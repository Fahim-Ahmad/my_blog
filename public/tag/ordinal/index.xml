<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ordinal | Fahim Ahmad</title>
    <link>/tag/ordinal/</link>
      <atom:link href="/tag/ordinal/index.xml" rel="self" type="application/rss+xml" />
    <description>ordinal</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Fahim Ahmad (2020)</copyright><lastBuildDate>Wed, 29 Dec 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>ordinal</title>
      <link>/tag/ordinal/</link>
    </image>
    
    <item>
      <title>Notes on the Bayesian cumulative probit</title>
      <link>/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/</link>
      <pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/</guid>
      <description>
&lt;script src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;whatwhy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What/why?&lt;/h2&gt;
&lt;p&gt;Prompted by a couple of my research projects, I’ve been fitting a lot of ordinal models, lately. Because of its nice interpretive properties, I’m fond of using the cumulative probit. Though I’ve written about cumulative logit &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingBrms2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020b&lt;/a&gt;, Section 11.1)&lt;/span&gt; and probit models &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2021&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;, Chapter 23)&lt;/span&gt; before, I still didn’t feel grounded enough to make rational decisions about priors and parameter interpretations. In this post, I have collected my recent notes and reformatted them into something of a tutorial on the Bayesian cumulative probit model. Using a single psychometric data set, we explore a variety of models, starting with the simplest single-level thresholds-only model and ending with a conditional multilevel distributional model.&lt;/p&gt;
&lt;p&gt;Be warned: This isn’t exactly a tutorial for beginners to the cumulative probit. For introductions, see some of the references cited within.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-it-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set it up&lt;/h2&gt;
&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt; for data wrangling and plotting. All models are fit with &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt;. In addition, there are a couple places where I make good use of the &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here we load the packages and adjust the global plotting theme.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load
library(tidyverse)
library(brms)
library(tidybayes)

# adjust the global plotting theme
theme_set(
  theme_gray(base_size = 13,
             base_family = &amp;quot;Times&amp;quot;) +
  theme(panel.grid = element_blank())
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our data will be a subset of the &lt;code&gt;bfi&lt;/code&gt; data &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-revelle2010individual&#34; role=&#34;doc-biblioref&#34;&gt;Revelle et al., 2010&lt;/a&gt;)&lt;/span&gt; from the &lt;a href=&#34;https://CRAN.R-project.org/package=psych&#34;&gt;&lt;strong&gt;psych&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-psych&#34; role=&#34;doc-biblioref&#34;&gt;Revelle, 2021&lt;/a&gt;)&lt;/span&gt;. Here we load, subset, and wrangle the data to suit our needs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

d &amp;lt;- psych::bfi %&amp;gt;% 
  mutate(male = ifelse(gender == 1, 1, 0),
         female = ifelse(gender == 2, 1, 0)) %&amp;gt;% 
  drop_na() %&amp;gt;% 
  slice_sample(n = 200) %&amp;gt;% 
  mutate(id = 1:n()) %&amp;gt;% 
  select(id, male, female, N1:N5) %&amp;gt;% 
  pivot_longer(N1:N5, names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;rating&amp;quot;)

# what is this?
glimpse(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1,000
## Columns: 5
## $ id     &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7…
## $ male   &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ female &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ item   &amp;lt;chr&amp;gt; &amp;quot;N1&amp;quot;, &amp;quot;N2&amp;quot;, &amp;quot;N3&amp;quot;, &amp;quot;N4&amp;quot;, &amp;quot;N5&amp;quot;, &amp;quot;N1&amp;quot;, &amp;quot;N2&amp;quot;, &amp;quot;N3&amp;quot;, &amp;quot;N4&amp;quot;, &amp;quot;N5&amp;quot;, &amp;quot;N1&amp;quot;, &amp;quot;N2&amp;quot;, &amp;quot;N3&amp;quot;, &amp;quot;N4&amp;quot;, &amp;quot;N5&amp;quot;, &amp;quot;N1&amp;quot;, &amp;quot;N2&amp;quot;, &amp;quot;…
## $ rating &amp;lt;int&amp;gt; 2, 2, 1, 1, 1, 3, 3, 2, 3, 3, 4, 4, 4, 4, 2, 5, 3, 4, 6, 2, 1, 1, 3, 2, 2, 2, 4, 4, 1, 2, 1, 2, 4, 4, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our focal variables will be &lt;code&gt;rating&lt;/code&gt;, which is a combination of the responses to the five questions in the Neuroticism scale of a version of the Big Five inventory &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-goldberg1999broad&#34; role=&#34;doc-biblioref&#34;&gt;Goldberg, 1999&lt;/a&gt;)&lt;/span&gt;. Here’s a quick plot of the responses, by item and sex.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  mutate(sex = ifelse(male == 0, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = rating)) +
  geom_bar() +
  scale_x_continuous(breaks = 1:6) +
  facet_grid(sex ~ item)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basics-of-the-ordered-probit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basics of the ordered probit&lt;/h2&gt;
&lt;p&gt;I’ve had a difficult time understanding the notation used in these models. As McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015, p. 335&lt;/a&gt;)&lt;/span&gt; remarked, there are a lot of different notational approaches for describing these models. In this post, I’ll be using a blend of sensibilities from McElreath, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner&lt;/a&gt; (&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;, and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke&lt;/a&gt; (&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;. If you see flaws in my equations, do &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1476641792263131143&#34;&gt;drop me a comment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Given a Likert-type variable &lt;code&gt;rating&lt;/code&gt; for which there are &lt;span class=&#34;math inline&#34;&gt;\(K + 1 = 6\)&lt;/span&gt; response options, you can model the relative probability of each ordinal category as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\text{rating} = k | \{ \tau_k \}) = f(\tau_k) - f(\tau_{k - 1}),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\tau_k\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(k^\text{th}\)&lt;/span&gt; threshold. On the left side of the equation, we stated the relative probabilities of the ordinal categories is conditional on our set of thresholds &lt;span class=&#34;math inline&#34;&gt;\(\{ \tau_k \}\)&lt;/span&gt;, which is a shorthand for writing out &lt;span class=&#34;math inline&#34;&gt;\(\{k_1, k_2, k_3, k_4, k_5\}\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; operator is a stand-in for some cumulative distribution function, which will allow us to map the cumulative probabilities onto an unbounded parameter space divided up by the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; thresholds.&lt;/p&gt;
&lt;p&gt;Because of its nice interpretative properties, I (and many others) like to use the cumulative standard normal distribution &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; as our function &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt;, which means we can rewrite the equation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\text{rating} = k | \{ \tau_k \}) = \Phi(\tau_k) - \Phi(\tau_{k - 1}),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, to be explicit, the parameters for &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; are fixed to &lt;span class=&#34;math inline&#34;&gt;\(\mu = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 1\)&lt;/span&gt; for purposes of identification. However, if you wanted two write out the equation to explicitly include our fixed &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters, it could look like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\text{rating} = k | \{ \tau_k \}) = \Phi([\tau_k - \mu] / \sigma) - \Phi([\tau_{k - 1} - \mu] / \sigma).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But again, since those are both held constant as &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, respectively, substituting those values into the equation reduces to what it was, before. This is one of the nice things about starting off with the cumulative standard normal distribution.&lt;/p&gt;
&lt;p&gt;Anyway, the thing to understand is that the area under the normal curve to the left of &lt;span class=&#34;math inline&#34;&gt;\(\tau_k\)&lt;/span&gt; is defined as &lt;span class=&#34;math inline&#34;&gt;\(\Phi(\tau_k)\)&lt;/span&gt;, which is the cumulative probability mass for the &lt;span class=&#34;math inline&#34;&gt;\(k^\text{th}\)&lt;/span&gt; rating. The reason we have to subtract the area to the left of &lt;span class=&#34;math inline&#34;&gt;\(\Phi(\tau_{k - 1})\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(\Phi(\tau_k)\)&lt;/span&gt; is so we can isolate a relative probability from a cumulative probability. This applies in the same way for all the middle ratings–&lt;code&gt;2&lt;/code&gt; through &lt;code&gt;5&lt;/code&gt; in our case. For the first and last ratings (&lt;code&gt;1&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;), we might add two &lt;em&gt;virtual&lt;/em&gt; thresholds &lt;span class=&#34;math inline&#34;&gt;\(-\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;, respectively. With the first rating, this means&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}) &amp;amp; = \Phi(\tau_1) - \Phi(\color{blue}{\tau_0} \color{black}{)} \\
  &amp;amp; = \Phi(\tau_1) - \Phi(\color{blue}{-\infty} \color{black}{)} \\
  &amp;amp; = \Phi(\tau_1) - \color{blue}0 \\
  &amp;amp; = \Phi(\tau_1).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a similar way, for the last rating we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}) &amp;amp; = \Phi(\color{blue}{\tau_{K + 1}} \color{black}{) - \Phi(\tau_K)} \\
  &amp;amp; = \Phi(\color{blue}\infty \color{black}{) - \Phi(\tau_K)} \\
  &amp;amp; = \color{blue}1 \color{black}{-} \color{black}{\Phi(\tau_K)}.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It might help to visualize this. Consider a case where we have a rating variable with 6 values (&lt;code&gt;1&lt;/code&gt; through &lt;code&gt;6&lt;/code&gt;), each with equal proportions, &lt;span class=&#34;math inline&#34;&gt;\(1 / 6 \approx .167.\)&lt;/span&gt; Here’s what that could look like mapped onto &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(z = seq(from = -3.75, to = 3.75, length.out = 1e3)) %&amp;gt;% 
  mutate(d = dnorm(x = z, mean = 0, sd = 1)) %&amp;gt;% 
  
  ggplot(aes(x = z, y = d)) +
  geom_line(color = &amp;quot;blue&amp;quot;) +
  geom_area(aes(fill = z &amp;gt;= qnorm(p = 0 / 6)), alpha = 1/4) + 
  geom_area(aes(fill = z &amp;gt;= qnorm(p = 1 / 6)), alpha = 1/4) + 
  geom_area(aes(fill = z &amp;gt;= qnorm(p = 2 / 6)), alpha = 1/4) + 
  geom_area(aes(fill = z &amp;gt;= qnorm(p = 3 / 6)), alpha = 1/4) + 
  geom_area(aes(fill = z &amp;gt;= qnorm(p = 4 / 6)), alpha = 1/4) + 
  geom_area(aes(fill = z &amp;gt;= qnorm(p = 5 / 6)), alpha = 1/4) + 
  geom_vline(xintercept = qnorm(p = 1:5 / 6), linetype = 3) +
  scale_fill_manual(values = c(&amp;quot;transparent&amp;quot;, &amp;quot;blue&amp;quot;), breaks = NULL) +
  scale_x_continuous(expression(Phi), breaks = -3:3,
                     sec.axis = dup_axis(
    name = NULL,
    breaks = qnorm(p = 1:5 / 6),
    labels = parse(text = str_c(&amp;quot;tau[&amp;quot;, 1:5, &amp;quot;]&amp;quot;))
    )) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-3.25, 3.25)) +
  labs(title = expression(Phi*&amp;quot; for an evenly-distributed 6-point ordinal variable&amp;quot;),
       subtitle = &amp;quot;Each shaded section, defined by the thresholds, has the same probability mass.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s how you might compute some of those values within a tibble.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(rating = 1:6) %&amp;gt;% 
  mutate(proportion = 1/6) %&amp;gt;% 
  mutate(cumulative_proportion = cumsum(proportion)) %&amp;gt;% 
  mutate(right_hand_threshold = qnorm(cumulative_proportion))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 4
##   rating proportion cumulative_proportion right_hand_threshold
##    &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;                 &amp;lt;dbl&amp;gt;                &amp;lt;dbl&amp;gt;
## 1      1      0.167                 0.167               -0.967
## 2      2      0.167                 0.333               -0.431
## 3      3      0.167                 0.5                  0    
## 4      4      0.167                 0.667                0.431
## 5      5      0.167                 0.833                0.967
## 6      6      0.167                 1                  Inf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, here’s how to use &lt;code&gt;qnorm()&lt;/code&gt; within &lt;code&gt;pnorm()&lt;/code&gt; to realize the various combinations of &lt;span class=&#34;math inline&#34;&gt;\(\Phi(\tau_k) - \Phi(\tau_{k - 1})\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(q = qnorm(p = 1 / 6)) - pnorm(q = qnorm(p = 0 / 6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1666667&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(q = qnorm(p = 2 / 6)) - pnorm(q = qnorm(p = 1 / 6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1666667&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(q = qnorm(p = 3 / 6)) - pnorm(q = qnorm(p = 2 / 6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1666667&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(q = qnorm(p = 4 / 6)) - pnorm(q = qnorm(p = 3 / 6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1666667&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(q = qnorm(p = 5 / 6)) - pnorm(q = qnorm(p = 4 / 6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1666667&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(q = qnorm(p = 6 / 6)) - pnorm(q = qnorm(p = 5 / 6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1666667&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models&lt;/h2&gt;
&lt;p&gt;For practice and building intuition, we’ll be fitting 8 models to the Neuroticism items. Starting simple and building up, they will be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fit1&lt;/code&gt;, the thresholds-only model;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit2&lt;/code&gt;, the single-level conditional mean model;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit3&lt;/code&gt;, the single-level conditional mean and dispersion model;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit4&lt;/code&gt;, the multilevel random participant-means model;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit5&lt;/code&gt;, the multilevel random participant- and item-level means model;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit6&lt;/code&gt;, the multilevel random participant- and item-level means model with varying thresholds;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit7&lt;/code&gt;, the multilevel unconditional distributional model; and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit8&lt;/code&gt;, the multilevel conditional distributional model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I make no claim these titles are canonical. They just make sense to me.&lt;/p&gt;
&lt;div id=&#34;thresholds-only.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thresholds only.&lt;/h3&gt;
&lt;p&gt;Over the years, I’ve become fond of McElreath’s style of writing out models. His style usually looks something like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{criterion}_i &amp;amp; \sim \operatorname{Some likelihood}(\phi_i) \\
\text{some link function}(\phi_i) &amp;amp; = \alpha + \beta x_i \\
\alpha &amp;amp; \sim \text{&amp;lt; some prior &amp;gt;} \\
\beta  &amp;amp; \sim \text{&amp;lt; some prior &amp;gt;},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\phi_i\)&lt;/span&gt; is a stand-in for the likelihood parameter(s). I’ve had a hard time applying this notation style to the cumulative probit model, so I’m going to detract a bit. Here’s my current attempt applied to the thresholds-only model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}) &amp;amp; = \Phi(\tau_k) - \Phi(\tau_{k - 1}) \\
\tau_k &amp;amp; \sim \mathcal N(0, 2),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the first line is the likelihood, which only contains the model parameters &lt;span class=&#34;math inline&#34;&gt;\(\tau_k\)&lt;/span&gt;. The second line is a simple non-committal prior for the thresholds, which places &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; of the prior probability mass for each threshold between &lt;span class=&#34;math inline&#34;&gt;\(-4\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt;. However, if we consider the plot above, we can come up with default prior that are a little less lazy. If we want our starting assumption to be a uniform distribution between our &lt;code&gt;rating&lt;/code&gt; data, we could update the model to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}) &amp;amp; = \Phi(\tau_k) - \Phi(\tau_{k - 1}) \\
\tau_1 &amp;amp; \sim \mathcal N(-0.97, 1) \\
\tau_2 &amp;amp; \sim \mathcal N(-0.43, 1) \\
\tau_3 &amp;amp; \sim \mathcal N(0, 1) \\
\tau_4 &amp;amp; \sim \mathcal N(0.43, 1) \\
\tau_5 &amp;amp; \sim \mathcal N(0.97, 1),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the priors are now all sequentially centered in different places in the &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; space. Though it might initially seem bold to use &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; for the standard deviations on each, that’s still rather permissive when you consider the figure, above. I think you could easily justify using a &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt; or so, instead.&lt;/p&gt;
&lt;p&gt;Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- brm(
  data = d,
  family = cumulative(probit),
  rating ~ 1,
  prior = c(prior(normal(-0.97, 1), class = Intercept, coef = 1),
            prior(normal(-0.43, 1), class = Intercept, coef = 2),
            prior(normal( 0.00, 1), class = Intercept, coef = 3),
            prior(normal( 0.43, 1), class = Intercept, coef = 4),
            prior(normal( 0.97, 1), class = Intercept, coef = 5)),
  cores = 4,
  seed = 1,
  init_r = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You don’t necessarily have to adjust &lt;code&gt;init_r&lt;/code&gt;, but my experience is these models often benefit from this adjustment.&lt;/p&gt;
&lt;p&gt;Here’s the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cumulative 
##   Links: mu = probit; disc = identity 
## Formula: rating ~ 1 
##    Data: d (Number of observations: 1000) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -0.94      0.05    -1.03    -0.85 1.00     3334     2650
## Intercept[2]    -0.25      0.04    -0.33    -0.17 1.00     5251     3510
## Intercept[3]     0.07      0.04    -0.00     0.15 1.00     4869     3513
## Intercept[4]     0.74      0.04     0.66     0.83 1.00     5449     3979
## Intercept[5]     1.32      0.05     1.21     1.43 1.00     5336     3509
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As Bürkner explained in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; tutorial, the &lt;code&gt;Intercept[k]&lt;/code&gt; rows are actually summarizing the &lt;span class=&#34;math inline&#34;&gt;\(\tau_k\)&lt;/span&gt; thresholds, not the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; intercept for the underlying latent variable. That, recall, is fixed at &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; for identification purposes. Also, notice that the &lt;code&gt;disc&lt;/code&gt; parameter is similarly held constant at &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; for identification.&lt;/p&gt;
&lt;p&gt;It might help to visualize the thresholds in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(x = seq(from = -3.5, to = 3.5, length.out = 200)) %&amp;gt;% 
  mutate(d = dnorm(x = x)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;black&amp;quot;, alpha = 1/3) +
  geom_vline(xintercept = fixef(fit1)[, 1], linetype = 3) +
  scale_x_continuous(expression(Phi), breaks = -3:3,
                     sec.axis = dup_axis(
    name = NULL,
    breaks = fixef(fit1)[, 1] %&amp;gt;% as.double(),
    labels = parse(text = str_c(&amp;quot;tau[&amp;quot;, 1:5, &amp;quot;]&amp;quot;))
    )) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-3, 3)) +
  labs(title = &amp;quot;Latent distribution for fit1&amp;quot;,
       subtitle = &amp;quot;By definition and identification constraints, the shape is a standardized normal.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To get more practice working with the thresholds and how they relate to the actual &lt;code&gt;rating&lt;/code&gt; data, it’ll help to to a couple posterior-predictive checks by hand. For our first version, we’ll make a custom function called &lt;code&gt;pp_check_pmap()&lt;/code&gt;, which will make use of the &lt;code&gt;pmap_dbl()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp_check_pmap &amp;lt;- function(seed = 0) {
  
  set.seed(seed)
  
  as_draws_df(fit1) %&amp;gt;% 
    slice_sample(n = 200) %&amp;gt;% 
    select(starts_with(&amp;quot;b_Intercept&amp;quot;)) %&amp;gt;% 
    set_names(str_c(&amp;quot;tau[&amp;quot;, 1:5, &amp;quot;]&amp;quot;)) %&amp;gt;% 
    mutate(p1 = pnorm(`tau[1]`),
           p2 = pnorm(`tau[2]`) - pnorm(`tau[1]`),
           p3 = pnorm(`tau[3]`) - pnorm(`tau[2]`),
           p4 = pnorm(`tau[4]`) - pnorm(`tau[3]`),
           p5 = pnorm(`tau[5]`) - pnorm(`tau[4]`),
           p6 = 1 - pnorm(`tau[5]`)) %&amp;gt;% 
    mutate(rating = pmap_dbl(.l = list(p1, p2, p3, p4, p5, p6),
                             .f = ~sample(
                               x = 1:6,
                               size = 1,
                               replace = TRUE,
                               prob = c(..1, ..2, ..3, ..4, ..5, ..6)
                             ))) %&amp;gt;% 
    select(rating)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use &lt;code&gt;pp_check_pmap()&lt;/code&gt; to simulate &lt;span class=&#34;math inline&#34;&gt;\(9\)&lt;/span&gt; data sets resembling the original data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(seed = 1:9) %&amp;gt;% 
  mutate(sim = map(seed, pp_check_pmap)) %&amp;gt;% 
  unnest(sim) %&amp;gt;% 
  
  ggplot(aes(x = rating)) +
  geom_bar() +
  scale_x_continuous(breaks = 1:6) +
  ggtitle(&amp;quot;PP-check with the pp_check_pmap() function&amp;quot;) +
  facet_wrap(~ seed, labeller = label_both)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can gain more insights into the model by using a different approach to computing the &lt;code&gt;rating&lt;/code&gt; probabilities. This time, we’ll use an approach that leverages the &lt;code&gt;case_when()&lt;/code&gt; function. We’ll wrap the approach in a custom function called &lt;code&gt;pp_check_case_when()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp_check_case_when &amp;lt;- function(seed = 0) {
  
  set.seed(seed)
  
  as_draws_df(fit1) %&amp;gt;% 
    slice_sample(n = 200) %&amp;gt;% 
    select(starts_with(&amp;quot;b_Intercept&amp;quot;)) %&amp;gt;% 
    set_names(str_c(&amp;quot;tau[&amp;quot;, 1:5, &amp;quot;]&amp;quot;)) %&amp;gt;% 
    mutate(Phi = rnorm(n = n(), mean = 0, sd = 1)) %&amp;gt;% 
    mutate(rating = case_when(
      Phi &amp;lt; `tau[1]` ~ 1,
      Phi &amp;gt;= `tau[1]` &amp;amp; Phi &amp;lt; `tau[2]`~ 2,
      Phi &amp;gt;= `tau[2]` &amp;amp; Phi &amp;lt; `tau[3]`~ 3,
      Phi &amp;gt;= `tau[3]` &amp;amp; Phi &amp;lt; `tau[4]`~ 4,
      Phi &amp;gt;= `tau[4]` &amp;amp; Phi &amp;lt; `tau[5]`~ 5,
      Phi &amp;gt;= `tau[5]` ~ 6
    )) %&amp;gt;% 
    select(rating)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use &lt;code&gt;pp_check_pmap()&lt;/code&gt; to simulate &lt;span class=&#34;math inline&#34;&gt;\(9\)&lt;/span&gt; data sets resembling the original data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(seed = 1:9) %&amp;gt;% 
  mutate(sim = map(seed, pp_check_case_when)) %&amp;gt;% 
  unnest(sim) %&amp;gt;% 
  
  ggplot(aes(x = rating)) +
  geom_bar() +
  scale_x_continuous(breaks = 1:6) +
  ggtitle(&amp;quot;PP-check with the pp_check_case_when() function&amp;quot;) +
  facet_wrap(~ seed, labeller = label_both)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both hand-done pp-checks returned similar results from those you can get with the &lt;code&gt;pp_check()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp_check(fit1, type = &amp;quot;hist&amp;quot;, ndraws = 8, binwidth = 1) +
  scale_x_continuous(breaks = 1:6) +
  ggtitle(&amp;quot;PP-check with the pp_check() function&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Posterior predictive distributions are cool and all, but one of the most common sense questions a researcher would ask is &lt;em&gt;How do I use the model to compute the mean of the data?&lt;/em&gt; When you have an ordinal model, the mean of the criterion variable is the sum of the &lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt; probabilities multiplied by the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; values of the criterion. We might express this in an equation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}(\text{rating}) = \sum_1^K p_k \times k,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}\)&lt;/span&gt; is the expectation operator (the model-based mean), &lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt; is the probability of the &lt;span class=&#34;math inline&#34;&gt;\(k^\text{th}\)&lt;/span&gt; ordinal value, and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the actual ordinal value. The trick, here, is that because we are computing all the &lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt; values with MCMC and expressing those values as posterior distributions, we have to perform this operation within each of our MCMC draws. If we take cues from our &lt;code&gt;pp_check_pmap()&lt;/code&gt;, above, we can compute this with our posterior draws like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_draws_df(fit1) %&amp;gt;% 
  select(.draw, starts_with(&amp;quot;b_Intercept&amp;quot;)) %&amp;gt;% 
  set_names(&amp;quot;.draw&amp;quot;, str_c(&amp;quot;tau[&amp;quot;, 1:5, &amp;quot;]&amp;quot;)) %&amp;gt;% 
  # compute the p_k distributions
  mutate(p1 = pnorm(`tau[1]`),
         p2 = pnorm(`tau[2]`) - pnorm(`tau[1]`),
         p3 = pnorm(`tau[3]`) - pnorm(`tau[2]`),
         p4 = pnorm(`tau[4]`) - pnorm(`tau[3]`),
         p5 = pnorm(`tau[5]`) - pnorm(`tau[4]`),
         p6 = 1 - pnorm(`tau[5]`)) %&amp;gt;% 
  # wrangle
  pivot_longer(starts_with(&amp;quot;p&amp;quot;), values_to = &amp;quot;p&amp;quot;) %&amp;gt;% 
  mutate(rating = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double()) %&amp;gt;% 
  # compute p_k * k
  mutate(`p * rating` = p * rating) %&amp;gt;% 
  # sum those values within each posterior draw
  group_by(.draw) %&amp;gt;% 
  summarise(mean_rating = sum(`p * rating`)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = mean_rating, y = 0)) +
  stat_halfeye(.width = .95) +
  geom_vline(xintercept = mean(d$rating), linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = &amp;quot;The postrior for the mean of the rating values&amp;quot;,
       subtitle = &amp;quot;The dashed vertical line marks off the sample mean. The posterior distribution is based on the\nsimple cumulative ordinal model fit1.&amp;quot;,
       x = expression(mu[rating])) +
  xlim(2, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our model did a great job finding the mean of the &lt;code&gt;rating&lt;/code&gt; data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;add-a-predictor.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Add a predictor.&lt;/h3&gt;
&lt;p&gt;If we would like to add a predictor, the most natural way is probably to explicitly add &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; in the likelihood and then attach a linear model to &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;. This would look like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}, \color{blue}{\mu_i} \color{black}{)} &amp;amp; = \Phi(\tau_k \color{blue}{- \mu_i} \color{black}{) - \Phi(\tau_{k - 1}} \color{blue}{- \mu_i} \color{black}) \\
\color{blue}{\mu_i} &amp;amp; = \color{blue}{\beta_0 + \sum_1^l \beta_l x_l} \\
\color{blue}{\beta_0} &amp;amp; = \color{blue}0 \color{black}{,}
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the intercept for the latent mean and &lt;span class=&#34;math inline&#34;&gt;\(\sum_1^l \beta_l x_l\)&lt;/span&gt; is the additive effect of the full set of &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; predictor variables and their &lt;span class=&#34;math inline&#34;&gt;\(\beta_l\)&lt;/span&gt; coefficients. Now &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the parameter we set to zero for identification purposes. As before, the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameter will remain set to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Since we are modeling a latent mean with a latent standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, this puts&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;single continuous standardized predictors in a correlation metric and&lt;/li&gt;
&lt;li&gt;single dummy variables in a Cohen’s-&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; metric.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As soon as you have multiple predictor variables in the mix, their metrics become increasingly difficult to interpret, but at least you can surmise that a partial correlation or a conditional Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; isn’t on a &lt;em&gt;radically&lt;/em&gt; different metric from their univariable counterparts.&lt;/p&gt;
&lt;p&gt;In the case of these data, our predictor of interest will be the dummy variable &lt;code&gt;male&lt;/code&gt;. Thus, it makes sense to assign it a weakly-regularizing prior like &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(0, 1)\)&lt;/span&gt;. Here’s the model formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}, \mu_i) &amp;amp; = \Phi(\tau_k - \mu_i) - \Phi(\tau_{k - 1} - \mu_i) \\
\mu_i &amp;amp; = \beta_1 \text{male}_i \\ 
\tau_1 &amp;amp; \sim \mathcal N(-0.97, 1) \\
\tau_2 &amp;amp; \sim \mathcal N(-0.43, 1) \\
\tau_3 &amp;amp; \sim \mathcal N(0, 1) \\
\tau_4 &amp;amp; \sim \mathcal N(0.43, 1) \\
\tau_5 &amp;amp; \sim \mathcal N(0.97, 1) \\
\beta_1 &amp;amp; \sim \mathcal N(0, 1),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where now the reference category is female. Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 49.33003 secs
fit2 &amp;lt;- brm(
  data = d,
  family = cumulative(probit),
  rating ~ 1 + male,
  prior = c(prior(normal(-0.97, 1), class = Intercept, coef = 1),
            prior(normal(-0.43, 1), class = Intercept, coef = 2),
            prior(normal( 0.00, 1), class = Intercept, coef = 3),
            prior(normal( 0.43, 1), class = Intercept, coef = 4),
            prior(normal( 0.97, 1), class = Intercept, coef = 5),
            prior(normal(0, 1), class = b)),
  cores = 4,
  seed = 1,
  init_r = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cumulative 
##   Links: mu = probit; disc = identity 
## Formula: rating ~ 1 + male 
##    Data: d (Number of observations: 1000) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -1.02      0.05    -1.12    -0.91 1.00     3167     3036
## Intercept[2]    -0.33      0.05    -0.42    -0.24 1.00     4809     3628
## Intercept[3]    -0.00      0.05    -0.09     0.09 1.00     4785     3571
## Intercept[4]     0.67      0.05     0.57     0.77 1.00     5309     3523
## Intercept[5]     1.25      0.06     1.13     1.37 1.00     5651     3799
## male            -0.22      0.07    -0.36    -0.09 1.00     5139     3154
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficient is a mean difference on the latent-&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; scale, it might be instructive to compare it to a Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; computed with the sample statistics. Here we compute the sample means, standard deviations, and sample sizes, by &lt;code&gt;male&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample means
m_female &amp;lt;- d %&amp;gt;% filter(female == 1) %&amp;gt;% summarise(m = mean(rating)) %&amp;gt;% pull()
m_male   &amp;lt;- d %&amp;gt;% filter(male == 1)   %&amp;gt;% summarise(m = mean(rating)) %&amp;gt;% pull()

# sample standard deviations
s_female &amp;lt;- d %&amp;gt;% filter(female == 1) %&amp;gt;% summarise(s = sd(rating)) %&amp;gt;% pull()
s_male   &amp;lt;- d %&amp;gt;% filter(male == 1)   %&amp;gt;% summarise(s = sd(rating)) %&amp;gt;% pull()

# sample sizes
n_female &amp;lt;- d %&amp;gt;% filter(female == 1) %&amp;gt;% summarise(n = n()) %&amp;gt;% pull()
n_male   &amp;lt;- d %&amp;gt;% filter(male == 1)   %&amp;gt;% summarise(n = n()) %&amp;gt;% pull()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now compute the pooled standard deviation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_pooled &amp;lt;- sqrt(((n_female - 1) * s_female^2 + (n_male - 1) * s_male^2) / (n_female + n_male - 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re finally ready to compute the sample Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(m_male - m_female) / s_pooled&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.2167152&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare that with the posterior mean and 95% CIs for our &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit2)[&amp;quot;male&amp;quot;, -2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Estimate        Q2.5       Q97.5 
## -0.22406785 -0.35812222 -0.08881928&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with the first model, it might also be helpful to plot the latent distributions along with the thresholds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(male = 0:1,
       mu   = c(0, fixef(fit2)[&amp;quot;male&amp;quot;, 1])) %&amp;gt;% 
  expand(nesting(male, mu),
         x = seq(from = -3.5, to = 3.5, length.out = 200)) %&amp;gt;% 
  mutate(d   = dnorm(x, mean = mu, sd = 1),
         sex = ifelse(male == 0, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d, fill = sex)) +
  geom_area(alpha = 1/2, position = &amp;quot;identity&amp;quot;) +
  geom_vline(xintercept = fixef(fit2)[1:5, 1], linetype = 3) +
  scale_fill_manual(NULL, values = c(&amp;quot;red3&amp;quot;, &amp;quot;blue3&amp;quot;)) +
  scale_x_continuous(expression(Phi), breaks = -3:3,
                     sec.axis = dup_axis(
    name = NULL,
    breaks = fixef(fit2)[1:5, 1] %&amp;gt;% as.double(),
    labels = parse(text = str_c(&amp;quot;tau[&amp;quot;, 1:5, &amp;quot;]&amp;quot;))
    )) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-3, 3)) +
  labs(title = &amp;quot;Latent distributions for fit2&amp;quot;,
       subtitle = expression(&amp;quot;The reference category is female. &amp;quot;*beta[1]*&amp;quot; is reflected in the leftward shift for men.&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As before, we might also want to use our conditional &lt;code&gt;fit2&lt;/code&gt; to compute the population means for &lt;code&gt;rating&lt;/code&gt;, by sex. We can extend our workflow from before by including the posterior draws for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;. Then when we use the &lt;code&gt;pnorm()&lt;/code&gt; functions to compute the conditional probabilities, we’ll have to use &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; to adjust the values in the &lt;code&gt;mean&lt;/code&gt; argument within &lt;code&gt;pnorm()&lt;/code&gt; to account for the &lt;span class=&#34;math inline&#34;&gt;\(\mu_i = \beta_1 \text{male}_i\)&lt;/span&gt; portion of our statistical model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_draws_df(fit2) %&amp;gt;% 
  select(.draw, starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  set_names(&amp;quot;.draw&amp;quot;, str_c(&amp;quot;tau[&amp;quot;, 1:5, &amp;quot;]&amp;quot;), &amp;quot;beta[1]&amp;quot;) %&amp;gt;% 
  # insert another copy of the data below
  bind_rows(., .) %&amp;gt;% 
  # add the two values for the dummy variable male
  mutate(male = rep(0:1, each = n() / 2)) %&amp;gt;% 
  # compute the p_k values conditional on the male dummy
  mutate(p1 = pnorm(`tau[1]`, mean = 0 + male * `beta[1]`),
         p2 = pnorm(`tau[2]`, mean = 0 + male * `beta[1]`) - pnorm(`tau[1]`, mean = 0 + male * `beta[1]`),
         p3 = pnorm(`tau[3]`, mean = 0 + male * `beta[1]`) - pnorm(`tau[2]`, mean = 0 + male * `beta[1]`),
         p4 = pnorm(`tau[4]`, mean = 0 + male * `beta[1]`) - pnorm(`tau[3]`, mean = 0 + male * `beta[1]`),
         p5 = pnorm(`tau[5]`, mean = 0 + male * `beta[1]`) - pnorm(`tau[4]`, mean = 0 + male * `beta[1]`),
         p6 = 1 - pnorm(`tau[5]`, mean = 0 + male * `beta[1]`)) %&amp;gt;%
  # wrangle
  pivot_longer(starts_with(&amp;quot;p&amp;quot;), values_to = &amp;quot;p&amp;quot;) %&amp;gt;% 
  mutate(rating = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double()) %&amp;gt;% 
  # compute p_k * k
  mutate(`p * rating` = p * rating) %&amp;gt;% 
  # sum those values within each posterior draw, by the male dummy
  group_by(.draw, male) %&amp;gt;% 
  summarise(mean_rating = sum(`p * rating`)) %&amp;gt;% 
  mutate(sex = ifelse(male == 0, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;)) %&amp;gt;% 
  
  # the trick with and without fct_rev() helps order the axes, colors, and legend labels
  ggplot(aes(x = mean_rating, y = fct_rev(sex), fill = sex)) +
  stat_halfeye(.width = .95) +
  geom_vline(xintercept = m_male, linetype = 2, color = &amp;quot;blue3&amp;quot;) +
  geom_vline(xintercept = m_female, linetype = 2, color = &amp;quot;red3&amp;quot;) +
  scale_fill_manual(NULL, values = c(alpha(&amp;quot;red3&amp;quot;, 0.5), alpha(&amp;quot;blue3&amp;quot;, 0.5))) +
  labs(title = &amp;quot;The postrior for the mean of the rating values, by sex&amp;quot;,
       subtitle = &amp;quot;The dashed vertical lines mark off the sample means, by sex. The posterior distributions\nare based on the simple conditional cumulative ordinal model fit2.&amp;quot;,
       x = expression(mu[rating]),
       y = NULL) +
  xlim(2, 4) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By making the &lt;code&gt;mean&lt;/code&gt; arguments within &lt;code&gt;pnorm()&lt;/code&gt; conditional on our dummy &lt;code&gt;male&lt;/code&gt;, we used our &lt;code&gt;fit2&lt;/code&gt; model to compute posteriors around the means of &lt;code&gt;rating&lt;/code&gt; with great success.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;varying-dispersion.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Varying dispersion.&lt;/h3&gt;
&lt;p&gt;If we have reason to presume differences in latent standard deviations, we can explicitly add &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; in the likelihood and then attach a linear model to &lt;span class=&#34;math inline&#34;&gt;\(\log(\sigma_i)\)&lt;/span&gt;. As in other contexts, it’s generally a good idea to model &lt;span class=&#34;math inline&#34;&gt;\(\log(\sigma_i)\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; because the former will ensure the model will only predict positive values. This would look like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}, \mu_i, \color{blue}{\sigma_i} \color{black}{)} &amp;amp; = \color{black}{\Phi([\tau_k - \mu_i]} \color{blue}{/ \sigma_i} \color{black}{) - \Phi([\tau_{k - 1} - \mu_i]} \color{blue}{/ \sigma_i} \color{black}{)} \\
\mu_i &amp;amp; = \beta_0 + \sum_1^l \beta_l x_l \\
\color{blue}{\log(\sigma_i)} &amp;amp; = \color{blue}{\eta_0 + \sum_1^m \eta_m x_m} \\
\beta_0 &amp;amp; = 0 \\
\color{blue}{\eta_0} &amp;amp; = \color{blue}0 \color{black}{,}
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\eta_0\)&lt;/span&gt; is the intercept for the logged latent standard deviation and &lt;span class=&#34;math inline&#34;&gt;\(\sum_1^m \eta_m x_m\)&lt;/span&gt; is the additive effect of the full set of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; predictor variables and their &lt;span class=&#34;math inline&#34;&gt;\(\eta_m\)&lt;/span&gt; coefficients. Now &lt;span class=&#34;math inline&#34;&gt;\(\eta_0\)&lt;/span&gt; is set to zero for identification purposes. Recall that &lt;span class=&#34;math inline&#34;&gt;\(\exp(0) = 1\)&lt;/span&gt;, which means that the default is still that &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i = 1\)&lt;/span&gt; when all predictors are set to zero. With this model, it is possible the &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; predictor sets are the same. Here we differentiate between them just to make clear that they can differ, as needed.&lt;/p&gt;
&lt;p&gt;A technical thing to keep in mind is that &lt;strong&gt;brms&lt;/strong&gt; actually parameterizes cumulative probit models in terms of the discrimination parameter &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, rather than &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Their relation is simple in that the one is the reciprocal of the other,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\sigma &amp;amp; = \frac{1}{\alpha}, \text{and} \\
\alpha &amp;amp; = \frac{1}{\sigma}.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It’s also the case that when you fit a model without a linear model attached to the discrimination parameter,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \frac{1}{\alpha} = \frac{1}{1} = 1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, things become more complicated when you want to model &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. First, here’s the updated likelihood:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}, \mu_i, \color{blue}{\alpha_i} \color{black}{)} &amp;amp; = \Phi(\color{blue}{\alpha_i} \color{black}{[\tau_k - \mu_i]) - \Phi( }\color{blue}{\alpha_i} \color{black}{[\tau_{k - 1} - \mu_i])} \\
\mu_i &amp;amp; = \beta_0 + \sum_1^l \beta_l x_l \\
\log(\color{blue}{\alpha_i} \color{black}{)} &amp;amp; = \eta_0 + \sum_1^m \eta_m x_m \\
\beta_0 &amp;amp; = 0 \\
\eta_0 &amp;amp; = 0.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you want to convert &lt;span class=&#34;math inline&#34;&gt;\(\log(\alpha_i)\)&lt;/span&gt; to the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; metric, you need the transformation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \frac{1}{\exp(\log \alpha)},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which puts the priors on the linear model for &lt;span class=&#34;math inline&#34;&gt;\(\log(\alpha_i)\)&lt;/span&gt; on an unfamiliar metric. As is often the case, something like &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(0, 1)\)&lt;/span&gt; is a good starting point. Here’s what it would look like if we simulated &lt;span class=&#34;math inline&#34;&gt;\(100{,}000\)&lt;/span&gt; draws from that prior and then converted them to the &lt;span class=&#34;math inline&#34;&gt;\(1 / \exp(\cdot)\)&lt;/span&gt; metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

tibble(prior = rnorm(n = 1e5, mean = 0, sd = 1)) %&amp;gt;%
  mutate(prior = 1 / exp(prior)) %&amp;gt;%
  
  ggplot(aes(x = prior)) +
  geom_histogram(binwidth = 0.1) +
  # the right tail is very long
  coord_cartesian(xlim = c(0, 10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We might extend this approach further to consider zero-centered normal priors with different values for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; hyperparameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

tibble(sigma_hyperparameter = 1:10 / 10) %&amp;gt;% 
  mutate(x = map(sigma_hyperparameter, ~ rnorm(n = 1e5, sd = .x))) %&amp;gt;% 
  unnest(x) %&amp;gt;% 
  mutate(x = 1 / exp(x)) %&amp;gt;% 
  group_by(sigma_hyperparameter) %&amp;gt;% 
  summarise(m = mean(x),
            mdn = median(x),
            s = sd(x),
            q2.5 = quantile(x, prob = .025),
            q97.5 = quantile(x, prob = .975)) %&amp;gt;% 
  mutate_all(round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 6
##    sigma_hyperparameter     m   mdn     s  q2.5 q97.5
##                   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1                  0.1  1.01     1  0.1   0.82  1.22
##  2                  0.2  1.02     1  0.21  0.67  1.48
##  3                  0.3  1.05     1  0.32  0.56  1.8 
##  4                  0.4  1.08     1  0.45  0.46  2.19
##  5                  0.5  1.13     1  0.6   0.38  2.66
##  6                  0.6  1.2      1  0.78  0.31  3.24
##  7                  0.7  1.28     1  1     0.25  3.93
##  8                  0.8  1.38     1  1.33  0.21  4.82
##  9                  0.9  1.5      1  1.72  0.17  5.85
## 10                  1    1.64     1  2.12  0.14  7.03&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The median is always at &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, with varying levels of dispersion. But this doesn’t seem principled. It might be better to think in terms of how much change would be reasonable to see on &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. In the present example, our sole predictor variable will be the dummy variable &lt;code&gt;male&lt;/code&gt;. I’m not sure how much more or less variation we’d expect to see between the sexes on the Neuroticism ratings, but I’d be shocked if the standard deviation for one was more than twice the size of the other. Thus, since the reference category will be set to &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 1\)&lt;/span&gt;, we want a distribution where the bulk of the prior mass is set to half and twice that value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma &amp;lt;- c(0.5, 1, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now convert those to &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha &amp;lt;- 1 / sigma&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, take the log of the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(alpha)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  0.6931472  0.0000000 -0.6931472&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{men}\)&lt;/span&gt; were half the size of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{women}\)&lt;/span&gt;, we’d expect &lt;span class=&#34;math inline&#34;&gt;\(\eta_1 = 0.693\)&lt;/span&gt;. Similarly, if &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{men}\)&lt;/span&gt; were &lt;em&gt;twice&lt;/em&gt; the size of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{women}\)&lt;/span&gt;, we’d expect &lt;span class=&#34;math inline&#34;&gt;\(\eta_1 = -0.693\)&lt;/span&gt;. Thus the prior &lt;span class=&#34;math inline&#34;&gt;\(\eta_i \sim \mathcal N(0, 0.693 / 2 \approx 0.347)\)&lt;/span&gt; would put &lt;span class=&#34;math inline&#34;&gt;\(95 \%\)&lt;/span&gt; of the prior mass within the desired range.&lt;/p&gt;
&lt;p&gt;We can compute that value with &lt;span class=&#34;math inline&#34;&gt;\(\log(2) / 2\)&lt;/span&gt;, too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(2) / 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3465736&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a histogram of what that would look like in the &lt;span class=&#34;math inline&#34;&gt;\(1 / \exp(\cdot)\)&lt;/span&gt; metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

tibble(prior = rnorm(n = 1e5, mean = 0, sd = log(2) / 2)) %&amp;gt;%
  mutate(prior = 1 / exp(prior)) %&amp;gt;%
  
  ggplot(aes(x = prior)) +
  geom_histogram(binwidth = 0.1) +
  xlab(expression(1/exp(prior))) +
  coord_cartesian(xlim = c(0, 10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With that all in mind, we can now update our model formula to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}, \mu_i, \alpha_i) &amp;amp; = \Phi(\alpha_i[\tau_k - \mu_i]) - \Phi(\alpha_i[\tau_{k - 1} - \mu_i]) \\
\mu_i          &amp;amp; = \beta_1 \text{male}_i \\ 
\log(\alpha_i) &amp;amp; = \eta_1 \text{male}_i\\
\tau_1 &amp;amp; \sim \mathcal N(-0.97, 1) \\
\tau_2 &amp;amp; \sim \mathcal N(-0.43, 1) \\
\tau_3 &amp;amp; \sim \mathcal N(0, 1) \\
\tau_4 &amp;amp; \sim \mathcal N(0.43, 1) \\
\tau_5 &amp;amp; \sim \mathcal N(0.97, 1) \\
\beta_1 &amp;amp; \sim \mathcal N(0, 1) \\
\eta_1 &amp;amp; \sim \mathcal N(0, 0.347).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 49.94408 secs
fit3 &amp;lt;- brm(
  data = d,
  family = cumulative(probit),
  bf(rating ~ 1 + male) +
    lf(disc ~ 0 + male,
       # this is really important
       cmc = FALSE),
  prior = c(prior(normal(-0.97, 1), class = Intercept, coef = 1),
            prior(normal(-0.43, 1), class = Intercept, coef = 2),
            prior(normal( 0.00, 1), class = Intercept, coef = 3),
            prior(normal( 0.43, 1), class = Intercept, coef = 4),
            prior(normal( 0.97, 1), class = Intercept, coef = 5),
            prior(normal(0, 1), class = b),
            # log(2) / 2 = 0.347
            prior(normal(0, log(2) / 2), class = b, dpar = disc)),
  cores = 4,
  seed = 1,
  init_r = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how we used the &lt;code&gt;0 + ...&lt;/code&gt; syntax in the first line in the &lt;code&gt;lf()&lt;/code&gt; function and that set we set &lt;code&gt;cmc = FALSE&lt;/code&gt; in the second line. As Bürkner noted in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; tutorial, it’s critical that you do this when attaching a model to &lt;span class=&#34;math inline&#34;&gt;\(\log(\alpha_i)\)&lt;/span&gt;. If you leave these bits out, you might find that you are no longer keeping the reference category for &lt;span class=&#34;math inline&#34;&gt;\(\log(\alpha_i)\)&lt;/span&gt; set to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, which can wreak havoc on the model.&lt;/p&gt;
&lt;p&gt;Anyway, check the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cumulative 
##   Links: mu = probit; disc = log 
## Formula: rating ~ 1 + male 
##          disc ~ 0 + male
##    Data: d (Number of observations: 1000) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -0.97      0.06    -1.08    -0.86 1.00     2359     2981
## Intercept[2]    -0.31      0.05    -0.41    -0.22 1.00     4558     3583
## Intercept[3]    -0.00      0.05    -0.09     0.09 1.00     5101     3616
## Intercept[4]     0.65      0.05     0.55     0.74 1.00     4746     3483
## Intercept[5]     1.22      0.06     1.10     1.33 1.00     4543     3337
## male            -0.20      0.07    -0.34    -0.08 1.00     4535     2831
## disc_male        0.12      0.06     0.00     0.24 1.00     2799     2612
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It can be challenging to interpret the &lt;code&gt;disc_male&lt;/code&gt; coefficient directly. Keep in mind that &lt;em&gt;larger&lt;/em&gt; values mean &lt;em&gt;smaller&lt;/em&gt; relative values on the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; scale. Given how the reference value is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, which equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 1\)&lt;/span&gt;, this is the estimate of the latent standard deviation for men.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 / exp(0 + fixef(fit3)[&amp;quot;disc_male&amp;quot;, -2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate      Q2.5     Q97.5 
## 0.8873625 0.9976675 0.7883039&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the two sample standard deviations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_female &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.616471&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_male&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.491827&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once again, let’s plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(male  = 0:1,
       mu    = c(0, fixef(fit3)[&amp;quot;male&amp;quot;, 1]),
       sigma = 1 / exp(c(0, fixef(fit3)[&amp;quot;disc_male&amp;quot;, 1]))) %&amp;gt;% 
  expand(nesting(male, mu, sigma),
         x = seq(from = -3.5, to = 3.5, length.out = 200)) %&amp;gt;% 
  mutate(d   = dnorm(x, mean = mu, sd = sigma),
         sex = ifelse(male == 0, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d, fill = sex)) +
  geom_area(alpha = 1/2, position = &amp;quot;identity&amp;quot;) +
  geom_vline(xintercept = fixef(fit3)[1:5, 1], linetype = 3) +
  scale_fill_manual(NULL, values = c(&amp;quot;red3&amp;quot;, &amp;quot;blue3&amp;quot;)) +
  scale_x_continuous(expression(Phi), breaks = -3:3,
                     sec.axis = dup_axis(
    name = NULL,
    breaks = fixef(fit3)[1:5, 1] %&amp;gt;% as.double(),
    labels = parse(text = str_c(&amp;quot;tau[&amp;quot;, 1:5, &amp;quot;]&amp;quot;))
    )) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = &amp;quot;Latent distributions for fit3&amp;quot;,
       subtitle = expression(&amp;quot;The shape for men is based on the posterior means of the &amp;quot;*beta[1]*&amp;quot; and &amp;quot;*eta[1]*&amp;quot; parameters.&amp;quot;)) +
  coord_cartesian(xlim = c(-3, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note, however, that because this new model &lt;code&gt;fit3&lt;/code&gt; now has separate &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; estimates by sex, the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; parameter isn’t quite in a standardized-mean-difference metric, anymore. To get it back in to a &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; metric, we’ll have to use the full posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{female}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{male}\)&lt;/span&gt; to compute a model-based pooled standard deviation, by which we can then standardize the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficient.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_draws_df(fit3) %&amp;gt;% 
  mutate(sigma_f = 1,
         sigma_m = 1 / exp(b_disc_male)) %&amp;gt;%
  mutate(sigma_pooled = sqrt(((n_female - 1) * sigma_f^2 + (n_male - 1) * sigma_m^2) / (n_female + n_male - 2))) %&amp;gt;% 
  transmute(d = b_male / sigma_pooled) %&amp;gt;% 
  mean_qi(d) %&amp;gt;% 
  mutate_if(is.double, round, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 6
##        d .lower .upper .width .point .interval
##    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
## 1 -0.212 -0.345  -0.08   0.95 mean   qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, consider the variance metric. Given that the standard deviation squared is the variance, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, an increase of doubling the standard deviation is the same as quadrupling the variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2^2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you wanted to set up the model so that &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; of the prior probability allowed for a doubling of the variance, you’d need to set the upper range to the square root of &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.414214&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Anyway, now that we’ve updated the model to account for differences in &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, we might also want to update our workflow for computing the population means for &lt;code&gt;rating&lt;/code&gt;. The trick is now the &lt;code&gt;sd&lt;/code&gt; argument within the &lt;code&gt;pnorm()&lt;/code&gt; function should be conditional on the values for males or females. Here’s one way how.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_draws_df(fit3) %&amp;gt;% 
  # this also includes the column for b_disc_male
  select(.draw, starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  set_names(&amp;quot;.draw&amp;quot;, str_c(&amp;quot;tau[&amp;quot;, 1:5, &amp;quot;]&amp;quot;), &amp;quot;beta[1]&amp;quot;, &amp;quot;eta[1]&amp;quot;) %&amp;gt;% 
  bind_rows(., .) %&amp;gt;% 
  mutate(male = rep(0:1, each = n() / 2)) %&amp;gt;% 
  # compute the conditional mu and sigma values
  mutate(mu_i    = 0 + male * `beta[1]`,
         sigma_i = 1 / exp(0 + male * `eta[1]`)) %&amp;gt;%
  # compute the p_k values conditional on the male dummy and the mu_i and sigma_i values
  mutate(p1 = pnorm(`tau[1]`, mean = mu_i, sd = sigma_i),
         p2 = pnorm(`tau[2]`, mean = mu_i, sd = sigma_i) - pnorm(`tau[1]`, mean = mu_i, sd = sigma_i),
         p3 = pnorm(`tau[3]`, mean = mu_i, sd = sigma_i) - pnorm(`tau[2]`, mean = mu_i, sd = sigma_i),
         p4 = pnorm(`tau[4]`, mean = mu_i, sd = sigma_i) - pnorm(`tau[3]`, mean = mu_i, sd = sigma_i),
         p5 = pnorm(`tau[5]`, mean = mu_i, sd = sigma_i) - pnorm(`tau[4]`, mean = mu_i, sd = sigma_i),
         p6 = 1 - pnorm(`tau[5]`, mean = mu_i, sd = sigma_i)) %&amp;gt;%
  # the rest is now the same as before
  pivot_longer(starts_with(&amp;quot;p&amp;quot;), values_to = &amp;quot;p&amp;quot;) %&amp;gt;% 
  mutate(rating = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double()) %&amp;gt;% 
  mutate(`p * rating` = p * rating) %&amp;gt;% 
  group_by(.draw, male) %&amp;gt;% 
  summarise(mean_rating = sum(`p * rating`)) %&amp;gt;% 
  mutate(sex = ifelse(male == 0, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = mean_rating, y = fct_rev(sex), fill = sex)) +
  stat_halfeye(.width = .95) +
  geom_vline(xintercept = m_male, linetype = 2, color = &amp;quot;blue3&amp;quot;) +
  geom_vline(xintercept = m_female, linetype = 2, color = &amp;quot;red3&amp;quot;) +
  scale_fill_manual(NULL, values = c(alpha(&amp;quot;red3&amp;quot;, 0.5), alpha(&amp;quot;blue3&amp;quot;, 0.5))) +
  labs(title = &amp;quot;The postrior for the mean of the rating values, by sex&amp;quot;,
       subtitle = &amp;quot;The dashed vertical lines mark off the sample means, by sex. The posterior distributions\nare based on the simple conditional cumulative ordinal model fit3.&amp;quot;,
       x = expression(mu[rating]),
       y = NULL) +
  xlim(2, 4) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results are very similar to those from above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;randomly-varying-latent-means.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Randomly-varying latent means.&lt;/h3&gt;
&lt;p&gt;Up until now, we’ve been ignoring how this model has a multilevel structure. We have ratings nested within persons (&lt;code&gt;id&lt;/code&gt;) and questions (&lt;code&gt;item&lt;/code&gt;). Within cumulative probit models, the simplest way to account for that nesting is to add random intercepts to the latent mean, which will now be &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ij}\)&lt;/span&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(i^\text{th}\)&lt;/span&gt; person and the &lt;span class=&#34;math inline&#34;&gt;\(j^\text{th}\)&lt;/span&gt; question.&lt;/p&gt;
&lt;p&gt;Starting slow, we’ll first just add random intercepts for the persons with the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}, \mu_i) &amp;amp; = \Phi(\tau_k - \mu_i) - \Phi(\tau_{k - 1} - \mu_i) \\
\mu_i          &amp;amp; = 0 \color{blue}+ \color{blue}{u_i} \\ 
\color{blue}{u_i} &amp;amp; \sim \color{blue}{\mathcal N(0, \sigma_u)} \\
\tau_1 &amp;amp; \sim \mathcal N(-0.97, 1) \\
\tau_2 &amp;amp; \sim \mathcal N(-0.43, 1) \\
\tau_3 &amp;amp; \sim \mathcal N(0, 1) \\
\tau_4 &amp;amp; \sim \mathcal N(0.43, 1) \\
\tau_5 &amp;amp; \sim \mathcal N(0.97, 1) \\
\color{blue}{\sigma_u} &amp;amp; \sim \color{blue}{\operatorname{Exponential}(1)} \color{black}{,}
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we’ve dropped the &lt;code&gt;male&lt;/code&gt; predictor and the entire linear model for &lt;span class=&#34;math inline&#34;&gt;\(\log(\alpha_i)\)&lt;/span&gt; for the sake of simplicity. They’ll come back in a bit. Given the baseline fixed-effects portion of the model is still on the standardized-normal metric, the good old &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Exponential}(1)\)&lt;/span&gt; prior is a good choice for the level-2 standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma_u\)&lt;/span&gt;. For more on the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Exponential}(1)\)&lt;/span&gt; prior, see &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 49.94408 secs
fit4 &amp;lt;- brm(
  data = d,
  family = cumulative(probit),
  rating ~ 1 + (1 | id),
  prior = c(prior(normal(-0.97, 1), class = Intercept, coef = 1),
            prior(normal(-0.43, 1), class = Intercept, coef = 2),
            prior(normal( 0.00, 1), class = Intercept, coef = 3),
            prior(normal( 0.43, 1), class = Intercept, coef = 4),
            prior(normal( 0.97, 1), class = Intercept, coef = 5),
            prior(exponential(1), class = sd)),
  cores = 4,
  seed = 1,
  init_r = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cumulative 
##   Links: mu = probit; disc = identity 
## Formula: rating ~ 1 + (1 | id) 
##    Data: d (Number of observations: 1000) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 200) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.03      0.07     0.90     1.18 1.00     1095     2370
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -1.33      0.10    -1.52    -1.15 1.00     1267     1809
## Intercept[2]    -0.37      0.09    -0.54    -0.21 1.00     1286     2043
## Intercept[3]     0.09      0.09    -0.08     0.25 1.00     1278     1982
## Intercept[4]     1.04      0.09     0.86     1.22 1.00     1369     2298
## Intercept[5]     1.86      0.10     1.66     2.07 1.00     1665     2743
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, the &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt; summaries from &lt;code&gt;ranef()&lt;/code&gt; will be in the latent-mean metric. This is because the grand mean is still set to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; for identification purposes. Here’s what the posterior means for those parameters look like in a dot plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(ranef = ranef(fit4)$id[, 1, &amp;quot;Intercept&amp;quot;]) %&amp;gt;% 
  
  ggplot(aes(x = ranef)) + 
  geom_rug(size = 1/6) +
  geom_dotplot(binwidth = 1/6.5) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu[italic(i)]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now it’s time to fit a proper cross-classified model accounting for the nesting within &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; persons and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; questions. That will follow the equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_k \}, \mu_{i\color{blue}j}) &amp;amp; = \Phi(\tau_k - \mu_{i\color{blue}j}) - \Phi(\tau_{k - 1} - \mu_{i\color{blue}j}) \\
\mu_{i\color{blue}j} &amp;amp; = 0 + u_i \color{blue}{ + } \color{blue}{v_j} \\ 
u_i               &amp;amp; \sim \mathcal N(0, \sigma_u) \\
\color{blue}{v_j} &amp;amp; \sim \color{blue}{\mathcal N(0, \sigma_v)} \\
\tau_1 &amp;amp; \sim \mathcal N(-0.97, 1) \\
\tau_2 &amp;amp; \sim \mathcal N(-0.43, 1) \\
\tau_3 &amp;amp; \sim \mathcal N(0, 1) \\
\tau_4 &amp;amp; \sim \mathcal N(0.43, 1) \\
\tau_5 &amp;amp; \sim \mathcal N(0.97, 1) \\
\sigma_u               &amp;amp; \sim \operatorname{Exponential}(1) \\
\color{blue}{\sigma_v} &amp;amp; \sim \color{blue}{\operatorname{Exponential}(1)} \color{black}{.}
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;At this point, we’re now fitting a Bayesian IRT model along the lines Bürkner showcased in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; tutorial. We have person-level parameters in &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt; and item-level parameters in &lt;span class=&#34;math inline&#34;&gt;\(v_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Fit the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1.278795 secs
fit5 &amp;lt;- brm(
  data = d,
  family = cumulative(probit),
  rating ~ 1 + (1 | id) + (1 | item),
  prior = c(prior(normal(-0.97, 1), class = Intercept, coef = 1),
            prior(normal(-0.43, 1), class = Intercept, coef = 2),
            prior(normal( 0.00, 1), class = Intercept, coef = 3),
            prior(normal( 0.43, 1), class = Intercept, coef = 4),
            prior(normal( 0.97, 1), class = Intercept, coef = 5),
            prior(exponential(1), class = sd)),
  cores = 4,
  seed = 1,
  init_r = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cumulative 
##   Links: mu = probit; disc = identity 
## Formula: rating ~ 1 + (1 | id) + (1 | item) 
##    Data: d (Number of observations: 1000) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 200) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.04      0.07     0.91     1.19 1.00     1497     2029
## 
## ~item (Number of levels: 5) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.21      0.12     0.07     0.52 1.00     1455     2003
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -1.36      0.14    -1.64    -1.08 1.00     2167     2333
## Intercept[2]    -0.38      0.13    -0.66    -0.13 1.00     2186     2486
## Intercept[3]     0.08      0.13    -0.19     0.34 1.00     2196     2537
## Intercept[4]     1.04      0.14     0.77     1.31 1.00     2396     2486
## Intercept[5]     1.88      0.15     1.59     2.17 1.00     2731     2506
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given there are only five levels of &lt;code&gt;item&lt;/code&gt;, we should not be surprised the posterior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_v\)&lt;/span&gt; is much more uncertain relative to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_u\)&lt;/span&gt;. It should also not be shocking that the posterior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_v\)&lt;/span&gt; is also centered around a relatively small value. These five questions are all from the Neuroticism scale and psychologists tend to publish scales with relatively homogeneous items.&lt;/p&gt;
&lt;p&gt;An issue that’s easy to miss with these two models is they’re holding the thresholds constant across both persons and items. At the moment, &lt;strong&gt;brms&lt;/strong&gt; is not capable of hierarchically modeling the thresholds (see &lt;a href=&#34;https://discourse.mc-stan.org/t/cumulative-models-for-multiple-likert-items/7282/24?u=solomon&#34;&gt;this thread&lt;/a&gt;), However, one can allow for differnt thresholds by question in a fixed-effects sort of way. Here’s what the updated statistical model would be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
p(\text{rating} = k | \{ \tau_{k\color{blue}j} \}, \mu_{ij}) &amp;amp; = \Phi(\tau_{k\color{blue}j} - \mu_{ij}) - \Phi(\tau_{k - 1,\color{blue}j} - \mu_{ij}) \\
\mu_{ij} &amp;amp; = 0 + u_i + v_j \\ 
u_i &amp;amp; \sim \mathcal N(0, \sigma_u) \\
v_j &amp;amp; \sim \mathcal N(0, \sigma_v) \\
\tau_{1\color{blue}j} &amp;amp; \sim \mathcal N(-0.97, 1) \\
\tau_{2\color{blue}j} &amp;amp; \sim \mathcal N(-0.43, 1) \\
\tau_{3\color{blue}j} &amp;amp; \sim \mathcal N(0, 1) \\
\tau_{4\color{blue}j} &amp;amp; \sim \mathcal N(0.43, 1) \\
\tau_{5\color{blue}j} &amp;amp; \sim \mathcal N(0.97, 1) \\
\sigma_u &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_v &amp;amp; \sim \operatorname{Exponential}(1),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where all the &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; parameters vary across the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; levels of &lt;code&gt;item&lt;/code&gt;. To fit such a model with &lt;strong&gt;brms&lt;/strong&gt;, you use the &lt;code&gt;thres(gr = &amp;lt;group&amp;gt;)&lt;/code&gt; helper function on the left-hand side of the model &lt;code&gt;formula&lt;/code&gt;. If you want to continue to manually set different priors for the thresholds, you’ll now need to include the &lt;code&gt;group&lt;/code&gt; argument within the &lt;code&gt;prior()&lt;/code&gt; function. In my experience, don’t be surprised if you have to adjust &lt;code&gt;adapt_delta&lt;/code&gt; at this point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 3.59341 mins
fit6 &amp;lt;- brm(
  data = d,
  family = cumulative(probit),
  rating | thres(gr = item) ~ 1 + (1 | id) + (1 | item),
  prior = c(prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N1),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N1),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N1),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N1),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N1),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N2),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N2),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N2),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N2),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N2),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N3),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N3),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N3),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N3),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N3),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N4),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N4),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N4),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N4),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N4),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N5),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N5),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N5),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N5),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N5),
            
            prior(exponential(1), class = sd)),
  cores = 4,
  seed = 1,
  init_r = 0.2,
  control = list(adapt_delta = .99)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The summary is now quite lengthy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cumulative 
##   Links: mu = probit; disc = identity 
## Formula: rating | thres(gr = item) ~ 1 + (1 | id) + (1 | item) 
##    Data: d (Number of observations: 1000) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 200) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.06      0.08     0.91     1.22 1.01     1416     2119
## 
## ~item (Number of levels: 5) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.23      0.20     0.01     0.73 1.00     1523     2400
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[N1,1]    -1.30      0.25    -1.87    -0.85 1.00     2717     2195
## Intercept[N1,2]    -0.46      0.24    -1.02    -0.02 1.00     2776     2162
## Intercept[N1,3]    -0.01      0.24    -0.57     0.43 1.00     2809     2281
## Intercept[N1,4]     1.12      0.25     0.56     1.57 1.00     3028     2808
## Intercept[N1,5]     1.99      0.27     1.40     2.47 1.00     3520     2772
## Intercept[N2,1]    -1.80      0.26    -2.33    -1.27 1.00     3003     2569
## Intercept[N2,2]    -0.64      0.24    -1.14    -0.15 1.00     3191     2365
## Intercept[N2,3]    -0.13      0.23    -0.64     0.35 1.00     3103     2017
## Intercept[N2,4]     0.73      0.24     0.24     1.24 1.00     3093     2758
## Intercept[N2,5]     1.75      0.25     1.24     2.28 1.00     3717     3134
## Intercept[N3,1]    -1.43      0.25    -2.02    -0.99 1.00     2855     2498
## Intercept[N3,2]    -0.34      0.24    -0.90     0.10 1.00     2965     2563
## Intercept[N3,3]     0.04      0.24    -0.51     0.48 1.00     3047     2545
## Intercept[N3,4]     1.08      0.25     0.52     1.52 1.00     3096     2459
## Intercept[N3,5]     1.82      0.26     1.23     2.28 1.00     3424     2966
## Intercept[N4,1]    -1.47      0.25    -2.03    -0.99 1.00     2880     2575
## Intercept[N4,2]    -0.58      0.24    -1.15    -0.13 1.00     3112     2709
## Intercept[N4,3]    -0.00      0.24    -0.54     0.45 1.00     3084     2786
## Intercept[N4,4]     0.96      0.24     0.41     1.42 1.00     3264     2669
## Intercept[N4,5]     1.89      0.27     1.32     2.39 1.00     3730     2695
## Intercept[N5,1]    -1.18      0.25    -1.76    -0.74 1.00     2872     2875
## Intercept[N5,2]    -0.23      0.24    -0.80     0.20 1.00     3071     2772
## Intercept[N5,3]     0.22      0.24    -0.34     0.67 1.00     3108     2855
## Intercept[N5,4]     1.04      0.24     0.48     1.48 1.00     3392     3014
## Intercept[N5,5]     1.68      0.26     1.09     2.18 1.00     3633     2985
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It might be easier to get a sense of the &lt;span class=&#34;math inline&#34;&gt;\(\tau_{kj}\)&lt;/span&gt; posteriors with a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_summary(fit6)[1:25, ] %&amp;gt;% 
  data.frame() %&amp;gt;% 
  mutate(tau = rep(1:5, times = 5),
         item = rep(1:5, each = 5)) %&amp;gt;% 
  mutate(tau = str_c(&amp;quot;tau[&amp;quot;, tau, &amp;quot;]&amp;quot;),
         item = factor(item)) %&amp;gt;% 
  
  ggplot(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, x = tau,
             group = item, color = item)) +
  geom_pointrange(position = position_dodge(width = -0.5), fatten = 1.5) +
  scale_color_viridis_d(expression(item~(italic(j))), option = &amp;quot;F&amp;quot;, end = 0.8, direction = -1) +
  scale_x_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_flip() +
  ylab(expression(Phi))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You could use information criteria comparisons to decide on whether to allow them to vary across items or if it’s okay to prefer the more parsimonious model. IMO, thresholds across items are &lt;em&gt;a priori&lt;/em&gt; different and should not be collapsed without strong theoretical or methodological justifications. In this case, it seems like I lose nothing by having them.&lt;/p&gt;
&lt;p&gt;Plus, there’s also the implications of the varying thresholds for the posterior-predictive distributions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
pp_check(fit5, type = &amp;quot;bars_grouped&amp;quot;, group = &amp;quot;item&amp;quot;,
         ndraws = 500, size = 1/2, fatten = 3/2) +
  ylim(0, 80) +
  labs(title = &amp;quot;fit5&amp;quot;,
       subtitle = &amp;quot;The thresholds are fixed across items.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-44-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
pp_check(fit6, type = &amp;quot;bars_grouped&amp;quot;, group = &amp;quot;item&amp;quot;,
         ndraws = 500, size = 1/2, fatten = 3/2) +
  ylim(0, 80) +
  labs(title = &amp;quot;fit6&amp;quot;,
       subtitle = &amp;quot;The thresholds can now vary across items.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-44-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well okay, maybe I do lose something by letting the thresholds vary across questions. If you look at the parameter summaries for &lt;code&gt;fit5&lt;/code&gt; and &lt;code&gt;fit6&lt;/code&gt;, you’ll see the posterior standard deviations (see the &lt;code&gt;Est.Error&lt;/code&gt; columns) are wider in &lt;code&gt;fit6&lt;/code&gt;. This corresponds to the wider prediction intervals in the &lt;code&gt;pp_check()&lt;/code&gt; plot for &lt;code&gt;fit6&lt;/code&gt;, compared to the plot for &lt;code&gt;fit5&lt;/code&gt;. You might think of this as trade-off of accuracy for precision or of reliability for validity. IMO, the trade-off was worth it. Allowing the thresholds to vary across the items makes for much better posterior predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;randomly-varying-latent-standard-deviations.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Randomly-varying latent standard deviations.&lt;/h3&gt;
&lt;p&gt;If the latent means can vary randomly, the latent dispersion parameters can vary randomly, too. Now &lt;span class=&#34;math inline&#34;&gt;\(\log(\alpha_{ij})\)&lt;/span&gt; will vary across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; persons and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; questions with the Bayesian ordinal IRT model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\small{p(\text{rating} = k | \{ \tau_{kj} \}, \mu_{ij}, \color{blue}{\alpha_{ij}} \color{black}{)}} &amp;amp; = \small{\Phi(\color{blue}{\alpha_{ij}} \color{black}{[\tau_{kj} - \mu_{ij}]) - \Phi(} \color{blue}{\alpha_{ij}} \color{black}{[\tau_{k - 1,j} - \mu_{ij}])}} \\
\mu_{ij} &amp;amp; = 0 + u_i + v_j \\ 
\color{blue}{\log(\alpha_{ij})} &amp;amp; = \color{blue}{0 + w_i + x_j} \\
u_i &amp;amp; \sim \mathcal N(0, \sigma_u) \\
v_j &amp;amp; \sim \mathcal N(0, \sigma_v) \\

\color{blue}{w_i} &amp;amp; \sim \color{blue}{\mathcal N(0, \sigma_w)} \\
\color{blue}{x_j} &amp;amp; \sim \color{blue}{\mathcal N(0, \sigma_x)} \\

\tau_{1j} &amp;amp; \sim \mathcal N(-0.97, 1) \\
\tau_{2j} &amp;amp; \sim \mathcal N(-0.43, 1) \\
\tau_{3j} &amp;amp; \sim \mathcal N(0, 1) \\
\tau_{4j} &amp;amp; \sim \mathcal N(0.43, 1) \\
\tau_{5j} &amp;amp; \sim \mathcal N(0.97, 1) \\
\sigma_u &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_v &amp;amp; \sim \operatorname{Exponential}(1) \\
\color{blue}{\sigma_w} &amp;amp; \sim \color{blue}{\operatorname{Exponential}(1)} \\
\color{blue}{\sigma_x} &amp;amp; \sim \color{blue}{\operatorname{Exponential}(1)} \color{black}{.}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Though the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Exponential}(1)\)&lt;/span&gt; prior might be a good place to start with our new &lt;span class=&#34;math inline&#34;&gt;\(\sigma_w\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x\)&lt;/span&gt;, let’s ponder this a bit. Recall these are for the distribution of logged discrimination parameters. Since I’m no statistician or psychometrician, I’m not even sure what that means, which makes it hard to intuit whether the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Exponential}(1)\)&lt;/span&gt; prior makes sense for my data. A plot might help. Here we’ll simulate &lt;span class=&#34;math inline&#34;&gt;\(100{,}000\)&lt;/span&gt; draws from the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Exponential}(1)\)&lt;/span&gt; prior, use those draws to simulate &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; values from &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(0, \sigma_w)\)&lt;/span&gt;, and then plot that distribution in the &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\exp(w_i)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1/\exp(w_i)\)&lt;/span&gt; metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

# simulate from the prior
tibble(sigma_w = rexp(n = 1e5, rate = 1)) %&amp;gt;% 
  # simulate w_i draws
  mutate(`italic(w[i])` = rnorm(n = n(), mean = 0, sd = sigma_w)) %&amp;gt;% 
  # transform
  mutate(`exp(italic(w[i]))` = exp(`italic(w[i])`)) %&amp;gt;% 
  mutate(`1/exp(italic(w[i]))` = 1 / `exp(italic(w[i]))`) %&amp;gt;% 
  # wrangle
  pivot_longer(-sigma_w) %&amp;gt;% 
  mutate(name = factor(name, levels = c(&amp;quot;italic(w[i])&amp;quot;, &amp;quot;exp(italic(w[i]))&amp;quot;, &amp;quot;1/exp(italic(w[i]))&amp;quot;))) %&amp;gt;% 
  # for computational simplicity, remove some values from the far right tail
  filter(value &amp;lt; 100) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 0.1) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-10, 10)) +
  ggtitle(expression(&amp;quot;Prior predictive distribution for &amp;quot;*sigma[italic(w)]%~%Exponential(1))) +
  facet_wrap(~ name, labeller = label_parsed, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The mean and median for &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; are both zero. Because of the very long right tails, both &lt;span class=&#34;math inline&#34;&gt;\(\exp(w_i)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1/\exp(w_i)\)&lt;/span&gt; have very large means and standard deviations. However, both &lt;span class=&#34;math inline&#34;&gt;\(\exp(w_i)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1/\exp(w_i)\)&lt;/span&gt; have medians at &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(1/\exp(w_i)\)&lt;/span&gt; distribution is the one that’s in the latent &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; metric and its percentile-based 95% interval ranges from &lt;span class=&#34;math inline&#34;&gt;\(0.05\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(20.28\)&lt;/span&gt;. That isn’t bad for a permissive level-2 standard deviation prior, but I think we can do better. If we set the mean of the exponential prior to &lt;span class=&#34;math inline&#34;&gt;\(0.463\)&lt;/span&gt;, the resulting prior-predictive distribution of &lt;span class=&#34;math inline&#34;&gt;\(1/\exp(w_i)\)&lt;/span&gt; will have a percentile-based 95% interval of &lt;span class=&#34;math inline&#34;&gt;\(0.25\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt;. Here’s what that looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

# save for this first line, this workflow is the same as before
tibble(sigma_w = rexp(n = 1e5, rate = 1 / 0.463)) %&amp;gt;% 
  mutate(`italic(w[i])` = rnorm(n = n(), mean = 0, sd = sigma_w)) %&amp;gt;% 
  mutate(`exp(italic(w[i]))` = exp(`italic(w[i])`)) %&amp;gt;% 
  mutate(`1/exp(italic(w[i]))` = 1 / `exp(italic(w[i]))`) %&amp;gt;% 
  pivot_longer(-sigma_w) %&amp;gt;% 
  mutate(name = factor(name, levels = c(&amp;quot;italic(w[i])&amp;quot;, &amp;quot;exp(italic(w[i]))&amp;quot;, &amp;quot;1/exp(italic(w[i]))&amp;quot;))) %&amp;gt;% 
  filter(value &amp;lt; 100) %&amp;gt;% 
  
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 0.1) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-10, 10)) +
  ggtitle(expression(&amp;quot;Prior predictive distribution for &amp;quot;*sigma[italic(w)]%~%Exponential(1/0.463))) +
  facet_wrap(~ name, labeller = label_parsed, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-48-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With that as our prior, our updated model formula will be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\small{p(\text{rating} = k | \{ \tau_{kj} \}, \mu_{ij}, \alpha_{ij})} &amp;amp; = \small{\Phi(\alpha_{ij}[\tau_{kj} - \mu_{ij}]) - \Phi(\alpha_{ij}[\tau_{k - 1,j} - \mu_{ij}])} \\
\mu_{ij} &amp;amp; = 0 + u_i + v_j \\ 
\log(\alpha_{ij}) &amp;amp; = 0 + w_i + x_j \\
u_i &amp;amp; \sim \mathcal N(0, \sigma_u) \\
v_j &amp;amp; \sim \mathcal N(0, \sigma_v) \\

w_i &amp;amp; \sim \mathcal N(0, \sigma_w) \\
x_j &amp;amp; \sim \mathcal N(0, \sigma_x) \\

\tau_{1j} &amp;amp; \sim \mathcal N(-0.97, 1) \\
\tau_{2j} &amp;amp; \sim \mathcal N(-0.43, 1) \\
\tau_{3j} &amp;amp; \sim \mathcal N(0, 1) \\
\tau_{4j} &amp;amp; \sim \mathcal N(0.43, 1) \\
\tau_{5j} &amp;amp; \sim \mathcal N(0.97, 1) \\
\sigma_u &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_v &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_w &amp;amp; \sim \operatorname{Exponential}(\color{blue}{1 / 0.463} \color{black}{)} \\
\sigma_x &amp;amp; \sim \operatorname{Exponential}(\color{blue}{1 / 0.463} \color{black}{).}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Put those new priors to work and fit the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 5.223564 mins
fit7 &amp;lt;- brm(
  data = d,
  family = cumulative(probit),
  bf(rating | thres(gr = item) ~ 1 + (1 | id) + (1 | item)) +
    lf(disc ~ 0 + (1 | id) + (1 | item)),
  prior = c(prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N1),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N1),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N1),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N1),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N1),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N2),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N2),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N2),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N2),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N2),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N3),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N3),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N3),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N3),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N3),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N4),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N4),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N4),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N4),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N4),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N5),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N5),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N5),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N5),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N5),
            
            prior(exponential(1), class = sd),
            # here&amp;#39;s the fancy new prior line
            prior(exponential(1 / 0.463), class = sd, dpar = disc)),
  cores = 4,
  seed = 1,
  init_r = 0.2,
  control = list(adapt_delta = .99)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Summarize.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cumulative 
##   Links: mu = probit; disc = log 
## Formula: rating | thres(gr = item) ~ 1 + (1 | id) + (1 | item) 
##          disc ~ 0 + (1 | id) + (1 | item)
##    Data: d (Number of observations: 1000) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 200) 
##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)          1.19      0.14     0.92     1.49 1.00      567     1045
## sd(disc_Intercept)     0.44      0.06     0.34     0.56 1.00     1322     2459
## 
## ~item (Number of levels: 5) 
##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)          0.26      0.23     0.01     0.85 1.00     1451     1997
## sd(disc_Intercept)     0.42      0.18     0.17     0.87 1.00     1101     1271
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[N1,1]    -1.32      0.30    -1.99    -0.77 1.00     1191     2072
## Intercept[N1,2]    -0.50      0.26    -1.09    -0.02 1.00     2149     2864
## Intercept[N1,3]    -0.06      0.25    -0.65     0.41 1.00     2637     2862
## Intercept[N1,4]     1.04      0.28     0.40     1.56 1.00     1859     3117
## Intercept[N1,5]     1.91      0.35     1.21     2.60 1.00     1311     2212
## Intercept[N2,1]    -1.81      0.33    -2.47    -1.17 1.00     1012     2135
## Intercept[N2,2]    -0.60      0.25    -1.08    -0.09 1.00     2020     2854
## Intercept[N2,3]    -0.12      0.24    -0.58     0.40 1.00     2941     2942
## Intercept[N2,4]     0.69      0.25     0.21     1.21 1.00     2323     2732
## Intercept[N2,5]     1.67      0.32     1.07     2.34 1.00     1441     2250
## Intercept[N3,1]    -1.52      0.33    -2.28    -0.96 1.00      980     1378
## Intercept[N3,2]    -0.36      0.27    -0.98     0.11 1.00     1980     2461
## Intercept[N3,3]     0.03      0.26    -0.56     0.51 1.00     2597     2703
## Intercept[N3,4]     1.18      0.29     0.55     1.74 1.00     2283     2869
## Intercept[N3,5]     2.08      0.37     1.36     2.83 1.00     1519     2576
## Intercept[N4,1]    -1.89      0.36    -2.63    -1.24 1.00     1058     2321
## Intercept[N4,2]    -0.73      0.28    -1.35    -0.23 1.00     2103     2903
## Intercept[N4,3]    -0.02      0.26    -0.59     0.47 1.00     3276     3331
## Intercept[N4,4]     1.21      0.30     0.58     1.81 1.00     2084     2892
## Intercept[N4,5]     2.57      0.44     1.74     3.50 1.00     1276     2514
## Intercept[N5,1]    -1.66      0.37    -2.53    -1.07 1.00     1009     1687
## Intercept[N5,2]    -0.35      0.29    -1.04     0.13 1.00     2181     2105
## Intercept[N5,3]     0.25      0.29    -0.42     0.74 1.00     2767     2573
## Intercept[N5,4]     1.40      0.34     0.68     2.04 1.00     2143     2835
## Intercept[N5,5]     2.40      0.42     1.58     3.25 1.00     1596     2798
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get a better sense of what those new level-2 random &lt;span class=&#34;math inline&#34;&gt;\(\log(\alpha_{ij})\)&lt;/span&gt; standard deviation estimates mean, let’s focus on the posterior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_w\)&lt;/span&gt;, which is &lt;span class=&#34;math inline&#34;&gt;\(0.44, 95\% \text{CI}\ [0.34, 0.56]\)&lt;/span&gt;. We can use the &lt;code&gt;ranef()&lt;/code&gt; function to pull the summary statistics for the corresponding &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; posteriors. To simplify things, we’ll just focus on the posterior means. After pulling the summaries, we’ll place them within a tibble, transform them to the &lt;span class=&#34;math inline&#34;&gt;\(\exp(w_i)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1 / \exp(w_i)\)&lt;/span&gt; metrics, and then plot like with the prior predictive distributions, before. But since we only have &lt;span class=&#34;math inline&#34;&gt;\(200\)&lt;/span&gt; values to visualize, this time, we’ll use dot plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(`italic(w[i])` = ranef(fit7)$id[, 1, &amp;quot;disc_Intercept&amp;quot;]) %&amp;gt;% 
  mutate(`exp(italic(w[i]))` = exp(`italic(w[i])`)) %&amp;gt;% 
  mutate(`1/exp(italic(w[i]))` = 1 / exp(`italic(w[i])`)) %&amp;gt;% 
  pivot_longer(everything()) %&amp;gt;% 
  mutate(name = factor(name, levels = c(&amp;quot;italic(w[i])&amp;quot;, &amp;quot;exp(italic(w[i]))&amp;quot;, &amp;quot;1/exp(italic(w[i]))&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = value)) + 
  geom_rug(size = 1/6) +
  geom_dotplot(binwidth = 1/9) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;posterior means&amp;quot;) +
  facet_wrap(~ name, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-50-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we turn to the task of using the model to compute the means of the &lt;code&gt;rating&lt;/code&gt; variable, this time separately for each of the &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt; Neuroticism items. One could generalized the workflow we used for &lt;code&gt;fit1&lt;/code&gt; through &lt;code&gt;fit3&lt;/code&gt;, but I wouldn’t recommend that. The &lt;code&gt;brms::fitted()&lt;/code&gt; function will be our friend, here. Though ultimately we’ll want to use &lt;code&gt;fitted()&lt;/code&gt; with &lt;code&gt;summary = FALSE&lt;/code&gt;, I’m going to save the results both ways to help explain the output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the new data
nd &amp;lt;- d %&amp;gt;% distinct(item)

# save the summarized results
f_summary &amp;lt;-
  fitted(fit7,
         newdata = nd,
         # note this line
         re_formula = ~ (1 | item))

# save the un-summarized results
f &amp;lt;-
  fitted(fit7,
         newdata = nd,
         re_formula = ~ (1 | item),
         summary = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First take a look at the structure of &lt;code&gt;f_summary&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f_summary %&amp;gt;% str()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  num [1:5, 1:4, 1:6] 0.0423 0.0055 0.0852 0.0998 0.1538 ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 3
##   ..$ : NULL
##   ..$ : chr [1:4] &amp;quot;Estimate&amp;quot; &amp;quot;Est.Error&amp;quot; &amp;quot;Q2.5&amp;quot; &amp;quot;Q97.5&amp;quot;
##   ..$ : chr [1:6] &amp;quot;P(Y = 1)&amp;quot; &amp;quot;P(Y = 2)&amp;quot; &amp;quot;P(Y = 3)&amp;quot; &amp;quot;P(Y = 4)&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt;-dimensional array. The &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt; levels of the first dimension are the &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt; levels of &lt;code&gt;item&lt;/code&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt; levels of the second dimension are the typical summary statistics, the posterior mean through the upper-level of the 95% CI. The &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; levels of the third dimension let us divide up the results by the &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; levels of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, the &lt;code&gt;rating&lt;/code&gt; levels &lt;code&gt;1&lt;/code&gt; through &lt;code&gt;6&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But we don’t want summarized results. We want the posterior draws. Enter &lt;code&gt;f&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f %&amp;gt;% str()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  num [1:4000, 1:5, 1:6] 0.0644 0.049 0.0359 0.0493 0.0258 ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 3
##   ..$ : chr [1:4000] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...
##   ..$ : NULL
##   ..$ : chr [1:6] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the dimension within the array is the &lt;span class=&#34;math inline&#34;&gt;\(4{,}000\)&lt;/span&gt; posterior draws. The second dimension is our &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt; levels of &lt;code&gt;item&lt;/code&gt; and the third dimension the &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; levels of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. Before we put this all to use, we’ll want to save a few values as external objects for the plotting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample means for the items
m_n1 &amp;lt;- d %&amp;gt;% filter(item == &amp;quot;N1&amp;quot;) %&amp;gt;% summarise(m = mean(rating)) %&amp;gt;% pull()
m_n2 &amp;lt;- d %&amp;gt;% filter(item == &amp;quot;N2&amp;quot;) %&amp;gt;% summarise(m = mean(rating)) %&amp;gt;% pull()
m_n3 &amp;lt;- d %&amp;gt;% filter(item == &amp;quot;N3&amp;quot;) %&amp;gt;% summarise(m = mean(rating)) %&amp;gt;% pull()
m_n4 &amp;lt;- d %&amp;gt;% filter(item == &amp;quot;N4&amp;quot;) %&amp;gt;% summarise(m = mean(rating)) %&amp;gt;% pull()
m_n5 &amp;lt;- d %&amp;gt;% filter(item == &amp;quot;N5&amp;quot;) %&amp;gt;% summarise(m = mean(rating)) %&amp;gt;% pull()

# save color values for the geom_vline() lines
colors &amp;lt;- viridis::viridis_pal(option = &amp;quot;A&amp;quot;, end = 0.85, direction = -1)(5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay, within &lt;code&gt;rbind()&lt;/code&gt;, we serially stack the levels of the third dimension within the &lt;code&gt;f&lt;/code&gt; array so we can use our &lt;strong&gt;tidyverse&lt;/strong&gt;-style wrangling. Then the workflow looks a lot like before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(f[, , 1],
      f[, , 2],
      f[, , 3],
      f[, , 4],
      f[, , 5],
      f[, , 6]) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  set_names(str_c(&amp;quot;N&amp;quot;, 1:5)) %&amp;gt;% 
  mutate(draw   = rep(1:4000, times = 6),
         rating = rep(1:6, each = 4000)) %&amp;gt;% 
  pivot_longer(N1:N5, names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;p&amp;quot;) %&amp;gt;% 
  mutate(`p * k` = p * rating) %&amp;gt;% 
  group_by(draw, item) %&amp;gt;% 
  summarise(mean_rating = sum(`p * k`)) %&amp;gt;% 
  
  ggplot(aes(x = mean_rating, y = fct_rev(item), fill = item)) +
  stat_halfeye(.width = .95) +
  geom_vline(xintercept = m_n1, linetype = 2, color = colors[1]) +
  geom_vline(xintercept = m_n2, linetype = 2, color = colors[2]) +
  geom_vline(xintercept = m_n3, linetype = 2, color = colors[3]) +
  geom_vline(xintercept = m_n4, linetype = 2, color = colors[4]) +
  geom_vline(xintercept = m_n5, linetype = 2, color = colors[5]) +
  scale_fill_viridis_d(NULL, option = &amp;quot;A&amp;quot;, end = 0.85, direction = -1, alpha = 2/3) +
  labs(title = &amp;quot;The postrior for the mean of the rating values, by item&amp;quot;,
       subtitle = &amp;quot;The dashed vertical lines mark off the sample means, by item. The posterior distributions\nare based on the multilevel unconditional distributional cumulative ordinal model fit7.&amp;quot;,
       x = expression(mu[rating]),
       y = NULL) +
  xlim(2, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-55-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At first glance, it might be upsetting that out posteriors for the item-level means no longer tightly line up with the sample means. However, keep in mind these were computed with a multilevel model, which will impose partial pooling toward the grand mean. Based on decades of hard work within statistics &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977&lt;/a&gt;)&lt;/span&gt;, we know that our partially-pooled means will be more likely generalize to new data than our sample means. This is why we model. Though we do want to understand our sample data, we also want to make inferences about other data from the population.&lt;/p&gt;
&lt;p&gt;Now that we have a full model accounting for all five Neuroticism items, we can use a similar approach to compute the population-level sum score. First, we’ll compute the sample mean of the sum score and save the value as &lt;code&gt;m_sum_score&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m_sum_score &amp;lt;- d %&amp;gt;% 
  group_by(id) %&amp;gt;% 
  summarise(sum = sum(rating)) %&amp;gt;% 
  summarise(mean_sum = mean(sum)) %&amp;gt;% 
  pull()

# what&amp;#39;s the value?
m_sum_score&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we make a small amendment the workflow above. Instead of grouping by posterior &lt;code&gt;draw&lt;/code&gt; and by &lt;code&gt;item&lt;/code&gt; before the &lt;code&gt;summarise()&lt;/code&gt; line, this time we only group by &lt;code&gt;draw&lt;/code&gt;. As a consequence, the values for all the items will be summed within each posterior draw, making a vector of sum scores. Then we plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(f[, , 1],
      f[, , 2],
      f[, , 3],
      f[, , 4],
      f[, , 5],
      f[, , 6]) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  set_names(str_c(&amp;quot;N&amp;quot;, 1:5)) %&amp;gt;% 
  mutate(draw   = rep(1:4000, times = 6),
         rating = rep(1:6, each = 4000)) %&amp;gt;% 
  pivot_longer(N1:N5, names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;p&amp;quot;) %&amp;gt;% 
  mutate(`p * k` = p * rating) %&amp;gt;% 
  # notice we are no longer grouping by draw AND item
  group_by(draw) %&amp;gt;% 
  summarise(mean_sum_score = sum(`p * k`)) %&amp;gt;% 
  
  ggplot(aes(x = mean_sum_score, y = 0)) +
  stat_halfeye(.width = .95) +
  geom_vline(xintercept = m_sum_score, linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = &amp;quot;The postrior for the mean of the Neuroticism sum cores&amp;quot;,
       subtitle = &amp;quot;The dashed vertical lines mark off the sample means. The posterior distributions are based on\nthe multilevel unconditional distributional cumulative ordinal model fit7.&amp;quot;,
       x = expression(mu[Neuroticism~sum~score]),
       y = NULL) +
  xlim(10, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-57-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model does a great job capturing the Neuroticism sum scores.&lt;/p&gt;
&lt;p&gt;Now if you wanted to, you could expand this model further to include correlations between &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; and between &lt;span class=&#34;math inline&#34;&gt;\(v_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_j\)&lt;/span&gt;. That would change the model formula to something like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\small{p(\text{rating} = k | \{ \tau_{kj} \}, \mu_{ij}, \alpha_{ij})} &amp;amp; = \small{\Phi(\alpha_{ij}[\tau_{kj} - \mu_{ij}]) - \Phi(\alpha_{ij}[\tau_{k - 1,j} - \mu_{ij}])} \\
\mu_{ij} &amp;amp; = 0 + u_i + v_j \\ 
\log(\alpha_{ij}) &amp;amp; = 0 + w_i + x_j \\

\begin{bmatrix} u_i \\ w_i \end{bmatrix} &amp;amp; \sim \mathcal{MVN}(\mathbf 0, \mathbf{S_a \color{blue}{R_a} \color{black}{S_a}}) \\
\begin{bmatrix} v_j \\ x_j \end{bmatrix} &amp;amp; \sim \mathcal{MVN}(\mathbf 0, \mathbf{S_b \color{blue}{R_b} \color{black}{S_b}}) \\

\mathbf{S_a} &amp;amp; = \begin{bmatrix} \sigma_u &amp;amp; 0 \\ 0 &amp;amp; \sigma_w \end{bmatrix} \\
\mathbf{S_b} &amp;amp; = \begin{bmatrix} \sigma_v &amp;amp; 0 \\ 0 &amp;amp; \sigma_x \end{bmatrix} \\
\color{blue}{\mathbf{R_a}} &amp;amp; = \color{blue}{\begin{bmatrix} 1 &amp;amp; \rho_a \\ \rho_a &amp;amp; 1 \end{bmatrix}} \\
\color{blue}{\mathbf{R_b}} &amp;amp; = \color{blue}{\begin{bmatrix} 1 &amp;amp; \rho_b \\ \rho_b &amp;amp; 1 \end{bmatrix}} \\

\tau_{1j} &amp;amp; \sim \mathcal N(-0.97, 1) \\
\tau_{2j} &amp;amp; \sim \mathcal N(-0.43, 1) \\
\tau_{3j} &amp;amp; \sim \mathcal N(0, 1) \\
\tau_{4j} &amp;amp; \sim \mathcal N(0.43, 1) \\
\tau_{5j} &amp;amp; \sim \mathcal N(0.97, 1) \\
\sigma_u &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_v &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_w &amp;amp; \sim \operatorname{Exponential}(1 / 0.463) \\
\sigma_x &amp;amp; \sim \operatorname{Exponential}(1 / 0.463) \\
\color{blue}{\rho_a} &amp;amp; \sim \color{blue}{\operatorname{LKJ}(2)} \\
\color{blue}{\rho_b} &amp;amp; \sim \color{blue}{\operatorname{LKJ}(2)} \color{black}{,}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where though our four random parameters are now modeled as two bivariate-normal pairs, there really are only two new parameters: &lt;span class=&#34;math inline&#34;&gt;\(\rho_a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho_b\)&lt;/span&gt;. The LKJ priors for our two new parameters would be weakly regularizing. In my experience, so far, adding those two level-2 correlations can make for notably longer fitting times and they often result in computational complications like the need to further adjust &lt;code&gt;adapt_delta&lt;/code&gt; and so on. So in the interest of space, I’m not going to fit this model. But it’s always a possibility to consider.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-conditional-distributional-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Full conditional distributional model.&lt;/h3&gt;
&lt;p&gt;A while back, we put our level-1 predictor &lt;code&gt;male&lt;/code&gt; to the side while we focused on expanding the random components. Now’s time to bring that predictor back and fit the full conditional distributional model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\small{p(\text{rating} = k | \{ \tau_{kj} \}, \mu_{ij}, \alpha_{ij})} &amp;amp; = \small{\Phi(\alpha_{ij}[\tau_{kj} - \mu_{ij}]) - \Phi(\alpha_{ij}[\tau_{k - 1,j} - \mu_{ij}])} \\
\mu_{ij}          &amp;amp; = \color{blue}{\beta_1 \text{male}_i} \color{black}{ + u_i + v_j} \\ 
\log(\alpha_{ij}) &amp;amp; = \color{blue}{\eta_1 \text{male}_i} \color{black}{ + w_i + x_j} \\
u_i &amp;amp; \sim \mathcal N(0, \sigma_u) \\
v_j &amp;amp; \sim \mathcal N(0, \sigma_v) \\
w_i &amp;amp; \sim \mathcal N(0, \sigma_w) \\
x_j &amp;amp; \sim \mathcal N(0, \sigma_x) \\
\tau_{1j} &amp;amp; \sim \mathcal N(-0.97, 1) \\
\tau_{2j} &amp;amp; \sim \mathcal N(-0.43, 1) \\
\tau_{3j} &amp;amp; \sim \mathcal N(0, 1) \\
\tau_{4j} &amp;amp; \sim \mathcal N(0.43, 1) \\
\tau_{5j} &amp;amp; \sim \mathcal N(0.97, 1) \\
\color{blue}{\beta_1} &amp;amp; \sim \color{blue}{\mathcal N(0, 1)} \\
\color{blue}{\eta_1}  &amp;amp; \sim \color{blue}{\mathcal N(0, 0.347)} \\
\sigma_u &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_v &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_w &amp;amp; \sim \operatorname{Exponential}(1 / 0.463) \\
\sigma_x &amp;amp; \sim \operatorname{Exponential}(1 / 0.463).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;At this point, the change to the &lt;strong&gt;brms&lt;/strong&gt; code is trivial.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 5.223564 mins
fit8 &amp;lt;- brm(
  data = d,
  family = cumulative(probit),
  bf(rating | thres(gr = item) ~ 1 + male + (1 | id) + (1 | item)) +
    lf(disc                    ~ 0 + male + (1 | id) + (1 | item),
       # don&amp;#39;t forget this line
       cmc = FALSE),
  prior = c(prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N1),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N1),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N1),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N1),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N1),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N2),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N2),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N2),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N2),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N2),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N3),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N3),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N3),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N3),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N3),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N4),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N4),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N4),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N4),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N4),
            
            prior(normal(-0.97, 1), class = Intercept, coef = 1, group = N5),
            prior(normal(-0.43, 1), class = Intercept, coef = 2, group = N5),
            prior(normal( 0.00, 1), class = Intercept, coef = 3, group = N5),
            prior(normal( 0.43, 1), class = Intercept, coef = 4, group = N5),
            prior(normal( 0.97, 1), class = Intercept, coef = 5, group = N5),
            
            # add those beta and eta priors back in
            prior(normal(0, 1), class = b),
            prior(normal(0, log(2) / 2), class = b, dpar = disc),
            
            prior(exponential(1), class = sd),
            prior(exponential(1 / 0.463), class = sd, dpar = disc)),
  cores = 4,
  seed = 1,
  init_r = 0.2,
  control = list(adapt_delta = .99)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check one last model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cumulative 
##   Links: mu = probit; disc = log 
## Formula: rating | thres(gr = item) ~ 1 + male + (1 | id) + (1 | item) 
##          disc ~ 0 + male + (1 | id) + (1 | item)
##    Data: d (Number of observations: 1000) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 200) 
##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)          1.19      0.14     0.95     1.48 1.01      561     1269
## sd(disc_Intercept)     0.45      0.06     0.34     0.57 1.00     1176     2156
## 
## ~item (Number of levels: 5) 
##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)          0.23      0.22     0.01     0.80 1.00     1568     2267
## sd(disc_Intercept)     0.39      0.18     0.13     0.83 1.01      692     1015
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[N1,1]    -1.41      0.28    -2.02    -0.88 1.01     1083     1934
## Intercept[N1,2]    -0.57      0.24    -1.10    -0.11 1.00     1649     2840
## Intercept[N1,3]    -0.13      0.23    -0.62     0.31 1.00     2094     2754
## Intercept[N1,4]     0.99      0.26     0.44     1.49 1.00     1695     2870
## Intercept[N1,5]     1.88      0.33     1.26     2.55 1.00     1190     2227
## Intercept[N2,1]    -1.91      0.34    -2.57    -1.25 1.00      903     1806
## Intercept[N2,2]    -0.68      0.25    -1.18    -0.14 1.00     1448     2595
## Intercept[N2,3]    -0.19      0.24    -0.66     0.34 1.00     2032     2858
## Intercept[N2,4]     0.63      0.24     0.17     1.18 1.00     2180     2880
## Intercept[N2,5]     1.62      0.30     1.07     2.26 1.00     1541     2157
## Intercept[N3,1]    -1.56      0.31    -2.23    -0.99 1.01      969     1801
## Intercept[N3,2]    -0.41      0.25    -0.97     0.05 1.00     1992     2320
## Intercept[N3,3]    -0.02      0.25    -0.57     0.43 1.00     2405     2756
## Intercept[N3,4]     1.12      0.28     0.52     1.65 1.00     2068     2839
## Intercept[N3,5]     2.02      0.35     1.35     2.71 1.00     1515     2714
## Intercept[N4,1]    -1.93      0.34    -2.67    -1.30 1.01      930     1596
## Intercept[N4,2]    -0.79      0.26    -1.38    -0.30 1.00     1480     2240
## Intercept[N4,3]    -0.08      0.25    -0.64     0.40 1.00     2758     2591
## Intercept[N4,4]     1.13      0.29     0.54     1.71 1.00     1781     2485
## Intercept[N4,5]     2.48      0.42     1.68     3.35 1.00     1169     1994
## Intercept[N5,1]    -1.66      0.33    -2.39    -1.06 1.01      985     1521
## Intercept[N5,2]    -0.38      0.27    -0.99     0.09 1.00     1916     2387
## Intercept[N5,3]     0.20      0.26    -0.36     0.68 1.00     2610     2768
## Intercept[N5,4]     1.33      0.32     0.70     1.96 1.00     2081     2967
## Intercept[N5,5]     2.31      0.41     1.52     3.13 1.00     1411     2346
## male               -0.30      0.19    -0.68     0.07 1.01      561     1352
## disc_male           0.07      0.10    -0.13     0.27 1.00     1711     2477
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last time we had posteriors for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta_1\)&lt;/span&gt; was way back with the much simpler &lt;code&gt;fit3&lt;/code&gt;. It might be instructive to compare their posteriors from the two models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  as_draws_df(fit3) %&amp;gt;% select(contains(&amp;quot;male&amp;quot;)),
  as_draws_df(fit8) %&amp;gt;% select(contains(&amp;quot;male&amp;quot;))
) %&amp;gt;% 
  set_names(&amp;quot;beta[1]&amp;quot;, &amp;quot;eta[1]&amp;quot;) %&amp;gt;% 
  mutate(fit = rep(c(&amp;quot;fit3&amp;quot;, &amp;quot;fit8&amp;quot;), each = n() / 2)) %&amp;gt;% 
  pivot_longer(-fit) %&amp;gt;% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(.width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_grid(fit ~ name, labeller = label_parsed, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-59-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For my money, the posteriors from the conditional distributional model &lt;code&gt;fit8&lt;/code&gt; are more valid than those from the highly-constrained &lt;code&gt;fit3&lt;/code&gt;. Now that &lt;code&gt;male&lt;/code&gt; is back in the game, let’s plot the latent distributions again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(male  = 0:1,
       mu    = c(0, fixef(fit8)[&amp;quot;male&amp;quot;, 1]),
       sigma = 1 / exp(c(0, fixef(fit8)[&amp;quot;disc_male&amp;quot;, 1]))) %&amp;gt;% 
  expand(nesting(male, mu, sigma),
         x = seq(from = -3.5, to = 3.5, length.out = 200)) %&amp;gt;% 
  mutate(d   = dnorm(x, mean = mu, sd = sigma),
         sex = ifelse(male == 0, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d, fill = sex)) +
  geom_area(alpha = 1/2, position = &amp;quot;identity&amp;quot;) +
  scale_fill_manual(NULL, values = c(&amp;quot;red3&amp;quot;, &amp;quot;blue3&amp;quot;)) +
  scale_x_continuous(expression(Phi), breaks = -3:3) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = &amp;quot;Latent distributions for fit8&amp;quot;,
       subtitle = expression(&amp;quot;The shape for men is based on the posterior means of the &amp;quot;*beta[1]*&amp;quot; and &amp;quot;*eta[1]*&amp;quot; parameters.&amp;quot;)) +
  coord_cartesian(xlim = c(-3, 3)) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-60-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unlike with the previous latent density plots, we have left out the threshold lines for this one. With &lt;span class=&#34;math inline&#34;&gt;\(25\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\tau_{kj}\)&lt;/span&gt; parameters, showing them all would just clutter things up. But since we now our latent &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters vary across all &lt;span class=&#34;math inline&#34;&gt;\(200\)&lt;/span&gt; participants, it might be fun to visualize those.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  distinct(id, male) %&amp;gt;% 
  mutate(beta0 = 0,
         beta1 = fixef(fit8)[&amp;quot;male&amp;quot;, 1],
         u_i = ranef(fit8)$id[, 1, &amp;quot;Intercept&amp;quot;],
         eta0 = 0,
         eta1 = fixef(fit8)[&amp;quot;disc_male&amp;quot;, 1],
         w_i = ranef(fit8)$id[, 1, &amp;quot;disc_Intercept&amp;quot;]) %&amp;gt;% 
  mutate(mu_i = beta0 + beta1 * male + u_i,
         sigma_i = 1 / exp(eta0 + eta1 * male + w_i)) %&amp;gt;% 
  expand(nesting(id, male, mu_i, sigma_i),
         x = seq(from = -6.5, to = 6.5, length.out = 200)) %&amp;gt;% 
  mutate(d   = dnorm(x, mean = mu_i, sd = sigma_i),
         sex = ifelse(male == 0, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;)) %&amp;gt;% 
  # unnecessary, but keeps male on the left, which matches the position of the previous plot
  mutate(sex = fct_rev(sex)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d, fill = sex, color = sex, group = id)) +
  geom_area(alpha = 1/20, size = 1/20, position = &amp;quot;identity&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;blue3&amp;quot;, &amp;quot;red3&amp;quot;), breaks = NULL) +
  scale_color_manual(values = c(&amp;quot;blue3&amp;quot;, &amp;quot;red3&amp;quot;), breaks = NULL) +
  scale_x_continuous(expression(Phi), breaks = -3:3 * 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = &amp;quot;Person-level latent distributions for fit8&amp;quot;,
       subtitle = expression(&amp;quot;The shapes are based on the posterior means of the &amp;quot;*beta[1]*&amp;quot;, &amp;quot;*eta[1]*&amp;quot;, &amp;quot;*italic(u[i])*&amp;quot;, and &amp;quot;*italic(w[i])*&amp;quot; parameters.&amp;quot;)) +
  coord_cartesian(xlim = c(-5.7, 5.7)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ sex)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-61-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As this is a fully cross-classified model, we can make a similar plot for the question-level latent densities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  distinct(item) %&amp;gt;% 
  mutate(beta0 = 0,
         beta1 = fixef(fit8)[&amp;quot;male&amp;quot;, 1],
         v_i = ranef(fit8)$item[, 1, &amp;quot;Intercept&amp;quot;],
         eta0 = 0,
         eta1 = fixef(fit8)[&amp;quot;disc_male&amp;quot;, 1],
         x_i = ranef(fit8)$item[, 1, &amp;quot;disc_Intercept&amp;quot;]) %&amp;gt;% 
  expand(nesting(item, beta0, beta1, v_i, eta0, eta1, x_i),
         male = 0:1) %&amp;gt;% 
  mutate(mu_j = beta0 + beta1 * male + v_i,
         sigma_j = 1 / exp(eta0 + eta1 * male + x_i)) %&amp;gt;% 
  expand(nesting(item, male, mu_j, sigma_j),
         x = seq(from = -5, to = 5, length.out = 200)) %&amp;gt;% 
  mutate(d   = dnorm(x, mean = mu_j, sd = sigma_j),
         sex = ifelse(male == 0, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;)) %&amp;gt;% 
  # unnecessary, but keeps male on the left, which matches the position of the previous plots
  mutate(sex = fct_rev(sex)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d, fill = item, color = item, group = item)) +
  geom_area(alpha = 1/4, size = 1/2, position = &amp;quot;identity&amp;quot;) +
  
  scale_fill_viridis_d(NULL, option = &amp;quot;A&amp;quot;, end = 0.85, direction = -1) +
  scale_color_viridis_d(NULL, option = &amp;quot;A&amp;quot;, end = 0.85, direction = -1) +
  scale_x_continuous(expression(Phi), breaks = -2:2 * 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = &amp;quot;Question-level latent distributions for fit8&amp;quot;,
       subtitle = expression(&amp;quot;The shapes are based on the posterior means of the &amp;quot;*beta[1]*&amp;quot;, &amp;quot;*eta[1]*&amp;quot;, &amp;quot;*italic(v[j])*&amp;quot;, and &amp;quot;*italic(x[j])*&amp;quot; parameters.&amp;quot;)) +
  coord_cartesian(xlim = c(-4.25, 4.25)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ sex) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-62-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even though you might think of &lt;code&gt;male&lt;/code&gt; as a person-level variable, you still need to account for it when plotting the question-level distributions because the grand-mean values for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; are both conditional on &lt;code&gt;male&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Do bear in mind, though, that the latent means and spreads the items share a very tenuous relationship with the naïve sample statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;%
  mutate(sex = ifelse(male == 0, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;)) %&amp;gt;%
  # unnecessary, but keeps male on the left, which matches the position of the previous plots
  mutate(sex = fct_rev(sex)) %&amp;gt;%
  group_by(sex, item) %&amp;gt;%
  summarise(m = mean(rating),
            s = sd(rating)) %&amp;gt;%
  mutate_if(is.double, round, digits = 1) %&amp;gt;%
  mutate(stat = str_c(m, &amp;quot; (&amp;quot;, s, &amp;quot;)&amp;quot;)) %&amp;gt;%
  select(-m, -s) %&amp;gt;%
  pivot_wider(names_from = sex, values_from = stat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 3
##   item  male      female   
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;    
## 1 N1    3.1 (1.5) 3.1 (1.6)
## 2 N2    3.4 (1.4) 3.5 (1.6)
## 3 N3    2.8 (1.3) 3.3 (1.7)
## 4 N4    3.2 (1.6) 3.3 (1.5)
## 5 N5    2.3 (1.4) 3.4 (1.6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I believe this apparent mismatch is because of how the cumulative probit model uses thresholds and the normal CDF to describe relative probabilities among the ordinal categories. The latent means and standard deviations have very different meanings from the sample statistics. So it goes…&lt;/p&gt;
&lt;p&gt;From an IRT perspective, we can think of the &lt;span class=&#34;math inline&#34;&gt;\(\mu_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_j\)&lt;/span&gt; parameters as item difficulty and discrimination parameters. Here we display them in a coefficient plot in the same way seen in Figure &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; in Bürkner’s IRT &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; tutorial.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(ranef(fit8)$item[, , &amp;quot;Intercept&amp;quot;],
      ranef(fit8)$item[, , &amp;quot;disc_Intercept&amp;quot;] %&amp;gt;% exp()) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  mutate(item = rep(str_c(&amp;quot;N&amp;quot;, 1:5), times = 2),
         parameter = rep(c(&amp;quot;difficulty&amp;quot;, &amp;quot;discrimination&amp;quot;), each = 5)) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = item)) +
  geom_pointrange(fatten = 1.5) +
  labs(title = &amp;quot;IRT-based item parameters for fit8&amp;quot;,
       x = &amp;quot;marginal posterior&amp;quot;) +
  facet_wrap(~ parameter)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-12-29-notes-on-the-bayesian-cumulative-probit/index_files/figure-html/unnamed-chunk-64-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I still haven’t figured out how to make item-characteristic curves for an ordinal model. If you know how, please chime in on twitter. And that last plot’s going to wrap this little blog post up. If you have any concerns about my equations or the workflows, herein, do chime in to twitter. This is a resource for us all and I’d hate to spread incorrect methods.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New blog up!&lt;a href=&#34;https://t.co/6T08AY8VDJ&#34;&gt;https://t.co/6T08AY8VDJ&lt;/a&gt;&lt;br&gt;&lt;br&gt;Some of my recent projects required the Bayesian cumulative probit model. To bet a better handle on the cumulative probit, I worked through some toy problems. This blog contains my notes.&lt;br&gt;&lt;br&gt;If you see errors, do chime in.&lt;a href=&#34;https://twitter.com/hashtag/brms?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#brms&lt;/a&gt;&lt;/p&gt;&amp;mdash; Solomon Kurz (@SolomonKurz) &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1476641792263131143?ref_src=twsrc%5Etfw&#34;&gt;December 30, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Happy modeling, friends.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.1 (2021-08-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_3.0.1 brms_2.16.2     Rcpp_1.0.7      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.7     purrr_0.3.4    
##  [8] readr_2.0.1     tidyr_1.1.3     tibble_3.1.6    ggplot2_3.3.5   tidyverse_1.3.1
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.3.0      plyr_1.8.6           igraph_1.2.6         svUnit_1.0.6        
##   [6] splines_4.1.1        crosstalk_1.1.1      TH.data_1.0-10       rstantools_2.1.1     inline_0.3.19       
##  [11] digest_0.6.28        htmltools_0.5.2      viridis_0.6.1        rsconnect_0.8.24     fansi_0.5.0         
##  [16] magrittr_2.0.1       checkmate_2.0.0      tzdb_0.1.2           modelr_0.1.8         RcppParallel_5.1.4  
##  [21] matrixStats_0.61.0   xts_0.12.1           sandwich_3.0-1       prettyunits_1.1.1    colorspace_2.0-2    
##  [26] rvest_1.0.1          ggdist_3.0.0         haven_2.4.3          xfun_0.25            callr_3.7.0         
##  [31] crayon_1.4.2         jsonlite_1.7.2       lme4_1.1-27.1        survival_3.2-11      zoo_1.8-9           
##  [36] glue_1.5.0           gtable_0.3.0         emmeans_1.7.1-1      V8_3.4.2             distributional_0.2.2
##  [41] pkgbuild_1.2.0       rstan_2.26.3         abind_1.4-5          scales_1.1.1         mvtnorm_1.1-2       
##  [46] DBI_1.1.1            miniUI_0.1.1.1       viridisLite_0.4.0    xtable_1.8-4         tmvnsim_1.0-2       
##  [51] diffobj_0.3.4        stats4_4.1.1         StanHeaders_2.26.3   DT_0.19              htmlwidgets_1.5.3   
##  [56] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0   posterior_1.1.0.9000 ellipsis_0.3.2      
##  [61] pkgconfig_2.0.3      loo_2.4.1            farver_2.1.0         sass_0.4.0           dbplyr_2.1.1        
##  [66] utf8_1.2.2           labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.12         reshape2_1.4.4      
##  [71] later_1.3.0          cellranger_1.1.0     munsell_0.5.0        tools_4.1.1          cli_3.1.0           
##  [76] generics_0.1.1       broom_0.7.9          ggridges_0.5.3       evaluate_0.14        fastmap_1.1.0       
##  [81] yaml_2.2.1           fs_1.5.0             processx_3.5.2       knitr_1.33           nlme_3.1-152        
##  [86] mime_0.11            projpred_2.0.2       xml2_1.3.2           compiler_4.1.1       bayesplot_1.8.1     
##  [91] shinythemes_1.2.0    rstudioapi_0.13      curl_4.3.2           gamm4_0.2-6          reprex_2.0.1        
##  [96] bslib_0.3.0          stringi_1.7.4        highr_0.9            ps_1.6.0             blogdown_1.5        
## [101] Brobdingnag_1.2-6    lattice_0.20-44      Matrix_1.3-4         psych_2.1.6          nloptr_1.2.2.2      
## [106] markdown_1.1         shinyjs_2.0.0        tensorA_0.36.2       vctrs_0.3.8          pillar_1.6.4        
## [111] lifecycle_1.0.1      jquerylib_0.1.4      bridgesampling_1.1-2 estimability_1.3     httpuv_1.6.2        
## [116] R6_2.5.1             bookdown_0.23        promises_1.2.0.1     gridExtra_2.3        codetools_0.2-18    
## [121] boot_1.3-28          colourpicker_1.1.0   MASS_7.3-54          gtools_3.9.2         assertthat_0.2.1    
## [126] withr_2.4.2          mnormt_2.0.2         shinystan_2.5.0      multcomp_1.4-17      mgcv_1.8-36         
## [131] parallel_4.1.1       hms_1.1.0            grid_4.1.1           coda_0.19-4          minqa_1.2.4         
## [136] rmarkdown_2.10       shiny_1.6.0          lubridate_1.7.10     base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-burknerBayesianItemResponse2020&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020a). Bayesian item response modeling in &lt;span&gt;R&lt;/span&gt; with brms and &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;arXiv:1905.09501 [Stat]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/1905.09501&#34;&gt;http://arxiv.org/abs/1905.09501&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-efronSteinParadoxStatistics1977&#34; class=&#34;csl-entry&#34;&gt;
Efron, B., &amp;amp; Morris, C. (1977). Stein’s paradox in statistics. &lt;em&gt;Scientific American&lt;/em&gt;, &lt;em&gt;236&lt;/em&gt;(5), 119–127. &lt;a href=&#34;https://doi.org/10.1038/scientificamerican0577-119&#34;&gt;https://doi.org/10.1038/scientificamerican0577-119&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-goldberg1999broad&#34; class=&#34;csl-entry&#34;&gt;
Goldberg, L. R. (1999). A broad-bandwidth, public domain, personality inventory measuring the lower-level facets of several five-factor models. In I. Mervielde, I. Deary, F. De Fruyt, &amp;amp; F. Ostendorf (Eds.), &lt;em&gt;Personality psychology in &lt;span&gt;Europe&lt;/span&gt;&lt;/em&gt; (Vol. 7, pp. 7–28). &lt;span&gt;Tilburg University Press&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzDoingBayesianData2021&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis in brms and the tidyverse&lt;/em&gt; (version 0.4.0). &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingBrms2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Statistical rethinking with brms, &lt;span class=&#34;nocase&#34;&gt;ggplot2&lt;/span&gt;, and the tidyverse&lt;/em&gt; (version 1.2.0). &lt;a href=&#34;https://doi.org/10.5281/zenodo.3693202&#34;&gt;https://doi.org/10.5281/zenodo.3693202&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-psych&#34; class=&#34;csl-entry&#34;&gt;
Revelle, W. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;psych&lt;/span&gt;: &lt;span&gt;Procedures&lt;/span&gt; for psychological, psychometric, and personality research&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=psych&#34;&gt;https://CRAN.R-project.org/package=psych&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-revelle2010individual&#34; class=&#34;csl-entry&#34;&gt;
Revelle, W., Wilt, J., &amp;amp; Rosenthal, A. (2010). Individual differences in cognition: &lt;span&gt;New&lt;/span&gt; methods for examining the personality-cognition link. In A. Gruszka, G. Matthews, &amp;amp; B. Szymura (Eds.), &lt;em&gt;Handbook of individual differences in cognition: &lt;span&gt;Attention&lt;/span&gt;, memory and executive control&lt;/em&gt; (pp. 27–49). &lt;span&gt;Springer&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

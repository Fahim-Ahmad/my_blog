<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>counts | Fahim Ahmad</title>
    <link>/tag/counts/</link>
      <atom:link href="/tag/counts/index.xml" rel="self" type="application/rss+xml" />
    <description>counts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>¬© Fahim Ahmad (2020)</copyright><lastBuildDate>Mon, 12 Jul 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>counts</title>
      <link>/tag/counts/</link>
    </image>
    
    <item>
      <title>Got overdispersion? Try observation-level random effects with the Poisson-lognormal mixture</title>
      <link>/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/</link>
      <pubDate>Mon, 12 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/</guid>
      <description>
&lt;script src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;what&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What?&lt;/h2&gt;
&lt;p&gt;One of &lt;a href=&#34;https://twitter.com/tjmahr&#34;&gt;Tristan Mahr&lt;/a&gt;‚Äôs recent Twitter threads almost broke my brain.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;wait when people talk about treating overdispersion by using random effects, they sometimes put a random intercept on each row?? &lt;a href=&#34;https://t.co/7NjG4uw3nz&#34;&gt;https://t.co/7NjG4uw3nz&lt;/a&gt; &lt;a href=&#34;https://t.co/fo8Ylcejqv&#34;&gt;pic.twitter.com/fo8Ylcejqv&lt;/a&gt;&lt;/p&gt;&amp;mdash; tj mahr (originally Doki Doki Panic in Japan) üççüçï (@tjmahr) &lt;a href=&#34;https://twitter.com/tjmahr/status/1413186646783242242?ref_src=twsrc%5Etfw&#34;&gt;July 8, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;It turns out that you can use random effects on cross-sectional count data. Yes, that‚Äôs right. Each count gets its own random effect. Some people call this observation-level random effects and it can be a tricky way to handle overdispersion. The purpose of this post is to show how to do this and to try to make sense of what it even means.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;First, I should clarify a bit. Mahr‚Äôs initial post and much of the thread to follow primarily focused on counts within the context of binomial data. If you‚Äôve ever read a book on the generalized linear model (GLM), you know that the two broad frameworks for modeling counts are as binomial or Poisson. The basic difference is if your counts are out of a known number of trials (e.g., I got 3 out of 5 questions correct in my pop quiz, last week&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;), the binomial is generally the way to go. However, if your counts aren‚Äôt out of a well-defined total (e.g., I drank 1497 cups of coffee&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, last year), the Poisson distribution offers a great way to think about your data. In this post, we‚Äôll be focusing on Poisson-like counts.&lt;/p&gt;
&lt;p&gt;The Poisson distribution is named after the French mathematician &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e8/E._Marcellot_Sim√©on-Denis_Poisson_1804.jpg&#34;&gt;Sim√©on Denis Poisson&lt;/a&gt;, who lived and died about 200 years ago. Poisson‚Äôs distribution is valid for non-negative integers, which is basically what counts are. The distribution has just one parameter, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, which controls both its mean and variance and imposes the assumption that the mean of your counts is the same as the variance. On the one hand, this is great because it keeps things simple‚Äìparsimony and all. On the other hand, holding the mean and variance the same is a really restrictive assumption and it just doesn‚Äôt match up well with a lot of real-world data.&lt;/p&gt;
&lt;p&gt;This Poisson assumption that the mean equals the variance is sometimes called &lt;em&gt;equidispersion&lt;/em&gt;. Count data violate the equidispersion assumption when their variance is smaller than their mean (&lt;em&gt;underdispersion&lt;/em&gt;) or when their variance is larger than their mean (&lt;em&gt;overdispersion&lt;/em&gt;). In practice, overdispersion tends to crop up most often. Real-world count data are overdispersed so often that statisticians have had to come up with a mess of strategies to handle the problem. In the applied statistics that I‚Äôm familiar with, the two most common ways to handle overdispersed count data are with the negative-binomial model, or with random effects. We‚Äôll briefly cover both.&lt;/p&gt;
&lt;div id=&#34;negative-binomial-counts.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Negative-binomial counts.&lt;/h3&gt;
&lt;p&gt;As its name implies, the negative-binomial model has a deep relationship with the binomial model. I‚Äôm not going to go into those details, but Hilbe covered them in his well-named &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hilbeNegativeBinomialRegression2011&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt; textbook, if you‚Äôre curious. Basically, the negative-binomial model adds a dispersion parameter to the Poisson. Different authors refer to it with different names. Hilbe, for example, called it both &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-B√ºrkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner&lt;/a&gt; (&lt;a href=&#34;#ref-B√ºrkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;2021b&lt;/a&gt;)&lt;/span&gt; and the &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-standevelopmentteamStanFunctionsReference2021&#34; role=&#34;doc-biblioref&#34;&gt;Stan Development Team&lt;/a&gt; (&lt;a href=&#34;#ref-standevelopmentteamStanFunctionsReference2021&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; both call it &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. By which ever name, the negative-binomial overdispersion parameter helps disentangle the mean from the variance in a set of counts. The way it does it is by re-expressing the count data as coming from a mixture where each count is from its own Poisson distribution with its own &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter. Importantly, the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;‚Äôs in this mixture of Poissons follow a gamma distribution, which is why the negative binomial is also sometimes referred to as a gamma-Poisson model. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;, for example, generally prefers to speak in terms of the gamma-Poisson.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poission-counts-with-random-intercepts.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poission counts with random intercepts.&lt;/h3&gt;
&lt;p&gt;Another way to handle overdispersion is to ask whether the data are grouped. In my field, this naturally occurs when you collect longitudinal data. My counts, over time, will differ form your counts, over time, and we accommodate that by adding a multilevel structure to the model. This, then, takes us to the generalized linear &lt;em&gt;mixed&lt;/em&gt; model (GLMM), which is covered in text books like &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-cameron2013regression&#34; role=&#34;doc-biblioref&#34;&gt;Cameron &amp;amp; Trivedi&lt;/a&gt; (&lt;a href=&#34;#ref-cameron2013regression&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gelmanDataAnalysisUsing2006&#34; role=&#34;doc-biblioref&#34;&gt;Gelman &amp;amp; Hill&lt;/a&gt; (&lt;a href=&#34;#ref-gelmanDataAnalysisUsing2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;. Say your data have &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; groups. With a simple random-intercept Poisson model, each group of counts gets its own &lt;span class=&#34;math inline&#34;&gt;\(\lambda_j\)&lt;/span&gt; parameter and the population of those &lt;span class=&#34;math inline&#34;&gt;\(\lambda_j\)&lt;/span&gt;‚Äôs is described in terms of a grand mean (an overall &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; intercept) and variation around that grand mean (typically a standard deviation or variance parameter). Thus, if your &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; data are counts from &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; cases clustered within &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; groups, the random-intercept Poisson model can be expressed as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ij} &amp;amp; \sim \operatorname{Poisson}(\lambda_{ij}) \\
\log(\lambda_{ij}) &amp;amp; = \beta_0 + \zeta_{0j} \\
\zeta_{0j} &amp;amp; \sim \operatorname{Normal}(0, \sigma_0)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the grand mean is &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, the group-specific deviations around the grand mean are the &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0j}\)&lt;/span&gt;‚Äôs, and the variation across those &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0j}\)&lt;/span&gt;‚Äôs is expressed by a standard-deviation parameter &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt;. Thus following the typical GLMM convention, we model the group-level deviations with the normal distribution. Also notice that whether we‚Äôre talking about single-level GLMs or multilevel GLMMs, we typically model &lt;span class=&#34;math inline&#34;&gt;\(\log \lambda\)&lt;/span&gt;, instead of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. This prevents the model from predicting negative counts. Keep this in mind.&lt;/p&gt;
&lt;p&gt;Anyway, the random-intercept Poisson model can go a long way for handling overdispersion when your data are grouped. It‚Äôs also possible to combine this approach with the last one and fit a negative-binomial model with a random intercept, too. Though I haven‚Äôt seen this used much in practice, you can even take a distributional model approach &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-B√ºrkner2021Distributional&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2021a&lt;/a&gt;)&lt;/span&gt; and set the negative-binomial dispersion parameter to random, too. That, for example, could look like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ij} &amp;amp; \sim \operatorname{Gamma-Poisson}(\lambda_{ij}, \phi_{ij}) \\
\log(\lambda_{ij}) &amp;amp; = \beta_0 + \zeta_{0j} \\
\log(\phi_{ij}) &amp;amp; = \gamma_0 + \zeta_{1j} \\
\zeta_{0j} &amp;amp; \sim \operatorname{Normal}(0, \sigma_0) \\
\zeta_{1j} &amp;amp; \sim \operatorname{Normal}(0, \sigma_1).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theres-a-third-option-the-poisson-lognormal.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;There‚Äôs a third option: The Poisson-lognormal.&lt;/h3&gt;
&lt;p&gt;Now a typical condition for a random-intercept model (whether using the Poison, the negative-binomial, or any other likelihood function) is that at least some of the &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; groups, if not most or all, contain two or more cases. For example, in a randomized controlled trial you might measure the outcome variable 3 or 5 or 10 times over the course of the trial. In a typical non-experimental experience-sampling study, you might get 10 or 50 or a few hundred measurements from each participant over the course of a few days, weeks, or months. Either way, we tend to have multiple &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;‚Äôs within each level of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt;. As it turns out, you don‚Äôt have to restrict yourself that way. With the observation-level random effects (OLRE) approach, each case (each level of &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;) gets its own random effect &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-harrison2014using&#34; role=&#34;doc-biblioref&#34;&gt;Harrison, 2014&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But why would you do that?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Think back to the conventional regression model where some variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is predicting some continuous variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. We can express the model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 x_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the residual variance not accounted for by &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is captured in &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Thus &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; can be seen as a residual-variance term. The conventional Poisson model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) &amp;amp; = \beta_0 + \beta_1 x_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;doesn‚Äôt have a residual-variance term. Rather, the variance in the data is deterministically controlled by the linear model on &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt;, which works great in the case of equidispersion, but fails when the data are overdispersed. Hence the negative-binomial and the random-intercept models. But what if we &lt;em&gt;could&lt;/em&gt; tack on a residual variance term? It might take on a form like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) &amp;amp; = \beta_0 + \beta_1 x_i + \epsilon_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; is the residual variation in &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; not captured by the deterministic part of the linear model for &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt;. Following the conventional regression model, we might make our lives simple and further presume &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i \sim \operatorname{Normal}(0, \sigma_\epsilon)\)&lt;/span&gt;. Though he didn‚Äôt use this style of notation, that‚Äôs basically the insight from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-bulmer1974OnFitting&#34; role=&#34;doc-biblioref&#34;&gt;Bulmer&lt;/a&gt; (&lt;a href=&#34;#ref-bulmer1974OnFitting&#34; role=&#34;doc-biblioref&#34;&gt;1974&lt;/a&gt;)&lt;/span&gt;. But rather than speak in terms of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; and residual variance, Bulmer proposed an alternative to the gamma-Poisson mixture and asked his audience to imagine each count in the data was from its own Poisson distribution with its own &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter, but that those &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameters were distributed according to the lognormal distribution. Now Bulmer had a substantive motivation for proposing the lognormal based on the species-abundance data and I‚Äôm not going to get into any of that. But the basic point was, if we can have a gamma-distributed mixture of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;‚Äôs, why not a lognormal mixture, instead?&lt;/p&gt;
&lt;p&gt;The trouble with Bulmer‚Äôs lognormal-mixture approach is it‚Äôs not readily available in most software packages. However, notice what happens when you specify an OLRE model with the Poisson likelihood:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) &amp;amp; = \beta_0 + \zeta_{0i} \\
\zeta_{0i} &amp;amp; \sim \operatorname{Normal}(0, \sigma_0).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0i}\)&lt;/span&gt; now looks a lot like the &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; term in a standard intercepts-only regression model. Further, since the linear model is defined for &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt;, that means the &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0i}\)&lt;/span&gt; terms will be log-normally distributed in the exponentiated &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt; space. In essence, the OLRE-Poisson model is a way to hack your multilevel regression software to fit a Poisson-lognormal model for overdispersed counts.&lt;/p&gt;
&lt;p&gt;Now we have a sense of the theory, it‚Äôs time to fit some models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-example-salamander-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Empirical example: Salamander data&lt;/h2&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;As per usual, we‚Äôll be working within &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;. We‚Äôll be fitting our models with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and most of our data wrangling and plotting work will be done with aid from the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt; and friends‚Äì&lt;strong&gt;patchwork&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-patchwork&#34; role=&#34;doc-biblioref&#34;&gt;Pedersen, 2019&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;tidybayes&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;. We‚Äôll take our data set from McElreath‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-rethinking&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt; &lt;strong&gt;rethinking&lt;/strong&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)
library(tidyverse)
library(tidybayes)
library(patchwork)

data(salamanders, package = &amp;quot;rethinking&amp;quot;)

glimpse(salamanders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 47
## Columns: 4
## $ SITE      &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1‚Ä¶
## $ SALAMAN   &amp;lt;int&amp;gt; 13, 11, 11, 9, 8, 7, 6, 6, 5, 5, 4, 3, 3, 3, 3, 3, 2, 2, 2, ‚Ä¶
## $ PCTCOVER  &amp;lt;int&amp;gt; 85, 86, 90, 88, 89, 83, 83, 91, 88, 90, 87, 83, 87, 89, 92, ‚Ä¶
## $ FORESTAGE &amp;lt;int&amp;gt; 316, 88, 548, 64, 43, 368, 200, 71, 42, 551, 675, 217, 212, ‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are in the &lt;code&gt;salamanders&lt;/code&gt; data frame, which contains counts of salamanders from 47 locations in northern California &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-welsh1995habitat&#34; role=&#34;doc-biblioref&#34;&gt;Welsh Jr &amp;amp; Lind, 1995&lt;/a&gt;)&lt;/span&gt;. Our count variable is &lt;code&gt;SALAMAN&lt;/code&gt;. The location for each count is indexed by the &lt;code&gt;SITE&lt;/code&gt; column. You could use the other two variables as covariates, but we won‚Äôt be focusing on those in this post. Here‚Äôs what &lt;code&gt;SALAMAN&lt;/code&gt; looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# adjust the global plotting theme
theme_set(theme_classic())

salamanders %&amp;gt;% 
  ggplot(aes(x = SALAMAN)) +
  geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Those data look overdispersed. We can get a quick sense of the overdispersion with sample statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;salamanders %&amp;gt;% 
  summarise(mean = mean(SALAMAN),
            variance = var(SALAMAN)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       mean variance
## 1 2.468085 11.38483&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For small-&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; data, we shouldn‚Äôt expect the mean to be exactly the same as the variance in Poisson data. This big of a difference, though, suggests&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; overdispersion even with a modest &lt;span class=&#34;math inline&#34;&gt;\(N = 47\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the models.&lt;/h3&gt;
&lt;p&gt;We‚Äôll fit three intercepts-only models. The first will be a conventional Poisson model and the second will be the negative binomial (a.k.a. the gamma-Poisson mixture). We‚Äôll finish off with our Poisson-lognormal mixture via the OLRE technique. Since we‚Äôre working with Bayesian software, we‚Äôll need priors. Though I‚Äôm not going to explain them in any detail, we‚Äôll be using the weakly-regularizing approach advocated for in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here‚Äôs how to fit the models with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# conventional Poisson
fit1 &amp;lt;-
  brm(data = salamanders, 
      family = poisson,
      SALAMAN ~ 1,
      prior(normal(log(3), 0.5), class = Intercept),
      cores = 4, seed = 1)

# gamma-Poisson mixture
fit2 &amp;lt;-
  brm(data = salamanders, 
      family = negbinomial,
      SALAMAN ~ 1,
      prior = c(prior(normal(log(3), 0.5), class = Intercept),
                prior(gamma(0.01, 0.01), class = shape)),
      cores = 4, seed = 1)

# Poisson-lognormal mixture
fit3 &amp;lt;-
  brm(data = salamanders, 
      family = poisson,
      SALAMAN ~ 1 + (1 | SITE),
      prior = c(prior(normal(log(3), 0.5), class = Intercept),
                prior(exponential(1), class = sd)),
      cores = 4, seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-the-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluate the models.&lt;/h3&gt;
&lt;p&gt;Here‚Äôs a quick parameter summary for each of the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: SALAMAN ~ 1 
##    Data: salamanders (Number of observations: 47) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.91      0.09     0.73     1.08 1.00     1483     1967
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: negbinomial 
##   Links: mu = log; shape = identity 
## Formula: SALAMAN ~ 1 
##    Data: salamanders (Number of observations: 47) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.95      0.21     0.54     1.36 1.00     2907     2195
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## shape     0.58      0.18     0.30     1.01 1.00     3608     2640
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: SALAMAN ~ 1 + (1 | SITE) 
##    Data: salamanders (Number of observations: 47) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~SITE (Number of levels: 47) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.28      0.23     0.89     1.80 1.00     1117     1435
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.34      0.22    -0.12     0.75 1.00     1687     2533
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might use the &lt;code&gt;pp_check()&lt;/code&gt; function to get a graphic sense of how well each model fit the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;-
  pp_check(fit1, type = &amp;quot;bars&amp;quot;, nsample = 150, fatten = 1, size = 1/2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 15),
                  ylim = c(0, 26)) +
  labs(title = &amp;quot;fit1&amp;quot;,
       subtitle = &amp;quot;Conventional Poisson&amp;quot;)

p2 &amp;lt;-
  pp_check(fit2, type = &amp;quot;bars&amp;quot;, nsample = 150, fatten = 1, size = 1/2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 15),
                  ylim = c(0, 26)) +
  labs(title = &amp;quot;fit2&amp;quot;,
       subtitle = &amp;quot;Gamma-Poisson mixture&amp;quot;)

p3 &amp;lt;-
  pp_check(fit3, type = &amp;quot;bars&amp;quot;, nsample = 150, fatten = 1, size = 1/2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 15),
                  ylim = c(0, 26)) +
  labs(title = &amp;quot;fit3&amp;quot;,
       subtitle = &amp;quot;Poisson-lognormal mixture&amp;quot;)

p1 + p2 + p3 + plot_layout(guides = &amp;quot;collect&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The conventional Poisson model seems like a disaster. Both the gamma-Poisson and the Poisson-lognormal models seemed to capture the data much better. We also might want to compare the models with information criteria. Here we‚Äôll use the LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- add_criterion(fit1, criterion = &amp;quot;loo&amp;quot;)
fit2 &amp;lt;- add_criterion(fit2, criterion = &amp;quot;loo&amp;quot;)
fit3 &amp;lt;- add_criterion(fit3, criterion = &amp;quot;loo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I first executed that code, I got the following warning message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Found 29 observations with a pareto_k &amp;gt; 0.7 in model ‚Äòfit3.‚Äô It is recommended to set ‚Äòmoment_match = TRUE‚Äô in order to perform moment matching for problematic observations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To use the &lt;code&gt;moment_match = TRUE&lt;/code&gt; option within the &lt;code&gt;add_criterion()&lt;/code&gt; function, you have to specify &lt;code&gt;save_pars = save_pars(all = TRUE)&lt;/code&gt; within &lt;code&gt;brm()&lt;/code&gt; when fitting the model. Here‚Äôs how to do that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit the Poisson-lognormal mixture, again
fit3 &amp;lt;-
  brm(data = salamanders, 
      family = poisson,
      SALAMAN ~ 1 + (1 | SITE),
      prior = c(prior(normal(log(3), 0.5), class = Intercept),
                prior(exponential(1), class = sd)),
      cores = 4, seed = 1,
      # here&amp;#39;s the new part
      save_pars = save_pars(all = TRUE))

# add the LOO
fit3 &amp;lt;- add_criterion(
  fit3, criterion = &amp;quot;loo&amp;quot;, 
  # this part is new, too
  moment_match = TRUE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we‚Äôre ready to compare the models with the LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(fit1, fit2, fit3, criterion = &amp;quot;loo&amp;quot;) %&amp;gt;% 
  print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic
## fit3    0.0       0.0   -89.8      6.6        20.2    1.4    179.6   13.2  
## fit2   -8.2       2.3   -98.0      8.3         1.6    0.2    196.0   16.6  
## fit1  -50.5      13.9  -140.4     17.4         4.3    1.2    280.7   34.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even after accounting for model complexity, the Poisson-lognormal model appears to be the best fit for the data. Next we consider how, exactly, does one interprets the parameters of the Poisson-lognormal model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-does-one-interpret-the-poisson-lognormal-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How does one interpret the Poisson-lognormal model?&lt;/h3&gt;
&lt;p&gt;A nice quality of both the conventional Poisson model and the gamma-Poisson model is the intercept for each corresponds directly with the mean of the original data, after exponentiation. The mean of the &lt;code&gt;SALAMAN&lt;/code&gt; variable, recall, was 2.5. Here are the summaries for their exponentiated intercepts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# conventional Poisson
fixef(fit1)[, -2] %&amp;gt;% exp() %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
##     2.48     2.06     2.94&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# gamma-Poisson
fixef(fit2)[, -2] %&amp;gt;% exp() %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
##     2.58     1.72     3.89&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both are really close to the sample mean. Here‚Äôs the exponentiated intercept for the Poisson-lognormal model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit3)[, -2] %&amp;gt;% exp() %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
##     1.40     0.89     2.12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow, that‚Äôs not even close! What gives? Well, keep in mind that with the OLRE Poisson-lognormal model, the intercept is the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; parameter for the lognormal distribution of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameters. In a similar way, the level-2 standard deviation (execute &lt;code&gt;posterior_summary(fit3)[&#34;sd_SITE__Intercept&#34;, ]&lt;/code&gt;) is the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameter for that lognormal distribution. Keeping things simple, for the moment, here‚Äôs what that lognormal distribution looks like if we take the posterior means for those parameters and insert them into the parameter arguments of the &lt;code&gt;dlnorm()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;-
  tibble(lambda = seq(from = 0, to = 13, length.out = 500)) %&amp;gt;% 
  mutate(d = dlnorm(lambda, 
                    meanlog = posterior_summary(fit3)[1, 1], 
                    sdlog = posterior_summary(fit3)[2, 1])) %&amp;gt;% 
  
  ggplot(aes(x = lambda, y = d)) +
  geom_area(fill = &amp;quot;grey50&amp;quot;) +
  scale_x_continuous(expression(lambda), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(&amp;quot;density&amp;quot;, breaks = NULL, 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 12),
                  ylim = c(0, 0.8)) +
  labs(title = &amp;quot;Population lognormal distribution&amp;quot;,
       subtitle = &amp;quot;The parameters are summarized by their posterior means.&amp;quot;)

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using just the posterior means for the parameters ignores the uncertainty in the distribution. To bring that into the plot, we‚Äôll want to work with the posterior samples, themselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many posterior ddraws would you like?
n_draw &amp;lt;- 100

set.seed(1)

p2 &amp;lt;-
  posterior_samples(fit3) %&amp;gt;% 
  slice_sample(n = n_draw) %&amp;gt;% 
  transmute(iter  = 1:n(),
            mu    = b_Intercept,
            sigma = sd_SITE__Intercept) %&amp;gt;% 
  expand(nesting(iter, mu, sigma),
         lambda = seq(from = 0, to = 13, length.out = 500)) %&amp;gt;% 
  mutate(d = dlnorm(lambda, meanlog = mu, sdlog = sigma)) %&amp;gt;% 
  
  ggplot(aes(x = lambda, y = d, group = iter)) +
  geom_line(size = 1/6, alpha = 1/2) +
  scale_x_continuous(expression(lambda), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(&amp;quot;density&amp;quot;, breaks = NULL, 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 12),
                  ylim = c(0, 0.8)) +
  labs(title = &amp;quot;Population lognormal distribution&amp;quot;,
       subtitle = &amp;quot;The parameters are summarized by 100 posterior draws.&amp;quot;)

p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These, recall, are 100 credible lognormal distributions for the case-level &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameters, not for the data themselves. We‚Äôll get to the data in a moment. Since we‚Äôre working with a multilevel model, we have posteriors for each of the case-level &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameters, too. Here they are in a dot plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;-
  coef(fit3)$SITE[, &amp;quot;Estimate&amp;quot;, &amp;quot;Intercept&amp;quot;] %&amp;gt;% 
  exp() %&amp;gt;% 
  data.frame() %&amp;gt;% 
  set_names(&amp;quot;lambda_i&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = lambda_i)) +
  geom_dots(fill = &amp;quot;grey50&amp;quot;, color = &amp;quot;grey50&amp;quot;) +
  scale_x_continuous(expression(lambda[italic(i)]), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(&amp;quot;normalized density&amp;quot;, breaks = NULL, 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 12)) +
  labs(title = expression(&amp;quot;Dotplot of individual &amp;quot;*lambda[italic(i)]*&amp;quot; parameters&amp;quot;),
       subtitle = &amp;quot;The parameters are summarized by their posterior means.&amp;quot;)

p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To reduce visual complexity, we just plotted the &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameters by their posterior means. But that might be frustrating the way it ignores uncertainty. A different way to look at them might be a rank-ordered coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p4 &amp;lt;-
  coef(fit3)$SITE[, -2, &amp;quot;Intercept&amp;quot;] %&amp;gt;% 
  exp() %&amp;gt;% 
  data.frame() %&amp;gt;% 
  arrange(Estimate) %&amp;gt;% 
  mutate(rank = 1:n()) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +
  geom_pointrange(fatten = 1, size = 1/2) +
  scale_x_continuous(expression(lambda[italic(i)]), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(breaks = NULL, expand = c(0.02, 0.02)) +
  coord_cartesian(xlim = c(0, 12)) +
  labs(title = expression(&amp;quot;Ranked coefficient plot of individual &amp;quot;*lambda[italic(i)]*&amp;quot; parameters&amp;quot;),
       subtitle = &amp;quot;The parameters are summarized by their posterior means and 95% CIs.&amp;quot;)

p4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since each &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameter is based in the data from a single case, it‚Äôs no surprise that their 95% intervals are all on the wide side. Just for kicks, here are the last four subplots all shown together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 + p2 + p3 + p4 &amp;amp; 
  theme_classic(base_size = 8.25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this point, though, you may be wondering how this model, with all its lognormal &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; glory, can inform us about actual counts. You know, the kind of counts that allowed us to fit such a wacky model. We‚Äôll want to work with the posterior draws for that, too. First we extract all of the posterior draws for the population parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;-
  posterior_samples(fit3) %&amp;gt;% 
  transmute(mu    = b_Intercept,
            sigma = sd_SITE__Intercept)

# what is this?
glimpse(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 2
## $ mu    &amp;lt;dbl&amp;gt; 0.16424013, 0.07509823, 0.26418683, 0.22422613, 0.36427113, -0.1‚Ä¶
## $ sigma &amp;lt;dbl&amp;gt; 1.4128424, 1.4236003, 1.2986505, 1.4523181, 1.2731463, 1.5761309‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next code block is a little chunky, so I‚Äôll try to explain what we‚Äôre doing before we dive in. Our goal is to use the posterior draws to make a posterior predictive check, by hand. My reasoning is doing this kind of check by hand, rather than relying on &lt;code&gt;pp_check()&lt;/code&gt;, requires you to understand the guts of the model. In our check, we are going to compare the histogram of the original &lt;code&gt;SALAMAN&lt;/code&gt; counts with the histograms of a few data sets simulated from the model. So first, we need to decide how many simulations we want. Since I want a faceted plot of 12 histograms, that means we‚Äôll need 11 simulations. We set that number with the opening &lt;code&gt;n_facet &amp;lt;- 12&lt;/code&gt; line. Next, we set our seed for reproducibility and took 11 random draws from the &lt;code&gt;post&lt;/code&gt; data frame. In the first &lt;code&gt;mutate()&lt;/code&gt; line, we added an iteration index. Then with the &lt;code&gt;purrr::map2()&lt;/code&gt; function, we drew 47 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values (47 was the original &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; in the &lt;code&gt;salamanders&lt;/code&gt; data) based on the lognormal distribution defined by the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; values from each iteration. After &lt;code&gt;unnest()&lt;/code&gt;-ing those results, we used &lt;code&gt;rpois()&lt;/code&gt; within the next &lt;code&gt;mutate()&lt;/code&gt; line to use those simulated &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values to simulate actual counts. The remaining lines clean up the data format a bit and tack on the original &lt;code&gt;salamanders&lt;/code&gt; data. Then we plot.&lt;/p&gt;
&lt;p&gt;Okay, here it is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many facets would you like?
n_facet &amp;lt;- 12

set.seed(1)

post %&amp;gt;% 
  # take 11 samples from the posterior iterations
  slice_sample(n = n_facet - 1) %&amp;gt;% 
  # take 47 random draws from each iteration
  mutate(iter   = 1:n(),
         lambda = map2(mu, sigma, ~ rlnorm(n = 47, meanlog = mu, sdlog = sigma))) %&amp;gt;% 
  unnest(lambda) %&amp;gt;% 
  # use the lambdas to generate the counts
  mutate(count = rpois(n(), lambda = lambda)) %&amp;gt;% 
  transmute(sample = str_c(&amp;quot;sample #&amp;quot;, iter),
            SALAMAN = count) %&amp;gt;% 
  # combine the original data
  bind_rows(
    salamanders %&amp;gt;% 
      select(SALAMAN) %&amp;gt;% 
      mutate(sample = &amp;quot;original data&amp;quot;)
  ) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = SALAMAN, fill = sample == &amp;quot;original data&amp;quot;)) +
  geom_bar() +
  scale_fill_viridis_d(option = &amp;quot;A&amp;quot;, begin = .15, end = .55, breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 30)) +
  labs(title = &amp;quot;Handmade posterior predictive check&amp;quot;) +
  facet_wrap(~sample) +
  theme(strip.background = element_rect(size = 0, fill = &amp;quot;grey92&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This, friends, is how you can use our intercepts-only Poisson-lognormal mixture model to simulate count data resembling the original count data. Data simulation is cool, but you might wonder how to compute the mean of the model-implied lognormal distribution. Recall that we can‚Äôt just exponentiate the model‚Äôs intercept. As it turns out, &lt;span class=&#34;math inline&#34;&gt;\(\exp \mu\)&lt;/span&gt; returns the &lt;strong&gt;median&lt;/strong&gt; for the lognormal distribution. The formula for the mean of the lognormal distribution is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{mean} = \exp \left ( \mu + \frac{\sigma^2}{2}\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So here‚Äôs how to work with the posterior draws to compute that value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  mutate(mean = exp(mu + sigma^2 / 2)) %&amp;gt;% 
  
  ggplot(aes(x = mean, y = 0)) +
  stat_halfeye(.width = c(.5, .95)) +
  geom_vline(xintercept = mean(salamanders$SALAMAN), 
             color = &amp;quot;purple4&amp;quot;, linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 10)) +
  xlab(expression(&amp;quot;mean of the lognormal &amp;quot;*lambda*&amp;quot; distribution&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For reference, we superimposed the mean of the &lt;code&gt;SALAMAN&lt;/code&gt; data with a dashed line.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrap-up&lt;/h2&gt;
&lt;p&gt;Okay, this is about as far as I‚Äôd like to go with this one. To be honest, the Poisson-lognormal mixture is a weird model and I‚Äôm not sure if it‚Äôs a good fit for the kind of data I tend to work with. But exposure to new options seems valuable and I‚Äôm content to low-key chew on this one for a while.&lt;/p&gt;
&lt;p&gt;If you‚Äôd like to learn more, do check out Bulmer‚Äôs original &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bulmer1974OnFitting&#34; role=&#34;doc-biblioref&#34;&gt;1974&lt;/a&gt;)&lt;/span&gt; paper and the more recent OLRE paper by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-harrison2014using&#34; role=&#34;doc-biblioref&#34;&gt;Harrison&lt;/a&gt; (&lt;a href=&#34;#ref-harrison2014using&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;. The great &lt;a href=&#34;https://twitter.com/bolkerb&#34;&gt;Ben Bolker&lt;/a&gt; wrote up a vignette (&lt;a href=&#34;https://glmm.wdfiles.com/local--files/examples/overdispersion.pdf&#34;&gt;here&lt;/a&gt;) on how to fit the OLRE Poisson-lognormal with &lt;strong&gt;lme4&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-lme4&#34; role=&#34;doc-biblioref&#34;&gt;Bates et al., 2015&lt;/a&gt;)&lt;/span&gt; and Michael Clark wrote up a very quick example of the model with &lt;strong&gt;brms&lt;/strong&gt; &lt;a href=&#34;https://m-clark.github.io/easy-bayes/posterior-predictive-checks.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy modeling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] patchwork_1.1.1 tidybayes_2.3.1 forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.6     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
##  [9] tibble_3.1.2    ggplot2_3.3.3   tidyverse_1.3.0 brms_2.15.0    
## [13] Rcpp_1.0.6     
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.3         splines_4.0.4       
##   [7] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0  
##  [19] xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [22] colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.23            callr_3.7.0         
##  [28] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2          
##  [34] gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2        
##  [40] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [43] DBI_1.1.0            miniUI_0.1.1.1       viridisLite_0.4.0   
##  [46] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.3    httr_1.4.2          
##  [52] threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.2      
##  [55] farver_2.1.0         pkgconfig_2.0.3      loo_2.4.1           
##  [58] sass_0.3.1           dbplyr_2.0.0         utf8_1.2.1          
##  [61] labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.11        
##  [64] reshape2_1.4.4       later_1.2.0          munsell_0.5.0       
##  [67] cellranger_1.1.0     tools_4.0.4          cli_2.5.0           
##  [70] generics_0.1.0       broom_0.7.6          ggridges_0.5.3      
##  [73] evaluate_0.14        fastmap_1.1.0        yaml_2.2.1          
##  [76] processx_3.5.2       knitr_1.33           fs_1.5.0            
##  [79] nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [82] xml2_1.3.2           rstudioapi_0.13      compiler_4.0.4      
##  [85] bayesplot_1.8.0      shinythemes_1.1.2    curl_4.3            
##  [88] gamm4_0.2-6          reprex_0.3.0         statmod_1.4.35      
##  [91] bslib_0.2.4          stringi_1.6.2        highr_0.9           
##  [94] ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6   
##  [97] lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
## [100] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.8         
## [103] pillar_1.6.1         lifecycle_1.0.0      jquerylib_0.1.4     
## [106] bridgesampling_1.0-0 estimability_1.3     httpuv_1.6.0        
## [109] R6_2.5.0             bookdown_0.22        promises_1.2.0.1    
## [112] gridExtra_2.3        codetools_0.2-18     boot_1.3-26         
## [115] colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [118] assertthat_0.2.1     withr_2.4.2          shinystan_2.5.0     
## [121] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [124] hms_0.5.3            grid_4.0.4           coda_0.19-4         
## [127] minqa_1.2.4          rmarkdown_2.8        shiny_1.6.0         
## [130] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-R-lme4&#34; class=&#34;csl-entry&#34;&gt;
Bates, D., M√§chler, M., Bolker, B., &amp;amp; Walker, S. (2015). Fitting linear mixed-effects models using &lt;span class=&#34;nocase&#34;&gt;lme4&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;67&lt;/em&gt;(1), 1‚Äì48. &lt;a href=&#34;https://doi.org/10.18637/jss.v067.i01&#34;&gt;https://doi.org/10.18637/jss.v067.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bulmer1974OnFitting&#34; class=&#34;csl-entry&#34;&gt;
Bulmer, M. (1974). On fitting the &lt;span&gt;Poisson&lt;/span&gt; lognormal distribution to species-abundance data. &lt;em&gt;Biometrics&lt;/em&gt;, &lt;em&gt;30&lt;/em&gt;(1), 101‚Äì110. &lt;a href=&#34;https://doi.org/10.2307/2529621&#34;&gt;https://doi.org/10.2307/2529621&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-B√ºrkner2021Distributional&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2021a). &lt;em&gt;Estimating distributional models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-B√ºrkner2021Parameterization&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2021b). &lt;em&gt;Parameterization of response distributions in brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1‚Äì28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395‚Äì411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ‚Äô&lt;span&gt;Stan&lt;/span&gt;‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cameron2013regression&#34; class=&#34;csl-entry&#34;&gt;
Cameron, A. C., &amp;amp; Trivedi, P. K. (2013). &lt;em&gt;Regression analysis of count data&lt;/em&gt; (Second Edition). &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9781139013567&#34;&gt;https://doi.org/10.1017/CBO9781139013567&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanDataAnalysisUsing2006&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., &amp;amp; Hill, J. (2006). &lt;em&gt;Data analysis using regression and multilevel/hierarchical models&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9780511790942&#34;&gt;https://doi.org/10.1017/CBO9780511790942&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-harrison2014using&#34; class=&#34;csl-entry&#34;&gt;
Harrison, X. A. (2014). Using observation-level random effects to model overdispersion in count data in ecology and evolution. &lt;em&gt;PeerJ&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;, e616. &lt;a href=&#34;https://doi.org/10.7717/peerj.616&#34;&gt;https://doi.org/10.7717/peerj.616&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hilbeNegativeBinomialRegression2011&#34; class=&#34;csl-entry&#34;&gt;
Hilbe, J. M. (2011). &lt;em&gt;Negative binomial regression&lt;/em&gt; (Second Edition). &lt;a href=&#34;https://doi.org/10.1017/CBO9780511973420&#34;&gt;https://doi.org/10.1017/CBO9780511973420&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ‚Äôgeoms‚Äô for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020a). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rethinking&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;rethinking&lt;/span&gt; &lt;span&gt;R&lt;/span&gt; package&lt;/em&gt;. &lt;a href=&#34;https://xcelab.net/rm/software/&#34;&gt;https://xcelab.net/rm/software/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-patchwork&#34; class=&#34;csl-entry&#34;&gt;
Pedersen, T. L. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;patchwork&lt;/span&gt;: &lt;span&gt;The&lt;/span&gt; composer of plots&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;https://CRAN.R-project.org/package=patchwork&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-standevelopmentteamStanFunctionsReference2021&#34; class=&#34;csl-entry&#34;&gt;
Stan Development Team. (2021). &lt;em&gt;Stan functions reference&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/docs/2_26/functions-reference/index.html&#34;&gt;https://mc-stan.org/docs/2_26/functions-reference/index.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-welsh1995habitat&#34; class=&#34;csl-entry&#34;&gt;
Welsh Jr, H. H., &amp;amp; Lind, A. J. (1995). Habitat correlates of the &lt;span&gt;Del Norte&lt;/span&gt; salamander, &lt;span&gt;Plethodon&lt;/span&gt; elongatus (&lt;span&gt;Caudata&lt;/span&gt;: &lt;span&gt;Plethodontidae&lt;/span&gt;), in northwestern &lt;span&gt;California&lt;/span&gt;. &lt;em&gt;Journal of Herpetology&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(2), 198‚Äì210. &lt;a href=&#34;https://doi.org/10.2307/1564557&#34;&gt;https://doi.org/10.2307/1564557&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ‚Äôtidyverse‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., Fran√ßois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., M√ºller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., ‚Ä¶ Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;That‚Äôs a lie. There was no pop quiz, last week.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I‚Äôm making this number up, too, but it‚Äôs probably not far off. ‚òï ‚òï ‚òï&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;One could also, of course, express that model as &lt;span class=&#34;math inline&#34;&gt;\(y_i = \beta_0 + \beta_1 x_i + \epsilon_i\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i \sim \operatorname{Normal}(0, \sigma)\)&lt;/span&gt;. But come on. That‚Äôs weak sauce. For more on why, see page 84 in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;I say ‚Äúsuggests‚Äù because a simple Poisson model can be good enough IF you have a set of high-quality predictors which can ‚Äúexplain‚Äù all that extra-looking variability. We, however, will be fitting intercept-only models.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian power analysis: Part III.a. Counts are special.</title>
      <link>/post/bayesian-power-analysis-part-iii-a/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/bayesian-power-analysis-part-iii-a/</guid>
      <description>
&lt;script src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;version-1.1.0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Version 1.1.0&lt;/h2&gt;
&lt;p&gt;Edited on April 21, 2021, to remove the &lt;code&gt;broom::tidy()&lt;/code&gt; portion of the workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;orientation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Orientation&lt;/h2&gt;
&lt;p&gt;So far we‚Äôve covered Bayesian power simulations from both a null hypothesis orientation (see &lt;a href=&#34;https://solomonkurz.netlify.com/post/bayesian-power-analysis-part-i/&#34;&gt;part I&lt;/a&gt;) and a parameter width perspective (see &lt;a href=&#34;https://solomonkurz.netlify.com/post/bayesian-power-analysis-part-ii/&#34;&gt;part II&lt;/a&gt;). In both instances, we kept things simple and stayed with Gaussian (i.e., normally distributed) data. But not all data follow that form, so it might do us well to expand our skill set a bit. In the next few posts, we‚Äôll cover how we might perform power simulations with other kinds of data. In this post, we‚Äôll focus on how to use the Poisson likelihood to model counts. In follow-up posts, we‚Äôll explore how to model binary and Likert-type data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-poisson-distribution-is-handy-for-counts.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Poisson distribution is handy for counts.&lt;/h2&gt;
&lt;p&gt;In the social sciences, count data arise when we ask questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How many sexual partners have you had?&lt;/li&gt;
&lt;li&gt;How many pets do you have at home?&lt;/li&gt;
&lt;li&gt;How many cigarettes did you smoke, yesterday?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The values these data will take are discrete&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; in that you‚Äôve either slept with 9 or 10 people, but definitely not 9.5. The values cannot go below zero in that even if you quit smoking cold turkey 15 years ago and have been a health nut since, you still could not have smoked -3 cigarettes, yesterday. Zero is as low as it goes.&lt;/p&gt;
&lt;p&gt;The canonical distribution for data of this type‚Äìnon-negative integers‚Äìis the Poisson. It‚Äôs named after the French mathematician Sim√©on Denis Poisson, &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e8/E._Marcellot_Sim√©on-Denis_Poisson_1804.jpg&#34;&gt;who had quite the confident stare in his youth&lt;/a&gt;. The Poisson distribution has one parameter, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, which controls both its mean and variance. Although the numbers the Poisson describes are counts, the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter does not need to be an integer. For example, here‚Äôs the plot of 1,000 draws from a Poisson for which &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 3.2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

theme_set(theme_gray() + theme(panel.grid = element_blank()))

tibble(x = rpois(n = 1e3, lambda = 3.2)) %&amp;gt;% 
  mutate(x = factor(x)) %&amp;gt;% 
  
  ggplot(aes(x = x)) +
  geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In case you missed it, the key function for generating those data was &lt;code&gt;rpois()&lt;/code&gt; (see &lt;code&gt;?rpois&lt;/code&gt;). I‚Äôm not going to go into a full-blown tutorial on the Poisson distribution or on count regression. For more thorough introductions, check out Atkins et al‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-atkinsTutorialOnCount2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3513584/pdf/nihms396181.pdf&#34;&gt;&lt;em&gt;A tutorial on count regression and zero-altered count models for longitudinal substance use data&lt;/em&gt;&lt;/a&gt;, chapters 9 through 11 in McElreath‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt;&lt;/a&gt;, or, if you really want to dive in, Agresti‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-agrestiFoundationsLinearGeneralized2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&#34;&gt;&lt;em&gt;Foundations of linear and generalized linear models&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For our power example, let‚Äôs say you were interested in drinking. Using data from &lt;a href=&#34;https://pubs.niaaa.nih.gov/publications/AA70/AA70.htm&#34;&gt;the National Epidemiologic Survey on Alcohol and Related Conditions&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-niaaaNationalEpidemiologicSurvey2006&#34; role=&#34;doc-biblioref&#34;&gt;{{National Institute on Alcohol Abuse and Alcoholism}}, 2006&lt;/a&gt;)&lt;/span&gt;, Christopher Ingraham &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ingrahamThinkYouDrink2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; presented &lt;a href=&#34;https://www.washingtonpost.com/news/wonk/wp/2014/09/25/think-you-drink-a-lot-this-chart-will-tell-you/?utm_term=.b81599bbbe25&#34;&gt;a data visualization&lt;/a&gt; of the average number of alcoholic drinks American adults consume, per week. By decile, the numbers were:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;0.00&lt;/li&gt;
&lt;li&gt;0.00&lt;/li&gt;
&lt;li&gt;0.00&lt;/li&gt;
&lt;li&gt;0.02&lt;/li&gt;
&lt;li&gt;0.14&lt;/li&gt;
&lt;li&gt;0.63&lt;/li&gt;
&lt;li&gt;2.17&lt;/li&gt;
&lt;li&gt;6.25&lt;/li&gt;
&lt;li&gt;15.28&lt;/li&gt;
&lt;li&gt;73.85&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let‚Äôs say you wanted to run a study where you planned on comparing two demographic groups by their weekly drinking levels. Let‚Äôs further say you suspected one of those groups drank like the American adults in the 7&lt;sup&gt;th&lt;/sup&gt; decile and the other drank like American adults in the 8&lt;sup&gt;th&lt;/sup&gt;. We‚Äôll call them low and high drinkers, respectively. For convenience, let‚Äôs further presume you‚Äôll be able to recruit equal numbers of participants from both groups. The objective for our power analysis‚Äìor sample size analysis if you prefer to avoid the language of &lt;em&gt;power&lt;/em&gt;‚Äìis to determine how many you‚Äôd need per group to detect reliable differences. Using &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt; as a starting point, here‚Äôs what the data for our hypothetical groups might look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu_7 &amp;lt;- 2.17
mu_8 &amp;lt;- 6.25

n &amp;lt;- 50

set.seed(3)

d &amp;lt;-
  tibble(low  = rpois(n = n, lambda = mu_7),
         high = rpois(n = n, lambda = mu_8)) %&amp;gt;% 
  gather(group, count) 

d %&amp;gt;%
  mutate(count = factor(count)) %&amp;gt;% 
  
  ggplot(aes(x = count)) +
  geom_bar() +
  facet_wrap(~group, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This will be our primary data type. Our next step is to determine how to express our research question as a regression model. Like with our two-group Gaussian models, we can predict counts in terms of an intercept (i.e., standing for the expected value on the reference group) and slope (i.e., standing for the expected difference between the reference group and the comparison group). If we coded our two groups by a &lt;code&gt;high&lt;/code&gt; variable for which 0 stood for low drinkers and 1 stood for high drinkers, the basic model would follow the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{drinks_per_week}_i         &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i)   &amp;amp; = \beta_0 + \beta_1 \text{high}_i.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here‚Äôs how to set the data up for that model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;% 
  mutate(high = ifelse(group == &amp;quot;low&amp;quot;, 0, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you were attending closely to our model formula, you noticed we ran into a detail. Count regression, such as with the Poisson likelihood, tends to use the log link. &lt;em&gt;Why?&lt;/em&gt; you ask. Recall that counts need to be 0 and above. Same deal for our &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter. In order to make sure our models don‚Äôt yield silly estimates for &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, like -2 or something, we typically use the log link. You don‚Äôt have to, of course. The world is your playground. But this is the method most of your colleagues are likely to use and it‚Äôs the one I suggest you use until you have compelling reasons to do otherwise.&lt;/p&gt;
&lt;p&gt;So then since we‚Äôre now fitting a model with a log link, it might seem challenging to pick good priors. As a place to start, we can use the &lt;code&gt;brms::get_prior()&lt;/code&gt; function to see the &lt;strong&gt;brms&lt;/strong&gt; defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)

get_prior(data = d,
          family = poisson,
          count ~ 0 + Intercept + high)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   prior class      coef group resp dpar nlpar bound       source
##  (flat)     b                                            default
##  (flat)     b      high                             (vectorized)
##  (flat)     b Intercept                             (vectorized)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hopefully two things popped out. First, there‚Äôs no prior of &lt;code&gt;class = sigma&lt;/code&gt;. Since the Poisson distribution only has one parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, we don‚Äôt need to set a prior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Our model won‚Äôt have one. Second, because we‚Äôre continuing to use the &lt;code&gt;0 + Intercept&lt;/code&gt; syntax for our model intercept, both our intercept and slope are of prior &lt;code&gt;class = b&lt;/code&gt; and those currently have default flat priors with &lt;strong&gt;brms&lt;/strong&gt;. To be sure, flat priors aren‚Äôt the best. But maybe if this was your first time playing around with a Poisson model, default flat priors might seem like a safe place to start. &lt;a href=&#34;https://xkcd.com/386/&#34;&gt;Feel free to disagree&lt;/a&gt;. In the meantime, here‚Äôs how to fit that default Poisson model with &lt;code&gt;brms::brm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d,
      family = poisson,
      count ~ 0 + Intercept + high,
      seed = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: count ~ 0 + Intercept + high 
##    Data: d (Number of observations: 100) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.59      0.11     0.38     0.79 1.01      917     1133
## high          1.27      0.12     1.03     1.51 1.01      935     1182
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we used the log link, our model results are in the log metric, too. If you‚Äôd like them in the metric of the data, you‚Äôd work directly with the poster samples and exponentiate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- 
  posterior_samples(fit1) %&amp;gt;% 
  mutate(`beta_0 (i.e., low)`                       = exp(b_Intercept),
         `beta_1 (i.e., difference score for high)` = exp(b_high))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then just summarize our parameters of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  select(starts_with(&amp;quot;beta_&amp;quot;)) %&amp;gt;% 
  gather() %&amp;gt;% 
  group_by(key) %&amp;gt;% 
  summarise(mean  = mean(value),
            lower = quantile(value, prob = .025),
            upper = quantile(value, prob = .975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
##   key                                       mean lower upper
##   &amp;lt;chr&amp;gt;                                    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 beta_0 (i.e., low)                        1.81  1.46  2.21
## 2 beta_1 (i.e., difference score for high)  3.58  2.81  4.53&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of simulation, it‚Äôll be easier if we press on with evaluating the parameters on the log metric, though. If you‚Äôre working within a null-hypothesis oriented power paradigm, you‚Äôll be happy to know zero is still the number to beat for evaluating our 95% intervals for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, even when that parameter is in the log metric. Here it is, again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;high&amp;quot;, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
## 1.2690437 0.1211455 1.0330613 1.5108894&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So our first fit suggests we‚Äôre on good footing to run a quick power simulation holding &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt;. As in the prior blog posts, our lives will be simpler if we set up a custom simulation function. Since we‚Äôll be using it to simulate the data and fit the model in one step, let‚Äôs call it &lt;code&gt;sim_data_fit()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_data_fit &amp;lt;- function(seed, n) {
  
  # define our mus in the function
  mu_7 &amp;lt;- 2.17
  mu_8 &amp;lt;- 6.25

  # make your results reproducible
  set.seed(seed)
  
  # simulate the data
  d &amp;lt;-
    tibble(high  = rep(0:1, each = n),
           count = c(rpois(n = n, lambda = mu_7),
                     rpois(n = n, lambda = mu_8)))
  
  # fit and summarize
  update(fit1,
         newdata = d,
         seed = seed) %&amp;gt;% 
    fixef() %&amp;gt;% 
    data.frame() %&amp;gt;% 
    rownames_to_column(&amp;quot;parameter&amp;quot;) %&amp;gt;% 
    filter(parameter == &amp;quot;high&amp;quot;) %&amp;gt;% 
    select(Q2.5:Q97.5 )
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here‚Äôs the simulation for a simple 100 iterations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n = 50)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That went quick‚Äìjust a little over a minute on my laptop. Here‚Äôs what those 100 &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; intervals look like in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 %&amp;gt;% 
  ggplot(aes(x = seed, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_linerange() +
  labs(x = &amp;quot;seed (i.e., simulation index)&amp;quot;,
       y = expression(beta[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;None of them are anywhere near the null value 0. So it appears we‚Äôre well above .8 power to reject the typical &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt;. Switching to the precision orientation, here‚Äôs the distribution of their widths.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 %&amp;gt;% 
  mutate(width = Q97.5 - Q2.5) %&amp;gt;% 
  
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = 0.01) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What if we wanted a mean width of 0.25 on the log scale? We might try the simulation with &lt;span class=&#34;math inline&#34;&gt;\(n = 150\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim2 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n = 150)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we‚Äôll summarize the widths both in terms of their mean and what proportion were smaller than 0.25.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim2 %&amp;gt;% 
  mutate(width = Q97.5 - Q2.5) %&amp;gt;% 
  summarise(`mean width` = mean(width),
            `below 0.25` = mean(width &amp;lt; 0.25))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   `mean width` `below 0.25`
##          &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1        0.252         0.43&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to focus on the mean, we did pretty good. Perhaps set the &lt;span class=&#34;math inline&#34;&gt;\(n = 155\)&lt;/span&gt; and simulate a full 1,000+ iterations for a serious power analysis. But if we wanted to make the stricter criteria of all below 0.25, we‚Äôd need to up the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; quite a bit more. And of course, once you have a little experience working with Poisson models, you might do the power simulations with more ambitious priors. For example, if your count values are lower than like 1,000, there‚Äôs a good chance a &lt;code&gt;normal(0, 6)&lt;/code&gt; prior on your &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters will be nearly flat within the reasonable neighborhoods of the parameter space.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-logs-are-hard.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;But logs are hard.&lt;/h2&gt;
&lt;p&gt;If we approach our Bayesian power analysis from a precision perspective, it can be difficult to settle on a reasonable interval width when they‚Äôre on the log scale. So let‚Äôs modify our simulation flow so it converts the width summaries back into the natural metric. Before we go big, let‚Äôs practice with a single iteration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seed &amp;lt;- 0
set.seed(seed)

# simulate the data
d &amp;lt;-
  tibble(high  = rep(0:1, each = n),
         count = c(rpois(n = n, lambda = mu_7),
                   rpois(n = n, lambda = mu_8)))

# fit the model
fit2 &amp;lt;-
  update(fit1,
         newdata = d,
         seed = seed) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now summarize.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

fit2 %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  transmute(`beta_1` = exp(b_high)) %&amp;gt;% 
  mean_qi()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     beta_1  .lower   .upper .width .point .interval
## 1 2.705404 2.16512 3.341729   0.95   mean        qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we used the &lt;code&gt;fixef()&lt;/code&gt; function to extract our intervals, which took the &lt;strong&gt;brms&lt;/strong&gt; fit object as input. Here we took a different approach. Because we are transforming &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, we used the &lt;code&gt;posterior_samples()&lt;/code&gt; function to work directly with the posterior draws. We then exponentiated within &lt;code&gt;transmute()&lt;/code&gt;, which returned a single-column tibble, not a &lt;strong&gt;brms&lt;/strong&gt; fit object. So instead of &lt;code&gt;fixef()&lt;/code&gt;, it‚Äôs easier to get our summary statistics with the &lt;code&gt;tidybayes::mean_qi()&lt;/code&gt; function. Do note that now our lower and upper levels are named &lt;code&gt;.lower&lt;/code&gt; and &lt;code&gt;.upper&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Now we‚Äôve practiced with the new flow, let‚Äôs redefine our simulation function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_data_fit &amp;lt;- function(seed, n) {
  
  # define our mus in the function
  mu_7 &amp;lt;- 2.17
  mu_8 &amp;lt;- 6.25

  # make your results reproducible
  set.seed(seed)
  
  # simulate the data
  d &amp;lt;-
    tibble(high  = rep(0:1, each = n),
           count = c(rpois(n = n, lambda = mu_7),
                     rpois(n = n, lambda = mu_8)))
  
  # fit and summarize
  update(fit1,
         newdata = d,
         seed = seed) %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  transmute(`beta_1` = exp(b_high)) %&amp;gt;% 
  mean_qi()
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim3 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n = 50)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here‚Äôs what those 100 &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; intervals look like in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim3 %&amp;gt;% 
  ggplot(aes(x = seed, y = beta_1, ymin = .lower, ymax = .upper)) +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_pointrange(fatten = 1) +
  labs(x = &amp;quot;seed (i.e., simulation index)&amp;quot;,
       y = expression(beta[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Inspect the distribution of their widths.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim3 %&amp;gt;% 
  mutate(width = .upper - .lower) %&amp;gt;% 
  
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = 0.05) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What if we wanted a mean 95% interval width of 1? Let‚Äôs run the simulation again, this time with &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim4 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n = 100)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  mutate(width = .upper - .lower)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here‚Äôs the new width distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim4 %&amp;gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = 0.05) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And the mean width is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim4 %&amp;gt;% 
  summarise(mean_width = mean(width))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   mean_width
##        &amp;lt;dbl&amp;gt;
## 1      0.913&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice! If we want a mean width of 1, it looks like we‚Äôre a little &lt;em&gt;overpowered&lt;/em&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt;. The next step would be to up your iterations to 1,000 or so to do a proper simulation.&lt;/p&gt;
&lt;p&gt;Now you‚Äôve got a sense of how to work with the Poisson likelihood, &lt;a href=&#34;https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-iii-b/&#34;&gt;next time&lt;/a&gt; we‚Äôll play with binary data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1  
##  [5] stringr_1.4.0   dplyr_1.0.6     purrr_0.3.4     readr_1.4.0    
##  [9] tidyr_1.1.3     tibble_3.1.2    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.3         splines_4.0.4       
##   [7] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0  
##  [19] xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [22] colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.23            callr_3.7.0         
##  [28] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2          
##  [34] gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2        
##  [40] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [43] DBI_1.1.0            miniUI_0.1.1.1       xtable_1.8-4        
##  [46] stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [49] htmlwidgets_1.5.3    httr_1.4.2           threejs_0.3.3       
##  [52] arrayhelpers_1.1-0   ellipsis_0.3.2       pkgconfig_2.0.3     
##  [55] loo_2.4.1            farver_2.1.0         sass_0.3.1          
##  [58] dbplyr_2.0.0         utf8_1.2.1           tidyselect_1.1.1    
##  [61] labeling_0.4.2       rlang_0.4.11         reshape2_1.4.4      
##  [64] later_1.2.0          munsell_0.5.0        cellranger_1.1.0    
##  [67] tools_4.0.4          cli_2.5.0            generics_0.1.0      
##  [70] broom_0.7.6          ggridges_0.5.3       evaluate_0.14       
##  [73] fastmap_1.1.0        yaml_2.2.1           processx_3.5.2      
##  [76] knitr_1.33           fs_1.5.0             nlme_3.1-152        
##  [79] mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [82] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [85] rstudioapi_0.13      gamm4_0.2-6          curl_4.3            
##  [88] reprex_0.3.0         statmod_1.4.35       bslib_0.2.4         
##  [91] stringi_1.6.2        highr_0.9            ps_1.6.0            
##  [94] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41     
##  [97] Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1        
## [100] shinyjs_2.0.0        vctrs_0.3.8          pillar_1.6.1        
## [103] lifecycle_1.0.0      jquerylib_0.1.4      bridgesampling_1.0-0
## [106] estimability_1.3     httpuv_1.6.0         R6_2.5.0            
## [109] bookdown_0.22        promises_1.2.0.1     gridExtra_2.3       
## [112] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0  
## [115] MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1    
## [118] withr_2.4.2          shinystan_2.5.0      multcomp_1.4-16     
## [121] mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [124] grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [127] rmarkdown_2.8        shiny_1.6.0          lubridate_1.7.9.2   
## [130] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-agrestiFoundationsLinearGeneralized2015&#34; class=&#34;csl-entry&#34;&gt;
Agresti, A. (2015). &lt;em&gt;Foundations of linear and generalized linear models&lt;/em&gt;. &lt;span&gt;John Wiley &amp;amp; Sons&lt;/span&gt;. &lt;a href=&#34;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&#34;&gt;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-atkinsTutorialOnCount2013&#34; class=&#34;csl-entry&#34;&gt;
Atkins, D. C., Baldwin, S. A., Zheng, C., Gallop, R. J., &amp;amp; Neighbors, C. (2013). A tutorial on count regression and zero-altered count models for longitudinal substance use data. &lt;em&gt;Psychology of Addictive Behaviors&lt;/em&gt;, &lt;em&gt;27&lt;/em&gt;(1), 166. &lt;a href=&#34;https://doi.org/10.1037/a0029508&#34;&gt;https://doi.org/10.1037/a0029508&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ingrahamThinkYouDrink2014&#34; class=&#34;csl-entry&#34;&gt;
Ingraham, C. (2014). Think you drink a lot? &lt;span&gt;This&lt;/span&gt; chart will tell you. &lt;em&gt;Wonkblog. The Washington Post&lt;/em&gt;. &lt;a href=&#34;https://www.washingtonpost.com/news/wonk/wp/2014/09/25/think-you-drink-a-lot-this-chart-will-tell-you/?utm_term=.b81599bbbe25&#34;&gt;https://www.washingtonpost.com/news/wonk/wp/2014/09/25/think-you-drink-a-lot-this-chart-will-tell-you/?utm_term=.b81599bbbe25&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-niaaaNationalEpidemiologicSurvey2006&#34; class=&#34;csl-entry&#34;&gt;
{{National Institute on Alcohol Abuse and Alcoholism}}. (2006). &lt;em&gt;National epidemiologic survey on alcohol and related conditions&lt;/em&gt;. &lt;a href=&#34;https://pubs.niaaa.nih.gov/publications/AA70/AA70.htm&#34;&gt;https://pubs.niaaa.nih.gov/publications/AA70/AA70.htm&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Yes, one can smoke half a cigarette or drink 1/3 of a drink. Ideally, we‚Äôd have the exact amount of nicotine in your blood at a given moment and over time and the same for the amount of alcohol in your system relative to your blood volume and such. But in practice, substance use researchers just don‚Äôt tend to have access to data of that quality. Instead, we‚Äôre typically stuck with simple counts. And I look forward to the day the right team of engineers, computer scientists, and substance use researchers (and whoever else I forgot to mention) release the cheap, non-invasive technology we need to passively measure these things. Until then: &lt;em&gt;How many standard servings of alcohol did you drink, last night?&lt;/em&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

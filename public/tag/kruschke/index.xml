<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kruschke | Fahim Ahmad</title>
    <link>/tag/kruschke/</link>
      <atom:link href="/tag/kruschke/index.xml" rel="self" type="application/rss+xml" />
    <description>Kruschke</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Fahim Ahmad (2020)</copyright><lastBuildDate>Wed, 17 Nov 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Kruschke</title>
      <link>/tag/kruschke/</link>
    </image>
    
    <item>
      <title>Conditional logistic models with brms: Rough draft.</title>
      <link>/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/</link>
      <pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/</guid>
      <description>
&lt;script src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble&lt;/h2&gt;
&lt;p&gt;After tremendous help from &lt;a href=&#34;http://singmann.org/&#34;&gt;Henrik Singmann&lt;/a&gt; and &lt;a href=&#34;https://sites.google.com/view/mattansb&#34;&gt;Mattan Ben-Shachar&lt;/a&gt;, I finally have two (!) workflows for conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. These workflows are on track to make it into the next update of my ebook translation of Kruschke’s text (see &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;here&lt;/a&gt;). But these models are new to me and I’m not entirely confident I’ve walked them out properly.&lt;/p&gt;
&lt;p&gt;The goal of this blog post is to present a draft of my workflow, which will eventually make it’s way into &lt;a href=&#34;https://bookdown.org/content/3686/nominal-predicted-variable.html&#34;&gt;Chapter 22&lt;/a&gt; of the ebook. If you have any constrictive criticisms, please pass them along on&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/22&#34;&gt;GitHub&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433&#34;&gt;this thread&lt;/a&gt; in the Stan forums,&lt;/li&gt;
&lt;li&gt;or on &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1461016859420024842&#34;&gt;twitter&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To streamline this post a little, I have removed the content on the softmax model. For that material, just go to the ebook proper.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# in the ebook, this code will have already been executed
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)

theme_set(
  theme_gray() +
    theme(panel.grid = element_blank())
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nominal-predicted-variable&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nominal Predicted Variable&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;This chapter considers data structures that have a nominal predicted variable. When the nominal predicted variable has only two possible values, this reduces to the case of the dichotomous predicted variable considered in the previous chapter. In the present chapter, we generalize to cases in which the predicted variable has three or more categorical values…&lt;/p&gt;
&lt;p&gt;The traditional treatment of this sort of data structure is called multinomial logistic regression or conditional logistic regression. We will consider Bayesian approaches to these methods. As usual, in Bayesian software it is easy to generalize the traditional models so they are robust to outliers, allow different variances within levels of a nominal predictor, and have hierarchical structure to share information across levels or factors as appropriate. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke, 2015, p. 649&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;softmax-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Softmax regression&lt;/h2&gt;
&lt;div id=&#34;softmax-reduces-to-logistic-for-two-outcomes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Softmax reduces to logistic for two outcomes.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;independence-from-irrelevant-attributes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Independence from irrelevant attributes.&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional logistic regression&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Softmax regression conceives of each outcome as an independent change in log odds from the reference outcome, and a special case of that is dichotomous logistic regression. But we can generalize logistic regression another way, which may better capture some patterns of data. The idea of this generalization is that we divide the set of outcomes into a hierarchy of two-set divisions, and use a logistic to describe the probability of each branch of the two-set divisions. (p. 655)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The model follows the generic equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\phi_{S^* | S} = \operatorname{logistic}(\lambda_{S^* | S}) \\
\lambda_{S^* | S} = \beta_{0, S^* | S} + \beta_{1, {S^* | S}} x,
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the conditional response probability (i.e., the goal of the analysis) is &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(S^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; denote the subset of outcomes and larger set of outcomes, respectively, and &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{S^* | S}\)&lt;/span&gt; is the propensity based on some linear model. The overall point is these “regression coefficients refer to the conditional probability of outcomes for the designated subsets, not necessarily to a single outcome among the full set of outcomes” (p. 655).&lt;/p&gt;
&lt;p&gt;In Figure 22.2 (p. 656), Kruschke depicted the two hierarchies of binary divisions of the models he fit to the data in his &lt;code&gt;CondLogistRegData1.csv&lt;/code&gt; and &lt;code&gt;CondLogistRegData2.csv&lt;/code&gt; files. Here we load those data, save them as &lt;code&gt;d3&lt;/code&gt; and &lt;code&gt;d4&lt;/code&gt;, and take a look at their structures.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d3 &amp;lt;- read_csv(&amp;quot;data.R/CondLogistRegData1.csv&amp;quot;)
d4 &amp;lt;- read_csv(&amp;quot;data.R/CondLogistRegData2.csv&amp;quot;)

glimpse(d3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 475
## Columns: 3
## $ X1 &amp;lt;dbl&amp;gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.5…
## $ X2 &amp;lt;dbl&amp;gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.774…
## $ Y  &amp;lt;dbl&amp;gt; 2, 1, 3, 1, 3, 3, 2, 3, 2, 4, 1, 2, 2, 3, 4, 2, 2, 4, 2, 3, 4, 2, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(d4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 475
## Columns: 3
## $ X1 &amp;lt;dbl&amp;gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.5…
## $ X2 &amp;lt;dbl&amp;gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.774…
## $ Y  &amp;lt;dbl&amp;gt; 4, 4, 3, 4, 2, 3, 4, 3, 4, 4, 2, 4, 4, 3, 3, 4, 4, 4, 4, 3, 4, 4, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In both data sets, the nominal criterion is &lt;code&gt;Y&lt;/code&gt; and the two predictors are &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt;. Though the data seem simple, the conditional logistic models are complex enough that it seems like we’ll be better served by focusing on them one at a time, which means I’m going to break up Figure 22.2. Here’s how to make the diagram in the left panel.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the big numbers
numbers &amp;lt;- tibble(
  x = c(3, 5, 2, 4, 1, 3, 2),
  y = c(0, 0, 1, 1, 2, 2, 3),
  label = c(&amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3,4&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2,3,4&amp;quot;, &amp;quot;1,2,3,4&amp;quot;)
)

# the smaller Greek numbers
greek &amp;lt;- tibble(
  x = c(3.4, 4.6, 2.4, 3.6, 1.4, 2.6),
  y = c(0.5, 0.5, 1.5, 1.5, 2.5, 2.5),
  hjust = c(1, 0, 1, 0, 1, 0),
  label = c(&amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;)
)

# arrows
tibble(
  x = c(4, 4, 3, 3, 2, 2),
  y = c(0.85, 0.85, 1.85, 1.85, 2.85, 2.85),
  xend = c(3, 5, 2, 4, 1, 3),
  yend = c(0.15, 0.15, 1.15, 1.15, 2.15, 2.15)
) %&amp;gt;%  
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_segment(aes(xend = xend, yend = yend),
               size = 1/4,
               arrow = arrow(length = unit(0.08, &amp;quot;in&amp;quot;), type = &amp;quot;closed&amp;quot;)) +
  geom_text(data = numbers,
            aes(label = label),
            size = 5, family = &amp;quot;Times&amp;quot;)+
  geom_text(data = greek,
            aes(label = label, hjust = hjust),
            size = 4.25, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-1, 7) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The large numbers are the four levels in the criterion &lt;code&gt;Y&lt;/code&gt; and the smaller numbers in the curly braces are various sets of those numbers. The diagram shows three levels of outcome-set divisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 versus 2, 3, or 4;&lt;/li&gt;
&lt;li&gt;2 versus 3 or 4; and&lt;/li&gt;
&lt;li&gt;3 versus 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The divisions in each of these levels can be expressed as linear models which we’ll denote &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Given our data with two predictors &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt;, we can express the three linear models as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\lambda_{\{ 1 \} | \{ 1,2,3,4 \}} &amp;amp; = \beta_{0,\{ 1 \} | \{ 1,2,3,4 \}} + \beta_{1,\{ 1 \} | \{ 1,2,3,4 \}} \text{X1} + \beta_{2,\{ 1 \} | \{ 1,2,3,4 \}} \text{X2} \\
\lambda_{\{ 2 \} | \{ 2,3,4 \}}   &amp;amp; = \beta_{0,\{ 2 \} | \{ 2,3,4 \}} + \beta_{1,\{ 2 \} | \{ 2,3,4 \}} \text{X1} + \beta_{2,\{ 2 \} | \{ 2,3,4 \}} \text{X2} \\
\lambda_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \beta_{0,\{ 3 \} | \{ 3,4 \}} + \beta_{1,\{ 3 \} | \{ 3,4 \}} \text{X1} + \beta_{2,\{ 3 \} | \{ 3,4 \}} \text{X2},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, for convenience, we’re omitting the typical &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; subscripts. As these linear models are all defined within the context of the logit link, we can express the conditional probabilities of the outcome sets as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_{\{ 1 \} | \{ 1,2,3,4 \}} &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_{\{ 2 \} | \{ 2,3,4 \}}   &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 2 \} | \{ 2,3,4 \}} \right) \\
\phi_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 3 \} | \{ 3,4 \}} \right),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 1 \} | \{ 1,2,3,4 \}}\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 3 \} | \{ 3,4 \}}\)&lt;/span&gt; are the conditional probabilities for the outcome sets. If, however, we want the conditional probabilities for the actual levels of the criterion &lt;code&gt;Y&lt;/code&gt;, we define those with a series of (in this case) four equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \phi_{\{ 2 \} | \{ 2,3,4 \}} \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the sum of the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\phi_1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(\phi_4\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. To get a sense of what this all means in practice, let’s visualize the data and the data-generating equations for our version of Figure 22.3. As with the previous figure, I’m going to break this figure up to focus on one model at a time. Thus, here’s the left panel of Figure 22.3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the various population parameters

# lambda 1
b01 &amp;lt;- -4
b11 &amp;lt;- -5
b21 &amp;lt;- 0.01  # rounding up to avoid dividing by zero
# lambda 2
b02 &amp;lt;- -2
b12 &amp;lt;- 1
b22 &amp;lt;- -5
# lambda 3
b03 &amp;lt;- -1
b13 &amp;lt;- 3
b23 &amp;lt;- 3

# use the parameters to define the lines 
lines &amp;lt;- tibble(
  intercept = c(-b01 / b21, -b02 / b22, -b03 / b23),
  slope = c(-b11 / b21, -b12 / b22, -b13 / b23),
  label = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;)
)

# wrangle
d3 %&amp;gt;% 
  mutate(Y = factor(Y)) %&amp;gt;% 
  
  # plot!
  ggplot() +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_text(aes(x = X1, y = X2, label = Y, color = Y),
            size = 3, show.legend = F) +
  geom_abline(data = lines,
              aes(intercept = intercept,
                  slope = slope,
                  linetype = label)) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]==-4+-5*x[1]+0*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]==-2+1*x[1]+-5*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]==-1+3*x[1]+3*x[2]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;240&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Recall back on page 629, Kruschke showed the equation for the 50% threshold of a logistic regression model given two continuous predictors was&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_2 = (-\beta_0 / \beta_2) + (-\beta_1 / \beta_2) x_1.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It was that equation that gave us the values for the &lt;code&gt;intercept&lt;/code&gt; and &lt;code&gt;slope&lt;/code&gt; arguments (&lt;span class=&#34;math inline&#34;&gt;\(-\beta_0 / \beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-\beta_1 / \beta_2\)&lt;/span&gt;, respectively) for the &lt;code&gt;geom_abline()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;It still might not be clear how the various &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt; values connect to the data. Though not in the text, here’s an alternative way of expressing the relations in Figure 22.3. This time the plot is faceted by the three levels of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt; and the background fill is based on those conditional probabilities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define a grid of X1 and X2 values
crossing(X1 = seq(from = -2, to = 2, length.out = 50),
         X2 = seq(from = -2, to = 2, length.out = 50)) %&amp;gt;% 
  # compute the lambda&amp;#39;s
  mutate(`lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]` = b01 + b11 * X1 + b21 * X2,
         `lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]`   = b02 + b12 * X1 + b22 * X2,
         `lambda[&amp;#39;{3}|{3,4}&amp;#39;]`     = b03 + b13 * X1 + b23 * X2) %&amp;gt;% 
  # compute the phi&amp;#39;s
  mutate(`phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]` = plogis(`lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]`),
         `phi[&amp;#39;{2}|{2,3,4}&amp;#39;]`   = plogis(`lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]`),
         `phi[&amp;#39;{3}|{3,4}&amp;#39;]`     = plogis(`lambda[&amp;#39;{3}|{3,4}&amp;#39;]`)) %&amp;gt;% 
  # wrangle
  pivot_longer(contains(&amp;quot;phi&amp;quot;), values_to = &amp;quot;phi&amp;quot;) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = X1, y = X2)) +
  geom_raster(aes(fill = phi),
              interpolate = T) +
  # note how we&amp;#39;re subsetting the d3 data by facet
  geom_text(data = bind_rows(
    d3 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;),
    d3 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;gt; 1),
    d3 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;gt; 2)),
            aes(label = Y),
            size = 2.5, color = &amp;quot;grey20&amp;quot;) +
  scale_fill_viridis_c(expression(phi[italic(S)*&amp;quot;*|&amp;quot;*italic(S)]),
                        option = &amp;quot;F&amp;quot;, breaks = 0:2 / 2, limits = c(0, 1)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_equal() +
  theme(legend.position = c(0.8, 0.2)) +
  facet_wrap(~ name, labeller = label_parsed, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how because each of the levels of &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is defined by a different subset of the data, each of the facets contains a different subset of the &lt;code&gt;d3&lt;/code&gt; data, too. For example, since &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 1 \} | \{ 1,2,3,4 \}}\)&lt;/span&gt; is defined by the full subset of the possible values of &lt;code&gt;Y&lt;/code&gt;, you see all the &lt;code&gt;Y&lt;/code&gt; data displayed by &lt;code&gt;geom_text()&lt;/code&gt; for that facet. In contrast, since &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 3 \} | \{ 3,4 \}}\)&lt;/span&gt; is defined by a subset of the data for which &lt;code&gt;Y&lt;/code&gt; is only &lt;code&gt;3&lt;/code&gt; or &lt;code&gt;4&lt;/code&gt;, those are the only values you see displayed within that facet of the plot.&lt;/p&gt;
&lt;p&gt;Now we’ll consider an alternative way to set up the binary-choices hierarchy, as seen in the right panel of Figure 22.2. First, here’s that half of the figure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the big numbers
numbers &amp;lt;- tibble(
  x = c(0, 2, 6, 8, 1, 7, 4),
  y = c(0, 0, 0, 0, 1, 1, 2),
  label = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;1,2&amp;quot;, &amp;quot;3,4&amp;quot;, &amp;quot;1,2,3,4&amp;quot;)
)

# the smaller Greek numbers
greek &amp;lt;- tibble(
  x = c(0.4, 1.6, 6.4, 7.6, 2.1, 5.8),
  y = c(0.5, 0.5, 0.5, 0.5, 1.5, 1.5),
  hjust = c(1, 0, 1, 0, 1, 0),
  label = c(&amp;quot;phi[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;)
)

# arrows
tibble(
  x = c(1, 1, 7, 7, 4, 4),
  y = c(0.85, 0.85, 0.85, 0.85, 1.85, 1.85),
  xend = c(0, 2, 6, 8, 1, 7),
  yend = c(0.15, 0.15, 0.15, 0.15, 1.15, 1.15)
) %&amp;gt;%  
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_segment(aes(xend = xend, yend = yend),
               size = 1/4,
               arrow = arrow(length = unit(0.08, &amp;quot;in&amp;quot;), type = &amp;quot;closed&amp;quot;)) +
  geom_text(data = numbers,
            aes(label = label),
            size = 5, family = &amp;quot;Times&amp;quot;)+
  geom_text(data = greek,
            aes(label = label, hjust = hjust),
            size = 4.25, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-1, 10) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This diagram shows three levels of outcome-set divisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 or 2 versus 3 or 4;&lt;/li&gt;
&lt;li&gt;1 versus 2; and&lt;/li&gt;
&lt;li&gt;3 versus 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given our data with two predictors &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt;, we can express the three linear models as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\lambda_{\{ 1,2 \} | \{ 1,2,3,4 \}} &amp;amp; = \beta_{0,\{ 1,2 \} | \{ 1,2,3,4 \}} + \beta_{1,\{ 1,2 \} | \{ 1,2,3,4 \}} \text{X1} + \beta_{2,\{ 1,2 \} | \{ 1,2,3,4 \}} \text{X2} \\
\lambda_{\{ 1 \} | \{ 1,2 \}}   &amp;amp; = \beta_{0,\{ 1 \} | \{ 1,2 \}} + \beta_{1,\{ 1 \} | \{ 1,2 \}} \text{X1} + \beta_{2,\{ 1 \} | \{ 1,2 \}} \text{X2} \\
\lambda_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \beta_{0,\{ 3 \} | \{ 3,4 \}} + \beta_{1,\{ 3 \} | \{ 3,4 \}} \text{X1} + \beta_{2,\{ 3 \} | \{ 3,4 \}} \text{X2}.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can then express the conditional probabilities of the outcome sets as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_{\{ 1 \} | \{ 1,2 \}}   &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 1 \} | \{ 1,2 \}} \right) \\
\phi_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 3 \} | \{ 3,4 \}} \right).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the conditional probabilities of the actual levels of the criterion &lt;code&gt;Y&lt;/code&gt;, we define those with a series of (in this case) four equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2 \}} \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \left ( 1 - \phi_{\{ 1 \} | \{ 1,2 \}} \right) \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the sum of the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\phi_1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(\phi_4\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. To get a sense of what this all means, let’s visualize the data and the data-generating equations in our version of the right panel of Figure 22.3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d4 %&amp;gt;% 
  mutate(Y = factor(Y)) %&amp;gt;% 
  
  ggplot() +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_text(aes(x = X1, y = X2, label = Y, color = Y),
            size = 3, show.legend = F) +
  geom_abline(data = lines,
              aes(intercept = intercept,
                  slope = slope,
                  linetype = label)) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]==-4+-5*x[1]+0*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{1}|{1,2}&amp;#39;]==-2+1*x[1]+-5*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]==-1+3*x[1]+3*x[2]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;240&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s an alternative way of expression the relations in the right panel of Figure 22.3. This time the plot is faceted by the three levels of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt; and the background fill is based on those conditional probabilities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define a grid of X1 and X2 values
crossing(X1 = seq(from = -2, to = 2, length.out = 50),
         X2 = seq(from = -2, to = 2, length.out = 50)) %&amp;gt;% 
  # compute the lambda&amp;#39;s
  mutate(`lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]` = b01 + b11 * X1 + b21 * X2,
         `lambda[&amp;#39;{1}|{1,2}&amp;#39;]`       = b02 + b12 * X1 + b22 * X2,
         `lambda[&amp;#39;{3}|{3,4}&amp;#39;]`       = b03 + b13 * X1 + b23 * X2) %&amp;gt;% 
  # compute the phi&amp;#39;s
  mutate(`phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]` = plogis(`lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]`),
         `phi[&amp;#39;{1}|{1,2}&amp;#39;]`       = plogis(`lambda[&amp;#39;{1}|{1,2}&amp;#39;]`),
         `phi[&amp;#39;{3}|{3,4}&amp;#39;]`       = plogis(`lambda[&amp;#39;{3}|{3,4}&amp;#39;]`)) %&amp;gt;% 
  # wrangle
  pivot_longer(contains(&amp;quot;phi&amp;quot;), values_to = &amp;quot;phi&amp;quot;) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = X1, y = X2)) +
  geom_raster(aes(fill = phi),
              interpolate = T) +
  # note how we&amp;#39;re subsetting the d3 data by facet
  geom_text(data = bind_rows(
    d4 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;),
    d4 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;lt; 3),
    d4 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;gt; 2)),
            aes(label = Y),
            size = 2.5, color = &amp;quot;grey20&amp;quot;) +
  scale_fill_viridis_c(expression(phi[italic(S)*&amp;quot;*|&amp;quot;*italic(S)]),
                       option = &amp;quot;F&amp;quot;, breaks = 0:2 / 2, limits = c(0, 1)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_equal() +
  theme(legend.position = c(0.8, 0.2)) +
  facet_wrap(~ name, labeller = label_parsed, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It could be easy to miss due to the way we broke up our workflow, but if you look closely at the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; equations at the top of both panels of Figure 22.3, you’ll see the right-hand side of the equations are the same. But because of the differences in the two data hierarchies, those &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; equations had different consequences for how the &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; values generated the &lt;code&gt;Y&lt;/code&gt; data. Also,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In general, conditional logistic regression requires that there is a linear division between two subsets of the outcomes, and then within each of those subsets there is a linear division of smaller subsets, and so on. This sort of linear division is not required of the softmax regression model… Real data can be extremely noisy, and there can be multiple predictors, so it can be challenging or impossible to visually ascertain which sort of model is most appropriate. The choice of model is driven primarily by theoretical meaningfulness. (p. 659)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-in-jags-brms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implementation in &lt;del&gt;JAGS&lt;/del&gt; brms&lt;/h2&gt;
&lt;div id=&#34;softmax-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Softmax model.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conditional logistic model.&lt;/h3&gt;
&lt;p&gt;The conditional logistic regression models are not natively supported in &lt;strong&gt;brms&lt;/strong&gt; at this time. Based on &lt;a href=&#34;https://github.com/paul-buerkner/brms/issues/560&#34;&gt;issue #560&lt;/a&gt; in the &lt;strong&gt;brms&lt;/strong&gt; GitHub, there are ways to fit them using the nonlinear syntax. If you compare the syntax Bürkner used in that thread on January 30&lt;sup&gt;th&lt;/sup&gt; to the JAGS syntax Kruschke showed on pages 661 and 662, you’ll see they appear to follow contrasting parameterizations.&lt;/p&gt;
&lt;p&gt;However, there are at least two other ways to fit conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. Based on insights from &lt;a href=&#34;http://singmann.org/&#34;&gt;Henrik Singmann&lt;/a&gt;, we can define conditional logistic models using the custom family approach. In contrast, &lt;a href=&#34;https://sites.google.com/view/mattansb&#34;&gt;Mattan Ben-Shachar&lt;/a&gt; has shown we can also fit conditional logistic models using a tricky application of sequential ordinal regression. Rather than present them in the abstract, here, we will showcase both of these approaches in the sections below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results-interpreting-the-regression-coefficients.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Results: Interpreting the regression coefficients.&lt;/h3&gt;
&lt;div id=&#34;softmax-model.-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Softmax model.&lt;/h4&gt;
&lt;div id=&#34;bonus-consider-the-interceps-only-softmax-model.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Bonus: Consider the interceps-only softmax model.&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-model.-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Conditional logistic model.&lt;/h4&gt;
&lt;p&gt;Since we will be fitting the conditional logistic model with two different strategies, I’m going to deviate from how Kruschke organized this part of the text and break this section up into two subsections:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First we’ll walk through the custom family approach.&lt;/li&gt;
&lt;li&gt;Second we’ll explore the sequential ordinal approach.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;conditional-logistic-models-with-custom-likelihoods.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Conditional logistic models with custom likelihoods.&lt;/h5&gt;
&lt;p&gt;As we briefly learned in [Section 8.6.1][Defining new likelihood functions.], &lt;strong&gt;brms&lt;/strong&gt; users can define their own custom likelihood functions, which Bürkner outlined in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Define&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; vignette, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html&#34;&gt;&lt;em&gt;Define custom response distributions with brms&lt;/em&gt;&lt;/a&gt;. As part of the &lt;a href=&#34;https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433&#34;&gt;&lt;em&gt;Nominal data and Kruschke’s “conditional logistic” approach&lt;/em&gt;&lt;/a&gt; thread on the Stan forums, Henrik Singmann showed how you can use this functionality to fit conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. We will practice how to do this for the models of both the &lt;code&gt;d3&lt;/code&gt; and &lt;code&gt;d4&lt;/code&gt; data sets, which were showcased in the left and right panels of Figure 22.3 in &lt;a href=&#34;#conditional-logistic-regression&#34;&gt;Section 22.2&lt;/a&gt;. Going in order, we’ll focus first on how to model the data in &lt;code&gt;d3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the first step, we use the &lt;code&gt;custom_family()&lt;/code&gt; function to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;name the new family with the &lt;code&gt;name&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;name the family’s parameters with the &lt;code&gt;dpars&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;name the link function(s) with the &lt;code&gt;links&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;define whether the distribution is discrete or continuous with the &lt;code&gt;type&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;provide the names of any variables that are part of the internal workings of the family but are not among the distributional parameters with the &lt;code&gt;vars&lt;/code&gt; argument, and&lt;/li&gt;
&lt;li&gt;provide supporting information with the &lt;code&gt;specials&lt;/code&gt; argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cond_log_1 &amp;lt;- custom_family(
  name     = &amp;quot;cond_log_1&amp;quot;, 
  dpars    = c(&amp;quot;mu&amp;quot;, &amp;quot;mub&amp;quot;, &amp;quot;muc&amp;quot;), 
  links    = &amp;quot;identity&amp;quot;, 
  type     = &amp;quot;int&amp;quot;,
  vars     = c(&amp;quot;n_cat&amp;quot;),
  specials = &amp;quot;categorical&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the second step, we use the &lt;code&gt;stanvar()&lt;/code&gt; function to define our custom probability mass function and the corresponding function that will allow us to return predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_lpmf_1 &amp;lt;- stanvar(block = &amp;quot;functions&amp;quot;, 
                       scode = &amp;quot;
real cond_log_1_lpmf(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mu;
  prob[2] = p_mub * (1 - p_mu);
  prob[3] = p_muc * (1 - p_mub) * (1 - p_mu);
  prob[4] = (1 - p_mu) * (1 - p_mub) * (1 - p_muc);
  return(categorical_lpmf(y | prob));
}

vector cond_log_1_pred(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mu;
  prob[2] = p_mub * (1 - p_mu);
  prob[3] = p_muc * (1 - p_mub) * (1 - p_mu);
  prob[4] = (1 - p_mu) * (1 - p_mub) * (1 - p_muc);
  return(prob);
}
&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how we have defined the four &lt;code&gt;prob[i]&lt;/code&gt; values based on the four equations from above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \phi_{\{ 2 \} | \{ 2,3,4 \}} \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Third, we save another &lt;code&gt;stanvar()&lt;/code&gt; object with additional information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stanvars &amp;lt;- stanvar(x = 4, name = &amp;quot;n_cat&amp;quot;, scode = &amp;quot;  int n_cat;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to fit the model with &lt;code&gt;brm()&lt;/code&gt;. Notice how our use of the &lt;code&gt;family&lt;/code&gt; and &lt;code&gt;stanvars&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.3 &amp;lt;-
  brm(data = d3, 
      family = cond_log_1,
      Y ~ 1 + X1 + X2,
      prior = c(prior(normal(0, 20), class = Intercept, dpar = mu2),
                prior(normal(0, 20), class = Intercept, dpar = mu3),
                prior(normal(0, 20), class = Intercept, dpar = mu4),
                prior(normal(0, 20), class = b, dpar = mu2),
                prior(normal(0, 20), class = b, dpar = mu3),
                prior(normal(0, 20), class = b, dpar = mu4)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      stanvars = stan_lpmf_1 + stanvars,
      file = &amp;quot;fits/fit22.03&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cond_log_1 
##   Links: mu2 = identity; mu3 = identity; mu4 = identity 
## Formula: Y ~ 1 + X1 + X2 
##    Data: d3 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## mu2_Intercept    -4.02      0.47    -5.01    -3.17 1.00     3108     2656
## mu3_Intercept    -2.13      0.36    -2.88    -1.50 1.00     2490     2211
## mu4_Intercept    -0.96      0.32    -1.62    -0.38 1.00     3014     2674
## mu2_X1           -4.92      0.54    -6.03    -3.94 1.00     3029     2832
## mu2_X2            0.01      0.20    -0.37     0.40 1.00     4950     2436
## mu3_X1            0.74      0.30     0.16     1.35 1.00     3343     2736
## mu3_X2           -5.21      0.63    -6.54    -4.06 1.00     2748     2794
## mu4_X1            3.00      0.49     2.10     4.05 1.00     3002     2280
## mu4_X2            3.10      0.53     2.16     4.25 1.00     2646     2412
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As they aren’t the most intuitive, here’s how to understand our three prefixes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mu2_&lt;/code&gt; has to do with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\{ 1 \} | \{ 1,2,3,4 \}}\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu3_&lt;/code&gt; has to do with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\{ 2 \} | \{ 2,3,4 \}}\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu4_&lt;/code&gt; has to do with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\{ 3 \} | \{ 3,4 \}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you compare those posterior means of each of those parameters from the data-generating equations at the top of Figure 22.3, you’ll see they are spot on (within simulation variance). Here’s how we might visualize those posteriors in our version of the histograms in the top right panel(s) of Figure 22.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws
post &amp;lt;- posterior_samples(fit22.3)

# wrangle
p1 &amp;lt;-
  post %&amp;gt;% 
  pivot_longer(-lp__) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(number = str_extract(name, &amp;quot;[2-4]+&amp;quot;)) %&amp;gt;% 
  mutate(lambda    = case_when(number == &amp;quot;2&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;,
                               number == &amp;quot;3&amp;quot; ~ &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;,
                               number == &amp;quot;4&amp;quot; ~ &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;),
         parameter = case_when(str_detect(name, &amp;quot;Intercept&amp;quot;) ~ &amp;quot;beta[0]&amp;quot;,
                               str_detect(name, &amp;quot;X1&amp;quot;)        ~ &amp;quot;beta[1]&amp;quot;,
                               str_detect(name, &amp;quot;X2&amp;quot;)        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95, size = 1,
                    normalize = &amp;quot;panels&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we use the threshold formula from above,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_2 = (-\beta_0 / \beta_2) + (-\beta_1 / \beta_2)x_1,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to the posterior draws, we can make our version of the upper left panel of Figure 22.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22)

p2 &amp;lt;-
  post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_mu&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;lambda&amp;quot;, &amp;quot;parameter&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  mutate(intercept = -Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now combine the two ggplots, add a little formatting, and show the full upper half of Figure 22.6, based on the &lt;code&gt;custom_family()&lt;/code&gt; approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p2 + p1) &amp;amp; 
  plot_layout(widths = c(1, 2)) &amp;amp;
  plot_annotation(title = &amp;quot;Figure 22.6, upper half&amp;quot;,
                  subtitle = &amp;quot;Results from the conditional logistic model fit to the d3 data via the custom-family approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though it isn’t necessary to reproduce any of the plots in this section of Kruschke’s text, we’ll want to use the &lt;code&gt;expose_functions()&lt;/code&gt; function if we wanted to use any of the &lt;strong&gt;brms&lt;/strong&gt; post-processing functions for our model fit with the custom likelihood.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expose_functions(fit22.3, vectorize = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what we’d need to do before computing information criteria estimates, such as with the WAIC.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_lik_cond_log_1 &amp;lt;- function(i, prep) {
  mu  &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mub &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  muc &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  cond_log_1_lpmf(y, mu, mub, muc, n_cat)
}

fit22.3 &amp;lt;- add_criterion(fit22.3, criterion = &amp;quot;waic&amp;quot;)

waic(fit22.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 4000 by 475 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -230.8 16.8
## p_waic         9.3  1.1
## waic         461.6 33.6
## 
## 2 (0.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to use one of the functions that relies on conditional expectations, such as &lt;code&gt;conditional_effects()&lt;/code&gt;, we’d execute something like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_epred_cond_log_1 &amp;lt;- function(prep) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y
  prob &amp;lt;- cond_log_1_pred(y = y, mu = mu, mu_b = mu_b, mu_c = mu_c, n_cat = n_cat)
  dim(prob) &amp;lt;- c(dim(prob)[1], dim(mu))
  prob &amp;lt;- aperm(prob, c(2,3,1))
  dimnames(prob) &amp;lt;- list(
    as.character(seq_len(dim(prob)[1])), 
    NULL, 
    as.character(seq_len(dim(prob)[3]))
  )
  prob
}

ce &amp;lt;- conditional_effects(
  fit22.3, 
  categorical = T,
  effects = &amp;quot;X1&amp;quot;)

plot(ce, plot = FALSE)[[1]] + 
  scale_fill_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we wanted to do a posterior predictive check with the &lt;code&gt;pp_check()&lt;/code&gt; function, we’d need to do something like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_predict_cond_log_1 &amp;lt;- function(i, prep, ...) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  prob &amp;lt;- cond_log_1_pred(y, mu, mu_b, mu_c, n_cat)
  # make sure you have the extraDistr package
  extraDistr::rcat(length(mu), t(prob))
}

pp_check(fit22.3, 
         type = &amp;quot;bars&amp;quot;, 
         ndraws = 100, 
         size = 1/2, 
         fatten = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So far all of this has been with the conditional logistic model based on the first hierarchy of two-set divisions, which Kruschke used to simulate the &lt;code&gt;d3&lt;/code&gt; data. Now we’ll switch to consider the second hierarchy of two-set divisions, with which Kruschke simulated the &lt;code&gt;d4&lt;/code&gt; data. That second hierarchy, recall, resulted in the following definition for the conditional probabilities for the four levels of &lt;code&gt;Y&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2 \}} \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \left ( 1 - \phi_{\{ 1 \} | \{ 1,2 \}} \right) \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This will require us to define a new custom family, which we’ll call &lt;code&gt;cond_log_2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cond_log_2 &amp;lt;- custom_family(
  name     = &amp;quot;cond_log_2&amp;quot;, 
  dpars    = c(&amp;quot;mu&amp;quot;, &amp;quot;mub&amp;quot;, &amp;quot;muc&amp;quot;), 
  links    = &amp;quot;identity&amp;quot;, 
  type     = &amp;quot;int&amp;quot;,
  vars     = c(&amp;quot;n_cat&amp;quot;),
  specials = &amp;quot;categorical&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we use the &lt;code&gt;stanvar()&lt;/code&gt; function to define our custom probability mass function and the corresponding function that will allow us to return predictions, which we’ll just save as &lt;code&gt;stan_lpmf_2&lt;/code&gt;. Other than the names, notice that the major change is how we have defined the &lt;code&gt;prob[i]&lt;/code&gt; parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_lpmf_2 &amp;lt;- stanvar(block = &amp;quot;functions&amp;quot;, 
                       scode = &amp;quot;
real cond_log_2_lpmf(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mub * p_mu;
  prob[2] = (1 - p_mub) * p_mu;
  prob[3] = p_muc * (1 - p_mu);
  prob[4] = (1 - p_muc) * (1 - p_mu);
  return(categorical_lpmf(y | prob));
}

vector cond_log_2_pred(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mub * p_mu;
  prob[2] = (1 - p_mub) * p_mu;
  prob[3] = p_muc * (1 - p_mu);
  prob[4] = (1 - p_muc) * (1 - p_mu);
  return(prob);
}
&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to fit the model with &lt;code&gt;brm()&lt;/code&gt;. Again, notice how our use of the &lt;code&gt;family&lt;/code&gt; and &lt;code&gt;stanvars&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.4 &amp;lt;-
  brm(data = d4, 
      family = cond_log_2,
      Y ~ 1 + X1 + X2,
      prior = c(prior(normal(0, 20), class = Intercept, dpar = mu2),
                prior(normal(0, 20), class = Intercept, dpar = mu3),
                prior(normal(0, 20), class = Intercept, dpar = mu4),
                prior(normal(0, 20), class = b, dpar = mu2),
                prior(normal(0, 20), class = b, dpar = mu3),
                prior(normal(0, 20), class = b, dpar = mu4)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      stanvars = stan_lpmf_2 + stanvars,
      file = &amp;quot;fits/fit22.04&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cond_log_2 
##   Links: mu2 = identity; mu3 = identity; mu4 = identity 
## Formula: Y ~ 1 + X1 + X2 
##    Data: d4 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## mu2_Intercept    -4.05      0.46    -5.02    -3.23 1.00     2757     2184
## mu3_Intercept    -1.39      1.19    -3.80     0.87 1.00     2632     2185
## mu4_Intercept    -1.02      0.23    -1.50    -0.60 1.00     2824     2668
## mu2_X1           -4.79      0.52    -5.90    -3.84 1.00     2769     2286
## mu2_X2            0.35      0.20    -0.04     0.74 1.00     4245     2441
## mu3_X1            1.54      0.88    -0.13     3.28 1.00     2668     2139
## mu3_X2           -5.36      1.16    -7.86    -3.39 1.00     3306     2241
## mu4_X1            3.03      0.38     2.34     3.81 1.00     1872     2428
## mu4_X2            3.13      0.36     2.49     3.86 1.00     2219     2511
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the same basic workflow as before to make our version of the upper half of Figure 22.7.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws
post &amp;lt;- posterior_samples(fit22.4)

# 2D thresholds on the left
set.seed(22)

p1 &amp;lt;-
  post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_mu&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;lambda&amp;quot;, &amp;quot;parameter&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  mutate(intercept = -Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d4,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)

# marginal posteriors on the right
p2 &amp;lt;-
  post %&amp;gt;% 
  pivot_longer(-lp__) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(number = str_extract(name, &amp;quot;[2-4]+&amp;quot;)) %&amp;gt;% 
  mutate(lambda    = case_when(number == &amp;quot;2&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;,
                               number == &amp;quot;3&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;,
                               number == &amp;quot;4&amp;quot; ~ &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;),
         parameter = case_when(str_detect(name, &amp;quot;Intercept&amp;quot;) ~ &amp;quot;beta[0]&amp;quot;,
                               str_detect(name, &amp;quot;X1&amp;quot;)        ~ &amp;quot;beta[1]&amp;quot;,
                               str_detect(name, &amp;quot;X2&amp;quot;)        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95, size = 1,
                    normalize = &amp;quot;panels&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)

# combine, entitle, and display the results
(p1 + p2) &amp;amp; 
  plot_layout(widths = c(1, 2)) &amp;amp;
  plot_annotation(title = &amp;quot;Figure 22.7, upper half&amp;quot;,
                  subtitle = &amp;quot;Results from the conditional logistic model fit to the d4 data via the custom-family approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As Kruschke pointed out in the text,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;notice that the estimates for &lt;span class=&#34;math inline&#34;&gt;\(\lambda_2\)&lt;/span&gt; are more uncertain, with wider HDI’s, than the other coefficients. This uncertainty is also shown in the threshold lines on the data: The lines separating the 1’s from the 2’s have a much wider spread than the other boundaries. Inspection of the scatter plot explains why: There is only a small zone of data that informs the separation of 1’s from 2’s, and therefore the estimate must be relatively ambiguous. (p. 665)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’m not going to go through a full demonstration like before, but if you want to use more &lt;strong&gt;brms&lt;/strong&gt; post processing functions for &lt;code&gt;fit22.4&lt;/code&gt; or any other model fit with our custom &lt;code&gt;cond_log_2&lt;/code&gt; function, you’d need to execute this block of code first. Then post process to your heart’s desire.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expose_functions(fit22.4, vectorize = TRUE)

# for information criteria
log_lik_cond_log_2 &amp;lt;- function(i, prep) {
  mu  &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mub &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  muc &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  cond_log_2_lpmf(y, mu, mub, muc, n_cat)
}

# for conditional expectations
posterior_epred_cond_log_2 &amp;lt;- function(prep) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y
  prob &amp;lt;- cond_log_2_pred(y = y, mu = mu, mu_b = mu_b, mu_c = mu_c, n_cat = n_cat)
  dim(prob) &amp;lt;- c(dim(prob)[1], dim(mu))
  prob &amp;lt;- aperm(prob, c(2,3,1))
  dimnames(prob) &amp;lt;- list(
    as.character(seq_len(dim(prob)[1])), 
    NULL, 
    as.character(seq_len(dim(prob)[3]))
  )
  prob
}

# for posterior predictions
posterior_predict_cond_log_2 &amp;lt;- function(i, prep, ...) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  prob &amp;lt;- cond_log_2_pred(y, mu, mu_b, mu_c, n_cat)
  # make sure you have the extraDistr package
  extraDistr::rcat(length(mu), t(prob))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this section of the text, Kruschke also showed the results of when he analyzed the two data sets with the non-data-generating likelihoods. In the lower half of Figure 22.6, he showed the results of his second version of the conditional logistic model applied to the &lt;code&gt;d3&lt;/code&gt; data. In the lower half of Figure 22.7, he showed the results of his first version of the conditional logistic model applied to the &lt;code&gt;d4&lt;/code&gt; data. Since this section is already complicated enough, we’re not going to do that. But if you’d like to see what happens, consider it a personal homework assignment.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In principle, the different conditional logistic models could be put into an overarching hierarchical model comparison. If you have only a few specific candidate models to compare, this could be a feasible approach. But it is not an easily pursued approach to selecting a partition of outcomes from all possible partitions of outcomes when there are many outcomes… Therefore, it is typical to consider a single model, or small set of models, that are motivated by being meaningful in the context of the application, and interpreting the parameter estimates in that meaningful context. (p. 667)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kruschke finished this section with:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Finally, when you run the models in JAGS, you may find that there is high autocorrelation in the MCMC chains (even with standardized data), which requires a very long chain for adequate ESS. This suggests that Stan might be a more efficient approach.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since we fit our models with Stan via &lt;strong&gt;brms&lt;/strong&gt;, high autocorrelations and low effective sample sizes weren’t a problem. For example, here are the bulk and tail effective sample sizes for both of our two models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(posterior)

bind_rows(
  posterior_samples(fit22.3) %&amp;gt;% summarise_draws(),
  posterior_samples(fit22.4) %&amp;gt;% summarise_draws()
  ) %&amp;gt;% 
  mutate(fit = rep(c(&amp;quot;fit22.3&amp;quot;, &amp;quot;fit22.4&amp;quot;), each = n() / 2)) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;ess&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 250) +
  xlim(0, NA) +
  facet_grid(fit ~ name)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The values look pretty good. We may as well look at the autocorrelations. To keep things simple, this time we’ll restrict our analysis to &lt;code&gt;fit22.4&lt;/code&gt;. [The results are largely the same for &lt;code&gt;fit22.3&lt;/code&gt;.]&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

ac &amp;lt;-
  posterior_samples(fit22.4, add_chain = T) %&amp;gt;% 
  select(b_mu2_Intercept:b_mu4_X2, chain) %&amp;gt;% 
  mcmc_acf(lags = 5)

ac$data %&amp;gt;% 
  filter(Lag &amp;gt; 0) %&amp;gt;% 
  
  ggplot(aes(x = AC)) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_histogram(binwidth = 0.05) +
  scale_x_continuous(&amp;quot;autocorrelation&amp;quot;, limits = c(-1, 1),
                     labels = c(&amp;quot;-1&amp;quot;, &amp;quot;-.5&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;.5&amp;quot;, &amp;quot;1&amp;quot;)) +
  facet_grid(Lag ~ Chain, labeller = label_both)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the whole, the autocorrelations are reasonably low across all parameters, chains, and lags.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-models-by-sequential-ordinal-regression.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Conditional logistic models by sequential ordinal regression.&lt;/h5&gt;
&lt;p&gt;In their &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerOrdinalRegressionModels2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; paper, &lt;a href=&#34;https://psyarxiv.com/x8swp/&#34;&gt;&lt;em&gt;Ordinal regression models in psychology: A tutorial&lt;/em&gt;&lt;/a&gt;, Bürkner and Vourre outlined a framework for fitting a variety of orginal models with &lt;strong&gt;brms&lt;/strong&gt;. We’ll learn more about ordinal models in [Chapter 23][Ordinal Predicted Variable]. In this section, we’ll use Mattan Ben-Shachar’s strategy and purpose one of the ordinal models to fit a conditional logistic model to our nominal data.&lt;/p&gt;
&lt;p&gt;As outlined in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-burknerOrdinalRegressionModels2019&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner &amp;amp; Vuorre&lt;/a&gt; (&lt;a href=&#34;#ref-burknerOrdinalRegressionModels2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, and as we will learn in greater detain in the next chapter, many ordinal regression models presume an underlying continuous process. However, you can use a sequential model in cases where one level of the criterion is only possible after the lower levels of the criterion have been achieved. Although this is not technically correct for the nominal variable &lt;code&gt;Y&lt;/code&gt; in the &lt;code&gt;d3&lt;/code&gt; data set, the simple hierarchical sequence Kruschke used to model those data does follow that same pattern. Ben-Shachar’s insight was that if we treat our nominal variable &lt;code&gt;Y&lt;/code&gt; as ordinal, the sequential model will mimic the sequential-ness of Kruschke’s binary-choices hierarchy. To get this to work, we first have to save an ordinal version of &lt;code&gt;Y&lt;/code&gt;, which we’ll call &lt;code&gt;Y_ord&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d3 &amp;lt;-
  d3 %&amp;gt;% 
  mutate(Y_ord = ordered(Y))

# what are the new attributes?
attributes(d3$Y_ord)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $levels
## [1] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot;
## 
## $class
## [1] &amp;quot;ordered&amp;quot; &amp;quot;factor&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within &lt;code&gt;brm()&lt;/code&gt; we fit sequential models using &lt;code&gt;family = sratio&lt;/code&gt;, which defaults to the logit link. If you want to use predictors in a model of this kind and you would like those coefficients to vary across the different levels of the criterion, you need to insert the predictor terms within the &lt;code&gt;cs()&lt;/code&gt; function. Here’s how to fit the model with &lt;code&gt;brm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.5 &amp;lt;-
  brm(data = d3, 
      family = sratio,
      Y_ord ~ 1 + cs(X1) + cs(X2),
      prior = c(prior(normal(0, 20), class = Intercept),
                prior(normal(0, 20), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      file = &amp;quot;fits/fit22.05&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: sratio 
##   Links: mu = logit; disc = identity 
## Formula: Y_ord ~ 1 + cs(X1) + cs(X2) 
##    Data: d3 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -4.01      0.47    -4.97    -3.14 1.00     2850     2689
## Intercept[2]    -2.12      0.35    -2.84    -1.48 1.00     2864     2875
## Intercept[3]    -0.97      0.32    -1.61    -0.37 1.00     3162     3039
## X1[1]            4.92      0.54     3.93     6.04 1.00     2820     2570
## X1[2]           -0.74      0.29    -1.30    -0.19 1.00     3511     3244
## X1[3]           -3.00      0.50    -4.02    -2.10 1.00     3307     2966
## X2[1]           -0.01      0.20    -0.38     0.38 1.00     4316     2519
## X2[2]            5.22      0.63     4.07     6.52 1.00     2837     2616
## X2[3]           -3.11      0.53    -4.23    -2.14 1.00     2902     2640
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing that might not be apparent at first glance is that although this model is essentially equivalent to the &lt;code&gt;family = cond_log_1&lt;/code&gt; version of the model we fit with &lt;code&gt;fit22.3&lt;/code&gt;, above, the parameters are a little different. The intercepts are largely the same. However, the coefficients for the &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; predictors have switched signs. This will be easier to see with a coefficient plot comparing &lt;code&gt;fit22.3&lt;/code&gt; and &lt;code&gt;fit22.5&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws for fit22.5
post &amp;lt;- posterior_samples(fit22.5)

# wrangle and combine the draws from the two models
bind_rows(
  # family = &amp;quot;cond_log_1&amp;quot;
  posterior_samples(fit22.3) %&amp;gt;% 
    pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
    mutate(name = str_remove(name, &amp;quot;b_mu&amp;quot;)) %&amp;gt;% 
    separate(name, into = c(&amp;quot;lambda&amp;quot;, &amp;quot;parameter&amp;quot;)) %&amp;gt;% 
    mutate(lambda = as.double(lambda) - 1,
           family = &amp;quot;cond_log_1&amp;quot;) %&amp;gt;% 
    select(family, lambda, parameter, value),
  # family = &amp;quot;sratio&amp;quot;
  post %&amp;gt;% 
    pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
    mutate(name = str_remove(name, &amp;quot;b_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
    separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;%
    mutate(lambda = as.double(lambda),
           family = &amp;quot;sratio&amp;quot;) %&amp;gt;% 
    select(family, lambda, parameter, value)
) %&amp;gt;% 
  mutate(lambda    = str_c(&amp;quot;lambda==&amp;quot;, lambda),
         parameter = case_when(str_detect(parameter, &amp;quot;Intercept&amp;quot;) ~ &amp;quot;beta[0]&amp;quot;,
                               str_detect(parameter, &amp;quot;X1&amp;quot;)        ~ &amp;quot;beta[1]&amp;quot;,
                               str_detect(parameter, &amp;quot;X2&amp;quot;)        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = value, y = family)) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  stat_pointinterval(.width = .95, point_size = 1.5, size = 1) +
  labs(x = &amp;quot;marginal posterior&amp;quot;,
       y = NULL) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even though the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; parameters switched signs, their magnitudes are about the same. Thus, if we want to use our &lt;code&gt;fit22.5&lt;/code&gt; to plot the thresholds as in Figure 22.6, we’ll have to update our threshold formula to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_2 = (\color{red}{+} \color{black}{\beta_0 / \beta_2) + (\beta_1 / \beta_2)x_1.}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With that adjustment in line, here’s our updated version of the left panel of Figure 22.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22)

post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  # this line is different
  mutate(intercept = Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though we used a different likelihood and a different formula for the thresholds, we got same basic model results. They’re just parameterized in a slightly different way. The nice thing with the &lt;code&gt;family = sratio&lt;/code&gt; approach is all of the typical &lt;strong&gt;brms&lt;/strong&gt; post processing functions will work out of the box. For example, here’s the posterior predictive check via &lt;code&gt;pp_check()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp_check(fit22.5, 
         type = &amp;quot;bars&amp;quot;, 
         ndraws = 100, 
         size = 1/2, 
         fatten = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Also note how the information criteria estimates for the two approaches are essentially the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.5 &amp;lt;- add_criterion(fit22.5, criterion = &amp;quot;waic&amp;quot;)

loo_compare(fit22.3, fit22.5, criterion = &amp;quot;waic&amp;quot;) %&amp;gt;% 
  print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  
## fit22.3    0.0       0.0  -230.8      16.8          9.3    1.1     461.6
## fit22.5   -0.1       0.1  -230.9      16.8          9.4    1.1     461.8
##         se_waic
## fit22.3   33.6 
## fit22.5   33.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A limitation of the &lt;code&gt;family = sratio&lt;/code&gt; method for conditional logistic models is it requires a simple binary-divisions hierarchy that resembles the one we just used, the one in the left panel of Figure 22.2. It is not well suited for the more complicated hierarchy displayed in the right panel of Figure 22.2, nor will it help you make sense of data generated by that kind of mechanism. For example, consider what happens when we try to use &lt;code&gt;family = sratio&lt;/code&gt; with the &lt;code&gt;d4&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make an ordinal version of Y
d4 &amp;lt;-
  d4 %&amp;gt;% 
  mutate(Y_ord = ordered(Y))

# fit the model
fit22.6 &amp;lt;-
  brm(data = d4, 
      family = sratio,
      Y_ord ~ 1 + cs(X1) + cs(X2),
      prior = c(prior(normal(0, 20), class = Intercept),
                prior(normal(0, 20), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      file = &amp;quot;fits/fit22.06&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: sratio 
##   Links: mu = logit; disc = identity 
## Formula: Y_ord ~ 1 + cs(X1) + cs(X2) 
##    Data: d4 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -6.03      0.76    -7.66    -4.67 1.00     2040     2169
## Intercept[2]    -5.40      0.68    -6.85    -4.18 1.00     1757     2104
## Intercept[3]    -1.02      0.24    -1.50    -0.59 1.00     2666     2522
## X1[1]            2.71      0.47     1.85     3.70 1.00     2251     2266
## X1[2]            5.56      0.70     4.30     7.04 1.00     1900     2424
## X1[3]           -3.03      0.39    -3.84    -2.32 1.00     2565     2447
## X2[1]            2.38      0.43     1.60     3.26 1.00     2525     2449
## X2[2]           -1.16      0.29    -1.75    -0.63 1.00     3524     2736
## X2[3]           -3.13      0.37    -3.89    -2.46 1.00     2718     2738
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look at the parameter summary, nothing obviously bad happened. The computer didn’t crash or anything. To get a better sense of the damage, we plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws for fit22.6
post &amp;lt;- posterior_samples(fit22.6)

# 2D thresholds on the left
set.seed(22)

p1 &amp;lt;- 
  post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  # still using the adjusted formula for the thresholds
  mutate(intercept = Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)

# marginal posteriors on the right
p2 &amp;lt;-
post %&amp;gt;% 
  pivot_longer(cols = c(-disc, -lp__)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;)%&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;% 
  mutate(lambda    = case_when(lambda == &amp;quot;1&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;,
                               lambda == &amp;quot;2&amp;quot; ~ &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;,
                               lambda == &amp;quot;3&amp;quot; ~ &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;),
         parameter = case_when(parameter == &amp;quot;Intercept&amp;quot; ~ &amp;quot;beta[0]&amp;quot;,
                               parameter == &amp;quot;X1&amp;quot;        ~ &amp;quot;beta[1]&amp;quot;,
                               parameter == &amp;quot;X2&amp;quot;        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95, size = 1,
                    normalize = &amp;quot;panels&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)

# combine, entitle, and display the results
(p1 + p2) &amp;amp; 
  plot_layout(widths = c(1, 2)) &amp;amp;
  plot_annotation(title = &amp;quot;Figure 22.7, lower half&amp;quot;,
                  subtitle = &amp;quot;Results from the conditional logistic model fit to the d4 data via the sequential-ordinal approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We ended up with our version of the lower half of Figure 22.7. As with the previous model, the sequential-ordinal approach reverses the signs for the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; parameters, which isn’t a big deal as long as you keep that in mind. The larger issue is that the thresholds displayed in the left panel do a poor job differentiating among the various &lt;code&gt;Y&lt;/code&gt; categories. The model underlying those thresholds is a bad match for the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-wrap-up.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Conditional logistic wrap-up.&lt;/h5&gt;
&lt;p&gt;To wrap this section up, we walked through approaches for fitting conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. First we considered Singmann’s method for using the &lt;strong&gt;brms&lt;/strong&gt; custom family functionality to define bespoke likelihood functions. Though it requires a lot of custom coding and an above-average knowledge of the inner workings of &lt;strong&gt;brms&lt;/strong&gt; and Stan, the custom family approach is very general and will possibly work for all your conditional-logistic needs. Then we considered Ben-Shachar sequential-ordinal approach. Ben-Shachar’s insight was that if we are willing to augment the nominal data with the &lt;code&gt;ordered()&lt;/code&gt; function, modeling them with a sequential-ordinal model via &lt;code&gt;family = sratio&lt;/code&gt; will return near equivalent results to the conditional logistic method. Though this method is attractive in that it uses a built-in likelihood and thus avoids a lot of custom coding, it is limited in that it will only handle nominal data which are well described by the simple binary-divisions hierarchy displayed in the left panel of Figure 22.2.&lt;/p&gt;
&lt;p&gt;In closing, I would like to thank Singmann and Ben-Shachar for their time and insights. 🍻 I could not have finished this section without them. If you would like more examples of both of their methods applied to different data sets, check out the Stan forum thread called &lt;a href=&#34;https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433&#34;&gt;Nominal data and Kruschke’s “conditional logistic” approach&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.1 (2021-08-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bayesplot_1.8.1      posterior_1.1.0.9000 patchwork_1.1.1     
##  [4] tidybayes_3.0.1      brms_2.16.2          Rcpp_1.0.7          
##  [7] forcats_0.5.1        stringr_1.4.0        dplyr_1.0.7         
## [10] purrr_0.3.4          readr_2.0.1          tidyr_1.1.3         
## [13] tibble_3.1.6         ggplot2_3.3.5        tidyverse_1.3.1     
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.3.0      RcppEigen_0.3.3.9.1 
##   [4] plyr_1.8.6           igraph_1.2.6         svUnit_1.0.6        
##   [7] splines_4.1.1        crosstalk_1.1.1      TH.data_1.0-10      
##  [10] rstantools_2.1.1     inline_0.3.19        digest_0.6.28       
##  [13] htmltools_0.5.2      rsconnect_0.8.24     fansi_0.5.0         
##  [16] BH_1.75.0-0          magrittr_2.0.1       checkmate_2.0.0     
##  [19] tzdb_0.1.2           modelr_0.1.8         RcppParallel_5.1.4  
##  [22] matrixStats_0.61.0   vroom_1.5.4          xts_0.12.1          
##  [25] sandwich_3.0-1       prettyunits_1.1.1    colorspace_2.0-2    
##  [28] rvest_1.0.1          ggdist_3.0.0         haven_2.4.3         
##  [31] xfun_0.25            callr_3.7.0          crayon_1.4.2        
##  [34] jsonlite_1.7.2       lme4_1.1-27.1        survival_3.2-11     
##  [37] zoo_1.8-9            glue_1.5.0           gtable_0.3.0        
##  [40] emmeans_1.6.3        V8_3.4.2             distributional_0.2.2
##  [43] pkgbuild_1.2.0       rstan_2.26.3         abind_1.4-5         
##  [46] scales_1.1.1         mvtnorm_1.1-2        emo_0.0.0.9000      
##  [49] DBI_1.1.1            miniUI_0.1.1.1       viridisLite_0.4.0   
##  [52] xtable_1.8-4         HDInterval_0.2.2     diffobj_0.3.4       
##  [55] bit_4.0.4            stats4_4.1.1         StanHeaders_2.26.3  
##  [58] DT_0.19              htmlwidgets_1.5.3    httr_1.4.2          
##  [61] threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.2      
##  [64] pkgconfig_2.0.3      loo_2.4.1            farver_2.1.0        
##  [67] sass_0.4.0           dbplyr_2.1.1         utf8_1.2.2          
##  [70] labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.12        
##  [73] reshape2_1.4.4       later_1.3.0          munsell_0.5.0       
##  [76] cellranger_1.1.0     tools_4.1.1          cli_3.1.0           
##  [79] generics_0.1.1       broom_0.7.9          ggridges_0.5.3      
##  [82] evaluate_0.14        fastmap_1.1.0        yaml_2.2.1          
##  [85] bit64_4.0.5          processx_3.5.2       knitr_1.33          
##  [88] fs_1.5.0             nlme_3.1-152         mime_0.11           
##  [91] projpred_2.0.2       xml2_1.3.2           compiler_4.1.1      
##  [94] shinythemes_1.2.0    rstudioapi_0.13      gamm4_0.2-6         
##  [97] curl_4.3.2           reprex_2.0.1         bslib_0.3.0         
## [100] stringi_1.7.4        highr_0.9            ps_1.6.0            
## [103] blogdown_1.5         Brobdingnag_1.2-6    lattice_0.20-44     
## [106] Matrix_1.3-4         nloptr_1.2.2.2       markdown_1.1        
## [109] shinyjs_2.0.0        tensorA_0.36.2       vctrs_0.3.8         
## [112] pillar_1.6.4         lifecycle_1.0.1      jquerylib_0.1.4     
## [115] bridgesampling_1.1-2 estimability_1.3     httpuv_1.6.2        
## [118] extraDistr_1.9.1     R6_2.5.1             bookdown_0.23       
## [121] promises_1.2.0.1     gridExtra_2.3        codetools_0.2-18    
## [124] boot_1.3-28          colourpicker_1.1.0   MASS_7.3-54         
## [127] gtools_3.9.2         assertthat_0.2.1     withr_2.4.2         
## [130] shinystan_2.5.0      multcomp_1.4-17      mgcv_1.8-36         
## [133] parallel_4.1.1       hms_1.1.0            grid_4.1.1          
## [136] coda_0.19-4          minqa_1.2.4          rmarkdown_2.10      
## [139] shiny_1.6.0          lubridate_1.7.10     base64enc_0.1-3     
## [142] dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-Bürkner2021Define&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Define custom response distributions with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerOrdinalRegressionModels2019&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C., &amp;amp; Vuorre, M. (2019). Ordinal regression models in psychology: &lt;span&gt;A&lt;/span&gt; tutorial. &lt;em&gt;Advances in Methods and Practices in Psychological Science&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;(1), 77–101. &lt;a href=&#34;https://doi.org/10.1177/2515245918823199&#34;&gt;https://doi.org/10.1177/2515245918823199&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multilevel models and the index-variable approach</title>
      <link>/post/2020-12-09-multilevel-models-and-the-index-variable-approach/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-12-09-multilevel-models-and-the-index-variable-approach/</guid>
      <description>
&lt;script src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The set-up&lt;/h2&gt;
&lt;p&gt;PhD candidate Huaiyu Liu recently reached out with a question about how to analyze clustered data. Liu’s basic setup was an experiment with four conditions. The dependent variable was binary, where success = 1, fail = 0. Each participant completed multiple trials under each of the four conditions. The catch was Liu wanted to model those four conditions with a multilevel model using the index-variable approach McElreath advocated for in the second edition of his text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Like any good question, this one got my gears turning. Thanks, Liu! The purpose of this post will be to show how to model data like this two different ways.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;In this post, I’m presuming you are familiar with Bayesian multilevel models and with logistic regression. All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. The statistical models will be fit with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. We’ll also make a little use of the &lt;strong&gt;tidybayes&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;rethinking&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-rethinking&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020b&lt;/a&gt;)&lt;/span&gt; packages. If you need to shore up, I list some educational resources at the &lt;a href=&#34;#next-steps&#34;&gt;end of the post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Load the primary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)
library(tidybayes)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for Liu’s question had the same basic structure as the &lt;code&gt;chimpanzees&lt;/code&gt; data from the &lt;strong&gt;rethinking&lt;/strong&gt; package. Happily, it’s also the case that Liu wanted to fit a model that was very similar to model &lt;code&gt;m14.3&lt;/code&gt; from Chapter 14 of McElreath’s text. Here we’ll load the data and wrangle a little.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(chimpanzees, package = &amp;quot;rethinking&amp;quot;)
d &amp;lt;- chimpanzees
rm(chimpanzees)

# wrangle
d &amp;lt;-
  d %&amp;gt;% 
  mutate(actor = factor(actor),
         treatment = factor(1 + prosoc_left + 2 * condition),
         # this will come in handy, later
         labels    = factor(treatment,
                            levels = 1:4,
                            labels = c(&amp;quot;r/n&amp;quot;, &amp;quot;l/n&amp;quot;, &amp;quot;r/p&amp;quot;, &amp;quot;l/p&amp;quot;)))

glimpse(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 504
## Columns: 10
## $ actor        &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ recipient    &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ condition    &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ block        &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5,…
## $ trial        &amp;lt;int&amp;gt; 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 4…
## $ prosoc_left  &amp;lt;int&amp;gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,…
## $ chose_prosoc &amp;lt;int&amp;gt; 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,…
## $ pulled_left  &amp;lt;int&amp;gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,…
## $ treatment    &amp;lt;fct&amp;gt; 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1,…
## $ labels       &amp;lt;fct&amp;gt; r/n, r/n, l/n, r/n, l/n, l/n, l/n, l/n, r/n, r/n, r/n, l/n, r/n, l/n, r/n, l/…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The focal variable will be &lt;code&gt;pulled_left&lt;/code&gt;, which is binary and coded yes = 1, no = 0. We have four experimental conditions, which are indexed &lt;code&gt;1&lt;/code&gt; through &lt;code&gt;4&lt;/code&gt; in the &lt;code&gt;treatment&lt;/code&gt; variable. The shorthand labels for those conditions are saved as &lt;code&gt;labels&lt;/code&gt;. These data are simple in that there are only seven participants, who are indexed in the &lt;code&gt;actor&lt;/code&gt; column.&lt;/p&gt;
&lt;p&gt;Within the generalized linear model framework, we typically model binary variables with binomial likelihood&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. When you use the conventional link function, you can call this &lt;em&gt;logistic regression&lt;/em&gt;. When you have a binary variable, the parameter of interest is the probability of a 1 in your criterion variable. When you want a quick sample statistic, you can estimate those probabilities with the mean. To get a sense of the data, here are the sample probabilities &lt;code&gt;pulled_left == 1&lt;/code&gt; for each of our seven participants, by the four levels of &lt;code&gt;treatment&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  mutate(treatment = str_c(&amp;quot;treatment &amp;quot;, treatment)) %&amp;gt;% 
  group_by(actor, treatment) %&amp;gt;% 
  summarise(p = mean(pulled_left) %&amp;gt;% round(digits = 2)) %&amp;gt;% 
  pivot_wider(values_from = p, names_from = treatment) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;actor&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models&lt;/h2&gt;
&lt;p&gt;We are going to analyze these data two kinds of multilevel models. The first way is the direct analogue to McElreath’s model &lt;code&gt;m14.3&lt;/code&gt;; it’ll be a multilevel model using the index-variable approach for the population-level intercepts. The second way is a multilevel Bayesian alternative to the ANOVA, based on Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text.&lt;/p&gt;
&lt;p&gt;However, some readers might benefit from a review of what I even mean by the “index-variable” approach. This approach is uncommon in my field of clinical psychology, for example. So before we get down to business, we’ll clear that up by contrasting it with the widely-used dummy-variable approach.&lt;/p&gt;
&lt;div id=&#34;warm-up-with-the-simple-index-variable-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Warm-up with the simple index-variable model.&lt;/h3&gt;
&lt;p&gt;Let’s forget the multilevel model for a moment. One of the more popular ways to use a categorical predictor variable is with the dummy-variable approach. Say we wanted to predict our criterion variable &lt;code&gt;pulled_left&lt;/code&gt; with &lt;code&gt;treatment&lt;/code&gt;, which is a four-category nominal variable. If we denote the number of categories &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;, &lt;code&gt;treatment&lt;/code&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(K = 4\)&lt;/span&gt; nominal variable. The dummy-variable approach would be to break &lt;code&gt;treatment&lt;/code&gt; into &lt;span class=&#34;math inline&#34;&gt;\(K - 1\)&lt;/span&gt; binary variables, which we’d simultaneously enter into the model. Say we broke &lt;code&gt;treatment&lt;/code&gt; into three dummies with the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;% 
  mutate(d2 = if_else(treatment == 2, 1, 0),
         d3 = if_else(treatment == 3, 1, 0),
         d4 = if_else(treatment == 4, 1, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dummy variables &lt;code&gt;d2&lt;/code&gt;, &lt;code&gt;d3&lt;/code&gt;, and &lt;code&gt;d4&lt;/code&gt; would capture the four levels of &lt;code&gt;treatment&lt;/code&gt; like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  distinct(treatment, d2, d3, d4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   treatment d2 d3 d4
## 1         1  0  0  0
## 2         2  1  0  0
## 3         3  0  1  0
## 4         4  0  0  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;d2 == 1&lt;/code&gt; only when &lt;code&gt;treatment == 2&lt;/code&gt;. Similarly, &lt;code&gt;d3 == 1&lt;/code&gt; only when &lt;code&gt;treatment == 3&lt;/code&gt; and &lt;code&gt;d4 == 1&lt;/code&gt; only when &lt;code&gt;treatment == 4&lt;/code&gt;. When &lt;code&gt;treatment == 1&lt;/code&gt;, all three dummies are &lt;code&gt;0&lt;/code&gt;, which makes &lt;code&gt;treatment == 1&lt;/code&gt; the reference category.&lt;/p&gt;
&lt;p&gt;You can write out the statistical model using these &lt;span class=&#34;math inline&#34;&gt;\(K - 1\)&lt;/span&gt; dummies as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \beta_0 + \beta_1 \text{d2}_i + \beta_2 \text{d3}_i + \beta_3 \text{d4}_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is both the “intercept” and the expected value for the first level of &lt;code&gt;treatment&lt;/code&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the expected change in value, relative to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, for the second level of &lt;code&gt;treatment&lt;/code&gt;. In the same way, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; are changes relative to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; for the third and fourth levels of &lt;code&gt;treatment&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;The index-variable approach takes a different stance. Rather than dividing &lt;code&gt;treatment&lt;/code&gt; into dummies, one simply allows each level of &lt;code&gt;treatment&lt;/code&gt; to have its own intercept. You can write that in statistical notation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \gamma_{\text{treatment}[i]},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\text{treatment}[i]\)&lt;/span&gt; subscript indicates the different levels of &lt;code&gt;treatment&lt;/code&gt;, which vary across cases &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, each get their own &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; parameter. Because &lt;code&gt;treatment&lt;/code&gt; has four levels, we end up with four &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;’s: &lt;span class=&#34;math inline&#34;&gt;\(\gamma_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_3\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_4\)&lt;/span&gt;. When you model intercepts in this way, none of the levels of &lt;code&gt;treatment&lt;/code&gt; end up as the reference category and none of the other levels of &lt;code&gt;treatment&lt;/code&gt; are parameterized in terms of deviations from the reference category. Each intercept is estimated in its own terms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick note on notation&lt;/strong&gt;: There’s nothing special about using the letter &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; for our index variable. We could just as easily have used &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;, or whatever. The only reason I’m using &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;, here, is because that’s what McElreath used for his model &lt;code&gt;m14.3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you’d like more practice with dummy variables, McElreath lectured on them &lt;a href=&#34;https://www.youtube.com/watch?v=e0tO64mtYMU&amp;amp;feature=youtu.be&amp;amp;t=3360&#34;&gt;here&lt;/a&gt;. If you’d like to hear McElreath walk out index variables a bit more, you can find that lecture &lt;a href=&#34;https://youtu.be/l_7yIUqWBmE?t=83&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mcelreaths-approach.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;McElreath’s approach.&lt;/h3&gt;
&lt;p&gt;Okay, now we’re up to speed on what Liu meant by wanting to fit a model with the index-variable approach, let’s see what that looks like in a multilevel model.&lt;/p&gt;
&lt;div id=&#34;the-statistical-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The statistical model.&lt;/h4&gt;
&lt;p&gt;Here’s how we might express McElreath’s index-variable approach to these data in statistical notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \gamma_{\text{treatment}[i]} + \alpha_{\text{actor}[i], \text{treatment}[i]} \\
\gamma_j &amp;amp; \sim \operatorname{Normal}(0, 1), \;\;\; \text{for } j = 1, \dots, 4 \\
\begin{bmatrix} \alpha_{j, 1} \\ \alpha_{j, 2} \\ \alpha_{j, 3} \\ \alpha_{j, 4} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \begin{pmatrix} \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \mathbf \Sigma_\text{actor} \end{pmatrix} \\
\mathbf \Sigma_\text{actor} &amp;amp; = \mathbf{S_\alpha R_\alpha S_\alpha} \\
\sigma_{\alpha, [1]}, \dots, \sigma_{\alpha, [4]} &amp;amp; \sim \operatorname{Exponential}(1) \\
\mathbf R_\alpha &amp;amp; \sim \operatorname{LKJ}(2).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this model, we have four population-level intercepts, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_1, \dots, \gamma_4\)&lt;/span&gt;, one for each of the four levels of &lt;code&gt;treatment&lt;/code&gt;. This is one of the critical features required by Liu’s question. &lt;code&gt;actor&lt;/code&gt; is our higher-level grouping variable. The third line spells out the priors for those four &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;’s. Though they all get the same prior in this model, you could use different priors for each, if you wanted.&lt;/p&gt;
&lt;p&gt;Going back to the second line, the term &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{actor}[i], \text{treatment}[i]}\)&lt;/span&gt; is meant to convey that each of the &lt;code&gt;treatment&lt;/code&gt; effects can vary by &lt;code&gt;actor&lt;/code&gt;. We can–and should–do this because each of our participants experienced each of the four levels of &lt;code&gt;treatment&lt;/code&gt; many times. The fourth line containing the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{MVNormal}(\cdot)\)&lt;/span&gt; operator might look intimidating. The vector on the left is just a way to list those four &lt;code&gt;actor&lt;/code&gt;-level deviations we just mentioned. We’ll be treating them much the same way you might treat a random intercept and slope in a multilevel growth model. That is, we presume they follow a multivariate normal distribution. Since these are all deviations, the 4-dimensional mean vector in our multivariate normal distribution contains four zeros. The spread around those zeros are controlled by the variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_\text{actor}\)&lt;/span&gt;. In the next line, we learn that &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_\text{actor}\)&lt;/span&gt; can be decomposed into two terms, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S_\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. It may not yet be clear by the notation, but &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S_\alpha\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; diagonal matrix of standard deviations,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf S_\alpha = \begin{bmatrix} \sigma_{\alpha, [1]} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_{\alpha, [2]} &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; \sigma_{\alpha, [3]} &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \sigma_{\alpha, [4]} \end{bmatrix}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a similar way, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; correlation matrix,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf R_\alpha = \begin{bmatrix} 1 &amp;amp; \rho_{\alpha, [1, 2]} &amp;amp; \rho_{\alpha, [1, 3]} &amp;amp; \rho_{\alpha, [1, 4]} \\ \rho_{\alpha, [2, 1]} &amp;amp; 1 &amp;amp; \rho_{\alpha, [2, 3]} &amp;amp; \rho_{\alpha, [2, 4]} \\ \rho_{\alpha, [3, 1]} &amp;amp; \rho_{\alpha, [3, 2]} &amp;amp; 1 &amp;amp; \rho_{\alpha, [3, 4]} \\ \rho_{\alpha, [4, 1]} &amp;amp; \rho_{\alpha, [4, 2]} &amp;amp; \rho_{\alpha, [4, 3]} &amp;amp; 1 \end{bmatrix}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As we see in the sixth line, all the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\alpha\)&lt;/span&gt; parameters have individual &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Exponential}(1)\)&lt;/span&gt; priors. The final line shows the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt; matrix has the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{LKJ}(2)\)&lt;/span&gt; prior. Though you could certainly use different priors, here we’re sticking close to those McElreath used in his text.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fit the model.&lt;/h4&gt;
&lt;p&gt;Though the statistical model might look intimidating, we can fit it pretty easily with &lt;code&gt;brms::brm()&lt;/code&gt;. We’ll call this &lt;code&gt;fit1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- 
  brm(data = d, 
      family = binomial,
      pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor),
      prior = c(prior(normal(0, 1), class = b),
                prior(exponential(1), class = sd),
                prior(lkj(2), class = cor)),
      cores = 4, seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From a syntax perspective, the important parts were the two occurrences of &lt;code&gt;0 + treatment&lt;/code&gt; in the model &lt;code&gt;formula&lt;/code&gt; line. The first occurrence was how we told &lt;strong&gt;brms&lt;/strong&gt; we wanted our population-level intercept to be indexed by the four levels of &lt;code&gt;treatment&lt;/code&gt;. The second occurrence was where we told &lt;strong&gt;brms&lt;/strong&gt; we wanted those to vary across our seven levels of &lt;code&gt;actor&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor) 
##    Data: d (Number of observations: 504) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~actor (Number of levels: 7) 
##                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(treatment1)                 1.36      0.48     0.69     2.52 1.00     1979     2494
## sd(treatment2)                 0.90      0.40     0.34     1.89 1.00     2188     2451
## sd(treatment3)                 1.84      0.56     1.00     3.15 1.00     2999     3036
## sd(treatment4)                 1.55      0.60     0.73     2.97 1.00     2550     2518
## cor(treatment1,treatment2)     0.42      0.28    -0.21     0.87 1.00     2511     2461
## cor(treatment1,treatment3)     0.52      0.25    -0.07     0.90 1.00     2313     2619
## cor(treatment2,treatment3)     0.48      0.27    -0.12     0.89 1.00     2989     3261
## cor(treatment1,treatment4)     0.44      0.27    -0.17     0.86 1.00     2515     3034
## cor(treatment2,treatment4)     0.44      0.28    -0.17     0.87 1.00     3205     3217
## cor(treatment3,treatment4)     0.57      0.24     0.00     0.92 1.00     3165     3234
## 
## Population-Level Effects: 
##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## treatment1     0.23      0.46    -0.66     1.16 1.00     1871     2504
## treatment2     0.66      0.36    -0.06     1.39 1.00     2780     2687
## treatment3    -0.02      0.56    -1.14     1.07 1.00     3017     2966
## treatment4     0.69      0.51    -0.32     1.72 1.00     3006     2575
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look at the lower level of the output, the four levels in the ‘Population-Level Effects’ section are the four levels of &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{\text{treatment}[i]}\)&lt;/span&gt; from our statistical formula. If you look above at the ‘Group-Level Effects’ section, the four lines beginning with “sd” correspond to our four &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\alpha, [1]}, \dots, \sigma_{\alpha, [4]}\)&lt;/span&gt; parameters. The correlations among those are depicted in the six rows beginning with “cor,” which correspond to the elements within the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;p&gt;It might help if we visualized the model in a plot. Here are the results depicted in a streamlined version of McElreath’s Figure 14.7 &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020a, p. 452&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# for annotation
text &amp;lt;-
  distinct(d, labels) %&amp;gt;% 
  mutate(actor = &amp;quot;actor[1]&amp;quot;,
         prop  = c(.07, .8, .08, .795))

# define the new data
nd &amp;lt;-
  d %&amp;gt;% 
  distinct(actor, condition, labels, prosoc_left, treatment)

# get the fitted draws
fitted(fit1,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  mutate(actor     = str_c(&amp;quot;actor[&amp;quot;, actor, &amp;quot;]&amp;quot;),
         condition = factor(condition)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = labels)) +
  geom_hline(yintercept = .5, color = &amp;quot;white&amp;quot;, linetype = 2) +
  # posterior predictions
  geom_line(aes(y = Estimate, group = prosoc_left),
            size = 3/4) +
  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, shape = condition),
                  fill = &amp;quot;transparent&amp;quot;, fatten = 10, size = 1/3, show.legend = F) + 
  # annotation for the conditions
  geom_text(data = text,
            aes(y = prop, label = labels), 
            size = 3) +
  scale_shape_manual(values = c(21, 19)) +
  scale_x_discrete(NULL, breaks = NULL) +
  scale_y_continuous(&amp;quot;proportion left lever&amp;quot;, breaks = 0:2 / 2, labels = c(&amp;quot;0&amp;quot;, &amp;quot;.5&amp;quot;, &amp;quot;1&amp;quot;)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~actor, nrow = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig1-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s an alternative version, this time faceting by treatment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit1,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  # add the gamma summaries
  left_join(
    tibble(treatment = as.character(1:4),
       gamma = inv_logit_scaled(fixef(fit1)[, 1])),
    by = &amp;quot;treatment&amp;quot;
  )  %&amp;gt;% 
  mutate(treatment = str_c(&amp;quot;treatment[&amp;quot;, treatment, &amp;quot;]&amp;quot;)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = reorder(actor, Estimate), y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(aes(yintercept = gamma),
             color = &amp;quot;white&amp;quot;) +
  geom_pointrange(size = 1/3) +
  scale_x_discrete(breaks = NULL) +
  labs(x = &amp;quot;actor, rank orderred by their average probability&amp;quot;,
       y = &amp;quot;probability of pulling the lever&amp;quot;) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~treatment, nrow = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig2-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The horizontal white lines mark off the posterior means for the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{\text{treatment}[i]}\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kruschkes-approach.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kruschke’s approach.&lt;/h3&gt;
&lt;p&gt;One way to think about our &lt;code&gt;pulled_left&lt;/code&gt; data is they are grouped by two factors. The first factor is the experimental condition, &lt;code&gt;treatment&lt;/code&gt;. The second factor is participant, &lt;code&gt;actor&lt;/code&gt;. Now imagine you arrange the number of times &lt;code&gt;pulled_left == 1&lt;/code&gt; within the cells of a &lt;span class=&#34;math inline&#34;&gt;\(2 \times 2\)&lt;/span&gt; contingency table where the four levels of the &lt;code&gt;treatment&lt;/code&gt; factor are in the rows and the seven levels of &lt;code&gt;actor&lt;/code&gt; are in the columns. Here’s what that might look like in a tile plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  group_by(actor, treatment) %&amp;gt;% 
  summarise(count = sum(pulled_left)) %&amp;gt;% 
  mutate(treatment = factor(treatment, levels = 4:1)) %&amp;gt;% 
  
  ggplot(aes(x = actor, y = treatment, fill = count, label = count)) +
  geom_tile() +
  geom_text(aes(color = count &amp;gt; 6)) +
  scale_color_viridis_d(option = &amp;quot;E&amp;quot;, direction = -1, breaks = NULL) +
  scale_fill_viridis_c(option = &amp;quot;E&amp;quot;, limits = c(0, 18), breaks = NULL) +
  scale_x_discrete(position = &amp;quot;top&amp;quot;, expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  theme(axis.ticks = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig3-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this arrangement, we can model &lt;span class=&#34;math inline&#34;&gt;\(\text{left_pull}_i \sim \operatorname{Binomial}(n_i = 1, p_i)\)&lt;/span&gt;, with three hierarchical grouping factors. The first will be &lt;code&gt;actor&lt;/code&gt;, the second will be &lt;code&gt;treatment&lt;/code&gt;, and the third will be their interaction. Kruschke gave a general depiction of this kind of statistical model in Figure 20.2&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; of his text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke, 2015, p. 588&lt;/a&gt;)&lt;/span&gt;. However, I generally prefer expressing my models using statistical notation similar to McElreath. Though I’m not exactly sure how McElreath would express a model like this, here’s my best attempt using his style of notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \gamma + \alpha_{\text{actor}[i]} + \alpha_{\text{treatment}[i]} + \alpha_{\text{actor}[i] \times \text{treatment}[i]} \\
\gamma &amp;amp; \sim \operatorname{Normal}(0, 1) \\
\alpha_\text{actor}  &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{actor}) \\
\alpha_\text{treatment}  &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{treatment}) \\
\alpha_{\text{actor} \times \text{treatment}} &amp;amp; \sim \operatorname{Normal}(0, \sigma_{\text{actor} \times \text{treatment}}) \\
\sigma_\text{actor} &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_\text{treatment} &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_{\text{actor} \times \text{treatment}} &amp;amp; \sim \operatorname{Exponential}(1).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is our overall intercept and the three &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{&amp;lt;group&amp;gt;}[i]}\)&lt;/span&gt; terms are our multilevel deviations around that overall intercept. Notice that because &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; nas no &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; index, we are not technically using the index variable approach we discussed earlier in this post. But we are still indexing the four levels of &lt;code&gt;treatment&lt;/code&gt; by way of higher-level deviations depicted by the &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{treatment}[i]}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{actor}[i] \times \text{treatment}[i]}\)&lt;/span&gt; parameters in the second line. In contrast to our first model based on McElreath’s work, notice our three &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{&amp;lt;group&amp;gt;}[i]}\)&lt;/span&gt; term are all modeled as &lt;em&gt;univariate&lt;/em&gt; normal. This makes this model an extension of the cross-classified model.&lt;/p&gt;
&lt;div id=&#34;fit-the-second-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fit the second model.&lt;/h4&gt;
&lt;p&gt;Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;. We’ll call it &lt;code&gt;fit2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- 
  brm(data = d, 
      family = binomial,
      pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | treatment) + (1 | actor:treatment),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(exponential(1), class = sd)),
      cores = 4, seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | treatment) + (1 | actor:treatment) 
##    Data: d (Number of observations: 504) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~actor (Number of levels: 7) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     2.00      0.66     1.07     3.68 1.00     1270     1894
## 
## ~actor:treatment (Number of levels: 28) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.25      0.18     0.01     0.70 1.00     1296     1810
## 
## ~treatment (Number of levels: 4) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.53      0.36     0.07     1.46 1.00     1144     1032
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.46      0.63    -0.81     1.71 1.00      989     1969
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a model like this, a natural first question is: &lt;em&gt;Where is the variance at?&lt;/em&gt; We can answer that by comparing the three lines in the output from the ‘Group-Level Effects’ section. It might be easier if we plotted the posteriors for those &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{&amp;lt;group&amp;gt;}\)&lt;/span&gt; parameters, instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

posterior_samples(fit2) %&amp;gt;% 
  select(starts_with(&amp;quot;sd&amp;quot;)) %&amp;gt;% 
  set_names(str_c(&amp;quot;sigma[&amp;quot;, c(&amp;quot;actor&amp;quot;, &amp;quot;actor~X~treatment&amp;quot;, &amp;quot;treatment&amp;quot;), &amp;quot;]&amp;quot;)) %&amp;gt;% 
  pivot_longer(everything()) %&amp;gt;% 
  mutate(name = factor(name,
                       levels = str_c(&amp;quot;sigma[&amp;quot;, c(&amp;quot;actor~X~treatment&amp;quot;, &amp;quot;treatment&amp;quot;, &amp;quot;actor&amp;quot;), &amp;quot;]&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = value, y = name)) +
  stat_halfeye(.width = .95, size = 1/2) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  xlab(&amp;quot;marginal posterior (log-odds scale)&amp;quot;) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig4-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like most of the action was between the seven actors. But there was some variation among the four levels of &lt;code&gt;treatment&lt;/code&gt; and even the interaction between the two factors wasn’t completely pushed against zero.&lt;/p&gt;
&lt;p&gt;Okay, here’s an alternative version of the first plot from &lt;code&gt;fit1&lt;/code&gt;, above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit2,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  mutate(actor     = str_c(&amp;quot;actor[&amp;quot;, actor, &amp;quot;]&amp;quot;),
         condition = factor(condition)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = labels)) +
  geom_hline(yintercept = .5, color = &amp;quot;white&amp;quot;, linetype = 2) +
  # posterior predictions
  geom_line(aes(y = Estimate, group = prosoc_left),
            size = 3/4) +
  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, shape = condition),
                  fill = &amp;quot;transparent&amp;quot;, fatten = 10, size = 1/3, show.legend = F) + 
  scale_shape_manual(values = c(21, 19)) +
  scale_x_discrete(NULL, breaks = NULL) +
  scale_y_continuous(&amp;quot;proportion left lever&amp;quot;, limits = 0:1,
                     breaks = 0:2 / 2, labels = c(&amp;quot;0&amp;quot;, &amp;quot;.5&amp;quot;, &amp;quot;1&amp;quot;)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~actor, nrow = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig5-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The two models made similar predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-not-make-the-horse-race-official&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why not make the horse race official?&lt;/h3&gt;
&lt;p&gt;Just for kicks and giggles, we’ll compare the two models with the LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- add_criterion(fit1, criterion = &amp;quot;loo&amp;quot;)
fit2 &amp;lt;- add_criterion(fit2, criterion = &amp;quot;loo&amp;quot;)

# LOO differences
loo_compare(fit1, fit2) %&amp;gt;% print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic
## fit2    0.0       0.0  -266.8      9.5        12.2    0.6    533.7   19.1  
## fit1   -4.5       3.0  -271.3      9.7        19.3    1.2    542.6   19.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# LOO weights
model_weights(fit1, fit2, weights = &amp;quot;loo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       fit1       fit2 
## 0.01111874 0.98888126&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like there’s a little bit of an edge for the Kruschke’s multilevel ANOVA model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-whats-the-difference-anyway&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;But what’s the difference, anyway?&lt;/h3&gt;
&lt;p&gt;Rather than attempt to chose one model based on information criteria, we might back up and focus on the conceptual differences between the two models.&lt;/p&gt;
&lt;p&gt;Our first model, based on McElreath’s index-variable approach, explicitly emphasized the four levels of &lt;code&gt;treatment&lt;/code&gt;. Each one got its own &lt;span class=&#34;math inline&#34;&gt;\(\gamma_j\)&lt;/span&gt;. By modeling those &lt;span class=&#34;math inline&#34;&gt;\(\gamma_j\)&lt;/span&gt;’s with the multivariate normal distribution, we also got an explicit accounting of the &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; correlation structure for those parameters.&lt;/p&gt;
&lt;p&gt;Our second model, based on Kruschke’s multilevel ANOVA approach, took a more general perspective. By modeling &lt;code&gt;actor&lt;/code&gt;, &lt;code&gt;treatment&lt;/code&gt; and their interaction as higher-level grouping factors, &lt;code&gt;fit2&lt;/code&gt; conceptualized both participants and experimental conditions as coming from populations of potential participants and conditions, respectively. No longer are those four &lt;code&gt;treatment&lt;/code&gt; levels inherently special. They’re just the four we happen to have in this iteration of the experiment. Were we to run the experiment again, after all, we might want to alter them a little. The &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{treatment}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{actor} \times \text{treatment}}\)&lt;/span&gt; parameters can help give us a sense of how much variation we’d expect among other similar experimental conditions.&lt;/p&gt;
&lt;p&gt;Since I’m not a chimpanzee researcher, I’m in no position to say which perspective is better for these data. At a predictive level, the models perform similarly. But if I were a researcher wanting to analyze these data or others with a similar structure, I’d want to think clearly about what kinds of points I’d want to make to my target audience. Would I want to make focused points about the four levels of &lt;code&gt;treatment&lt;/code&gt;, or would it make sense to generalize from those four levels to other similar conditions? Each model has its rhetorical strengths and weaknesses.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;If you’re new to the Bayesian multilevel model, I recommend the introductory text by either McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020a&lt;/a&gt;)&lt;/span&gt; or Kruschke &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke, 2015&lt;/a&gt;)&lt;/span&gt;. I have ebook versions of both wherein I translated their code into the &lt;strong&gt;tidyverse&lt;/strong&gt; style and fit their models with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;, &lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2020&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt;. Both McElreath and Kruschke have blogs (&lt;a href=&#34;https://elevanth.org/blog/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://doingbayesiandataanalysis.blogspot.com/&#34;&gt;here&lt;/a&gt;). Also, though it doesn’t cover the multilevel model, you can get a lot of practice with Bayesian regression with the new book by Gelman, Hill, and Vehtari &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. And for more hot Bayesian regression talk, you always have the Stan forums, which even have a &lt;a href=&#34;https://discourse.mc-stan.org/c/interfaces/brms/36&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; section&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5    
##  [7] purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] splines_4.0.4        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] viridisLite_0.3.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3       
##  [53] arrayhelpers_1.1-0   ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3     
##  [57] loo_2.4.1            dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2      
##  [61] tidyselect_1.1.0     rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1       
##  [65] munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [69] generics_0.1.0       broom_0.7.5          ggridges_0.5.2       evaluate_0.14       
##  [73] fastmap_1.0.1        yaml_2.2.1           processx_3.4.5       knitr_1.31          
##  [77] fs_1.5.0             nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [81] xml2_1.3.2           compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [85] rstudioapi_0.13      gamm4_0.2-6          curl_4.3             reprex_0.3.0        
##  [89] statmod_1.4.35       stringi_1.5.3        highr_0.8            ps_1.6.0            
##  [93] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2        
##  [97] nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
## [101] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3    
## [105] httpuv_1.5.4         R6_2.5.0             bookdown_0.21        promises_1.1.1      
## [109] gridExtra_2.3        codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0  
## [113] MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1     withr_2.4.1         
## [117] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [125] rmarkdown_2.7        shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3     
## [129] dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-Bürkner2021Parameterization&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Parameterization of response distributions in brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzDoingBayesianData2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis in brms and the tidyverse&lt;/em&gt; (version 0.3.0). &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Statistical rethinking with brms, Ggplot2, and the tidyverse: &lt;span&gt;Second&lt;/span&gt; edition&lt;/em&gt; (version 0.1.1). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020a). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rethinking&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;rethinking&lt;/span&gt; &lt;span&gt;R&lt;/span&gt; package&lt;/em&gt;. &lt;a href=&#34;https://xcelab.net/rm/software/&#34;&gt;https://xcelab.net/rm/software/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Given the data are coded 0/1, one could also use the Bernoulli likelihood &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2021&lt;/a&gt;, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html#binary-and-count-data-models&#34; role=&#34;doc-biblioref&#34;&gt;&lt;em&gt;Binary and count data models&lt;/em&gt;&lt;/a&gt;)&lt;/span&gt;. I’m just partial to the binomial.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This is the typical parameterization for multilevel models fit with &lt;strong&gt;brms&lt;/strong&gt;. Though he used different notation, Bürkner spelled this all out in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; overview paper, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_overview.pdf&#34;&gt;&lt;em&gt;brms: An R package for Bayesian multilevel models using Stan&lt;/em&gt;&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The careful reader might notice that the models Kruschke focused on in Chapter 20 were all based on the Gaussian likelihood. So in the most technical sense, the model in Figure 20.2 is not a perfect match to our &lt;code&gt;fit2&lt;/code&gt;. I’m hoping my readers might look past those details to see the more general point. For more practice, &lt;a href=&#34;https://bookdown.org/content/3686/count-predicted-variable.html#example-hair-eye-go-again&#34;&gt;Section 24.2&lt;/a&gt; and &lt;a href=&#34;https://bookdown.org/content/3686/count-predicted-variable.html#example-interaction-contrasts-shrinkage-and-omnibus-test&#34;&gt;Section 24.3&lt;/a&gt; of my translation of Kruschke’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;)&lt;/span&gt; show variants of this model type using the Poisson likelihood. In Section &lt;a href=&#34;https://bookdown.org/content/3686/count-predicted-variable.html#log-linear-models-for-contingency-tables-bonus-alternative-parameterization&#34;&gt;24.4&lt;/a&gt; you can even find a variant using the aggregated binomial likelihood.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Make model diagrams, Kruschke style</title>
      <link>/post/2020-03-09-make-model-diagrams-kruschke-style/</link>
      <pubDate>Mon, 09 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-03-09-make-model-diagrams-kruschke-style/</guid>
      <description>
&lt;script src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;You too can make model diagrams with the &lt;strong&gt;tidyverse&lt;/strong&gt; and &lt;strong&gt;patchwork&lt;/strong&gt; packages. Here’s how.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diagrams-can-help-us-understand-statistical-models.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagrams can help us understand statistical models.&lt;/h2&gt;
&lt;p&gt;I’ve been working through John Kruschke’s &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;&lt;em&gt;Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan&lt;/em&gt;&lt;/a&gt; and translating it into &lt;strong&gt;brms&lt;/strong&gt; and &lt;strong&gt;tidyverse&lt;/strong&gt;-style workflow. At this point, the bulk of the work is done and you can check it out at &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;. One of Kruschke’s unique contributions was the way he used diagrams to depict his statistical models. Here’s an example from the text (Figure 8.2 on page 196):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kruschke_figure8.2.png&#34; style=&#34;width:33.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the figure’s caption, we read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Diagram of model with Bernoulli likelihood and beta prior. The pictures of the distributions are intended as stereotypical icons, and are not meant to indicate the exact forms of the distributions. Diagrams like this should be scanned from the bottom up, starting with the data &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; and working upward through the likelihood function and prior distribution. Every arrow in the diagram has a corresponding line of code in a JAGS model specification.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Making diagrams like this is a bit of a challenge because even Kruchke, who is no &lt;strong&gt;R&lt;/strong&gt; slouch, used other software to make his diagrams. In the comments section from his blog post, &lt;a href=&#34;http://doingbayesiandataanalysis.blogspot.com/2012/05/graphical-model-diagrams-in-doing.html&#34;&gt;&lt;em&gt;Graphical model diagrams in Doing Bayesian Data Analysis versus traditional convention&lt;/em&gt;&lt;/a&gt;, Kruschke remarked he made these “‘by hand’ in OpenOffice.” If you look over to the &lt;a href=&#34;https://tex.stackexchange.com/questions/55869/how-to-produce-john-kruschkes-bayesian-model-diagrams-using-tikz-or-similar-too&#34;&gt;&lt;em&gt;How to produce John Kruschke’s Bayesian model diagrams using TikZ or similar tools?&lt;/em&gt;&lt;/a&gt; thread in StackExchange, you’ll find a workflow to make plots like this with TikZ. In a related &lt;a href=&#34;https://github.com/rasmusab/distribution_diagrams/blob/master/readme.md&#34;&gt;GitHub repo&lt;/a&gt;, the great Rasmus Bååth showed how to make diagrams like this with a combination of base &lt;strong&gt;R&lt;/strong&gt; and &lt;a href=&#34;https://www.libreoffice.org/discover/draw/&#34;&gt;Libre Office Draw&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It’d be nice, however, if one could make plots like this entirely within &lt;strong&gt;R&lt;/strong&gt;, preferably with a &lt;strong&gt;tidyverse&lt;/strong&gt;-style workflow. With help from the handy new &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;&lt;strong&gt;patchwork&lt;/strong&gt; package&lt;/a&gt;, I believe we can make it work. In this post, I’ll walk through a few attempts.&lt;/p&gt;
&lt;div id=&#34;my-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;My assumptions.&lt;/h3&gt;
&lt;p&gt;For the sake of this post, I’m presuming you’re familiar with &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html&#34;&gt;&lt;strong&gt;R&lt;/strong&gt;&lt;/a&gt;, aware of the &lt;a href=&#34;https://www.rstudio.com/resources/videos/data-science-in-the-tidyverse/&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt;, and have fit a &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&#34;&gt;Bayesian model&lt;/a&gt; or two.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;figure-8.2-keep-it-simple.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Figure 8.2: Keep it simple.&lt;/h3&gt;
&lt;p&gt;One way to conceptualize Figure 8.2, above, is to break it down into discrete parts. To my mind, there are five. Starting from the top and going down, we have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a plot of a beta density,&lt;/li&gt;
&lt;li&gt;an annotated arrow,&lt;/li&gt;
&lt;li&gt;a bar plot of Bernoulli data,&lt;/li&gt;
&lt;li&gt;another annotated arrow, and&lt;/li&gt;
&lt;li&gt;some text.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we make and save each component separately with &lt;strong&gt;ggplot2&lt;/strong&gt;, we can then combine them with &lt;strong&gt;patchwork&lt;/strong&gt; syntax. First we’ll load the necessary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork)
library(ggforce)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We won’t need &lt;strong&gt;ggforce&lt;/strong&gt; for this first diagram, but it’ll come in handy in the next section. Before we start making our subplots, we can use the &lt;code&gt;ggplot2::theme_set()&lt;/code&gt; function to adjust the global theme.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_grey() +
            theme_void() +
            theme(plot.margin = margin(0, 5.5, 0, 5.5)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we’ll make the 5 subplots, saving them as &lt;code&gt;p1&lt;/code&gt;, &lt;code&gt;p2&lt;/code&gt;, and so on. Since I’m presuming a working fluency with &lt;strong&gt;ggplot2&lt;/strong&gt; and &lt;strong&gt;tidyverse&lt;/strong&gt; basics, I’m not going to explain the plot code in detail. If you’re new to plotting like this, execute the code for a given plot line by line to see how each layer builds on the last.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot of a beta density
p1 &amp;lt;-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;beta&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .6,
           label = &amp;quot;italic(A)*&amp;#39;, &amp;#39;*italic(B)&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))

## an annotated arrow
# save our custom arrow settings
my_arrow &amp;lt;- arrow(angle = 20, length = unit(0.35, &amp;quot;cm&amp;quot;), type = &amp;quot;closed&amp;quot;)
p2 &amp;lt;-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = 0) %&amp;gt;%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .375, y = 1/3,
           label = &amp;quot;&amp;#39;~&amp;#39;&amp;quot;,
           size = 10, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(0, 1)

# bar plot of Bernoulli data
p3 &amp;lt;-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = &amp;quot;skyblue&amp;quot;, width = .4) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;Bernoulli&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .94,
           label = &amp;quot;theta&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-.75, 1.75) +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p4 &amp;lt;-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c(&amp;quot;&amp;#39;~&amp;#39;&amp;quot;, &amp;quot;italic(i)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = &amp;quot;Times&amp;quot;) +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1)

# some text
p5 &amp;lt;-
  tibble(x     = 1,
         y     = .5,
         label = &amp;quot;italic(y[i])&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = &amp;quot;Times&amp;quot;) +
  xlim(0, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ve saved each of the components as subplots, we can combine them with a little &lt;strong&gt;patchwork&lt;/strong&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout &amp;lt;- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 3, b = 3, l = 1, r = 1),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 7, l = 1, r = 1)
)

(p1 + p2 + p3 + p4 + p5) + 
  plot_layout(design = layout) &amp;amp;
  ylim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;192&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For that plot, the settings in the R Markdown code chunk were &lt;code&gt;fig.width = 2, fig.height = 3.5&lt;/code&gt;. An obvious difference between our plot and Kruschke’s is whereas he depicted the beta density with a line, we used &lt;code&gt;geom_area()&lt;/code&gt; to make the shape a solid blue. If you prefer Kruschke’s approach, just use something like &lt;code&gt;geom_line()&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;Within some of the &lt;code&gt;annotate()&lt;/code&gt; and &lt;code&gt;geom_text()&lt;/code&gt; functions, above, you may have noticed we set &lt;code&gt;parse = T&lt;/code&gt;. Though it wasn’t always necessary, it helps streamline the workflow. I found this particularly helpful when setting the coordinates for the tildes (i.e., the &lt;span class=&#34;math inline&#34;&gt;\(\sim\)&lt;/span&gt; signs).&lt;/p&gt;
&lt;p&gt;The main thing to focus on is the &lt;strong&gt;patchwork&lt;/strong&gt; syntax from that last code block. We combined the five subplots with the &lt;code&gt;(p1 + p2 + p3 + p4 + p5)&lt;/code&gt; code. It was the &lt;code&gt;plot_layout(design = layout)&lt;/code&gt; part and the associated code defining &lt;code&gt;layout&lt;/code&gt; that helped us arrange the subplots in the right order and according to the desired size ratios. For each subplot, we used the &lt;code&gt;t&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;l&lt;/code&gt;, and &lt;code&gt;r&lt;/code&gt; parameters to define the four bounds (top, bottom, left, and right) in overall plot grid. You can learn more about how this works from Thomas Lin Pedersen’s &lt;a href=&#34;https://patchwork.data-imaginist.com/articles/guides/layout.html&#34;&gt;&lt;em&gt;Controlling Layouts&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://patchwork.data-imaginist.com/reference/area.html&#34;&gt;&lt;em&gt;Specify a plotting area in a layout&lt;/em&gt;&lt;/a&gt; vignettes.&lt;/p&gt;
&lt;p&gt;Now we’ve covered the basics, it’s time to build.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;figure-9.1-add-an-offset-formula-and-some-curvy-lines.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Figure 9.1: Add an offset formula and some curvy lines.&lt;/h3&gt;
&lt;p&gt;For our next challenge, we’ll tackle Kruschke’s Figure 9.1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kruschke_figure9.1.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From a statistical perspective, this model is interesting in that it uses a hierarchical prior specification wherein the lower-level beta density is parameterized in terms &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; (mode) and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; (concentration). From a plotting perspective, adding more density and arrow subplots isn’t a big deal. But see how the &lt;span class=&#34;math inline&#34;&gt;\(\omega(K-2)+1, (1-\omega)(K-2)+1\)&lt;/span&gt; formula extends way out past the right bound of that second beta density? Also, check those wavy arrows right above. These require an amended workflow. Let’s go step by step. The top subplot is fairly simple.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;-
  tibble(x = seq(from = .01, to = .99, by = .01),
       d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&amp;gt;% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;beta&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .6,
           label = &amp;quot;italic(A[omega])*&amp;#39;, &amp;#39;*italic(B[omega])&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;192&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now things get wacky.&lt;/p&gt;
&lt;p&gt;We are going to make the formula and the wavy lies in one subplot. We can define the basic coordinates for the wavy lines with the &lt;code&gt;ggforce::geom_bspline()&lt;/code&gt; function (learn more &lt;a href=&#34;https://ggforce.data-imaginist.com/reference/geom_bspline.html&#34;&gt;here&lt;/a&gt;). For each line segment, we just need about 5 pairs of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; coordinates. There’s no magic solution to these coordinates. I came to them by trial and error. As far as the formula goes, it isn’t much more complicated from what we’ve been doing. It’s all just a bunch of &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/grDevices/html/plotmath.html&#34;&gt;plotmath syntax&lt;/a&gt;. The main deal is to notice how we set the &lt;code&gt;limits&lt;/code&gt; in &lt;code&gt;the scale_x_continuous()&lt;/code&gt; function to &lt;code&gt;(0, 2)&lt;/code&gt;. In the other plots, those are restricted to &lt;code&gt;0, 1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2 &amp;lt;-
  tibble(x = c(.5, .475, .26, .08, .06,
               .5, .55, .85, 1.15, 1.2),
         y = c(1, .7, .6, .5, .2,
               1, .7, .6, .5, .2),
         line = rep(letters[2:1], each = 5)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y)) +
  geom_bspline(aes(color = line),
               size = 2/3, show.legend = F) + 
  annotate(geom = &amp;quot;text&amp;quot;,
           x = 0, y = .125,
           label = &amp;quot;omega(italic(K)-2)+1*&amp;#39;, &amp;#39;*(1-omega)(italic(K)-2)+1&amp;quot;,
           size = 7, parse = T, family = &amp;quot;Times&amp;quot;, hjust = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = 1/3, y = .7,
           label = &amp;quot;&amp;#39;~&amp;#39;&amp;quot;,
           size = 10, parse = T, family = &amp;quot;Times&amp;quot;) +
  scale_color_manual(values = c(&amp;quot;grey75&amp;quot;, &amp;quot;black&amp;quot;)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1) +
  theme_void()

p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You’ll see how this will works when we combine all the subplots, below. The rest of the subplots are similar or identical to the ones from the first section. Here we’ll make them in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# another beta density
p3 &amp;lt;-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&amp;gt;% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;beta&amp;quot;,
           size = 7) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))

# an annotated arrow
p4 &amp;lt;-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = 0) %&amp;gt;%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .375, y = 1/3,
           label = &amp;quot;&amp;#39;~&amp;#39;&amp;quot;,
           size = 10, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(0, 1)

# bar plot of Bernoulli data
p5 &amp;lt;-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = &amp;quot;skyblue&amp;quot;, width = .4) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;Bernoulli&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .94,
           label = &amp;quot;theta&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-.75, 1.75) +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p6 &amp;lt;-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c(&amp;quot;&amp;#39;~&amp;#39;&amp;quot;, &amp;quot;italic(i)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = &amp;quot;Times&amp;quot;) +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1)

# some text
p7 &amp;lt;-
  tibble(x     = .5,
         y     = .5,
         label = &amp;quot;italic(y[i])&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = &amp;quot;Times&amp;quot;) +
  xlim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now combine the subplots with &lt;strong&gt;patchwork&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout &amp;lt;- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 3, b = 4, l = 1, r = 2),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 8, l = 1, r = 1),
  area(t = 9, b = 9, l = 1, r = 1),
  area(t = 10, b = 10, l = 1, r = 1)
)

(p1 + p3 + p2 + p4 + p5 + p6 + p7) + 
  plot_layout(design = layout) &amp;amp;
  ylim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;369.6&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a few reasons why this worked. First, we superimposed the subplot with the formula and wavy lines (&lt;code&gt;p2&lt;/code&gt;) atop of the second density (&lt;code&gt;p3&lt;/code&gt;) by ordering the plots as &lt;code&gt;(p1 + p3 + p2 + p4 + p5 + p6 + p7)&lt;/code&gt;. Placing one plot atop another was made easy by our use of &lt;code&gt;theme_void()&lt;/code&gt;, which made the backgrounds for all the subplots transparent. But also look at how we set the &lt;code&gt;r&lt;/code&gt; argument to 2 within the &lt;code&gt;area()&lt;/code&gt; function for our &lt;code&gt;p2&lt;/code&gt;. That’s what bought us that extra space for the formula.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;figure-9.7-add-more-curvy-lines-and-a-second-density-to-the-top-row.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Figure 9.7: Add more curvy lines and a second density to the top row.&lt;/h3&gt;
&lt;p&gt;For our next challenge, we’ll tackle Kruschke’s Figure 9.7:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kruschke_figure9.7.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a mild extension of the previous one. From a plotting perspective, the noteworthy new features are we have two density plots on the top row and now we have to juggle two pairs of wiggly lines in the subplot with the formula. The two subplots in the top row are no big deal. To make the gamma density, just use the &lt;code&gt;dgamma()&lt;/code&gt; function in place of &lt;code&gt;dbeta()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a beta density
p1 &amp;lt;-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&amp;gt;% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;beta&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .6,
           label = &amp;quot;italic(A[omega])*&amp;#39;, &amp;#39;*italic(B[omega])&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))

# a gamma density
p2 &amp;lt;-
  tibble(x = seq(from = 0, to = 5, by = .01),
         d = (dgamma(x, 1.75, .85) / max(dgamma(x, 1.75, .85)))) %&amp;gt;% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = 2.5, y = .2,
           label = &amp;quot;gamma&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = 2.5, y = .6,
           label = &amp;quot;list(italic(S)[kappa], italic(R)[kappa])&amp;quot;,
           size = 7, family = &amp;quot;Times&amp;quot;, parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The third subplot contains our offset formula and two sets of wiggly lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;-
  tibble(x = c(.5, .475, .26, .08, .06,
               .5, .55, .85, 1.15, 1.175,
               1.5, 1.4, 1, .25, .2,
               1.5, 1.49, 1.445, 1.4, 1.39),
         y = c(1, .7, .6, .5, .2,
               1, .7, .6, .5, .2,
               1, .7, .6, .5, .2,
               1, .75, .6, .45, .2),
         line = rep(letters[2:1], each = 5) %&amp;gt;% rep(., times = 2),
         plot = rep(1:2, each = 10)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, group = interaction(plot, line))) +
  geom_bspline(aes(color = line),
               size = 2/3, show.legend = F) + 
  annotate(geom = &amp;quot;text&amp;quot;,
           x = 0, y = .1,
           label = &amp;quot;omega(kappa-2)+1*&amp;#39;, &amp;#39;*(1-omega)(kappa-2)+1&amp;quot;,
           size = 7, parse = T, family = &amp;quot;Times&amp;quot;, hjust = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = c(1/3, 1.15), y = .7,
           label = &amp;quot;&amp;#39;~&amp;#39;&amp;quot;,
           size = 10, parse = T, family = &amp;quot;Times&amp;quot;) +
  scale_color_manual(values = c(&amp;quot;grey75&amp;quot;, &amp;quot;black&amp;quot;)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1)

p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The rest of the subplots are similar or identical to the ones from the last section. Here we’ll make them in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# another beta density
p4 &amp;lt;-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&amp;gt;% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;beta&amp;quot;,
           size = 7) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))

# an annotated arrow
p5 &amp;lt;-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c(&amp;quot;&amp;#39;~&amp;#39;&amp;quot;, &amp;quot;italic(s)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = &amp;quot;Times&amp;quot;) +
  geom_segment(x = 0.5, xend = 0.5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1)

# bar plot of Bernoulli data
p6 &amp;lt;-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = &amp;quot;skyblue&amp;quot;, width = .4) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;Bernoulli&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .94,
           label = &amp;quot;theta&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-.75, 1.75) +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p7 &amp;lt;-
  tibble(x     = c(.35, .65),
         y     = c(1/3, 1/3),
         label = c(&amp;quot;&amp;#39;~&amp;#39;&amp;quot;, &amp;quot;italic(i)*&amp;#39;|&amp;#39;*italic(s)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = &amp;quot;Times&amp;quot;) +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1)

# some text
p8 &amp;lt;-
  tibble(x     = .5,
         y     = .5,
         label = &amp;quot;italic(y[i])[&amp;#39;|&amp;#39;][italic(s)]&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = &amp;quot;Times&amp;quot;) +
  xlim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now combine the subplots with &lt;strong&gt;patchwork&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout &amp;lt;- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 1, b = 2, l = 2, r = 2),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 3, b = 4, l = 1, r = 2),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 8, l = 1, r = 1),
  area(t = 9, b = 9, l = 1, r = 1),
  area(t = 10, b = 10, l = 1, r = 1)
)

(p1 + p2 + p4 + p3 + p5 + p6 + p7 + p8) + 
  plot_layout(design = layout) &amp;amp;
  ylim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;374.4&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boom; we did it!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;Though I’m overall pleased with this workflow, it’s not without limitations. To keep the values in our &lt;code&gt;area()&lt;/code&gt; functions simple, we scaled the density plots to be twice the size of the arrow plots. With simple ratios like 1/2, this works well but it can be a bit of a pain with more exotic ratios. The size and proportions of the fonts are quite sensitive to the overall height and width values for the final plot. You’ll find similar issues with the coordinates for the wiggly &lt;code&gt;geom_bspline()&lt;/code&gt; lines. Getting these right will likely take a few iterations. Speaking of &lt;code&gt;geom_bspline()&lt;/code&gt;, I’m also not happy that there doesn’t appear to be an easy way to have them end with arrow heads. Perhaps you could hack some in with another layer of &lt;code&gt;geom_segment()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Limitations aside, I hope this helps makes it one step easier for applied researchers to create their own Kruschke-stype model diagrams. Happy plotting!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggforce_0.3.2   patchwork_1.1.1 forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
##  [9] tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.1.0  xfun_0.22         haven_2.3.1       colorspace_2.0-0 
##  [5] vctrs_0.3.6       generics_0.1.0    htmltools_0.5.1.1 yaml_2.2.1       
##  [9] utf8_1.1.4        rlang_0.4.10      pillar_1.5.1      withr_2.4.1      
## [13] glue_1.4.2        DBI_1.1.0         tweenr_1.0.1      dbplyr_2.0.0     
## [17] modelr_0.1.8      readxl_1.3.1      lifecycle_1.0.0   munsell_0.5.0    
## [21] blogdown_1.3      gtable_0.3.0      cellranger_1.1.0  rvest_0.3.6      
## [25] evaluate_0.14     labeling_0.4.2    knitr_1.31        fansi_0.4.2      
## [29] highr_0.8         broom_0.7.5       Rcpp_1.0.6        scales_1.1.1     
## [33] backports_1.2.1   jsonlite_1.7.2    farver_2.0.3      fs_1.5.0         
## [37] hms_0.5.3         digest_0.6.27     stringi_1.5.3     bookdown_0.21    
## [41] polyclip_1.10-0   grid_4.0.4        cli_2.3.1         tools_4.0.4      
## [45] magrittr_2.0.1    crayon_1.4.1      pkgconfig_2.0.3   MASS_7.3-53      
## [49] ellipsis_0.3.1    xml2_1.3.2        reprex_0.3.0      lubridate_1.7.9.2
## [53] assertthat_0.2.1  rmarkdown_2.7     httr_1.4.2        rstudioapi_0.13  
## [57] R6_2.5.0          compiler_4.0.4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Make rotated Gaussians, Kruschke style</title>
      <link>/post/2018-12-20-make-rotated-gaussians-kruschke-style/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-12-20-make-rotated-gaussians-kruschke-style/</guid>
      <description>
&lt;script src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;[edited Apr 21, 2021]&lt;/p&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;You too can make sideways Gaussian density curves within the tidyverse. Here’s how.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;heres-the-deal-i-like-making-pictures.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Here’s the deal: I like making pictures.&lt;/h2&gt;
&lt;p&gt;Over the past several months, I’ve been slowly chipping away&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; at John Kruschke’s &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;&lt;em&gt;Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan&lt;/em&gt;&lt;/a&gt;. Kruschke has a unique plotting style. One of the quirks is once in a while he likes to express the results of his analyses in plots where he shows the data alongside density curves of the model-implied data-generating distributions. Here’s an example from chapter 19 (p. 563).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kruschke_sideways_Gaussians.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, he has lifespan data (i.e., &lt;code&gt;Longevity&lt;/code&gt;) for fruit flies from five experimental conditions (i.e., &lt;code&gt;CompanionNumber&lt;/code&gt;). Those are the black circles. In this section of the chapter, he used a Gaussian multilevel model in which the mean value for &lt;code&gt;Longevity&lt;/code&gt; had a grand mean in addition to random effects for the five experimental conditions. Those sideways-turned blue Gaussians are his attempt to express the model-implied data generating distributions for each group.&lt;/p&gt;
&lt;p&gt;If you haven’t gone through Kruschke’s text, you should know he relies on base R and all its &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/control-structures.html#for-loops&#34;&gt;loop&lt;/a&gt;y glory. If you carefully go through his code, you can reproduce his plots in that fashion. I’m a &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;tidyverse&lt;/a&gt; man and prefer to avoid writing a &lt;code&gt;for()&lt;/code&gt; loop at all costs. At first, I tried to work with convenience functions within ggplot2 and friends, but only had limited success. After staring long and hard at Kruschke’s base code, I came up with a robust solution, which I’d like to share here.&lt;/p&gt;
&lt;p&gt;In this post, we’ll practice making sideways Gaussians in the Kruschke style. We’ll do so with a simple intercept-only single-level model and then expand our approach to an intercept-only multilevel model like the one in the picture, above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My assumptions&lt;/h2&gt;
&lt;p&gt;For the sake of this post, I’m presuming you’re familiar with &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html&#34;&gt;R&lt;/a&gt;, aware of the &lt;a href=&#34;https://www.rstudio.com/resources/videos/data-science-in-the-tidyverse/&#34;&gt;tidyverse&lt;/a&gt;, and have fit a &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&#34;&gt;Bayesian model&lt;/a&gt; or two. Yes. I admit that’s a narrow crowd. Sometimes the target’s a small one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need data.&lt;/h2&gt;
&lt;p&gt;First, we need data. Here we’ll borrow code from Matthew Kay’s nice &lt;a href=&#34;https://mjskay.github.io/tidybayes/articles/tidy-brms.html&#34;&gt;tutorial&lt;/a&gt; on how to use his great &lt;a href=&#34;https://github.com/mjskay/tidybayes&#34;&gt;tidybayes package&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

set.seed(5)
n           &amp;lt;- 10
n_condition &amp;lt;- 5

abc &amp;lt;-
  tibble(condition = rep(letters[1:5], times = n),
         response  = rnorm(n * 5, mean = c(0, 1, 2, 1, -1), sd = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data structure looks like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(abc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [50 × 2] (S3: tbl_df/tbl/data.frame)
##  $ condition: chr [1:50] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; &amp;quot;d&amp;quot; ...
##  $ response : num [1:50] -0.42 1.692 1.372 1.035 -0.144 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With Kay’s code, we have &lt;code&gt;response&lt;/code&gt; values for five &lt;code&gt;condition&lt;/code&gt;s. All follow the normal distribution and share a common standard deviation. However, they differ in their group means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;% 
  group_by(condition) %&amp;gt;% 
  summarise(mean = mean(response) %&amp;gt;% round(digits = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   condition  mean
##   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 a          0.18
## 2 b          1.01
## 3 c          1.87
## 4 d          1.03
## 5 e         -0.94&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Altogether, the data look like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_grey() + 
            theme(panel.grid = element_blank()))

abc %&amp;gt;%
  ggplot(aes(y = condition, x = response)) +
  geom_point(shape = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s get ready to model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;just-one-intercept&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Just one intercept&lt;/h2&gt;
&lt;p&gt;If you’ve read this far, you know we’re going Bayesian. Let’s open up our favorite Bayesian modeling package, Bürkner’s &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our first model, we’ll ignore the groups and just estimate a grand mean and a standard deviation. Relative to the scale of the &lt;code&gt;abc&lt;/code&gt; data, our priors are modestly &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;regularizing&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- 
  brm(data = abc,
      response ~ 1,
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(student_t(3, 0, 1), class = sigma)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extract the posterior draws and save them as a data frame we’ll call &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(fit1)

glimpse(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 3
## $ b_Intercept &amp;lt;dbl&amp;gt; 0.5729167, 0.6185517, 0.4430281, 0.4383346, 0.8541620, 0.6280931, 0.5159498, 0…
## $ sigma       &amp;lt;dbl&amp;gt; 1.1595969, 1.0350395, 1.0101029, 0.9758173, 1.1676389, 0.9694168, 1.0725615, 1…
## $ lp__        &amp;lt;dbl&amp;gt; -77.17416, -76.99795, -77.92546, -78.35923, -78.25847, -77.55006, -77.13486, -…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If all you want is a quick and dirty way to plot a few of the model-implied Gaussians from the simple model, you can just nest &lt;code&gt;stat_function()&lt;/code&gt; within &lt;code&gt;mapply()&lt;/code&gt; and tack on the original data in a &lt;code&gt;geom_jitter()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How many Gaussians would you like?
n_iter &amp;lt;- 20

tibble(response = c(-4, 4)) %&amp;gt;%
  ggplot(aes(x = response)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 1/2, 
                  color = &amp;quot;steelblue&amp;quot;)
    }, 
    # Enter means and standard deviations here
    mean = post[1:n_iter, &amp;quot;b_Intercept&amp;quot;],
    sd   = post[1:n_iter, &amp;quot;sigma&amp;quot;]
    ) +
  geom_jitter(data = abc, aes(y = -0.02),
              height = .025, shape = 1, alpha = 2/3) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This works pretty okay. But notice the orientation is the usual horizontal. Kruschke’s Gaussians were on their sides. If we switch out our &lt;code&gt;scale_y_continuous()&lt;/code&gt; line with &lt;code&gt;scale_y_reverse()&lt;/code&gt; and add in &lt;code&gt;coord_flip()&lt;/code&gt;, we’ll have it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(response = c(-4, 4)) %&amp;gt;%
  ggplot(aes(x = response)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 1/2, 
                  color = &amp;quot;steelblue&amp;quot;)
    }, 
    mean = post[1:n_iter, &amp;quot;b_Intercept&amp;quot;],
    sd   = post[1:n_iter, &amp;quot;sigma&amp;quot;]
    ) +
  geom_jitter(data = abc, aes(y = -0.02),
              height = .025, shape = 1, alpha = 2/3) +
  scale_y_reverse(NULL, breaks = NULL) +
  coord_flip() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boom. It won’t always be this easy, though.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-intercepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple intercepts&lt;/h2&gt;
&lt;p&gt;Since the &lt;code&gt;response&lt;/code&gt; values are from a combination of five &lt;code&gt;condition&lt;/code&gt; groups, we can fit a multilevel model to compute both the grand mean and the group-level deviations from the grand mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- 
  brm(data = abc,
      response ~ 1 + (1 | condition),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(student_t(3, 0, 1), class = sigma),
                prior(student_t(3, 0, 1), class = sd)),
      cores = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;“Wait. Whoa. I’m so confused”—you say. “What’s a multilevel model, again?” Read this &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;book&lt;/a&gt;, or this &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;book&lt;/a&gt;; start &lt;a href=&#34;https://www.youtube.com/watch?v=2sTQ7TG_85Q&#34;&gt;here&lt;/a&gt; on this lecture series; or even check out &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;my ebook&lt;/a&gt;, starting with Chapter 12.&lt;/p&gt;
&lt;p&gt;Once again, extract the posterior draws and save them as a data frame, &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(fit2)

str(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    4000 obs. of  9 variables:
##  $ b_Intercept             : num  1.16 1.41 1.17 1.49 1.39 ...
##  $ sd_condition__Intercept : num  1.012 0.892 0.871 0.846 0.813 ...
##  $ sigma                   : num  0.581 0.537 0.491 0.537 0.541 ...
##  $ r_condition[a,Intercept]: num  -0.934 -1.313 -0.9 -1.363 -1.337 ...
##  $ r_condition[b,Intercept]: num  -0.336 -0.176 -0.523 -0.273 -0.241 ...
##  $ r_condition[c,Intercept]: num  0.969 0.413 0.8 0.483 0.394 ...
##  $ r_condition[d,Intercept]: num  -0.287 -0.198 -0.31 -0.105 -0.101 ...
##  $ r_condition[e,Intercept]: num  -2.12 -2.32 -2.12 -2.37 -2.25 ...
##  $ lp__                    : num  -52 -53 -54.6 -55.5 -54.1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is where our task becomes difficult. Now each level of &lt;code&gt;condition&lt;/code&gt; has its own mean estimate, which is a combination of the grand mean &lt;code&gt;b_Intercept&lt;/code&gt; and the group-specific deviation, &lt;code&gt;r_condition[a,Intercept]&lt;/code&gt; through &lt;code&gt;r_condition[e,Intercept]&lt;/code&gt;. If all we wanted to do was show the model-implied Gaussians for, say, &lt;code&gt;condition == a&lt;/code&gt;, that’d be a small extension of our last approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(response = c(-4, 4)) %&amp;gt;%
  ggplot(aes(x = response)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 1/2, 
                  color = &amp;quot;steelblue&amp;quot;)
    }, 
    # Here&amp;#39;s the small extension, part a
    mean = post[1:n_iter, &amp;quot;b_Intercept&amp;quot;] + post[1:n_iter, &amp;quot;r_condition[a,Intercept]&amp;quot;],
    sd   = post[1:n_iter, &amp;quot;sigma&amp;quot;]
    ) +
  # The small extension, part b:
  geom_jitter(data = abc %&amp;gt;% filter(condition == &amp;quot;a&amp;quot;), aes(y = 0),
              height = .025, shape = 1, alpha = 2/3) +
  scale_y_reverse(NULL, breaks = NULL) +
  coord_flip() +
  labs(subtitle = &amp;quot;This is just for condition a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The main thing we did was add to the definition of the &lt;code&gt;mean&lt;/code&gt; within &lt;code&gt;mapply()&lt;/code&gt;: &lt;code&gt;mean = post[1:n_iter, &#34;b_Intercept&#34;] + post[1:n_iter, &#34;r_condition[a,Intercept]&#34;]&lt;/code&gt;. Within &lt;code&gt;geom_jitter()&lt;/code&gt;, we also isolated the &lt;code&gt;condition == &#34;a&#34;&lt;/code&gt; cases with &lt;code&gt;filter()&lt;/code&gt;. Simple. However, it’s more of a pickle if we want multiple densities stacked atop/next to one another within the same plot.&lt;/p&gt;
&lt;p&gt;Unfortunately, we can’t extend our &lt;code&gt;mapply(stat_function())&lt;/code&gt; method to the group-level estimates–at least not that I’m aware. But there are other ways. We’ll need a little help from &lt;code&gt;tidybayes::spread_draws()&lt;/code&gt;, about which you can learn more &lt;a href=&#34;https://mjskay.github.io/tidybayes/articles/tidy-brms.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

sd &amp;lt;-
  fit2 %&amp;gt;% 
  spread_draws(b_Intercept, sigma, r_condition[condition,])
  
head(sd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
## # Groups:   condition [5]
##   .chain .iteration .draw b_Intercept sigma condition r_condition
##    &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
## 1      1          1     1        1.16 0.581 a              -0.934
## 2      1          1     1        1.16 0.581 b              -0.336
## 3      1          1     1        1.16 0.581 c               0.969
## 4      1          1     1        1.16 0.581 d              -0.287
## 5      1          1     1        1.16 0.581 e              -2.12 
## 6      1          2     2        1.41 0.537 a              -1.31&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our &lt;code&gt;sp&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html&#34;&gt;tibble&lt;/a&gt;, we have much of the same information we’d get from &lt;code&gt;brms::posterior_samples()&lt;/code&gt;, but in the long format with respect to the random effects for &lt;code&gt;condition&lt;/code&gt;. Also notice that each row is indexed by the chain, iteration, and draw number. Among those, &lt;code&gt;.draw&lt;/code&gt; is the column that corresponds to a unique row like what we’d get from &lt;code&gt;brms::posterior_samples()&lt;/code&gt;. This is the index that ranges from 1 to the number of chains multiplied by the number of post-warmup iterations (i.e., default 4000 in our case).&lt;/p&gt;
&lt;p&gt;But we need to wrangle a bit. Within the &lt;code&gt;expand()&lt;/code&gt; function, we’ll select the columns we’d like to keep within the &lt;code&gt;nesting()&lt;/code&gt; function and then expand the tibble by adding a sequence of &lt;code&gt;response&lt;/code&gt; values ranging from -4 to 4, for each. This sets us up to use the &lt;code&gt;dnorm()&lt;/code&gt; function in the next line to compute the density for each of those &lt;code&gt;response&lt;/code&gt; values based on 20 unique normal distributions for each of the five &lt;code&gt;condition&lt;/code&gt; groups. “Why 20?” Because we need some reasonably small number and 20’s the one Kruschke tended to use in his text and because, well, we set &lt;code&gt;filter(.draw &amp;lt; 21)&lt;/code&gt;. But choose whatever number you like.&lt;/p&gt;
&lt;p&gt;The difficulty, however, is that all of these densities will have a minimum value of around 0 and all will be on the same basic scale. So we need a way to serially shift the density values up the y-axis in such a way that they’ll be sensibly separated by group. As far as I can figure, this’ll take us a couple steps. For the first step, we’ll create an intermediary variable, &lt;code&gt;g&lt;/code&gt;, with which we’ll arbitrarily assign each of our five groups an integer index ranging from 0 to 4.&lt;/p&gt;
&lt;p&gt;The second step is tricky. There we use our &lt;code&gt;g&lt;/code&gt; integers to sequentially shift the density values up. Since our &lt;code&gt;g&lt;/code&gt; value for &lt;code&gt;a == 0&lt;/code&gt;, those we’ll keep 0 as their baseline. As our &lt;code&gt;g&lt;/code&gt; value for &lt;code&gt;b == 1&lt;/code&gt;, the baseline for those will now increase by 1. And so on for the other groups. But we still need to do a little more fiddling. What we want is for the maximum values of the density estimates to be a little lower than the baselines of the ones one grouping variable up. That is, we want the maximum values for the &lt;code&gt;a&lt;/code&gt; densities to fall a little bit below 1 on the y-axis. It’s with the &lt;code&gt;* .75 / max(density)&lt;/code&gt; part of the code that we accomplish that task. If you want to experiment with more or less room between the top and bottom of each density, play around with increasing/decreasing that .75 value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  filter(.draw &amp;lt; 21) %&amp;gt;% 
  expand(nesting(.draw, b_Intercept, sigma, condition, r_condition), 
         response = seq(from = -4, to = 4, length.out = 200)) %&amp;gt;%
  mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma),
         g       = recode(condition,
                          a = 0,
                          b = 1,
                          c = 2,
                          d = 3,
                          e = 4)) %&amp;gt;% 
  mutate(density = g + density * .75 / max(density))

glimpse(sd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 8
## Groups: condition [5]
## $ .draw       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
## $ b_Intercept &amp;lt;dbl&amp;gt; 1.164396, 1.164396, 1.164396, 1.164396, 1.164396, 1.164396, 1.164396, 1.164396…
## $ sigma       &amp;lt;dbl&amp;gt; 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0…
## $ condition   &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;…
## $ r_condition &amp;lt;dbl&amp;gt; -0.9342895, -0.9342895, -0.9342895, -0.9342895, -0.9342895, -0.9342895, -0.934…
## $ response    &amp;lt;dbl&amp;gt; -4.000000, -3.959799, -3.919598, -3.879397, -3.839196, -3.798995, -3.758794, -…
## $ density     &amp;lt;dbl&amp;gt; 1.874794e-12, 3.094499e-12, 5.083339e-12, 8.310546e-12, 1.352172e-11, 2.189555…
## $ g           &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we’ll now be using the same axis for both the densities and the five &lt;code&gt;condition&lt;/code&gt; groups, we’ll need to add a &lt;code&gt;density&lt;/code&gt; column to our &lt;code&gt;abc&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc &amp;lt;-
  abc %&amp;gt;% 
  mutate(density = recode(condition,
                          a = 0,
                          b = 1,
                          c = 2,
                          d = 3,
                          e = 4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time to plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  ggplot(aes(x = response, y = density)) +
  # here we make our density lines
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  # use the original data for the jittered points
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we’re rolling. Let’s make a cosmetic adjustment. Recall that the full range of the normal distribution spans from &lt;span class=&#34;math inline&#34;&gt;\(-\infty\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. At a certain point, it’s just not informative to show the left and right tails. If you look back up at our motivating example, you’ll note Kruschke’s densities stopped well before trailing off into the tails. If you look closely to the code from his text, you’ll see he’s just showing the inner 95-percentile range for each. To follow suit, we can compute those ranges with &lt;code&gt;qnorm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  mutate(ll = qnorm(.025, mean = b_Intercept + r_condition, sd = sigma),
         ul = qnorm(.975, mean = b_Intercept + r_condition, sd = sigma))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have our lower- and upper-level points for each iteration, we can limit the ranges of our Gaussians with &lt;code&gt;filter()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  
  ggplot(aes(x = response, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Oh man, just look how sweet that is. Although I prefer our current method, another difference between it and Kruschke’s example is all of his densities are the same relative height. In all our plots so far, though, the densities differ by their heights. We’ll need a slight adjustment in our &lt;code&gt;sd&lt;/code&gt; workflow for that. All we need to do is insert a &lt;code&gt;group_by()&lt;/code&gt; statement between the two &lt;code&gt;mutate()&lt;/code&gt; lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma),
         g       = recode(condition,
                          a = 0,
                          b = 1,
                          c = 2,
                          d = 3,
                          e = 4)) %&amp;gt;% 
  # here&amp;#39;s the new line
  group_by(.draw) %&amp;gt;% 
  mutate(density = g + density * .75 / max(density))

# now plot
sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  ggplot(aes(x = response, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nice. “But wait!”, you say. “We wanted our Gaussians to be on their sides.” We can do that in at least two ways. At this point, the quickest way is to use our &lt;code&gt;scale_y_reverse() + coord_flip()&lt;/code&gt; combo from before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  
  ggplot(aes(x = response, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_reverse(&amp;quot;condition&amp;quot;,
                  breaks = 0:4,
                  labels = letters[1:5]) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way to get those sideways Gaussians is to alter our &lt;code&gt;sd&lt;/code&gt; data workflow. The main difference is this time we change the original &lt;code&gt;mutate(density = g + density * .75 / max(density))&lt;/code&gt; line to &lt;code&gt;mutate(density = g - density * .75 / max(density))&lt;/code&gt;. In case you missed it, the only difference is we changed the &lt;code&gt;+&lt;/code&gt; to a &lt;code&gt;-&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  # step one: starting fresh
  mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma)) %&amp;gt;% 
  group_by(.draw) %&amp;gt;% 
  # step two: now SUBTRACTING density from g within the equation
  mutate(density = g - density * .75 / max(density))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now in our global &lt;code&gt;aes()&lt;/code&gt; statement in the plot, we put &lt;code&gt;density&lt;/code&gt; on the x and &lt;code&gt;response&lt;/code&gt; on the y. We need to take a few other subtle steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Switch out &lt;code&gt;geom_line()&lt;/code&gt; for &lt;code&gt;geom_path()&lt;/code&gt; (see &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/geom_path.html&#34;&gt;here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Drop the &lt;code&gt;height&lt;/code&gt; argument within &lt;code&gt;geom_jitter()&lt;/code&gt; for &lt;code&gt;width&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Switch out &lt;code&gt;scale_y_continuous()&lt;/code&gt; for &lt;code&gt;scale_x_continuous()&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Though totally not necessary, we’ll add a little something extra by coloring the Gaussians by their means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  
  ggplot(aes(x = density, y = response)) +
  geom_path(aes(group = interaction(.draw, g), 
                color = b_Intercept + r_condition),
            alpha = 1/2, size = 1/3, show.legend = F) +
  geom_jitter(data = abc,
              width = .05, shape = 1, alpha = 2/3) +
  scale_x_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5]) +
  scale_color_viridis_c(option = &amp;quot;A&amp;quot;, end = .92)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you have it–Kruschke-style sideways Gaussians for your model plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;afterward&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Afterward&lt;/h2&gt;
&lt;p&gt;After releasing the initial version of this post, some of us had a lively twitter discussion on how to improve the code.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Ah, hrm. Took some digging, but it looks like negative density + setting `min_height = NA` (otherwise negative values are cut off) might work &lt;a href=&#34;https://t.co/gmF9kpo2T7&#34;&gt;pic.twitter.com/gmF9kpo2T7&lt;/a&gt;&lt;/p&gt;&amp;mdash; Matthew Kay (@mjskay) &lt;a href=&#34;https://twitter.com/mjskay/status/1076395687020056576?ref_src=twsrc%5Etfw&#34;&gt;December 22, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Part of that discussion had to do with the possibility of using functions from &lt;a href=&#34;https://twitter.com/ClausWilke/&#34;&gt;Claus Wilke&lt;/a&gt;’s great &lt;a href=&#34;https://github.com/clauswilke/ggridges&#34;&gt;ggridges package&lt;/a&gt;. After some great efforts, especially from &lt;a href=&#34;https://twitter.com/mjskay/&#34;&gt;Matthew Kay&lt;/a&gt;, we came up with solutions. In this section, we’ll cover them in some detail.&lt;/p&gt;
&lt;p&gt;First, here’s a more compact way to prepare the data for the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  mutate(density  = pmap(list(response, mu, sigma), dnorm)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  group_by(.draw) %&amp;gt;% 
  mutate(density  = density * .75 / max(density)) %&amp;gt;% 
  
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required when using unnest().
## Please use `cols = c(response, density)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 12
## Groups: .draw [20]
## $ condition  &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;,…
## $ .row       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ .chain     &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .iteration &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .draw      &amp;lt;int&amp;gt; 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,…
## $ .value     &amp;lt;dbl&amp;gt; -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -…
## $ mu         &amp;lt;dbl&amp;gt; -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -…
## $ sigma      &amp;lt;dbl&amp;gt; 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.…
## $ lower      &amp;lt;dbl&amp;gt; -1.103438, -1.103438, -1.103438, -1.103438, -1.103438, -1.103438, -1.103438, -1…
## $ upper      &amp;lt;dbl&amp;gt; 1.011903, 1.011903, 1.011903, 1.011903, 1.011903, 1.011903, 1.011903, 1.011903,…
## $ response   &amp;lt;dbl&amp;gt; -1.1034383, -1.0928085, -1.0821786, -1.0715488, -1.0609189, -1.0502890, -1.0396…
## $ density    &amp;lt;dbl&amp;gt; 0.1098804, 0.1141834, 0.1186089, 0.1231581, 0.1278322, 0.1326322, 0.1375591, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This could use some walking out. With the first two lines, we made a &lt;span class=&#34;math inline&#34;&gt;\(5 \times 1\)&lt;/span&gt; tibble containing the five levels of &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; through &lt;code&gt;f&lt;/code&gt;. The &lt;code&gt;add_fitted_draws()&lt;/code&gt; function comes from tidybayes. The first argument took our brms model fit, &lt;code&gt;fit2&lt;/code&gt;. With the &lt;code&gt;n&lt;/code&gt; argument, we indicated we just wanted &lt;code&gt;20&lt;/code&gt; draws. With &lt;code&gt;dpar&lt;/code&gt;, we requested distributional regression parameters in the output. In our case, those were the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; values for each level of &lt;code&gt;condition&lt;/code&gt;. Here’s what that looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
## # Groups:   condition, .row [1]
##   condition  .row .chain .iteration .draw .value     mu sigma
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 a             1     NA         NA    57 0.562  0.562  0.645
## 2 a             1     NA         NA    97 0.163  0.163  0.593
## 3 a             1     NA         NA   277 0.0490 0.0490 0.682
## 4 a             1     NA         NA   305 0.386  0.386  0.607
## 5 a             1     NA         NA   360 0.162  0.162  0.563
## 6 a             1     NA         NA   496 0.0224 0.0224 0.650&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we established the lower- and upper-bounds bounds for the density lines, which were 95% intervals in this example. Within the second &lt;code&gt;mutate()&lt;/code&gt; function, we used the &lt;a href=&#34;https://purrr.tidyverse.org/reference/map2.html&#34;&gt;&lt;code&gt;purrr::map2()&lt;/code&gt;&lt;/a&gt; function to feed those two values into the first two arguments of the &lt;code&gt;seq()&lt;/code&gt; function. Those arguments, recall, are &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt;. We then hard coded &lt;code&gt;200&lt;/code&gt; into the &lt;code&gt;length.out&lt;/code&gt; argument. As a result, we turned our regular old tibble into a &lt;a href=&#34;https://tidyr.tidyverse.org/reference/nest.html&#34;&gt;nested tibble&lt;/a&gt;. In each row of our new &lt;code&gt;response&lt;/code&gt; column, we now have a &lt;span class=&#34;math inline&#34;&gt;\(200 \times 1\)&lt;/span&gt; data frame containing the &lt;code&gt;seq()&lt;/code&gt; output. If you’re new to nested data structures, I recommend checking out Hadley Wickham’s &lt;a href=&#34;https://www.youtube.com/watch?v=rz3_FDVt9eg&#34;&gt;&lt;em&gt;Managing many models with R&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 11
## # Groups:   condition, .row [1]
##   condition  .row .chain .iteration .draw .value    mu sigma  lower upper response   
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;     
## 1 a             1     NA         NA   130  0.105 0.105 0.553 -0.979  1.19 &amp;lt;dbl [200]&amp;gt;
## 2 a             1     NA         NA   647  0.121 0.121 0.602 -1.06   1.30 &amp;lt;dbl [200]&amp;gt;
## 3 a             1     NA         NA  1087  0.260 0.260 0.727 -1.17   1.68 &amp;lt;dbl [200]&amp;gt;
## 4 a             1     NA         NA  1343  0.396 0.396 0.587 -0.754  1.55 &amp;lt;dbl [200]&amp;gt;
## 5 a             1     NA         NA  1618  0.342 0.342 0.500 -0.638  1.32 &amp;lt;dbl [200]&amp;gt;
## 6 a             1     NA         NA  1701  0.238 0.238 0.569 -0.878  1.35 &amp;lt;dbl [200]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Much as the &lt;code&gt;purrr::map2()&lt;/code&gt; function allowed us to iterate over two arguments, the &lt;code&gt;purrr::pmap()&lt;/code&gt; function will allow us to iterate over an arbitrary number of arguments. In the case of our third &lt;code&gt;mutate()&lt;/code&gt; function, we’ll iterate over the first three arguments of the &lt;code&gt;dnorm()&lt;/code&gt; function. In case you forgot, those arguments are &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;mean&lt;/code&gt;, and &lt;code&gt;sd&lt;/code&gt;, respectively. Within our &lt;code&gt;list()&lt;/code&gt;, we indicated we wanted to insert into them the &lt;code&gt;response&lt;/code&gt;, &lt;code&gt;mu&lt;/code&gt;, and &lt;code&gt;sigma&lt;/code&gt; values. This returns the desired &lt;code&gt;density&lt;/code&gt; values. Since our &lt;code&gt;map2()&lt;/code&gt; and &lt;code&gt;pmap()&lt;/code&gt; operations returned a nested tibble, we then followed them up with the &lt;code&gt;unnest()&lt;/code&gt; function to make it easier to access the results.&lt;/p&gt;
&lt;p&gt;Before &lt;code&gt;unnest&lt;/code&gt;ing, our nested tibble had 100 observations. After &lt;code&gt;unnest()&lt;/code&gt;, we converted it to the long format, resulting in &lt;span class=&#34;math inline&#34;&gt;\(100 \times 200 = 20,000\)&lt;/span&gt; observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  mutate(density  = pmap(list(response, mu, sigma), dnorm)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required when using unnest().
## Please use `cols = c(response, density)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 12
## Groups: condition, .row [5]
## $ condition  &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;,…
## $ .row       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ .chain     &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .iteration &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .draw      &amp;lt;int&amp;gt; 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,…
## $ .value     &amp;lt;dbl&amp;gt; 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.…
## $ mu         &amp;lt;dbl&amp;gt; 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.…
## $ sigma      &amp;lt;dbl&amp;gt; 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.…
## $ lower      &amp;lt;dbl&amp;gt; -0.9057199, -0.9057199, -0.9057199, -0.9057199, -0.9057199, -0.9057199, -0.9057…
## $ upper      &amp;lt;dbl&amp;gt; 1.278939, 1.278939, 1.278939, 1.278939, 1.278939, 1.278939, 1.278939, 1.278939,…
## $ response   &amp;lt;dbl&amp;gt; -0.9057199, -0.8947417, -0.8837635, -0.8727853, -0.8618071, -0.8508289, -0.8398…
## $ density    &amp;lt;dbl&amp;gt; 0.1048678, 0.1089746, 0.1131982, 0.1175399, 0.1220008, 0.1265818, 0.1312839, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hopefully, our last two lines look familiar. We &lt;code&gt;group_by(.draw)&lt;/code&gt; just like in previous examples. However, our final &lt;code&gt;mutate()&lt;/code&gt; line is a little simpler than in previous versions. Before we had to make that intermediary variable, &lt;code&gt;g&lt;/code&gt;. Because we intend to plot these data with help from ggridges, we no longer have need for &lt;code&gt;g&lt;/code&gt;. You’ll see. But the upshot is the only reason we’re adding this last &lt;code&gt;mutate()&lt;/code&gt; line is to scale all the Gaussians to have the same maximum height the way Kruschke did.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afd &amp;lt;-
  abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  mutate(density  = pmap(list(response, mu, sigma), dnorm)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  group_by(.draw) %&amp;gt;% 
  mutate(density  = density * .75 / max(density))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required when using unnest().
## Please use `cols = c(response, density)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(afd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 12
## Groups: .draw [20]
## $ condition  &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;,…
## $ .row       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ .chain     &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .iteration &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .draw      &amp;lt;int&amp;gt; 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,…
## $ .value     &amp;lt;dbl&amp;gt; 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.…
## $ mu         &amp;lt;dbl&amp;gt; 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.…
## $ sigma      &amp;lt;dbl&amp;gt; 0.570355, 0.570355, 0.570355, 0.570355, 0.570355, 0.570355, 0.570355, 0.570355,…
## $ lower      &amp;lt;dbl&amp;gt; -0.857776, -0.857776, -0.857776, -0.857776, -0.857776, -0.857776, -0.857776, -0…
## $ upper      &amp;lt;dbl&amp;gt; 1.377974, 1.377974, 1.377974, 1.377974, 1.377974, 1.377974, 1.377974, 1.377974,…
## $ response   &amp;lt;dbl&amp;gt; -0.8577760, -0.8465410, -0.8353061, -0.8240712, -0.8128362, -0.8016013, -0.7903…
## $ density    &amp;lt;dbl&amp;gt; 0.1098804, 0.1141834, 0.1186089, 0.1231581, 0.1278322, 0.1326322, 0.1375591, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s open &lt;a href=&#34;https://github.com/clauswilke/ggridges&#34;&gt;ggridges&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggridges)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how contrary to before, we set the global y axis to our &lt;code&gt;condition&lt;/code&gt; grouping variable. It’s within the &lt;code&gt;geom_ridgeline()&lt;/code&gt; function that we now specify &lt;code&gt;height = density&lt;/code&gt;. Other than that, the main thing to point out is you might want to adjust the &lt;code&gt;ylim&lt;/code&gt; parameters. Otherwise the margins aren’t the best.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afd %&amp;gt;%
  
  ggplot(aes(x = response, y = condition)) +
  geom_ridgeline(aes(height = density, group = interaction(condition, .draw)),
                 fill = NA, size = 1/3, color = adjustcolor(&amp;quot;steelblue&amp;quot;, alpha.f = 1/2)) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  coord_cartesian(ylim = c(1.25, 5.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;“But I wanted my Gaussians tipped to the left!”, you say. Yep, we can do that, too. Three things: First, we’ll want to adjust the &lt;code&gt;height&lt;/code&gt; parameter to &lt;code&gt;-density&lt;/code&gt;. We want our Gaussians to extend under their baselines. Along with that, we need to include &lt;code&gt;min_height = NA&lt;/code&gt;. Finally, we’ll switch out &lt;code&gt;coord_cartesian()&lt;/code&gt; for good old &lt;code&gt;coord_flip()&lt;/code&gt;. And you can adjust your &lt;code&gt;ylim&lt;/code&gt; parameters as desired.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afd %&amp;gt;%
  
  ggplot(aes(x = response, y = condition)) +
  geom_ridgeline(aes(height = -density, group = interaction(condition, .draw)),
                 fill = NA, size = 1/3, color = adjustcolor(&amp;quot;steelblue&amp;quot;, alpha.f = 1/2),
                 min_height = NA) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  coord_flip(ylim = c(0.5, 4.75))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think it’s important to note that I’ve never met any of the people who helped me with this project. Academic twitter, man–it’s a good place to be.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggridges_0.5.2  tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0  
##  [7] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3  
## [13] tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] svUnit_1.0.3         splines_4.0.4        crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] viridisLite_0.3.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3       
##  [53] arrayhelpers_1.1-0   ellipsis_0.3.1       pkgconfig_2.0.3      loo_2.4.1           
##  [57] farver_2.0.3         dbplyr_2.0.0         utf8_1.1.4           tidyselect_1.1.0    
##  [61] labeling_0.4.2       rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1       
##  [65] munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [69] generics_0.1.0       broom_0.7.5          evaluate_0.14        fastmap_1.0.1       
##  [73] yaml_2.2.1           processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [77] nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [81] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [85] gamm4_0.2-6          curl_4.3             reprex_0.3.0         statmod_1.4.35      
##  [89] stringi_1.5.3        highr_0.8            ps_1.6.0             blogdown_1.3        
##  [93] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [97] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1        
## [101] lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [105] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [109] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [113] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [117] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [125] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I’ve made a lot of progress working through Kruschke’s material since the initial release of this blog post. You can find the results in an ebook, &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

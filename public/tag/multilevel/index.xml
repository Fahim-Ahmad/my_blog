<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>multilevel | Fahim Ahmad</title>
    <link>/tag/multilevel/</link>
      <atom:link href="/tag/multilevel/index.xml" rel="self" type="application/rss+xml" />
    <description>multilevel</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>¬© Fahim Ahmad (2020)</copyright><lastBuildDate>Mon, 12 Jul 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>multilevel</title>
      <link>/tag/multilevel/</link>
    </image>
    
    <item>
      <title>Got overdispersion? Try observation-level random effects with the Poisson-lognormal mixture</title>
      <link>/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/</link>
      <pubDate>Mon, 12 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/</guid>
      <description>
&lt;script src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;what&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What?&lt;/h2&gt;
&lt;p&gt;One of &lt;a href=&#34;https://twitter.com/tjmahr&#34;&gt;Tristan Mahr&lt;/a&gt;‚Äôs recent Twitter threads almost broke my brain.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;wait when people talk about treating overdispersion by using random effects, they sometimes put a random intercept on each row?? &lt;a href=&#34;https://t.co/7NjG4uw3nz&#34;&gt;https://t.co/7NjG4uw3nz&lt;/a&gt; &lt;a href=&#34;https://t.co/fo8Ylcejqv&#34;&gt;pic.twitter.com/fo8Ylcejqv&lt;/a&gt;&lt;/p&gt;&amp;mdash; tj mahr (originally Doki Doki Panic in Japan) üççüçï (@tjmahr) &lt;a href=&#34;https://twitter.com/tjmahr/status/1413186646783242242?ref_src=twsrc%5Etfw&#34;&gt;July 8, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;It turns out that you can use random effects on cross-sectional count data. Yes, that‚Äôs right. Each count gets its own random effect. Some people call this observation-level random effects and it can be a tricky way to handle overdispersion. The purpose of this post is to show how to do this and to try to make sense of what it even means.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;First, I should clarify a bit. Mahr‚Äôs initial post and much of the thread to follow primarily focused on counts within the context of binomial data. If you‚Äôve ever read a book on the generalized linear model (GLM), you know that the two broad frameworks for modeling counts are as binomial or Poisson. The basic difference is if your counts are out of a known number of trials (e.g., I got 3 out of 5 questions correct in my pop quiz, last week&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;), the binomial is generally the way to go. However, if your counts aren‚Äôt out of a well-defined total (e.g., I drank 1497 cups of coffee&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, last year), the Poisson distribution offers a great way to think about your data. In this post, we‚Äôll be focusing on Poisson-like counts.&lt;/p&gt;
&lt;p&gt;The Poisson distribution is named after the French mathematician &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e8/E._Marcellot_Sim√©on-Denis_Poisson_1804.jpg&#34;&gt;Sim√©on Denis Poisson&lt;/a&gt;, who lived and died about 200 years ago. Poisson‚Äôs distribution is valid for non-negative integers, which is basically what counts are. The distribution has just one parameter, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, which controls both its mean and variance and imposes the assumption that the mean of your counts is the same as the variance. On the one hand, this is great because it keeps things simple‚Äìparsimony and all. On the other hand, holding the mean and variance the same is a really restrictive assumption and it just doesn‚Äôt match up well with a lot of real-world data.&lt;/p&gt;
&lt;p&gt;This Poisson assumption that the mean equals the variance is sometimes called &lt;em&gt;equidispersion&lt;/em&gt;. Count data violate the equidispersion assumption when their variance is smaller than their mean (&lt;em&gt;underdispersion&lt;/em&gt;) or when their variance is larger than their mean (&lt;em&gt;overdispersion&lt;/em&gt;). In practice, overdispersion tends to crop up most often. Real-world count data are overdispersed so often that statisticians have had to come up with a mess of strategies to handle the problem. In the applied statistics that I‚Äôm familiar with, the two most common ways to handle overdispersed count data are with the negative-binomial model, or with random effects. We‚Äôll briefly cover both.&lt;/p&gt;
&lt;div id=&#34;negative-binomial-counts.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Negative-binomial counts.&lt;/h3&gt;
&lt;p&gt;As its name implies, the negative-binomial model has a deep relationship with the binomial model. I‚Äôm not going to go into those details, but Hilbe covered them in his well-named &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hilbeNegativeBinomialRegression2011&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt; textbook, if you‚Äôre curious. Basically, the negative-binomial model adds a dispersion parameter to the Poisson. Different authors refer to it with different names. Hilbe, for example, called it both &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-B√ºrkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner&lt;/a&gt; (&lt;a href=&#34;#ref-B√ºrkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;2021b&lt;/a&gt;)&lt;/span&gt; and the &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-standevelopmentteamStanFunctionsReference2021&#34; role=&#34;doc-biblioref&#34;&gt;Stan Development Team&lt;/a&gt; (&lt;a href=&#34;#ref-standevelopmentteamStanFunctionsReference2021&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; both call it &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. By which ever name, the negative-binomial overdispersion parameter helps disentangle the mean from the variance in a set of counts. The way it does it is by re-expressing the count data as coming from a mixture where each count is from its own Poisson distribution with its own &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter. Importantly, the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;‚Äôs in this mixture of Poissons follow a gamma distribution, which is why the negative binomial is also sometimes referred to as a gamma-Poisson model. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;, for example, generally prefers to speak in terms of the gamma-Poisson.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poission-counts-with-random-intercepts.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poission counts with random intercepts.&lt;/h3&gt;
&lt;p&gt;Another way to handle overdispersion is to ask whether the data are grouped. In my field, this naturally occurs when you collect longitudinal data. My counts, over time, will differ form your counts, over time, and we accommodate that by adding a multilevel structure to the model. This, then, takes us to the generalized linear &lt;em&gt;mixed&lt;/em&gt; model (GLMM), which is covered in text books like &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-cameron2013regression&#34; role=&#34;doc-biblioref&#34;&gt;Cameron &amp;amp; Trivedi&lt;/a&gt; (&lt;a href=&#34;#ref-cameron2013regression&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gelmanDataAnalysisUsing2006&#34; role=&#34;doc-biblioref&#34;&gt;Gelman &amp;amp; Hill&lt;/a&gt; (&lt;a href=&#34;#ref-gelmanDataAnalysisUsing2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;. Say your data have &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; groups. With a simple random-intercept Poisson model, each group of counts gets its own &lt;span class=&#34;math inline&#34;&gt;\(\lambda_j\)&lt;/span&gt; parameter and the population of those &lt;span class=&#34;math inline&#34;&gt;\(\lambda_j\)&lt;/span&gt;‚Äôs is described in terms of a grand mean (an overall &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; intercept) and variation around that grand mean (typically a standard deviation or variance parameter). Thus, if your &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; data are counts from &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; cases clustered within &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; groups, the random-intercept Poisson model can be expressed as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ij} &amp;amp; \sim \operatorname{Poisson}(\lambda_{ij}) \\
\log(\lambda_{ij}) &amp;amp; = \beta_0 + \zeta_{0j} \\
\zeta_{0j} &amp;amp; \sim \operatorname{Normal}(0, \sigma_0)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the grand mean is &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, the group-specific deviations around the grand mean are the &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0j}\)&lt;/span&gt;‚Äôs, and the variation across those &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0j}\)&lt;/span&gt;‚Äôs is expressed by a standard-deviation parameter &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt;. Thus following the typical GLMM convention, we model the group-level deviations with the normal distribution. Also notice that whether we‚Äôre talking about single-level GLMs or multilevel GLMMs, we typically model &lt;span class=&#34;math inline&#34;&gt;\(\log \lambda\)&lt;/span&gt;, instead of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. This prevents the model from predicting negative counts. Keep this in mind.&lt;/p&gt;
&lt;p&gt;Anyway, the random-intercept Poisson model can go a long way for handling overdispersion when your data are grouped. It‚Äôs also possible to combine this approach with the last one and fit a negative-binomial model with a random intercept, too. Though I haven‚Äôt seen this used much in practice, you can even take a distributional model approach &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-B√ºrkner2021Distributional&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2021a&lt;/a&gt;)&lt;/span&gt; and set the negative-binomial dispersion parameter to random, too. That, for example, could look like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ij} &amp;amp; \sim \operatorname{Gamma-Poisson}(\lambda_{ij}, \phi_{ij}) \\
\log(\lambda_{ij}) &amp;amp; = \beta_0 + \zeta_{0j} \\
\log(\phi_{ij}) &amp;amp; = \gamma_0 + \zeta_{1j} \\
\zeta_{0j} &amp;amp; \sim \operatorname{Normal}(0, \sigma_0) \\
\zeta_{1j} &amp;amp; \sim \operatorname{Normal}(0, \sigma_1).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theres-a-third-option-the-poisson-lognormal.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;There‚Äôs a third option: The Poisson-lognormal.&lt;/h3&gt;
&lt;p&gt;Now a typical condition for a random-intercept model (whether using the Poison, the negative-binomial, or any other likelihood function) is that at least some of the &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; groups, if not most or all, contain two or more cases. For example, in a randomized controlled trial you might measure the outcome variable 3 or 5 or 10 times over the course of the trial. In a typical non-experimental experience-sampling study, you might get 10 or 50 or a few hundred measurements from each participant over the course of a few days, weeks, or months. Either way, we tend to have multiple &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;‚Äôs within each level of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt;. As it turns out, you don‚Äôt have to restrict yourself that way. With the observation-level random effects (OLRE) approach, each case (each level of &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;) gets its own random effect &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-harrison2014using&#34; role=&#34;doc-biblioref&#34;&gt;Harrison, 2014&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But why would you do that?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Think back to the conventional regression model where some variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is predicting some continuous variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. We can express the model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 x_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the residual variance not accounted for by &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is captured in &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Thus &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; can be seen as a residual-variance term. The conventional Poisson model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) &amp;amp; = \beta_0 + \beta_1 x_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;doesn‚Äôt have a residual-variance term. Rather, the variance in the data is deterministically controlled by the linear model on &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt;, which works great in the case of equidispersion, but fails when the data are overdispersed. Hence the negative-binomial and the random-intercept models. But what if we &lt;em&gt;could&lt;/em&gt; tack on a residual variance term? It might take on a form like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) &amp;amp; = \beta_0 + \beta_1 x_i + \epsilon_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; is the residual variation in &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; not captured by the deterministic part of the linear model for &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt;. Following the conventional regression model, we might make our lives simple and further presume &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i \sim \operatorname{Normal}(0, \sigma_\epsilon)\)&lt;/span&gt;. Though he didn‚Äôt use this style of notation, that‚Äôs basically the insight from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-bulmer1974OnFitting&#34; role=&#34;doc-biblioref&#34;&gt;Bulmer&lt;/a&gt; (&lt;a href=&#34;#ref-bulmer1974OnFitting&#34; role=&#34;doc-biblioref&#34;&gt;1974&lt;/a&gt;)&lt;/span&gt;. But rather than speak in terms of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; and residual variance, Bulmer proposed an alternative to the gamma-Poisson mixture and asked his audience to imagine each count in the data was from its own Poisson distribution with its own &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter, but that those &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameters were distributed according to the lognormal distribution. Now Bulmer had a substantive motivation for proposing the lognormal based on the species-abundance data and I‚Äôm not going to get into any of that. But the basic point was, if we can have a gamma-distributed mixture of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;‚Äôs, why not a lognormal mixture, instead?&lt;/p&gt;
&lt;p&gt;The trouble with Bulmer‚Äôs lognormal-mixture approach is it‚Äôs not readily available in most software packages. However, notice what happens when you specify an OLRE model with the Poisson likelihood:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) &amp;amp; = \beta_0 + \zeta_{0i} \\
\zeta_{0i} &amp;amp; \sim \operatorname{Normal}(0, \sigma_0).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0i}\)&lt;/span&gt; now looks a lot like the &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; term in a standard intercepts-only regression model. Further, since the linear model is defined for &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt;, that means the &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0i}\)&lt;/span&gt; terms will be log-normally distributed in the exponentiated &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt; space. In essence, the OLRE-Poisson model is a way to hack your multilevel regression software to fit a Poisson-lognormal model for overdispersed counts.&lt;/p&gt;
&lt;p&gt;Now we have a sense of the theory, it‚Äôs time to fit some models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-example-salamander-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Empirical example: Salamander data&lt;/h2&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;As per usual, we‚Äôll be working within &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;. We‚Äôll be fitting our models with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and most of our data wrangling and plotting work will be done with aid from the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt; and friends‚Äì&lt;strong&gt;patchwork&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-patchwork&#34; role=&#34;doc-biblioref&#34;&gt;Pedersen, 2019&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;tidybayes&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;. We‚Äôll take our data set from McElreath‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-rethinking&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt; &lt;strong&gt;rethinking&lt;/strong&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)
library(tidyverse)
library(tidybayes)
library(patchwork)

data(salamanders, package = &amp;quot;rethinking&amp;quot;)

glimpse(salamanders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 47
## Columns: 4
## $ SITE      &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1‚Ä¶
## $ SALAMAN   &amp;lt;int&amp;gt; 13, 11, 11, 9, 8, 7, 6, 6, 5, 5, 4, 3, 3, 3, 3, 3, 2, 2, 2, ‚Ä¶
## $ PCTCOVER  &amp;lt;int&amp;gt; 85, 86, 90, 88, 89, 83, 83, 91, 88, 90, 87, 83, 87, 89, 92, ‚Ä¶
## $ FORESTAGE &amp;lt;int&amp;gt; 316, 88, 548, 64, 43, 368, 200, 71, 42, 551, 675, 217, 212, ‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are in the &lt;code&gt;salamanders&lt;/code&gt; data frame, which contains counts of salamanders from 47 locations in northern California &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-welsh1995habitat&#34; role=&#34;doc-biblioref&#34;&gt;Welsh Jr &amp;amp; Lind, 1995&lt;/a&gt;)&lt;/span&gt;. Our count variable is &lt;code&gt;SALAMAN&lt;/code&gt;. The location for each count is indexed by the &lt;code&gt;SITE&lt;/code&gt; column. You could use the other two variables as covariates, but we won‚Äôt be focusing on those in this post. Here‚Äôs what &lt;code&gt;SALAMAN&lt;/code&gt; looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# adjust the global plotting theme
theme_set(theme_classic())

salamanders %&amp;gt;% 
  ggplot(aes(x = SALAMAN)) +
  geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Those data look overdispersed. We can get a quick sense of the overdispersion with sample statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;salamanders %&amp;gt;% 
  summarise(mean = mean(SALAMAN),
            variance = var(SALAMAN)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       mean variance
## 1 2.468085 11.38483&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For small-&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; data, we shouldn‚Äôt expect the mean to be exactly the same as the variance in Poisson data. This big of a difference, though, suggests&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; overdispersion even with a modest &lt;span class=&#34;math inline&#34;&gt;\(N = 47\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the models.&lt;/h3&gt;
&lt;p&gt;We‚Äôll fit three intercepts-only models. The first will be a conventional Poisson model and the second will be the negative binomial (a.k.a. the gamma-Poisson mixture). We‚Äôll finish off with our Poisson-lognormal mixture via the OLRE technique. Since we‚Äôre working with Bayesian software, we‚Äôll need priors. Though I‚Äôm not going to explain them in any detail, we‚Äôll be using the weakly-regularizing approach advocated for in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here‚Äôs how to fit the models with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# conventional Poisson
fit1 &amp;lt;-
  brm(data = salamanders, 
      family = poisson,
      SALAMAN ~ 1,
      prior(normal(log(3), 0.5), class = Intercept),
      cores = 4, seed = 1)

# gamma-Poisson mixture
fit2 &amp;lt;-
  brm(data = salamanders, 
      family = negbinomial,
      SALAMAN ~ 1,
      prior = c(prior(normal(log(3), 0.5), class = Intercept),
                prior(gamma(0.01, 0.01), class = shape)),
      cores = 4, seed = 1)

# Poisson-lognormal mixture
fit3 &amp;lt;-
  brm(data = salamanders, 
      family = poisson,
      SALAMAN ~ 1 + (1 | SITE),
      prior = c(prior(normal(log(3), 0.5), class = Intercept),
                prior(exponential(1), class = sd)),
      cores = 4, seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-the-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluate the models.&lt;/h3&gt;
&lt;p&gt;Here‚Äôs a quick parameter summary for each of the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: SALAMAN ~ 1 
##    Data: salamanders (Number of observations: 47) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.91      0.09     0.73     1.08 1.00     1483     1967
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: negbinomial 
##   Links: mu = log; shape = identity 
## Formula: SALAMAN ~ 1 
##    Data: salamanders (Number of observations: 47) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.95      0.21     0.54     1.36 1.00     2907     2195
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## shape     0.58      0.18     0.30     1.01 1.00     3608     2640
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: SALAMAN ~ 1 + (1 | SITE) 
##    Data: salamanders (Number of observations: 47) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~SITE (Number of levels: 47) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.28      0.23     0.89     1.80 1.00     1117     1435
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.34      0.22    -0.12     0.75 1.00     1687     2533
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might use the &lt;code&gt;pp_check()&lt;/code&gt; function to get a graphic sense of how well each model fit the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;-
  pp_check(fit1, type = &amp;quot;bars&amp;quot;, nsample = 150, fatten = 1, size = 1/2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 15),
                  ylim = c(0, 26)) +
  labs(title = &amp;quot;fit1&amp;quot;,
       subtitle = &amp;quot;Conventional Poisson&amp;quot;)

p2 &amp;lt;-
  pp_check(fit2, type = &amp;quot;bars&amp;quot;, nsample = 150, fatten = 1, size = 1/2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 15),
                  ylim = c(0, 26)) +
  labs(title = &amp;quot;fit2&amp;quot;,
       subtitle = &amp;quot;Gamma-Poisson mixture&amp;quot;)

p3 &amp;lt;-
  pp_check(fit3, type = &amp;quot;bars&amp;quot;, nsample = 150, fatten = 1, size = 1/2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 15),
                  ylim = c(0, 26)) +
  labs(title = &amp;quot;fit3&amp;quot;,
       subtitle = &amp;quot;Poisson-lognormal mixture&amp;quot;)

p1 + p2 + p3 + plot_layout(guides = &amp;quot;collect&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The conventional Poisson model seems like a disaster. Both the gamma-Poisson and the Poisson-lognormal models seemed to capture the data much better. We also might want to compare the models with information criteria. Here we‚Äôll use the LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- add_criterion(fit1, criterion = &amp;quot;loo&amp;quot;)
fit2 &amp;lt;- add_criterion(fit2, criterion = &amp;quot;loo&amp;quot;)
fit3 &amp;lt;- add_criterion(fit3, criterion = &amp;quot;loo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I first executed that code, I got the following warning message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Found 29 observations with a pareto_k &amp;gt; 0.7 in model ‚Äòfit3.‚Äô It is recommended to set ‚Äòmoment_match = TRUE‚Äô in order to perform moment matching for problematic observations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To use the &lt;code&gt;moment_match = TRUE&lt;/code&gt; option within the &lt;code&gt;add_criterion()&lt;/code&gt; function, you have to specify &lt;code&gt;save_pars = save_pars(all = TRUE)&lt;/code&gt; within &lt;code&gt;brm()&lt;/code&gt; when fitting the model. Here‚Äôs how to do that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit the Poisson-lognormal mixture, again
fit3 &amp;lt;-
  brm(data = salamanders, 
      family = poisson,
      SALAMAN ~ 1 + (1 | SITE),
      prior = c(prior(normal(log(3), 0.5), class = Intercept),
                prior(exponential(1), class = sd)),
      cores = 4, seed = 1,
      # here&amp;#39;s the new part
      save_pars = save_pars(all = TRUE))

# add the LOO
fit3 &amp;lt;- add_criterion(
  fit3, criterion = &amp;quot;loo&amp;quot;, 
  # this part is new, too
  moment_match = TRUE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we‚Äôre ready to compare the models with the LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(fit1, fit2, fit3, criterion = &amp;quot;loo&amp;quot;) %&amp;gt;% 
  print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic
## fit3    0.0       0.0   -89.8      6.6        20.2    1.4    179.6   13.2  
## fit2   -8.2       2.3   -98.0      8.3         1.6    0.2    196.0   16.6  
## fit1  -50.5      13.9  -140.4     17.4         4.3    1.2    280.7   34.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even after accounting for model complexity, the Poisson-lognormal model appears to be the best fit for the data. Next we consider how, exactly, does one interprets the parameters of the Poisson-lognormal model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-does-one-interpret-the-poisson-lognormal-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How does one interpret the Poisson-lognormal model?&lt;/h3&gt;
&lt;p&gt;A nice quality of both the conventional Poisson model and the gamma-Poisson model is the intercept for each corresponds directly with the mean of the original data, after exponentiation. The mean of the &lt;code&gt;SALAMAN&lt;/code&gt; variable, recall, was 2.5. Here are the summaries for their exponentiated intercepts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# conventional Poisson
fixef(fit1)[, -2] %&amp;gt;% exp() %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
##     2.48     2.06     2.94&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# gamma-Poisson
fixef(fit2)[, -2] %&amp;gt;% exp() %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
##     2.58     1.72     3.89&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both are really close to the sample mean. Here‚Äôs the exponentiated intercept for the Poisson-lognormal model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit3)[, -2] %&amp;gt;% exp() %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
##     1.40     0.89     2.12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow, that‚Äôs not even close! What gives? Well, keep in mind that with the OLRE Poisson-lognormal model, the intercept is the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; parameter for the lognormal distribution of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameters. In a similar way, the level-2 standard deviation (execute &lt;code&gt;posterior_summary(fit3)[&#34;sd_SITE__Intercept&#34;, ]&lt;/code&gt;) is the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameter for that lognormal distribution. Keeping things simple, for the moment, here‚Äôs what that lognormal distribution looks like if we take the posterior means for those parameters and insert them into the parameter arguments of the &lt;code&gt;dlnorm()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;-
  tibble(lambda = seq(from = 0, to = 13, length.out = 500)) %&amp;gt;% 
  mutate(d = dlnorm(lambda, 
                    meanlog = posterior_summary(fit3)[1, 1], 
                    sdlog = posterior_summary(fit3)[2, 1])) %&amp;gt;% 
  
  ggplot(aes(x = lambda, y = d)) +
  geom_area(fill = &amp;quot;grey50&amp;quot;) +
  scale_x_continuous(expression(lambda), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(&amp;quot;density&amp;quot;, breaks = NULL, 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 12),
                  ylim = c(0, 0.8)) +
  labs(title = &amp;quot;Population lognormal distribution&amp;quot;,
       subtitle = &amp;quot;The parameters are summarized by their posterior means.&amp;quot;)

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using just the posterior means for the parameters ignores the uncertainty in the distribution. To bring that into the plot, we‚Äôll want to work with the posterior samples, themselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many posterior ddraws would you like?
n_draw &amp;lt;- 100

set.seed(1)

p2 &amp;lt;-
  posterior_samples(fit3) %&amp;gt;% 
  slice_sample(n = n_draw) %&amp;gt;% 
  transmute(iter  = 1:n(),
            mu    = b_Intercept,
            sigma = sd_SITE__Intercept) %&amp;gt;% 
  expand(nesting(iter, mu, sigma),
         lambda = seq(from = 0, to = 13, length.out = 500)) %&amp;gt;% 
  mutate(d = dlnorm(lambda, meanlog = mu, sdlog = sigma)) %&amp;gt;% 
  
  ggplot(aes(x = lambda, y = d, group = iter)) +
  geom_line(size = 1/6, alpha = 1/2) +
  scale_x_continuous(expression(lambda), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(&amp;quot;density&amp;quot;, breaks = NULL, 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 12),
                  ylim = c(0, 0.8)) +
  labs(title = &amp;quot;Population lognormal distribution&amp;quot;,
       subtitle = &amp;quot;The parameters are summarized by 100 posterior draws.&amp;quot;)

p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These, recall, are 100 credible lognormal distributions for the case-level &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameters, not for the data themselves. We‚Äôll get to the data in a moment. Since we‚Äôre working with a multilevel model, we have posteriors for each of the case-level &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameters, too. Here they are in a dot plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;-
  coef(fit3)$SITE[, &amp;quot;Estimate&amp;quot;, &amp;quot;Intercept&amp;quot;] %&amp;gt;% 
  exp() %&amp;gt;% 
  data.frame() %&amp;gt;% 
  set_names(&amp;quot;lambda_i&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = lambda_i)) +
  geom_dots(fill = &amp;quot;grey50&amp;quot;, color = &amp;quot;grey50&amp;quot;) +
  scale_x_continuous(expression(lambda[italic(i)]), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(&amp;quot;normalized density&amp;quot;, breaks = NULL, 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 12)) +
  labs(title = expression(&amp;quot;Dotplot of individual &amp;quot;*lambda[italic(i)]*&amp;quot; parameters&amp;quot;),
       subtitle = &amp;quot;The parameters are summarized by their posterior means.&amp;quot;)

p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To reduce visual complexity, we just plotted the &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameters by their posterior means. But that might be frustrating the way it ignores uncertainty. A different way to look at them might be a rank-ordered coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p4 &amp;lt;-
  coef(fit3)$SITE[, -2, &amp;quot;Intercept&amp;quot;] %&amp;gt;% 
  exp() %&amp;gt;% 
  data.frame() %&amp;gt;% 
  arrange(Estimate) %&amp;gt;% 
  mutate(rank = 1:n()) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +
  geom_pointrange(fatten = 1, size = 1/2) +
  scale_x_continuous(expression(lambda[italic(i)]), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(breaks = NULL, expand = c(0.02, 0.02)) +
  coord_cartesian(xlim = c(0, 12)) +
  labs(title = expression(&amp;quot;Ranked coefficient plot of individual &amp;quot;*lambda[italic(i)]*&amp;quot; parameters&amp;quot;),
       subtitle = &amp;quot;The parameters are summarized by their posterior means and 95% CIs.&amp;quot;)

p4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since each &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameter is based in the data from a single case, it‚Äôs no surprise that their 95% intervals are all on the wide side. Just for kicks, here are the last four subplots all shown together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 + p2 + p3 + p4 &amp;amp; 
  theme_classic(base_size = 8.25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this point, though, you may be wondering how this model, with all its lognormal &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; glory, can inform us about actual counts. You know, the kind of counts that allowed us to fit such a wacky model. We‚Äôll want to work with the posterior draws for that, too. First we extract all of the posterior draws for the population parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;-
  posterior_samples(fit3) %&amp;gt;% 
  transmute(mu    = b_Intercept,
            sigma = sd_SITE__Intercept)

# what is this?
glimpse(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 2
## $ mu    &amp;lt;dbl&amp;gt; 0.16424013, 0.07509823, 0.26418683, 0.22422613, 0.36427113, -0.1‚Ä¶
## $ sigma &amp;lt;dbl&amp;gt; 1.4128424, 1.4236003, 1.2986505, 1.4523181, 1.2731463, 1.5761309‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next code block is a little chunky, so I‚Äôll try to explain what we‚Äôre doing before we dive in. Our goal is to use the posterior draws to make a posterior predictive check, by hand. My reasoning is doing this kind of check by hand, rather than relying on &lt;code&gt;pp_check()&lt;/code&gt;, requires you to understand the guts of the model. In our check, we are going to compare the histogram of the original &lt;code&gt;SALAMAN&lt;/code&gt; counts with the histograms of a few data sets simulated from the model. So first, we need to decide how many simulations we want. Since I want a faceted plot of 12 histograms, that means we‚Äôll need 11 simulations. We set that number with the opening &lt;code&gt;n_facet &amp;lt;- 12&lt;/code&gt; line. Next, we set our seed for reproducibility and took 11 random draws from the &lt;code&gt;post&lt;/code&gt; data frame. In the first &lt;code&gt;mutate()&lt;/code&gt; line, we added an iteration index. Then with the &lt;code&gt;purrr::map2()&lt;/code&gt; function, we drew 47 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values (47 was the original &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; in the &lt;code&gt;salamanders&lt;/code&gt; data) based on the lognormal distribution defined by the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; values from each iteration. After &lt;code&gt;unnest()&lt;/code&gt;-ing those results, we used &lt;code&gt;rpois()&lt;/code&gt; within the next &lt;code&gt;mutate()&lt;/code&gt; line to use those simulated &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values to simulate actual counts. The remaining lines clean up the data format a bit and tack on the original &lt;code&gt;salamanders&lt;/code&gt; data. Then we plot.&lt;/p&gt;
&lt;p&gt;Okay, here it is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many facets would you like?
n_facet &amp;lt;- 12

set.seed(1)

post %&amp;gt;% 
  # take 11 samples from the posterior iterations
  slice_sample(n = n_facet - 1) %&amp;gt;% 
  # take 47 random draws from each iteration
  mutate(iter   = 1:n(),
         lambda = map2(mu, sigma, ~ rlnorm(n = 47, meanlog = mu, sdlog = sigma))) %&amp;gt;% 
  unnest(lambda) %&amp;gt;% 
  # use the lambdas to generate the counts
  mutate(count = rpois(n(), lambda = lambda)) %&amp;gt;% 
  transmute(sample = str_c(&amp;quot;sample #&amp;quot;, iter),
            SALAMAN = count) %&amp;gt;% 
  # combine the original data
  bind_rows(
    salamanders %&amp;gt;% 
      select(SALAMAN) %&amp;gt;% 
      mutate(sample = &amp;quot;original data&amp;quot;)
  ) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = SALAMAN, fill = sample == &amp;quot;original data&amp;quot;)) +
  geom_bar() +
  scale_fill_viridis_d(option = &amp;quot;A&amp;quot;, begin = .15, end = .55, breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 30)) +
  labs(title = &amp;quot;Handmade posterior predictive check&amp;quot;) +
  facet_wrap(~sample) +
  theme(strip.background = element_rect(size = 0, fill = &amp;quot;grey92&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This, friends, is how you can use our intercepts-only Poisson-lognormal mixture model to simulate count data resembling the original count data. Data simulation is cool, but you might wonder how to compute the mean of the model-implied lognormal distribution. Recall that we can‚Äôt just exponentiate the model‚Äôs intercept. As it turns out, &lt;span class=&#34;math inline&#34;&gt;\(\exp \mu\)&lt;/span&gt; returns the &lt;strong&gt;median&lt;/strong&gt; for the lognormal distribution. The formula for the mean of the lognormal distribution is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{mean} = \exp \left ( \mu + \frac{\sigma^2}{2}\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So here‚Äôs how to work with the posterior draws to compute that value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  mutate(mean = exp(mu + sigma^2 / 2)) %&amp;gt;% 
  
  ggplot(aes(x = mean, y = 0)) +
  stat_halfeye(.width = c(.5, .95)) +
  geom_vline(xintercept = mean(salamanders$SALAMAN), 
             color = &amp;quot;purple4&amp;quot;, linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 10)) +
  xlab(expression(&amp;quot;mean of the lognormal &amp;quot;*lambda*&amp;quot; distribution&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For reference, we superimposed the mean of the &lt;code&gt;SALAMAN&lt;/code&gt; data with a dashed line.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrap-up&lt;/h2&gt;
&lt;p&gt;Okay, this is about as far as I‚Äôd like to go with this one. To be honest, the Poisson-lognormal mixture is a weird model and I‚Äôm not sure if it‚Äôs a good fit for the kind of data I tend to work with. But exposure to new options seems valuable and I‚Äôm content to low-key chew on this one for a while.&lt;/p&gt;
&lt;p&gt;If you‚Äôd like to learn more, do check out Bulmer‚Äôs original &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bulmer1974OnFitting&#34; role=&#34;doc-biblioref&#34;&gt;1974&lt;/a&gt;)&lt;/span&gt; paper and the more recent OLRE paper by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-harrison2014using&#34; role=&#34;doc-biblioref&#34;&gt;Harrison&lt;/a&gt; (&lt;a href=&#34;#ref-harrison2014using&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;. The great &lt;a href=&#34;https://twitter.com/bolkerb&#34;&gt;Ben Bolker&lt;/a&gt; wrote up a vignette (&lt;a href=&#34;https://glmm.wdfiles.com/local--files/examples/overdispersion.pdf&#34;&gt;here&lt;/a&gt;) on how to fit the OLRE Poisson-lognormal with &lt;strong&gt;lme4&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-lme4&#34; role=&#34;doc-biblioref&#34;&gt;Bates et al., 2015&lt;/a&gt;)&lt;/span&gt; and Michael Clark wrote up a very quick example of the model with &lt;strong&gt;brms&lt;/strong&gt; &lt;a href=&#34;https://m-clark.github.io/easy-bayes/posterior-predictive-checks.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy modeling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] patchwork_1.1.1 tidybayes_2.3.1 forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.6     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
##  [9] tibble_3.1.2    ggplot2_3.3.3   tidyverse_1.3.0 brms_2.15.0    
## [13] Rcpp_1.0.6     
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.3         splines_4.0.4       
##   [7] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0  
##  [19] xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [22] colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.23            callr_3.7.0         
##  [28] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2          
##  [34] gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2        
##  [40] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [43] DBI_1.1.0            miniUI_0.1.1.1       viridisLite_0.4.0   
##  [46] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.3    httr_1.4.2          
##  [52] threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.2      
##  [55] farver_2.1.0         pkgconfig_2.0.3      loo_2.4.1           
##  [58] sass_0.3.1           dbplyr_2.0.0         utf8_1.2.1          
##  [61] labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.11        
##  [64] reshape2_1.4.4       later_1.2.0          munsell_0.5.0       
##  [67] cellranger_1.1.0     tools_4.0.4          cli_2.5.0           
##  [70] generics_0.1.0       broom_0.7.6          ggridges_0.5.3      
##  [73] evaluate_0.14        fastmap_1.1.0        yaml_2.2.1          
##  [76] processx_3.5.2       knitr_1.33           fs_1.5.0            
##  [79] nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [82] xml2_1.3.2           rstudioapi_0.13      compiler_4.0.4      
##  [85] bayesplot_1.8.0      shinythemes_1.1.2    curl_4.3            
##  [88] gamm4_0.2-6          reprex_0.3.0         statmod_1.4.35      
##  [91] bslib_0.2.4          stringi_1.6.2        highr_0.9           
##  [94] ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6   
##  [97] lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
## [100] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.8         
## [103] pillar_1.6.1         lifecycle_1.0.0      jquerylib_0.1.4     
## [106] bridgesampling_1.0-0 estimability_1.3     httpuv_1.6.0        
## [109] R6_2.5.0             bookdown_0.22        promises_1.2.0.1    
## [112] gridExtra_2.3        codetools_0.2-18     boot_1.3-26         
## [115] colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [118] assertthat_0.2.1     withr_2.4.2          shinystan_2.5.0     
## [121] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [124] hms_0.5.3            grid_4.0.4           coda_0.19-4         
## [127] minqa_1.2.4          rmarkdown_2.8        shiny_1.6.0         
## [130] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-R-lme4&#34; class=&#34;csl-entry&#34;&gt;
Bates, D., M√§chler, M., Bolker, B., &amp;amp; Walker, S. (2015). Fitting linear mixed-effects models using &lt;span class=&#34;nocase&#34;&gt;lme4&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;67&lt;/em&gt;(1), 1‚Äì48. &lt;a href=&#34;https://doi.org/10.18637/jss.v067.i01&#34;&gt;https://doi.org/10.18637/jss.v067.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bulmer1974OnFitting&#34; class=&#34;csl-entry&#34;&gt;
Bulmer, M. (1974). On fitting the &lt;span&gt;Poisson&lt;/span&gt; lognormal distribution to species-abundance data. &lt;em&gt;Biometrics&lt;/em&gt;, &lt;em&gt;30&lt;/em&gt;(1), 101‚Äì110. &lt;a href=&#34;https://doi.org/10.2307/2529621&#34;&gt;https://doi.org/10.2307/2529621&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-B√ºrkner2021Distributional&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2021a). &lt;em&gt;Estimating distributional models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-B√ºrkner2021Parameterization&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2021b). &lt;em&gt;Parameterization of response distributions in brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1‚Äì28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395‚Äì411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ‚Äô&lt;span&gt;Stan&lt;/span&gt;‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cameron2013regression&#34; class=&#34;csl-entry&#34;&gt;
Cameron, A. C., &amp;amp; Trivedi, P. K. (2013). &lt;em&gt;Regression analysis of count data&lt;/em&gt; (Second Edition). &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9781139013567&#34;&gt;https://doi.org/10.1017/CBO9781139013567&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanDataAnalysisUsing2006&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., &amp;amp; Hill, J. (2006). &lt;em&gt;Data analysis using regression and multilevel/hierarchical models&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9780511790942&#34;&gt;https://doi.org/10.1017/CBO9780511790942&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-harrison2014using&#34; class=&#34;csl-entry&#34;&gt;
Harrison, X. A. (2014). Using observation-level random effects to model overdispersion in count data in ecology and evolution. &lt;em&gt;PeerJ&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;, e616. &lt;a href=&#34;https://doi.org/10.7717/peerj.616&#34;&gt;https://doi.org/10.7717/peerj.616&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hilbeNegativeBinomialRegression2011&#34; class=&#34;csl-entry&#34;&gt;
Hilbe, J. M. (2011). &lt;em&gt;Negative binomial regression&lt;/em&gt; (Second Edition). &lt;a href=&#34;https://doi.org/10.1017/CBO9780511973420&#34;&gt;https://doi.org/10.1017/CBO9780511973420&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ‚Äôgeoms‚Äô for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020a). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rethinking&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;rethinking&lt;/span&gt; &lt;span&gt;R&lt;/span&gt; package&lt;/em&gt;. &lt;a href=&#34;https://xcelab.net/rm/software/&#34;&gt;https://xcelab.net/rm/software/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-patchwork&#34; class=&#34;csl-entry&#34;&gt;
Pedersen, T. L. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;patchwork&lt;/span&gt;: &lt;span&gt;The&lt;/span&gt; composer of plots&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;https://CRAN.R-project.org/package=patchwork&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-standevelopmentteamStanFunctionsReference2021&#34; class=&#34;csl-entry&#34;&gt;
Stan Development Team. (2021). &lt;em&gt;Stan functions reference&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/docs/2_26/functions-reference/index.html&#34;&gt;https://mc-stan.org/docs/2_26/functions-reference/index.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-welsh1995habitat&#34; class=&#34;csl-entry&#34;&gt;
Welsh Jr, H. H., &amp;amp; Lind, A. J. (1995). Habitat correlates of the &lt;span&gt;Del Norte&lt;/span&gt; salamander, &lt;span&gt;Plethodon&lt;/span&gt; elongatus (&lt;span&gt;Caudata&lt;/span&gt;: &lt;span&gt;Plethodontidae&lt;/span&gt;), in northwestern &lt;span&gt;California&lt;/span&gt;. &lt;em&gt;Journal of Herpetology&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(2), 198‚Äì210. &lt;a href=&#34;https://doi.org/10.2307/1564557&#34;&gt;https://doi.org/10.2307/1564557&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ‚Äôtidyverse‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., Fran√ßois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., M√ºller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., ‚Ä¶ Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;That‚Äôs a lie. There was no pop quiz, last week.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I‚Äôm making this number up, too, but it‚Äôs probably not far off. ‚òï ‚òï ‚òï&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;One could also, of course, express that model as &lt;span class=&#34;math inline&#34;&gt;\(y_i = \beta_0 + \beta_1 x_i + \epsilon_i\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i \sim \operatorname{Normal}(0, \sigma)\)&lt;/span&gt;. But come on. That‚Äôs weak sauce. For more on why, see page 84 in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;I say ‚Äúsuggests‚Äù because a simple Poisson model can be good enough IF you have a set of high-quality predictors which can ‚Äúexplain‚Äù all that extra-looking variability. We, however, will be fitting intercept-only models.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example power analysis report</title>
      <link>/post/2021-07-02-example-power-analysis-report/</link>
      <pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-07-02-example-power-analysis-report/</guid>
      <description>
&lt;script src=&#34;/post/2021-07-02-example-power-analysis-report/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;context&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Context&lt;/h2&gt;
&lt;p&gt;In one of my recent Twitter posts, I got pissy and complained about a vague power-analysis statement I saw while reviewing a manuscript submitted to a scientific journal.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;If you submit a manuscript for publication that involves HLMs and SEMs of longitudinal data and you vaguely summarize your power analysis in one sentence, I, as your friendly neighborhood Reviewer #2, am requesting a full power-analysis write-up as a supplementary material.&lt;/p&gt;&amp;mdash; Solomon Kurz (@SolomonKurz) &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1409626961161371648?ref_src=twsrc%5Etfw&#34;&gt;June 28, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;It wasn‚Äôt my best moment and I ended up apologizing for my tone.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Okay, I apologize for getting a little pissy with the tweet. Yet the issue is real and it leads to a natural question: What would go into a good power analysis report? I‚Äôve done a few for work and I promise to morph one into a blog post, by the end of the week.&lt;/p&gt;&amp;mdash; Solomon Kurz (@SolomonKurz) &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1409634560485199876?ref_src=twsrc%5Etfw&#34;&gt;June 28, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;However, the broader issue remains. If you plan to analyze your data with anything more complicated than a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test, the power analysis phase gets tricky. The manuscript I was complaining about used a complicated multilevel model as its primary analysis. I‚Äôm willing to bet that most applied researchers (including the authors of that manuscript) have never done a power analysis for a multilevel model and probably have never seen what one might look like, either. The purpose of this post is to give a real-world example of just such an analysis.&lt;/p&gt;
&lt;p&gt;Over the past couple years, I‚Äôve done a few multilevel power analyses as part of my day job. In this post, I will reproduce one of them. For the sake of confidentiality, some of the original content will be omitted or slightly altered. But the overall workflow will be about 90% faithful to the original report I submitted to my boss. To understand this report, you should know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;my boss has some experience fitting multilevel models, but they‚Äôre not a stats jock;&lt;/li&gt;
&lt;li&gt;we had pilot data from two different sources, each with its strengths and weaknesses; and&lt;/li&gt;
&lt;li&gt;this document was meant for internal purposes only, though I believe some of its contents did make it into other materials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the end, I‚Äôll wrap this post up with a few comments. Here‚Äôs the report:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;executive-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Executive summary&lt;/h2&gt;
&lt;p&gt;A total sample size of &lt;strong&gt;164&lt;/strong&gt; is the minimum number to detect an effect size similar to that in the pilot data (i.e., Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d = 0.3\)&lt;/span&gt;). This recommendation assumes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a study design of three time points,&lt;/li&gt;
&lt;li&gt;random assignment of participants into two equal groups, and&lt;/li&gt;
&lt;li&gt;20% dropout on the second time point and another 20% dropout by the third time point.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we presume a more conservative effect size of &lt;span class=&#34;math inline&#34;&gt;\(0.2\)&lt;/span&gt; and a larger dropout rate of 30% the second and third time points, the minimum recommended total sample size is &lt;strong&gt;486&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The remainder of this report details how I came to these conclusions. For full transparency, I will supplement prose with figures, tables, and the statistical code used used for all computations. By default, the code is hidden is this document. However, if you are interested in the code, you should be able to make it appear by selecting ‚ÄúShow All Code‚Äù in the dropdown menu from the ‚ÄúCode‚Äù button on the upper-right corner.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cohens-d&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;In this report, Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is meant to indicate a standardized mean difference. The &lt;span class=&#34;math inline&#34;&gt;\(d = 0.3\)&lt;/span&gt; from above is based on the &lt;code&gt;some_file.docx&lt;/code&gt; file you shared with me last week. In Table 1, you provided the following summary information for the intervention group:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

tibble(summary = c(&amp;quot;mean&amp;quot;, &amp;quot;sd&amp;quot;),
       baseline = c(1.29, 1.13),
       followup = c(0.95, 1.09)) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;summary&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;baseline&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;followup&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;mean&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;sd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.09&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With that information, we can compute a within-subject‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; by hand. With this formula, we will be using the pooled standard deviation in the denominator.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- (1.29 - .95) / sqrt((1.13^2 + 1.09^2) / 2)
d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3062566&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, 0.306 is just a point estimate. We can express the uncertainty in that point estimate with 95% confidence intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ci &amp;lt;-
  MBESS::ci.smd(smd = d,
                n.1 = 50, 
                n.2 = 26)

ci %&amp;gt;% 
  data.frame() %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1
## Columns: 3
## $ Lower.Conf.Limit.smd &amp;lt;dbl&amp;gt; -0.1712149
## $ smd                  &amp;lt;dbl&amp;gt; 0.3062566
## $ Upper.Conf.Limit.smd &amp;lt;dbl&amp;gt; 0.7816834&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this output, &lt;code&gt;smd&lt;/code&gt; refers to ‚Äústandardized mean difference,‚Äù what what we have been referring to as Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. The output indicates the effect size for the experimental group from the pilot study was &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; of 0.31 [-0.17, .78]. The data look promising for a small/moderate effect. But those confidence intervals swing from small negative to large.&lt;/p&gt;
&lt;p&gt;For reference, here are the 50% intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MBESS::ci.smd(smd = d,
              n.1 = 50, 
              n.2 = 26,
              conf.level = .5) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1
## Columns: 3
## $ Lower.Conf.Limit.smd &amp;lt;dbl&amp;gt; 0.1412595
## $ smd                  &amp;lt;dbl&amp;gt; 0.3062566
## $ Upper.Conf.Limit.smd &amp;lt;dbl&amp;gt; 0.4691839&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 50% CIs range from 0.14 to 0.47.&lt;/p&gt;
&lt;div id=&#34;power-analyses-can-be-tailor-made.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Power analyses can be tailor made.&lt;/h3&gt;
&lt;p&gt;Whenever possible, it is preferable to tailor a power analysis to the statistical models researchers plan to use to analyze the data they intend to collect. Based on your previous analyses, I suspect you intend to fit a series of hierarchical models. I would have done the same thing with those data and I further recommend you analyze the data you intend to collect within a hierarchical growth model paradigm. With that in mind, the power analyses in the report are all based on the following model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
y_{ij} &amp;amp; = \beta_{0i} + \beta_{1i} \text{time}_{ij} + \epsilon_{ij} \\
\beta_{0i} &amp;amp; = \gamma_{00} + \gamma_{01} \text{treatment}_i +  u_{0i} \\
\beta_{1i} &amp;amp; = \gamma_{10} + \gamma_{11} \text{treatment}_i +  u_{1i}, 
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the dependent variable of interest, which varies across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; participants and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; measurement occasions. The model is linear with an intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0i}\)&lt;/span&gt; and slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1i}\)&lt;/span&gt;. As indicated by the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; subscripts, both intercepts and slopes vary across participants with grand means &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{10}\)&lt;/span&gt;, respectively, and participant-specific deviations around those means &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}\)&lt;/span&gt;, respectively. There is a focal between-participant predictor in the model, &lt;span class=&#34;math inline&#34;&gt;\(\text{treatment}_i\)&lt;/span&gt;, which is coded 0 = &lt;em&gt;control&lt;/em&gt; 1 = &lt;em&gt;treatment&lt;/em&gt;. Rearranging the the formulas into the composite form will make it clear this is an interaction model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
y_{ij} &amp;amp; = \gamma_{00} + \gamma_{01} \text{treatment}_i \\
       &amp;amp; \;\;\; + \gamma_{10} \text{time}_{ij} + \gamma_{11} \text{treatment}_i \times \text{time}_{ij} \\
       &amp;amp; \;\;\; + u_{0i} +  u_{1i} \text{time}_{ij} + \epsilon_{ij},
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the parameter of primary interest for the study is &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{11} \text{treatment}_i \times \text{time}_{ij}\)&lt;/span&gt;, the difference between the two &lt;span class=&#34;math inline&#34;&gt;\(\text{treatment}\)&lt;/span&gt; conditions in their change in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(\text{time}\)&lt;/span&gt;. As such, the focus of the power analyses reported above are on the power to reject the null hypothesis the &lt;span class=&#34;math inline&#34;&gt;\(\text{treatment}\)&lt;/span&gt; conditions do not differ in their change over &lt;span class=&#34;math inline&#34;&gt;\(\text{time}\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0: \gamma_{11} = 0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To finish out the equations, this approach makes the typical assumptions the within-participant residual term, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij}\)&lt;/span&gt;, is normally distributed around zero,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\epsilon_{ij} \sim \operatorname{Normal} (0, \sigma_\epsilon^2),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the between-participant variances &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}\)&lt;/span&gt; have a multivariate normal distribution with a mean vector of zeros,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{bmatrix} u_{0i} \\ u_{1i} \end{bmatrix} \sim \operatorname{Normal} \Bigg ( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \sigma_0^2 &amp;amp; \sigma_{01} \\ \sigma_{01} &amp;amp; \sigma_1^2 \end{bmatrix} \Bigg ).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Following convention, the within-participant residuals &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij}\)&lt;/span&gt; are orthogonal to the between-participant variances &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For simplicity, another assumption of this model that the control condition will remain constant over time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;main-results-power-curves.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Main results: Power curves.&lt;/h3&gt;
&lt;p&gt;I computed a series of power curves to examine the necessary sample size given different assumptions. Due to the uncertainty in the effect size from the pilot data, &lt;span class=&#34;math inline&#34;&gt;\(d = 0.31 [-0.17, .78]\)&lt;/span&gt;, varied the effect size from 0.1 to 0.3. I also examined different levels of missing data via dropout. These followed four patterns of dropout and were extensions of the missing data pattern described in the &lt;code&gt;some_other_file.docx&lt;/code&gt; file. They were:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(`dropout rate` = str_c(c(0, 10, 20, 30), &amp;quot;%&amp;quot;),
       baseline = &amp;quot;100%&amp;quot;,
       `1st followup` = str_c(c(100, 90, 80, 70), &amp;quot;%&amp;quot;),
       `2nd followup` = str_c(c(100, 80, 60, 40), &amp;quot;%&amp;quot;)) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;dropout rate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;baseline&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;1st followup&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;2nd followup&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;0%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;100%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;100%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;100%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;10%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;100%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;90%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;80%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;20%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;100%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;80%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;60%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;30%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;100%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;70%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;40%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The row with the 20% dropout rate, for example, corresponds directly to the dropout rate entertained in the &lt;code&gt;some_other_file.docx&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;The power simulations of this kind required two more bits of information. The first was that we specify an expected intraclass correlation coefficient (ICC). I used ICC = .9, which is the ICC value you reported in your previous work (p.¬†41).&lt;/p&gt;
&lt;p&gt;The second value needed is the ratio of &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}/ \epsilon_{ij}\)&lt;/span&gt;, sometimes called the ‚Äúvariance ratio.‚Äù I was not able to determine that value from the &lt;code&gt;some_file.docx&lt;/code&gt; or the &lt;code&gt;some_other_file.docx&lt;/code&gt;. However, I was able to compute one based on data from a different project on participants from a similar population. The data are from several hundred participants in a longitudinal survey study. The data do not include your primary variable of interest. Instead, I took the &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}/ \epsilon_{ij}\)&lt;/span&gt; from recent hierarchical analyses of two related measures. These left me with two values: 0.018 on the low end and 0.281 on the high end. Thus, I performed the power curves using both.&lt;/p&gt;
&lt;p&gt;Here is the code for the simulations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(powerlmm)

t &amp;lt;- 3
n &amp;lt;- 100

# variance ratio 0.018
icc0.9_vr_0.018_d0.1 &amp;lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.018,
                  effect_size = cohend(-0.2, 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;),
                 dropout = dropout_manual(0, 0.1, 0.2)) %&amp;gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;))

icc0.9_vr_0.018_d0.2 &amp;lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.018,
                  effect_size = cohend(-0.2, 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;),
                 dropout = dropout_manual(0, 0.2, 0.4)) %&amp;gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;))

icc0.9_vr_0.018_d0.3 &amp;lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.018,
                  effect_size = cohend(-0.2, 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;),
                 dropout = dropout_manual(0, 0.3, 0.6)) %&amp;gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;))

# variance ratio 0.281
icc0.9_vr_0.281_d0.1 &amp;lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.281,
                  effect_size = cohend(-0.2, 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;),
                 dropout = dropout_manual(0, 0.1, 0.2)) %&amp;gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;))

icc0.9_vr_0.281_d0.2 &amp;lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.281,
                  effect_size = cohend(-0.2, 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;),
                 dropout = dropout_manual(0, 0.2, 0.4)) %&amp;gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;))

icc0.9_vr_0.281_d0.3 &amp;lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.281,
                  effect_size = cohend(-0.2, 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;),
                 dropout = dropout_manual(0, 0.3, 0.6)) %&amp;gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &amp;quot;pretest_SD&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the power curve plots, beginning with the plot for the smaller variance ratio of 0.018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  icc0.9_vr_0.018_d0.1 %&amp;gt;% filter(dropout == &amp;quot;with missing&amp;quot;),
  icc0.9_vr_0.018_d0.2 %&amp;gt;% filter(dropout == &amp;quot;with missing&amp;quot;),
  icc0.9_vr_0.018_d0.3
) %&amp;gt;% 
  mutate(missing = c(rep(str_c(c(10, 20, 30, 00), &amp;quot;% missing per time point after baseline&amp;quot;), each = n() / 4))) %&amp;gt;% 
  
  mutate(d = factor(effect_size,
                    levels = c(&amp;quot;0.1&amp;quot;, &amp;quot;0.15&amp;quot;, &amp;quot;0.2&amp;quot;, &amp;quot;0.25&amp;quot;, &amp;quot;0.3&amp;quot;),
                    labels = c(&amp;quot;.10&amp;quot;, &amp;quot;.15&amp;quot;, &amp;quot;.20&amp;quot;, &amp;quot;.25&amp;quot;, &amp;quot;.30&amp;quot;))) %&amp;gt;% 
  mutate(d = fct_rev(d)) %&amp;gt;% 
  
  ggplot(aes(x = tot_n, y = power, color = d)) +
  geom_vline(xintercept = 500, color = &amp;quot;white&amp;quot;, size = 1) +
  geom_hline(yintercept = .8, color = &amp;quot;white&amp;quot;, size = 1) +
  geom_line(size = 1.5) +
  scale_color_viridis_d(expression(paste(&amp;quot;Cohen&amp;#39;s &amp;quot;, italic(d))),
                        option = &amp;quot;A&amp;quot;, end = .67, direction = -1) +
  scale_x_continuous(expression(paste(italic(N), &amp;quot; (i.e., the total sample size)&amp;quot;)), 
                     breaks = seq(from = 0, to = 1000, by = 100), limits = c(0, 1000)) +
  scale_y_continuous(breaks = c(0, .2, .4, .6, .8, 1), limits = c(0, 1)) +
  ggtitle(&amp;quot;Power curves based on a variance ratio of 0.018&amp;quot;) +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_wrap(~missing)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-02-example-power-analysis-report/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;816&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is the power curve plot for the larger variance ratio of 0.281.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  icc0.9_vr_0.281_d0.1 %&amp;gt;% filter(dropout == &amp;quot;with missing&amp;quot;),
  icc0.9_vr_0.281_d0.2 %&amp;gt;% filter(dropout == &amp;quot;with missing&amp;quot;),
  icc0.9_vr_0.281_d0.3
) %&amp;gt;% 
  mutate(missing = c(rep(str_c(c(10, 20, 30, 00), &amp;quot;% missing per time point after baseline&amp;quot;), each = n() / 4))) %&amp;gt;% 
  
  mutate(d = factor(effect_size,
                    levels = c(&amp;quot;0.1&amp;quot;, &amp;quot;0.15&amp;quot;, &amp;quot;0.2&amp;quot;, &amp;quot;0.25&amp;quot;, &amp;quot;0.3&amp;quot;),
                    labels = c(&amp;quot;.10&amp;quot;, &amp;quot;.15&amp;quot;, &amp;quot;.20&amp;quot;, &amp;quot;.25&amp;quot;, &amp;quot;.30&amp;quot;))) %&amp;gt;% 
  mutate(d = fct_rev(d)) %&amp;gt;% 
  
  ggplot(aes(x = tot_n, y = power, color = d)) +
  geom_vline(xintercept = 500, color = &amp;quot;white&amp;quot;, size = 1) +
  geom_hline(yintercept = .8, color = &amp;quot;white&amp;quot;, size = 1) +
  geom_line(size = 1.5) +
  scale_color_viridis_d(expression(paste(&amp;quot;Cohen&amp;#39;s &amp;quot;, italic(d))),
                        option = &amp;quot;A&amp;quot;, end = .67, direction = -1) +
  scale_x_continuous(expression(paste(italic(N), &amp;quot; (i.e., the total sample size)&amp;quot;)), 
                     breaks = seq(from = 0, to = 1000, by = 100), limits = c(0, 1000)) +
  scale_y_continuous(breaks = c(0, .2, .4, .6, .8, 1), limits = c(0, 1)) +
  ggtitle(&amp;quot;Power curves based on a variance ratio of 0.281&amp;quot;) +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_wrap(~missing)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-02-example-power-analysis-report/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;816&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The upshot of the variance ratio issue is that a higher variance ratio led to lower power. To be on the safe side, &lt;em&gt;I recommend leaning on the more conservative power curve estimates from the simulations based on the larger variance ratio&lt;/em&gt;, &lt;strong&gt;0.281&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A more succinct way to summarize the information in the power curves in with two tables. Here is the minimum total sample size required to reach a power of .8 based on the smaller evidence ratio of 0.018 and the various combinations of Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and dropout:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  icc0.9_vr_0.018_d0.1 %&amp;gt;% filter(dropout == &amp;quot;with missing&amp;quot;),
  icc0.9_vr_0.018_d0.2 %&amp;gt;% filter(dropout == &amp;quot;with missing&amp;quot;),
  icc0.9_vr_0.018_d0.3
) %&amp;gt;% 
  mutate(missing = c(rep(c(10, 20, 30, 00), each = n() / 4))) %&amp;gt;% 

  filter(power &amp;gt; .8) %&amp;gt;% 
  group_by(missing, effect_size) %&amp;gt;% 
  top_n(-1, power) %&amp;gt;% 
  select(-n2, -power, -dropout) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(`Cohen&amp;#39;s d` = effect_size) %&amp;gt;% 
  
  ggplot(aes(x = `Cohen&amp;#39;s d`, y = missing)) +
  geom_tile(aes(fill = tot_n),
            show.legend = F) +
  geom_text(aes(label = tot_n, color = tot_n &amp;lt; 700),
            show.legend = F) +
  scale_fill_viridis_c(option = &amp;quot;B&amp;quot;, begin = .1, end = .70 ,limits = c(0, 1000)) +
  scale_color_manual(values = c(&amp;quot;black&amp;quot;, &amp;quot;white&amp;quot;)) +
  labs(title = expression(paste(&amp;quot;Total &amp;quot;, italic(N), &amp;quot; required for .8 power, based on a variance ratio of 0.018&amp;quot;)),
       subtitle = expression(paste(&amp;quot;The power simulations only considered up to &amp;quot;, italic(N), &amp;quot; = 1,000.&amp;quot;)),
       x = expression(paste(&amp;quot;Cohen&amp;#39;s &amp;quot;, italic(d))),
       y = &amp;quot;% missing\nper follow-up&amp;quot;) +
  theme(axis.ticks = element_blank(),
        panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-02-example-power-analysis-report/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;624&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is an alternative version of that plot, this time based on the more conservative variance ratio of 0.281.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  icc0.9_vr_0.281_d0.1 %&amp;gt;% filter(dropout == &amp;quot;with missing&amp;quot;),
  icc0.9_vr_0.281_d0.2 %&amp;gt;% filter(dropout == &amp;quot;with missing&amp;quot;),
  icc0.9_vr_0.281_d0.3
) %&amp;gt;% 
  mutate(missing = c(rep(c(10, 20, 30, 00), each = n() / 4))) %&amp;gt;% 

  filter(power &amp;gt; .8) %&amp;gt;% 
  group_by(missing, effect_size) %&amp;gt;% 
  top_n(-1, power) %&amp;gt;% 
  select(-n2, -power, -dropout) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(`Cohen&amp;#39;s d` = effect_size) %&amp;gt;% 
  
  ggplot(aes(x = `Cohen&amp;#39;s d`, y = missing)) +
  geom_tile(aes(fill = tot_n),
            show.legend = F) +
  geom_text(aes(label = tot_n, color = tot_n &amp;lt; 700),
            show.legend = F) +
  scale_fill_viridis_c(option = &amp;quot;B&amp;quot;, begin = .1, end = .70 ,limits = c(0, 1000)) +
  scale_color_manual(values = c(&amp;quot;black&amp;quot;, &amp;quot;white&amp;quot;)) +
  labs(title = expression(paste(&amp;quot;Total &amp;quot;, italic(N), &amp;quot; required for .8 power, based on variance ratio of 0.281&amp;quot;)),
       subtitle = expression(paste(&amp;quot;The power simulations only considered up to &amp;quot;, italic(N), &amp;quot; = 1,000.&amp;quot;)),
       x = expression(paste(&amp;quot;Cohen&amp;#39;s &amp;quot;, italic(d))),
       y = &amp;quot;% missing\nper follow-up&amp;quot;) +
  theme(axis.ticks = element_blank(),
        panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-02-example-power-analysis-report/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;624&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, I recommend playing it safe and relying on the power estimates based on the larger variance ratio of 0.281. Those power curves indicate that even with rather large dropout (i.e., 30% at the second time point and another 30% at the final time point), &lt;span class=&#34;math inline&#34;&gt;\(N = 486\)&lt;/span&gt; is sufficient to detect a small effect size (i.e., &lt;span class=&#34;math inline&#34;&gt;\(d = 0.2\)&lt;/span&gt;) at the conventional .8 power threshold. Note that because we cut off the power simulations at &lt;span class=&#34;math inline&#34;&gt;\(N = 1{,}000\)&lt;/span&gt;, we never reached .8 power in the conditions where &lt;span class=&#34;math inline&#34;&gt;\(d = 0.1\)&lt;/span&gt; and there was missingness at or greater than 0% dropout at each follow-up time point.&lt;/p&gt;
&lt;p&gt;To clarify, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; in each cell is the total sample size presuming both the control and experimental conditions have equal numbers in each. Thus, &lt;span class=&#34;math inline&#34;&gt;\(n_\text{control} = n_\text{experimental} = N/2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrap up&lt;/h2&gt;
&lt;p&gt;I presented the original report with an HTML document, which used the R Markdown &lt;a href=&#34;https://community.rstudio.com/t/notebook-with-code-folding-hide-by-default/55845&#34;&gt;code folding&lt;/a&gt; option, which hid my code, by default. Since I‚Äôm not aware of a good way to use code folding with &lt;strong&gt;blogdown&lt;/strong&gt; blog posts, here you see the code in all its glory.&lt;/p&gt;
&lt;p&gt;All you Bayesian freaks may have noticed that this was a conventional frequentist power analysis. I‚Äôm not always a Bayesian. ü§∑ When you intend to analyze experimental RCT-like data with frequentist software, the &lt;a href=&#34;https://github.com/rpsychologist/powerlmm&#34;&gt;&lt;strong&gt;powerlmm&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-powerlmm&#34; role=&#34;doc-biblioref&#34;&gt;Magnusson, 2018&lt;/a&gt;)&lt;/span&gt; can come in really handy.&lt;/p&gt;
&lt;p&gt;Had I intended to share a report like this for a broader audience, possibly as supplemental material for a paper, I might have explained the &lt;strong&gt;powerlmm&lt;/strong&gt; code a bit more. Since this was originally meant for internal use, my main goal was to present the results with an extra bit of transparency for the sake of building trust with a new collaborator. It worked, by the way. This person‚Äôs grant money now pays for part of my salary.&lt;/p&gt;
&lt;p&gt;If this was supplementary material, I would have also spent more time explicitly showing where I got the Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, ICC, and variance ratio values.&lt;/p&gt;
&lt;p&gt;If you didn‚Äôt notice, the context for this power analysis wasn‚Äôt ideal. Even though I pulled information from two different data sources, neither was ideal and their combination wasn‚Äôt, either. Though my collaborator‚Äôs pilot data let me compute the Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the ICC, I didn‚Äôt have access to the raw data, themselves. Without that, I had no good way to compute the variance ratio. As it turns out, that was a big deal. Though I was able to compute variance ratios from different data from a similar population, it wasn‚Äôt on the same criterion variable. The best place to be in is if you have pilot data from the same population and on the same criterion variable. Outside of that, you‚Äôre making assumptions about model parameters you might not have spent a lot of time pondering, before. Welcome to the world of multilevel power analyses, friends. Keep your chins up. It‚Äôs rough, out there.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] forcats_0.5.1   stringr_1.4.0   dplyr_1.0.6     purrr_0.3.4    
## [5] readr_1.4.0     tidyr_1.1.3     tibble_3.1.2    ggplot2_3.3.3  
## [9] tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.1.1  xfun_0.23         bslib_0.2.4       haven_2.3.1      
##  [5] colorspace_2.0-0  vctrs_0.3.8       generics_0.1.0    viridisLite_0.4.0
##  [9] htmltools_0.5.1.1 emo_0.0.0.9000    yaml_2.2.1        utf8_1.2.1       
## [13] rlang_0.4.11      jquerylib_0.1.4   pillar_1.6.1      withr_2.4.2      
## [17] glue_1.4.2        DBI_1.1.0         dbplyr_2.0.0      modelr_0.1.8     
## [21] readxl_1.3.1      lifecycle_1.0.0   munsell_0.5.0     blogdown_1.3     
## [25] gtable_0.3.0      cellranger_1.1.0  rvest_0.3.6       evaluate_0.14    
## [29] labeling_0.4.2    knitr_1.33        MBESS_4.8.0       fansi_0.4.2      
## [33] highr_0.9         broom_0.7.6       Rcpp_1.0.6        backports_1.2.1  
## [37] scales_1.1.1      jsonlite_1.7.2    farver_2.1.0      fs_1.5.0         
## [41] hms_0.5.3         digest_0.6.27     stringi_1.6.2     bookdown_0.22    
## [45] grid_4.0.4        cli_2.5.0         tools_4.0.4       magrittr_2.0.1   
## [49] sass_0.3.1        crayon_1.4.1      pkgconfig_2.0.3   ellipsis_0.3.2   
## [53] xml2_1.3.2        reprex_0.3.0      lubridate_1.7.9.2 rstudioapi_0.13  
## [57] assertthat_0.2.1  rmarkdown_2.8     httr_1.4.2        R6_2.5.0         
## [61] compiler_4.0.4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-R-powerlmm&#34; class=&#34;csl-entry&#34;&gt;
Magnusson, K. (2018). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;powerlmm&lt;/span&gt;: &lt;span&gt;Power&lt;/span&gt; analysis for longitudinal multilevel models&lt;/em&gt; [Manual]. &lt;a href=&#34;https://github.com/rpsychologist/powerlmm&#34;&gt;https://github.com/rpsychologist/powerlmm&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Make ICC plots for your brms IRT models</title>
      <link>/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/</guid>
      <description>
&lt;script src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;context&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Context&lt;/h2&gt;
&lt;p&gt;Someone recently posted a &lt;a href=&#34;https://discourse.mc-stan.org/t/item-characteristic-curves-and-item-information-curves-from-item-response-models/22964&#34;&gt;thread on the Stan forums&lt;/a&gt; asking how one might make item-characteristic curve (ICC) and item-information curve (IIC) plots for an item-response theory (IRT) model fit with &lt;strong&gt;brms&lt;/strong&gt;. People were slow to provide answers and I came up disappointingly empty handed after a quick web search. The purpose of this blog post is to show how one might make ICC and IIC plots for &lt;strong&gt;brms&lt;/strong&gt; IRT models using general-purpose data wrangling steps.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;This tutorial is for those with a passing familiarity with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You‚Äôll want to be familiar with the &lt;strong&gt;brms&lt;/strong&gt; package &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt;. In addition to the references I just cited, you can find several helpful vignettes at &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;https://github.com/paul-buerkner/brms&lt;/a&gt;. I‚Äôve also written a few ebooks highlighting &lt;strong&gt;brms&lt;/strong&gt;, which you can find at &lt;a href=&#34;https://solomonkurz.netlify.app/bookdown/&#34;&gt;https://solomonkurz.netlify.app/bookdown/&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You‚Äôll want to be familiar with Bayesian multilevel regression. In addition to the resources, above, I recommend either edition of McElreath‚Äôs introductory text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; or Kruschke‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; introductory text.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You‚Äôll want to be familiar with IRT. The framework in this blog comes most directly from B√ºrkner‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; preprint. Though I‚Äôm not in a position to vouch for them myself, I‚Äôve had people recommend the texts by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-crockerIntroductionToClassical2006&#34; role=&#34;doc-biblioref&#34;&gt;Crocker &amp;amp; Algina&lt;/a&gt; (&lt;a href=&#34;#ref-crockerIntroductionToClassical2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-deayalaTheoryAndPractice2008&#34; role=&#34;doc-biblioref&#34;&gt;De Ayala&lt;/a&gt; (&lt;a href=&#34;#ref-deayalaTheoryAndPractice2008&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-reckaseMultidimensionalIRT2009&#34; role=&#34;doc-biblioref&#34;&gt;Reckase&lt;/a&gt; (&lt;a href=&#34;#ref-reckaseMultidimensionalIRT2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-bonifayMultidimensionalIRT2019&#34; role=&#34;doc-biblioref&#34;&gt;Bonifay&lt;/a&gt; (&lt;a href=&#34;#ref-bonifayMultidimensionalIRT2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-albanoIntroductionToEducational2020&#34; role=&#34;doc-biblioref&#34;&gt;Albano&lt;/a&gt; (&lt;a href=&#34;#ref-albanoIntroductionToEducational2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. Probably the best place to learn about the &lt;strong&gt;tidyverse&lt;/strong&gt;-style of coding, as well as an introduction to &lt;strong&gt;R&lt;/strong&gt;, is Grolemund and Wickham‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-grolemundDataScience2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; freely-available online text, &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R for data science&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Load the primary &lt;strong&gt;R&lt;/strong&gt; packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for this post come from the preprint by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;Loram et al.&lt;/a&gt; (&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, who generously shared their data and code on &lt;a href=&#34;https://github.com/Lingtax/2018_measures_study&#34;&gt;GitHub&lt;/a&gt; and the &lt;a href=&#34;https://osf.io/t9w2x/&#34;&gt;Open Science Framework&lt;/a&gt;. In their paper, they used IRT to make a self-report measure of climate change denial. After pruning their initial item set, Loram and colleagues settled on eight binary items for their measure. Here we load the data for those items&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;data/ccdrefined02.rda&amp;quot;)

ccdrefined02 %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 206
## Columns: 8
## $ ccd05 &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0‚Ä¶
## $ ccd18 &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0‚Ä¶
## $ ccd11 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0‚Ä¶
## $ ccd13 &amp;lt;dbl&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0‚Ä¶
## $ ccd08 &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0‚Ä¶
## $ ccd06 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0‚Ä¶
## $ ccd09 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0‚Ä¶
## $ ccd16 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you walk through the code in Loram and colleagues‚Äô &lt;a href=&#34;https://github.com/Lingtax/2018_measures_study/blob/master/Rcode/2018_Loram_CC_IRT.R&#34;&gt;&lt;code&gt;2018_Loram_CC_IRT.R&lt;/code&gt;&lt;/a&gt; file, you‚Äôll see where this version of the data comes from. For our purposes, we‚Äôll want to make an explicit participant number column and then convert the data to the long format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_long &amp;lt;- ccdrefined02 %&amp;gt;% 
  mutate(id = 1:n()) %&amp;gt;% 
  pivot_longer(-id, names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;y&amp;quot;) %&amp;gt;% 
  mutate(item = str_remove(item, &amp;quot;ccd&amp;quot;))

# what did we do?
head(dat_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##      id item      y
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 05        0
## 2     1 18        0
## 3     1 11        1
## 4     1 13        1
## 5     1 08        0
## 6     1 06        1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now responses (&lt;code&gt;y&lt;/code&gt;) are nested within participants (&lt;code&gt;id&lt;/code&gt;) and items (&lt;code&gt;item&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;irt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IRT&lt;/h2&gt;
&lt;p&gt;In his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; preprint, B√ºrkner outlined the framework for the multilevel Bayesian approach to IRT, as implemented in &lt;strong&gt;brms&lt;/strong&gt;. In short, IRT allows one to decompose the information from assessment measures into person parameters &lt;span class=&#34;math inline&#34;&gt;\((\theta)\)&lt;/span&gt; and item parameters &lt;span class=&#34;math inline&#34;&gt;\((\xi)\)&lt;/span&gt;. The IRT framework offers a large variety of model types. In this post, we‚Äôll focus on the widely-used 1PL and 2PL models. First, we‚Äôll briefly introduce them within the context of B√ºrkner‚Äôs multilevel Bayesian approach. Then we‚Äôll fit those models to the &lt;code&gt;dat_long&lt;/code&gt; data. Finally, we‚Äôll show how to explore those models using ICC and IIC plots.&lt;/p&gt;
&lt;div id=&#34;what-is-the-1pl&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is the 1PL?&lt;/h3&gt;
&lt;p&gt;With a set of binary data &lt;span class=&#34;math inline&#34;&gt;\(y_{pi}\)&lt;/span&gt;, which vary across &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; persons and &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; items, we can express the simple one-parameter logistic (1PL) model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \theta_p + \xi_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(p_{pi}\)&lt;/span&gt; parameter from the Bernoulli distribution indicates the probability of &lt;code&gt;1&lt;/code&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(p\text{th}\)&lt;/span&gt; person on the &lt;span class=&#34;math inline&#34;&gt;\(i\text{th}\)&lt;/span&gt; item. To constrain the model predictions to within the &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt; probability space, we use the logit link. Note that with this parameterization, the linear model itself is just the additive sum of the person parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and item parameter &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Within our multilevel Bayesian framework, we will expand this a bit to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \beta_0 + \theta_p + \xi_i \\
\theta_p &amp;amp; \sim \operatorname{Normal}(0, \sigma_\theta) \\
\xi_i    &amp;amp; \sim \operatorname{Normal}(0, \sigma_\xi),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the new parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the grand mean. Now our &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; parameters are expressed as deviations around the grand mean &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;. As is typical within the multilevel framework, we model these deviations as normally distributed with means set to zero and standard deviations (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\xi\)&lt;/span&gt;) estimated from the data&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To finish off our multilevel Bayesian version the 1PL, we just need to add in our priors. In this blog post, we‚Äôll follow the weakly-regularizing approach and set&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\beta_0 &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\sigma_\theta &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\sigma_\xi    &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; superscripts indicate the Student-&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; priors for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters are restricted to non-negative values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-about-the-2pl&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How about the 2PL?&lt;/h3&gt;
&lt;p&gt;We can express the two-parameter logistic (2PL) model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \alpha_i \theta_p + \alpha_i \xi_i \\
                             &amp;amp; = \alpha_i(\theta_p + \xi_i),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; parameters are now both multiplied by the discrimination parameter &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; subscript indicates the discrimination parameter varies across the items, but not across persons. We should note that because we are now multiplying parameters, this makes the 2PL a non-liner model. Within our multilevel Bayesian framework, we might express the 2PL as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \alpha (\beta_0 + \theta_p + \xi_i) \\
\alpha &amp;amp; = \beta_1 + \alpha_i \\
\theta_p &amp;amp; \sim \operatorname{Normal}(0, \sigma_\theta) \\
\begin{bmatrix} \alpha_i \\ \xi_i \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal}(\mathbf 0, \mathbf \Sigma) \\
\Sigma    &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_\alpha &amp;amp; 0 \\ 0 &amp;amp; \sigma_\xi \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix} ,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; term is multiplied by &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, in addition to the &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; parameters. But note that &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is itself a composite of its own grand mean &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and the item-level deviations around it, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt;. Since both &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; vary across items, they are modeled as multivariate normal, with a mean vector of zeros and variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt;. As is typical with &lt;strong&gt;brms&lt;/strong&gt;, we will decompose &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; into a diagonal matrix of standard deviations &lt;span class=&#34;math inline&#34;&gt;\((\mathbf S)\)&lt;/span&gt; and a correlation matrix &lt;span class=&#34;math inline&#34;&gt;\((\mathbf R)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As B√ºrkner &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; discussed in Section 5, this particular model might have identification problems without strong priors. The issue is ‚Äúa switch in the sign of &lt;span class=&#34;math inline&#34;&gt;\([\alpha]\)&lt;/span&gt; can be corrected for by a switch in the sign of &lt;span class=&#34;math inline&#34;&gt;\([(\beta_0 + \theta_p + \xi_i)]\)&lt;/span&gt; without a change in the overall likelihood.‚Äù One solution, then, would be to constrain &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; to be positive. We can do that with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \color{#8b0000}{ \exp(\log \alpha) } \color{#000000}{\times (\beta_0 + \theta_p + \xi_i)} \\
\color{#8b0000}{\log \alpha} &amp;amp; = \beta_1 + \alpha_i \\
\theta_p &amp;amp; \sim \operatorname{Normal}(0, \sigma_\theta) \\
\begin{bmatrix} \alpha_i \\ \xi_i \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal}(\mathbf 0, \mathbf \Sigma) \\
\Sigma    &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_\alpha &amp;amp; 0 \\ 0 &amp;amp; \sigma_\xi \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;wherein we are now modeling &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; on the log scale and then exponentiating &lt;span class=&#34;math inline&#34;&gt;\(\log \alpha\)&lt;/span&gt; within the linear formula for &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit}(p_{pi})\)&lt;/span&gt;. Continuing on with our weakly-regularizing approach, we will express our priors for this model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\beta_0 &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\beta_1 &amp;amp; \sim \operatorname{Normal}(0, 1) \\
\sigma_\theta &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\sigma_\alpha &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\sigma_\xi    &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\mathbf R &amp;amp; \sim \operatorname{LKJ}(2),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where LKJ is the Lewandowski, Kurowicka, and Joe prior for correlation matrices &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lewandowski2009generating&#34; role=&#34;doc-biblioref&#34;&gt;Lewandowski et al., 2009&lt;/a&gt;)&lt;/span&gt;. With &lt;span class=&#34;math inline&#34;&gt;\(\eta = 2\)&lt;/span&gt;, the LKJ weakly regularizes the correlations away from extreme values&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fire-up-brms.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fire up &lt;strong&gt;brms&lt;/strong&gt;.&lt;/h3&gt;
&lt;p&gt;With &lt;code&gt;brms::brm()&lt;/code&gt;, we can fit our 1PL model with conventional multilevel syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;irt1 &amp;lt;- brm(
  data = dat_long,
  family = brmsfamily(&amp;quot;bernoulli&amp;quot;, &amp;quot;logit&amp;quot;),
  y ~ 1 + (1 | item) + (1 | id),
  prior = c(prior(normal(0, 1.5), class = Intercept),
            prior(student_t(10, 0, 1), class = sd)),
  cores = 4, seed = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our non-linear 2PL model, however, will require the &lt;strong&gt;brms&lt;/strong&gt; non-linear syntax &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-B√ºrkner2021Non_linear&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2021&lt;/a&gt;)&lt;/span&gt;. Here we‚Äôll follow the same basic configuration B√ºrkner used in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; IRT preprint.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;irt2 &amp;lt;- brm(
  data = dat_long,
  family = brmsfamily(&amp;quot;bernoulli&amp;quot;, &amp;quot;logit&amp;quot;),
  bf(
    y ~ exp(logalpha) * eta,
    eta ~ 1 + (1 |i| item) + (1 | id),
    logalpha ~ 1 + (1 |i| item),
    nl = TRUE
  ),
  prior = c(prior(normal(0, 1.5), class = b, nlpar = eta),
            prior(normal(0, 1), class = b, nlpar = logalpha),
            prior(student_t(10, 0, 1), class = sd, nlpar = eta),
            prior(student_t(10, 0, 1), class = sd, nlpar = logalpha),
            prior(lkj(2), class = cor)),
  cores = 4, seed = 1,
  control = list(adapt_delta = .99)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that for &lt;code&gt;irt2&lt;/code&gt;, we had to adjust the &lt;code&gt;adapt_delta&lt;/code&gt; settings to stave off a few divergent transitions. Anyway, here are the parameter summaries for the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(irt1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: bernoulli 
##   Links: mu = logit 
## Formula: y ~ 1 + (1 | item) + (1 | id) 
##    Data: dat_long (Number of observations: 1648) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 206) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     3.81      0.38     3.12     4.61 1.01     1129     2107
## 
## ~item (Number of levels: 8) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.96      0.29     0.55     1.67 1.00     1815     2835
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -2.88      0.47    -3.82    -1.94 1.00     1308     2040
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(irt2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: bernoulli 
##   Links: mu = logit 
## Formula: y ~ exp(logalpha) * eta 
##          eta ~ 1 + (1 | i | item) + (1 | id)
##          logalpha ~ 1 + (1 | i | item)
##    Data: dat_long (Number of observations: 1648) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 206) 
##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(eta_Intercept)     1.78      0.69     0.68     3.39 1.00     1933     2321
## 
## ~item (Number of levels: 8) 
##                                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(eta_Intercept)                         0.50      0.25     0.16     1.16 1.00     1521     2181
## sd(logalpha_Intercept)                    0.36      0.16     0.13     0.74 1.00     1544     2075
## cor(eta_Intercept,logalpha_Intercept)     0.44      0.31    -0.26     0.90 1.00     2847     2922
## 
## Population-Level Effects: 
##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## eta_Intercept         -1.41      0.57    -2.71    -0.51 1.00     1700     2140
## logalpha_Intercept     0.91      0.42     0.16     1.82 1.00     1944     2282
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I‚Äôm not going to bother interpreting these results because, well, this isn‚Äôt a full-blown IRT tutorial. For our purposes, we‚Äôll just note that the &lt;span class=&#34;math inline&#34;&gt;\(\widehat R\)&lt;/span&gt; and effective sample size values all look good and nothing seems off with the parameter summaries. They‚Äôre not shown here, but the trace plots look good, too. We‚Äôre on good footing to explore the models with our ICC and IIC plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iccs.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ICCs.&lt;/h3&gt;
&lt;p&gt;For IRT models of binary items, item-characteristic curves (ICCs) show the expected relation between one‚Äôs underlying ‚Äúability‚Äù and the probability of scoring 1 on a given item. In our models, above, each participant in the data had a their underlying ability estimated by way of the &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters. However, what we want, here, is is to specify the relevant part of the parameter space for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; without reference to any given participant. Since the the 1PL and 2PL models are fit with the logit link, this will mean entertaining &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values ranging within an interval like &lt;span class=&#34;math inline&#34;&gt;\([-4, 4]\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\([-6, 6]\)&lt;/span&gt;. This range will define our &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; axis. Since our &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis has to do with probabilities, it will range from 0 to 1. The trick is knowing how to work with the posterior draws to compute the relevant probability values for their corresponding &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values.&lt;/p&gt;
&lt;p&gt;We‚Äôll start with our 1PL model, &lt;code&gt;irt1&lt;/code&gt;. First, we extract the posterior draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(irt1)

# what is this?
glimpse(post)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I‚Äôm not showing the output for &lt;code&gt;glimpse(post)&lt;/code&gt; because &lt;code&gt;post&lt;/code&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(4{,}000 \times 218\)&lt;/span&gt; data frame and all that output is just too much for a blog post. Here‚Äôs a more focused look at the primary columns of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 9
## $ b_Intercept            &amp;lt;dbl&amp;gt; -2.347027, -2.710615, -2.613853, -3.166867, -2.676470, -2.794820, -3.079290, ‚Ä¶
## $ `r_item[05,Intercept]` &amp;lt;dbl&amp;gt; -1.6323003, -1.8058115, -1.8621189, -1.6090339, -2.5323344, -1.5353327, -1.43‚Ä¶
## $ `r_item[06,Intercept]` &amp;lt;dbl&amp;gt; 0.1559534455, -0.2575624215, 0.0149043141, 0.4396739597, -0.2644399951, 0.115‚Ä¶
## $ `r_item[08,Intercept]` &amp;lt;dbl&amp;gt; -0.4202201485, -0.3789631590, -0.5076833653, -0.1256742164, -0.9035688171, -0‚Ä¶
## $ `r_item[09,Intercept]` &amp;lt;dbl&amp;gt; 0.16016701, 0.04171247, 0.19869473, 0.92882887, 0.07287032, 0.41171578, 0.935‚Ä¶
## $ `r_item[11,Intercept]` &amp;lt;dbl&amp;gt; -0.80410817, -1.31470954, -0.98800140, -0.58681735, -1.75515227, -1.20933771,‚Ä¶
## $ `r_item[13,Intercept]` &amp;lt;dbl&amp;gt; -0.78569824, -0.68159922, -0.69046836, -0.33288882, -0.96657408, -0.01837113,‚Ä¶
## $ `r_item[16,Intercept]` &amp;lt;dbl&amp;gt; 0.2638089, 0.4226998, 0.1968664, 0.9465201, 0.1394158, 0.3445774, 1.0695805, ‚Ä¶
## $ `r_item[18,Intercept]` &amp;lt;dbl&amp;gt; -1.1746881, -1.2400993, -1.4093647, -1.1124993, -1.8502372, -0.9874308, -1.10‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each of our 8 questionnaire items, we compute their conditional probability with the equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y = 1) = \operatorname{logit}^{-1}(\beta_0 + \xi_i + \theta),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit}^{-1}\)&lt;/span&gt; is the inverse logit function&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\exp(x)}{1 + \exp(x)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;brms&lt;/strong&gt;, we have access to the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit}^{-1}\)&lt;/span&gt; function by way of the convenience function called &lt;code&gt;inv_logit_scaled()&lt;/code&gt;. Before we put the &lt;code&gt;inv_logit_scaled()&lt;/code&gt; function to use, we‚Äôll want to rearrange our &lt;code&gt;post&lt;/code&gt; samples into the long format so that all the &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; draws for each of the eight items are nested within a single column, which we‚Äôll call &lt;code&gt;xi&lt;/code&gt;. We‚Äôll index which draw corresponds to which of the eight items with a nominal &lt;code&gt;item&lt;/code&gt; column. And to make this all work within the context of 4,000 posterior draws, we‚Äôll also need to make an iteration index, which we‚Äôll call &lt;code&gt;iter&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;), names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;xi&amp;quot;) %&amp;gt;% 
  mutate(item = str_extract(item, &amp;quot;\\d+&amp;quot;)) 

# what is this?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   b_Intercept  iter item      xi
##         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
## 1       -2.35     1 05    -1.63 
## 2       -2.35     1 06     0.156
## 3       -2.35     1 08    -0.420
## 4       -2.35     1 09     0.160
## 5       -2.35     1 11    -0.804
## 6       -2.35     1 13    -0.786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we‚Äôre ready to compute our probabilities, conditional in different ability &lt;span class=&#34;math inline&#34;&gt;\((\theta)\)&lt;/span&gt; levels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  expand(nesting(iter, b_Intercept, item, xi),
         theta = seq(from = -6, to = 6, length.out = 100)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(b_Intercept + xi + theta)) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(p = mean(p))

# what have we done?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
## # Groups:   theta [1]
##   theta item          p
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1    -6 05    0.0000364
## 2    -6 06    0.000241 
## 3    -6 08    0.000162 
## 4    -6 09    0.000300 
## 5    -6 11    0.0000797
## 6    -6 13    0.000121&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With those summaries in hand, it‚Äôs trivial to make the ICC plot with good old &lt;strong&gt;ggplot2&lt;/strong&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  ggplot(aes(x = theta, y = p, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;ICCs for the 1PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior mean.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = expression(italic(p)(y==1))) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since each item had a relatively low response probability, you have to go pretty far into the right-hand side of the &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; range before the curves start to approach the top of the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis.&lt;/p&gt;
&lt;p&gt;To make the ICCs for the 2PL model, the data wrangling will require a couple more steps. First, we extract the posterior draws and take a quick look at the columns of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(irt2) 

# what do we care about?
post %&amp;gt;% 
  select(b_eta_Intercept, b_logalpha_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 18
## $ b_eta_Intercept                  &amp;lt;dbl&amp;gt; -1.3855075, -1.4968294, -1.6918246, -1.6252046, -1.7095222, -1.4090‚Ä¶
## $ b_logalpha_Intercept             &amp;lt;dbl&amp;gt; 0.7814705, 0.6435611, 0.3971659, 0.5084503, 0.7547493, 1.2154113, 1‚Ä¶
## $ `r_item__eta[05,Intercept]`      &amp;lt;dbl&amp;gt; -0.7922848, -0.7423704, -1.1893399, -1.0718978, -0.2636685, -0.3965‚Ä¶
## $ `r_item__eta[06,Intercept]`      &amp;lt;dbl&amp;gt; 0.26895443, 0.21153136, 0.24488884, 0.20332673, 0.52650711, 0.51766‚Ä¶
## $ `r_item__eta[08,Intercept]`      &amp;lt;dbl&amp;gt; 0.1730248531, -0.0251183662, 0.0966877581, -0.0626794892, 0.3235805‚Ä¶
## $ `r_item__eta[09,Intercept]`      &amp;lt;dbl&amp;gt; 0.50089389, 0.39236895, 0.35678623, 0.41680088, 0.66285740, 0.53730‚Ä¶
## $ `r_item__eta[11,Intercept]`      &amp;lt;dbl&amp;gt; -0.17619531, -1.00068058, -0.41426442, -0.71686852, -0.15409689, -0‚Ä¶
## $ `r_item__eta[13,Intercept]`      &amp;lt;dbl&amp;gt; 0.02890276, -0.19689487, 0.12965889, -0.13639774, 0.35999998, 0.250‚Ä¶
## $ `r_item__eta[16,Intercept]`      &amp;lt;dbl&amp;gt; 0.5650797, 0.3484697, 0.5692639, 0.4691862, 0.5286982, 0.7387770, 0‚Ä¶
## $ `r_item__eta[18,Intercept]`      &amp;lt;dbl&amp;gt; -0.03100911, -0.94785287, -0.40602161, -1.06680784, -0.29232278, 0.‚Ä¶
## $ `r_item__logalpha[05,Intercept]` &amp;lt;dbl&amp;gt; -0.445733584, -0.168897075, -0.175487957, -0.023652177, -0.38447241‚Ä¶
## $ `r_item__logalpha[06,Intercept]` &amp;lt;dbl&amp;gt; 0.106865933, 0.183709540, 0.352641061, 0.372592174, 0.606879092, 0.‚Ä¶
## $ `r_item__logalpha[08,Intercept]` &amp;lt;dbl&amp;gt; 0.279715442, 0.224570579, 0.613387942, 0.344415036, 0.249641920, -0‚Ä¶
## $ `r_item__logalpha[09,Intercept]` &amp;lt;dbl&amp;gt; 0.46984590, 0.59974910, 0.61973239, 0.57372330, 0.67782554, 0.38758‚Ä¶
## $ `r_item__logalpha[11,Intercept]` &amp;lt;dbl&amp;gt; -0.23306444, -0.85055396, -0.31303715, -0.60098567, -0.40559846, -0‚Ä¶
## $ `r_item__logalpha[13,Intercept]` &amp;lt;dbl&amp;gt; -0.008225223, 0.116253481, 0.122579334, 0.291166675, 0.689798894, 0‚Ä¶
## $ `r_item__logalpha[16,Intercept]` &amp;lt;dbl&amp;gt; 0.092113896, 0.104240677, 0.092359783, 0.097844755, 0.494488271, 0.‚Ä¶
## $ `r_item__logalpha[18,Intercept]` &amp;lt;dbl&amp;gt; 0.09498269, -0.15278053, 0.01792382, -0.01208098, -0.31325790, -0.0‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now there are 16 &lt;code&gt;r_item__&lt;/code&gt; columns, half of which correspond to the &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; deviations and the other half of which correspond to the &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; deviations. In addition, we also have the &lt;code&gt;b_logalpha_Intercept&lt;/code&gt; columns to content with. So this time, we‚Äôll follow up our &lt;code&gt;pivot_longer()&lt;/code&gt; code with subsequent &lt;code&gt;mutate()&lt;/code&gt; and &lt;code&gt;select()&lt;/code&gt; steps, and complete the task with &lt;code&gt;pivot_wider()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  select(b_eta_Intercept, b_logalpha_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(item      = str_extract(name, &amp;quot;\\d+&amp;quot;),
         parameter = ifelse(str_detect(name, &amp;quot;eta&amp;quot;), &amp;quot;xi&amp;quot;, &amp;quot;logalpha&amp;quot;)) %&amp;gt;% 
  select(-name) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value)

# what does this look like, now?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   b_eta_Intercept b_logalpha_Intercept  iter item       xi logalpha
##             &amp;lt;dbl&amp;gt;                &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1           -1.39                0.781     1 05    -0.792  -0.446  
## 2           -1.39                0.781     1 06     0.269   0.107  
## 3           -1.39                0.781     1 08     0.173   0.280  
## 4           -1.39                0.781     1 09     0.501   0.470  
## 5           -1.39                0.781     1 11    -0.176  -0.233  
## 6           -1.39                0.781     1 13     0.0289 -0.00823&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this configuration, it‚Äôs only a little more complicated to compute the probability summaries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  expand(nesting(iter, b_eta_Intercept, b_logalpha_Intercept, item, xi, logalpha),
         theta = seq(from = -6, to = 6, length.out = 100)) %&amp;gt;% 
  # note the difference in the equation
  mutate(p = inv_logit_scaled(exp(b_logalpha_Intercept + logalpha) * (b_eta_Intercept + theta + xi))) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(p = mean(p))

# what have we done?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
## # Groups:   theta [1]
##   theta item           p
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;
## 1    -6 05    0.0000152 
## 2    -6 06    0.00000543
## 3    -6 08    0.00000967
## 4    -6 09    0.00000105
## 5    -6 11    0.000127  
## 6    -6 13    0.00000111&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  ggplot(aes(x = theta, y = p, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;ICCs for the 2PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior mean.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = expression(italic(p)(y==1))) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like those &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; parameters made a big difference for the ICCs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iics.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;IICs.&lt;/h3&gt;
&lt;p&gt;From a computational standpoint, item information curves (IICs) are a transformation of the ICCs. Recall that the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis for the ICC is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, the probability &lt;span class=&#34;math inline&#34;&gt;\(y = 1\)&lt;/span&gt; for a given item. For the IIC plots, the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis shows information, which is a simple transformation of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{information} = p(1 - p).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So here‚Äôs how to use that equation and make the IIC plot for our 1PL model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these wrangling steps are all the same as before
posterior_samples(irt1) %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;), names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;xi&amp;quot;) %&amp;gt;% 
  mutate(item = str_extract(item, &amp;quot;\\d+&amp;quot;)) %&amp;gt;% 
  expand(nesting(iter, b_Intercept, item, xi),
         theta = seq(from = -6, to = 6, length.out = 200)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(b_Intercept + xi + theta)) %&amp;gt;% 
  
  # this part, right here, is what&amp;#39;s new
  mutate(i = p * (1 - p)) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(i = median(i)) %&amp;gt;%
  
  # now plot!
  ggplot(aes(x = theta, y = i, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;IICs for the 1PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior median.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = &amp;quot;information&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For kicks and giggles, we used the posterior medians, rather than the means. It‚Äôs similarly easy to compute the item-level information for the 2PL.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these wrangling steps are all the same as before
posterior_samples(irt2) %&amp;gt;% 
  select(b_eta_Intercept, b_logalpha_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(item      = str_extract(name, &amp;quot;\\d+&amp;quot;),
         parameter = ifelse(str_detect(name, &amp;quot;eta&amp;quot;), &amp;quot;xi&amp;quot;, &amp;quot;logalpha&amp;quot;)) %&amp;gt;% 
  select(-name) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  expand(nesting(iter, b_eta_Intercept, b_logalpha_Intercept, item, xi, logalpha),
         theta = seq(from = -6, to = 6, length.out = 200)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(exp(b_logalpha_Intercept + logalpha) * (b_eta_Intercept + theta + xi))) %&amp;gt;% 

  # again, here&amp;#39;s the new part
  mutate(i = p * (1 - p)) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(i = median(i)) %&amp;gt;%
  
  # now plot!
  ggplot(aes(x = theta, y = i, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;IICs for the 2PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior median.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = &amp;quot;information&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;tic.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;TIC.&lt;/h4&gt;
&lt;p&gt;Sometimes researchers want to get a overall sense of the information in a group of items. For simplicity, here, we‚Äôll just call groups of items a &lt;em&gt;test&lt;/em&gt;. The test information curve (TIC) is a special case of the IIC, but applied to the whole test. In short, you compute the TIC by summing up the information for the individual items at each level of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Using the 1PL as an example, here‚Äôs how we might do that by hand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(irt1) %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;), names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;xi&amp;quot;) %&amp;gt;% 
  mutate(item = str_extract(item, &amp;quot;\\d+&amp;quot;)) %&amp;gt;% 
  expand(nesting(iter, b_Intercept, item, xi),
         theta = seq(from = -6, to = 6, length.out = 200)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(b_Intercept + xi + theta)) %&amp;gt;% 
  mutate(i = p * (1 - p)) %&amp;gt;% 
  
  # this is where the TIC magic happens
  group_by(theta, iter) %&amp;gt;% 
  summarise(sum_i = sum(i)) %&amp;gt;% 
  group_by(theta) %&amp;gt;% 
  summarise(i = median(sum_i)) %&amp;gt;%
  
  # we plot
  ggplot(aes(x = theta, y = i)) +
  geom_line() +
  labs(title = &amp;quot;The test information curve for the 1PL&amp;quot;,
       subtitle = &amp;quot;The curve is based on the posterior median.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = &amp;quot;information&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Taken as a whole, the combination of the eight items &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;Loram et al.&lt;/a&gt; (&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; settled on does a reasonable job differentiating among those with high &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; values. But this combination of items isn‚Äôt going to be the best at differentiating among those on the lower end of the &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; scale. You might say these eight items make for a difficult test.&lt;/p&gt;
&lt;p&gt;Our method of extending the 1PL IIC to the TIC should work the same for the 2PL. I‚Äôll leave it as an exercise for the interested reader.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;We might outlines the steps in this post as:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fit your &lt;strong&gt;brms&lt;/strong&gt; IRT model.&lt;/li&gt;
&lt;li&gt;Inspect the model with all your standard quality checks (e.g., &lt;span class=&#34;math inline&#34;&gt;\(\widehat R\)&lt;/span&gt; values, trace plots).&lt;/li&gt;
&lt;li&gt;Extract your posterior draws with the &lt;code&gt;posterior_samples()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;Isolate the item-related columns. Within the multilevel IRT context, this will typically involve an overall intercept (e.g., &lt;code&gt;b_Intercept&lt;/code&gt; for our 1PL &lt;code&gt;irt1&lt;/code&gt;) and item-specific deviations (e.g., the columns starting with &lt;code&gt;r_item&lt;/code&gt; in our 1PL &lt;code&gt;irt1&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Arrange the data into a format that makes it easy to add the overall intercept in question to each of the item-level deviations in question. For me, this seemed easiest with the long format via the &lt;code&gt;pivot_longer()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;Expand the data over a range of ability &lt;span class=&#34;math inline&#34;&gt;\((\theta)\)&lt;/span&gt; values. For me, this worked well with the &lt;code&gt;expand()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;Use the model-implied formula to compute the &lt;span class=&#34;math inline&#34;&gt;\(p(y = 1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Group the results by item and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and summarize the &lt;span class=&#34;math inline&#34;&gt;\(p(y = 1)\)&lt;/span&gt; distributions with something like the mean or median.&lt;/li&gt;
&lt;li&gt;Plot the results with &lt;code&gt;ggplot2::geom_line()&lt;/code&gt; and friends.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;You should be able to generalize this workflow to IRT models for data with more than two categories. You‚Äôll just have to be careful about juggling your thresholds. You might find some inspiration along these lines &lt;a href=&#34;https://bookdown.org/content/4857/monsters-and-mixtures.html#ordered-categorical-outcomes&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://bookdown.org/content/3686/ordinal-predicted-variable.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You could totally switch up this workflow to use some of the data wrangling helpers from the &lt;a href=&#34;https://CRAN.R-project.org/package=tidybayes&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;. That could be a nifty little blog post in and of itself.&lt;/p&gt;
&lt;p&gt;One thing that‚Äôs super lame about conventional ICC/IIC plots is there‚Äôs no expression of uncertainty. To overcome that, you could compute the 95% intervals (or 50% or whatever) in the same &lt;code&gt;summarise()&lt;/code&gt; line where you computed the mean and then express those interval bounds with something like &lt;code&gt;geom_ribbon()&lt;/code&gt; in your plot. The difficulty I foresee is it will result in overplotting for any models with more than like five items. Perhaps faceting would be the solution, there.&lt;/p&gt;
&lt;p&gt;I‚Äôm no IRT jock and may have goofed some of the steps or equations. To report mistakes or provide any other constructive criticism, just chime in on this Twitter thread:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New blog up!&lt;a href=&#34;https://t.co/ZOy9Cxrqat&#34;&gt;https://t.co/ZOy9Cxrqat&lt;/a&gt;&lt;br&gt;&lt;br&gt;This time we practice making ICC plots for &lt;a href=&#34;https://twitter.com/hashtag/brms?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#brms&lt;/a&gt;-based multilevel IRT models.&lt;/p&gt;&amp;mdash; Solomon Kurz (@SolomonKurz) &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1409951540228628482?ref_src=twsrc%5Etfw&#34;&gt;June 29, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.6     purrr_0.3.4    
##  [7] readr_1.4.0     tidyr_1.1.3     tibble_3.1.2    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6         splines_4.0.4       
##   [6] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17        digest_0.6.27       
##  [11] htmltools_0.5.1.1    rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [16] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [21] colorspace_2.0-0     rvest_0.3.6          haven_2.3.1          xfun_0.23            callr_3.7.0         
##  [26] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [31] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0             pkgbuild_1.2.0      
##  [36] rstan_2.21.2         abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0           
##  [41] miniUI_0.1.1.1       viridisLite_0.4.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [46] DT_0.16              htmlwidgets_1.5.3    httr_1.4.2           threejs_0.3.3        ellipsis_0.3.2      
##  [51] farver_2.1.0         pkgconfig_2.0.3      loo_2.4.1            sass_0.3.1           dbplyr_2.0.0        
##  [56] utf8_1.2.1           labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.11         reshape2_1.4.4      
##  [61] later_1.2.0          munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.5.0           
##  [66] generics_0.1.0       broom_0.7.6          ggridges_0.5.3       evaluate_0.14        fastmap_1.1.0       
##  [71] yaml_2.2.1           processx_3.5.2       knitr_1.33           fs_1.5.0             nlme_3.1-152        
##  [76] mime_0.10            projpred_2.0.2       xml2_1.3.2           rstudioapi_0.13      compiler_4.0.4      
##  [81] bayesplot_1.8.0      shinythemes_1.1.2    curl_4.3             gamm4_0.2-6          reprex_0.3.0        
##  [86] statmod_1.4.35       bslib_0.2.4          stringi_1.6.2        highr_0.9            ps_1.6.0            
##  [91] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [96] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.8          pillar_1.6.1         lifecycle_1.0.0     
## [101] jquerylib_0.1.4      bridgesampling_1.0-0 estimability_1.3     httpuv_1.6.0         R6_2.5.0            
## [106] bookdown_0.22        promises_1.2.0.1     gridExtra_2.3        codetools_0.2-18     boot_1.3-26         
## [111] colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1     withr_2.4.2         
## [116] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.8        shiny_1.6.0         
## [126] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-albanoIntroductionToEducational2020&#34; class=&#34;csl-entry&#34;&gt;
Albano, T. (2020). &lt;em&gt;Introduction to educational and psychological measurement using &lt;span&gt;R&lt;/span&gt;&lt;/em&gt;. &lt;a href=&#34;https://www.thetaminusb.com/intro-measurement-r/&#34;&gt;https://www.thetaminusb.com/intro-measurement-r/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bonifayMultidimensionalIRT2019&#34; class=&#34;csl-entry&#34;&gt;
Bonifay, W. (2019). &lt;em&gt;Multidimensional item response theory&lt;/em&gt;. &lt;span&gt;SAGE Publications&lt;/span&gt;. &lt;a href=&#34;https://us.sagepub.com/en-us/nam/multidimensional-item-response-theory/book257740&#34;&gt;https://us.sagepub.com/en-us/nam/multidimensional-item-response-theory/book257740&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBayesianItemResponse2020&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2020a). Bayesian item response modeling in &lt;span&gt;R&lt;/span&gt; with brms and &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;arXiv:1905.09501 [Stat]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/1905.09501&#34;&gt;http://arxiv.org/abs/1905.09501&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-B√ºrkner2021Non_linear&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2021). &lt;em&gt;Estimating non-linear models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1‚Äì28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395‚Äì411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ‚Äô&lt;span&gt;Stan&lt;/span&gt;‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-crockerIntroductionToClassical2006&#34; class=&#34;csl-entry&#34;&gt;
Crocker, L., &amp;amp; Algina, J. (2006). &lt;em&gt;Introduction to classical and modern test theory&lt;/em&gt;. &lt;span&gt;Cengage Learning&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-deayalaTheoryAndPractice2008&#34; class=&#34;csl-entry&#34;&gt;
De Ayala, R. J. (2008). &lt;em&gt;The theory and practice of item response theory&lt;/em&gt;. &lt;span&gt;Guilford Publications&lt;/span&gt;. &lt;a href=&#34;https://www.guilford.com/books/The-Theory-and-Practice-of-Item-Response-Theory/R-de-Ayala/9781593858698&#34;&gt;https://www.guilford.com/books/The-Theory-and-Practice-of-Item-Response-Theory/R-de-Ayala/9781593858698&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-grolemundDataScience2017&#34; class=&#34;csl-entry&#34;&gt;
Grolemund, G., &amp;amp; Wickham, H. (2017). &lt;em&gt;R for data science&lt;/em&gt;. &lt;span&gt;O‚ÄôReilly&lt;/span&gt;. &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;https://r4ds.had.co.nz&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ‚Äôgeoms‚Äô for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lewandowski2009generating&#34; class=&#34;csl-entry&#34;&gt;
Lewandowski, D., Kurowicka, D., &amp;amp; Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt;, &lt;em&gt;100&lt;/em&gt;(9), 1989‚Äì2001. &lt;a href=&#34;https://doi.org/10.1016/j.jmva.2009.04.008&#34;&gt;https://doi.org/10.1016/j.jmva.2009.04.008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-loramValidationOfANovel2019&#34; class=&#34;csl-entry&#34;&gt;
Loram, G., Ling, M., Head, A., &amp;amp; Clarke, E. J. R. (2019). &lt;em&gt;Validation of a novel climate change denial measure using item response theory&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/57nbk&#34;&gt;https://doi.org/10.31234/osf.io/57nbk&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-reckaseMultidimensionalIRT2009&#34; class=&#34;csl-entry&#34;&gt;
Reckase, M. D. (2009). &lt;em&gt;Multidimensional item response theory models&lt;/em&gt;. &lt;span&gt;Springer&lt;/span&gt;. &lt;a href=&#34;https://www.springer.com/gp/book/9780387899756&#34;&gt;https://www.springer.com/gp/book/9780387899756&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ‚Äôtidyverse‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., Fran√ßois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., M√ºller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., ‚Ä¶ Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I should disclose that although I have not read through Bonifay‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bonifayMultidimensionalIRT2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; text, he offered to send me a copy around the time I uploaded this post.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;You can find a copy of these data on my GitHub &lt;a href=&#34;https://github.com/ASKurz/blogdown/tree/main/content/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/data&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Adopting the three-term multilevel structure‚Äì&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \theta_p + \xi_i\)&lt;/span&gt;, where the latter two terms are &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, \sigma_x)\)&lt;/span&gt;‚Äìplaces this form of the 1PL model squarely within the generalized linear multilevel model (GLMM). McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;, Chapter 12)&lt;/span&gt; referred to this particular model type as a cross-classified model. Coming from another perspective, Kruschke &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;, Chapters 19 and 20)&lt;/span&gt; described this as a kind of multilevel analysis of variance (ANOVA).&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;For a nice blog post on the LKJ, check out Stephen Martin‚Äôs &lt;a href=&#34;http://srmart.in/is-the-lkj1-prior-uniform-yes/&#34;&gt;&lt;em&gt;Is the LKJ(1) prior uniform? ‚ÄúYes‚Äù&lt;/em&gt;&lt;/a&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Don&#39;t forget your inits</title>
      <link>/post/2021-06-05-don-t-forget-your-inits/</link>
      <pubDate>Sat, 05 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-06-05-don-t-forget-your-inits/</guid>
      <description>
&lt;script src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;When your MCMC chains look a mess, you might have to manually set your initial values. If you‚Äôre a fancy pants, you can use a custom function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;context&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Context&lt;/h2&gt;
&lt;p&gt;A collaborator asked me to help model some reaction-time data. One of the first steps was to decide on a reasonable likelihood function. You can see a productive Twitter thread on that process &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1398000353875005444&#34;&gt;here&lt;/a&gt;. Although I‚Äôve settled on the shifted-lognormal function, I also considered the exponentially modified Gaussian function (a.k.a. exGaussian). As it turns out, the exGaussian can be fussy to work with! After several frustrating attempts, I solved the problem by fiddling with my initial values. The purpose of this post is to highlight the issue and give you some options.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This post is for Bayesians. For thorough introductions to contemporary Bayesian regression, I recommend either edition of McElreath‚Äôs text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Kruschke‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text; or Gelman, Hill, and Vehtari‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; text.&lt;/li&gt;
&lt;li&gt;Though not necessary, it will help if you‚Äôre familiar with multilevel regression. The texts by McElreath and Kruschke, from above, can both help with that.&lt;/li&gt;
&lt;li&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with an emphasis on the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. We will also make good use of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;, the &lt;strong&gt;patchwork&lt;/strong&gt; package &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-patchwork&#34; role=&#34;doc-biblioref&#34;&gt;Pedersen, 2019&lt;/a&gt;)&lt;/span&gt;, and &lt;strong&gt;ggmcmc&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-marinGgmcmcAnalysisMCMC2016&#34; role=&#34;doc-biblioref&#34;&gt;Fern√°ndez i Mar√≠n, 2016&lt;/a&gt;, &lt;a href=&#34;#ref-R-ggmcmc&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. We will also use the &lt;strong&gt;lisa&lt;/strong&gt; package &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-lisa&#34; role=&#34;doc-biblioref&#34;&gt;Littlefield, 2020&lt;/a&gt;)&lt;/span&gt; to select the color palette for our figures.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Load the primary &lt;strong&gt;R&lt;/strong&gt; packages and adjust the global plotting theme defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load
library(tidyverse)
library(brms)
library(patchwork)
library(ggmcmc)
library(lisa)

# define the color palette
fk &amp;lt;- lisa_palette(&amp;quot;FridaKahlo&amp;quot;, n = 31, type = &amp;quot;continuous&amp;quot;)

# adjust the global plotting theme
theme_set(
  theme_gray(base_size = 13) +
    theme(
      text = element_text(family = &amp;quot;Times&amp;quot;, color = fk[1]),
      axis.text = element_text(family = &amp;quot;Times&amp;quot;, color = fk[1]),
      axis.ticks = element_line(color = fk[1]),
      legend.background = element_blank(),
      legend.box.background = element_blank(),
      legend.key = element_blank(),
      panel.background = element_rect(fill = alpha(fk[16], 1/4), color = &amp;quot;transparent&amp;quot;),
      panel.grid = element_blank(),
      plot.background = element_rect(fill = alpha(fk[16], 1/4), color = &amp;quot;transparent&amp;quot;)
    )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The color palette in this post is inspired by &lt;a href=&#34;https://en.wikipedia.org/wiki/Frida_Kahlo&#34;&gt;Frida Kahlo&lt;/a&gt;‚Äôs &lt;a href=&#34;https://en.wikipedia.org/wiki/Self-Portrait_with_Thorn_Necklace_and_Hummingbird&#34;&gt;&lt;em&gt;Self-Portrait with Thorn Necklace and Hummingbird&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need data&lt;/h2&gt;
&lt;p&gt;I‚Äôm not at liberty to share the original data. However, I have simulated a new data set that has the essential features of the original and I have saved the file on GitHub. You can load it like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(url(&amp;quot;https://github.com/ASKurz/blogdown/raw/main/content/post/2021-06-05-don-t-forget-your-inits/data/dat.rda?raw=true&amp;quot;))

# what is this?
glimpse(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 29,281
## Columns: 2
## $ id &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a‚Ä¶
## $ rt &amp;lt;dbl&amp;gt; 689.0489, 552.8998, 901.0891, 992.2104, 1218.2256, 1356.5888, 679.0385, 663.7340, 771.3938, 996.2‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our primary variable of interest is &lt;code&gt;rt&lt;/code&gt;, which is simulated reaction times in milliseconds. The reaction times are nested within 26 participants, who are denoted by the &lt;code&gt;id&lt;/code&gt; column. The data are not balanced.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  count(id, name = &amp;quot;trials&amp;quot;) %&amp;gt;% 
  count(trials)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 2
##   trials     n
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
## 1    320     2
## 2    640     4
## 3    960     3
## 4   1121     1
## 5   1280    14
## 6   1600     1
## 7   2560     1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whereas most participants have 1,280 trials, their numbers range from 320 to 2,560, which means we‚Äôll want a multilevel model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-can-describe-the-data-with-the-exgaussian-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We can describe the data with the exGaussian function&lt;/h2&gt;
&lt;p&gt;To start getting a sense of the &lt;code&gt;rt&lt;/code&gt; data, we‚Äôll make a density plot of the overall distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  ggplot(aes(x = rt)) +
  geom_density(fill = fk[3], color = fk[3])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As is typical of reaction times, the data are continuous, non-negative, and strongly skewed to the right. There are any number of likelihood functions one can use to model data of this kind. One popular choice is the exGaussian. The exGaussian distribution has three parameters: &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters govern the mean and standard deviation for the central Gaussian portion of the distribution. The &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter governs the rate of the exponential distribution, which is tacked on to the right-hand side of the distribution. Within &lt;strong&gt;R&lt;/strong&gt;, you can compute the density of various exGaussian distributions using the &lt;code&gt;brms::dexgaussian()&lt;/code&gt; function. If you fool around with the parameter settings, a bit, you can make an exGaussian curve that fits pretty closely to the shape of our &lt;code&gt;rt&lt;/code&gt; data. For example, here‚Äôs what it looks like when we set &lt;code&gt;mu = 1300&lt;/code&gt;, &lt;code&gt;sigma = 150&lt;/code&gt;, and &lt;code&gt;beta = 520&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(rt = seq(from = 0, to = 5500, length.out = 300),
       d = dexgaussian(rt, mu = 1300, sigma = 150, beta = 520)) %&amp;gt;% 
  
  ggplot(aes(x = rt)) +
  geom_density(data = dat,
               fill = fk[3], color = fk[3]) +
  geom_line(aes(y = d), 
            color = fk[31], size = 5/4) +
  # zoom in on the bulk of the values
  coord_cartesian(xlim = c(0, 5000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The fit isn‚Äôt perfect, but it gives a sense of where things are headed. It‚Äôs time to talk about modeling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models&lt;/h2&gt;
&lt;p&gt;In this post, we will explore three options for modeling the reaction-time data. The first will use default options. The second option will employ manually-set starting points. For the third option, we will use pseudorandom number generators to define the starting points, all within a custom function.&lt;/p&gt;
&lt;div id=&#34;model-1-use-the-exgaussian-with-default-settings.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model 1: Use the exGaussian with default settings.&lt;/h3&gt;
&lt;p&gt;When using &lt;strong&gt;brms&lt;/strong&gt;, you can fit an exGaussian model by setting &lt;code&gt;family = exgaussian()&lt;/code&gt;. Here we‚Äôll allow the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; parameters to vary by participant, but keep the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters fixed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- brm(
  data = dat,
  family = exgaussian(),
  formula = rt ~ 1 + (1 | id),
  cores = 4, seed = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I‚Äôm not going to show them all, here, for the sake of space, but this model returned warnings about 604 transitions, 1 chain for which the estimated Bayesian Fraction of Missing Information was low, a large R-hat value of 2.85, and low bulk and tail effective sample sizes. In other words, this was a disaster. To help bring these all into focus, we‚Äôll want to take a look at the chains in a trace plot. Since we‚Äôll be doing this a few times, let‚Äôs go ahead and make a custom trace plot geom to suit our purposes. We‚Äôll call it &lt;code&gt;geom_trace()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;geom_trace &amp;lt;- function(subtitle = NULL, 
                       xlab = &amp;quot;iteration&amp;quot;, 
                       xbreaks = 0:4 * 500) {
  
  list(
    annotate(geom = &amp;quot;rect&amp;quot;, 
             xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,
             fill = fk[16], alpha = 1/2, size = 0),
    geom_line(size = 1/3),
    scale_color_manual(values = fk[c(3, 8, 27, 31)]),
    scale_x_continuous(xlab, breaks = xbreaks, expand = c(0, 0)),
    labs(subtitle = subtitle),
    theme(panel.grid = element_blank())
  )
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For your real-world models, it‚Äôs good to look at the tract plots for all major model parameters. Here we‚Äôll just focus on the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; intercept.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggs(fit1, burnin = TRUE) %&amp;gt;%
  filter(Parameter == &amp;quot;b_Intercept&amp;quot;) %&amp;gt;% 
  mutate(chain = factor(Chain),
         intercept = value) %&amp;gt;% 
  
  ggplot(aes(x = Iteration, y = intercept, color = chain)) +
  geom_trace(subtitle = &amp;quot;fit1 (default settings)&amp;quot;) +
  scale_y_continuous(breaks = c(0, 650, 1300), limits = c(NA, 1430))

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since we pulled the chains using the &lt;code&gt;ggmcmc::ggs()&lt;/code&gt; function, we were able to plot the warmup iterations (darker beige background on the left) along with the post-warmup iterations (lighter beige background on the right). Although one of our chains eventually made its way to the posterior, three out of the four stagnated near their starting values. This brings us to a major point in this post: &lt;em&gt;Starting points can be a big deal&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-points-can-be-a-big-deal.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Starting points can be a big deal.&lt;/h3&gt;
&lt;p&gt;I‚Äôm not going to go into the theory underlying Markov chain Monte Carlo (MCMC) methods in any detail. For that, check out some of the latter chapters in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gillBayesianMethods2015&#34; role=&#34;doc-biblioref&#34;&gt;Gill&lt;/a&gt; (&lt;a href=&#34;#ref-gillBayesianMethods2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gelman2013bayesian&#34; role=&#34;doc-biblioref&#34;&gt;Gelman et al.&lt;/a&gt; (&lt;a href=&#34;#ref-gelman2013bayesian&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;. In brief, if you run a Markov chain for an infinite number of iterations, it will converge on the correct posterior distribution. The problem is we can‚Äôt run our chains for that long, which means we have to be careful about whether our finite-length chains have converged properly. Starting points are one of the factors that can influence this process.&lt;/p&gt;
&lt;p&gt;One of the ways to help make sure your MCMC chains are sampling well is to run multiple chains for a while and check to see whether they have all converged around the same parameter space. Ideally, each chain will start from a different initial value. In practice, the first several iterations following the starting values are typically discarded. With older methods, like the Gibbs sampler, this was called the ‚Äúburn-in‚Äù period. With Hamiltonian Monte Carlo (HMC), which is what &lt;strong&gt;brms&lt;/strong&gt; uses, we have a similar period called ‚Äúwarmup.‚Äù When everything goes well, the MCMC chains will all have traversed from their starting values to sampling probabilistically from the posterior distribution once they have emerged from the warmup phase. However, this isn‚Äôt always the case. Sometimes the chains get stuck around their stating values and continue to linger there, even after you have terminated the warmup period. When this happens, you‚Äôll end up with samples that are still tainted by their starting values and are not yet representative of the posterior distribution.&lt;/p&gt;
&lt;p&gt;In our example, above, we used the &lt;strong&gt;brms&lt;/strong&gt; default settings of four chains, each of which ran for 1,000 warmup iterations and then 1,000 post-warmup iterations. We also used the &lt;strong&gt;brms&lt;/strong&gt; default for the starting values. These defaults are based on the Stan defaults, which is to randomly select the starting points from a uniform distribution ranging from -2 to 2. For details, see the &lt;a href=&#34;https://mc-stan.org/docs/2_25/reference-manual/initialization.html#random-initial-values&#34;&gt;&lt;em&gt;Random initial values&lt;/em&gt;&lt;/a&gt; section of the &lt;em&gt;Stan Reference Manual&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-standevelopmentteamStanReferenceManual2021&#34; role=&#34;doc-biblioref&#34;&gt;Stan Development Team, 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In my experience, the &lt;strong&gt;brms&lt;/strong&gt; defaults are usually pretty good. My models often quickly traverse from their starting values to concentrate in the posterior, just like our second chain did, above. When things go wrong, sometimes adding stronger priors can work. Other times it makes sense to rescale or reparameterize the model, somehow. In this case, I have reasons to want to (a) use default priors and to (b) stick to the default parameterization applied to the transformed data. Happily, we have another trick at out disposal: We can adjust the starting points.&lt;/p&gt;
&lt;p&gt;Within &lt;code&gt;brms::brm()&lt;/code&gt;, we can control the starting values with the &lt;code&gt;inits&lt;/code&gt; argument. The default is &lt;code&gt;inits = &#34;random&#34;&lt;/code&gt;, which follows the Stan convention of sampling from &lt;span class=&#34;math inline&#34;&gt;\((-2, 2)\)&lt;/span&gt;, as discussed above. Another option is to fix all starting values to zero by setting &lt;code&gt;inits = &#34;0&#34;&lt;/code&gt;. This often works surprisingly well, but it wasn‚Äôt the solution in this case. If you look at the trace plot, above, you‚Äôll see that all the starting values are a long ways from the target range, which is somewhere around 1,300. So why not just put the starting values near there?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-2-fit-the-model-with-initial-values-set-by-hand.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model 2: Fit the model with initial values set by hand.&lt;/h3&gt;
&lt;p&gt;When you specify start values for the parameters in your Stan models, you need to do so with a list of lists. Each MCMC chain will need its own list. In our case, that means we‚Äôll need four separate lists, each of which will be nested within a single higher-order list. For example, here we‚Äôll define a single list called &lt;code&gt;inits&lt;/code&gt;, which will have starting values defined for our primary three population-level parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inits &amp;lt;- list(
  Intercept = 1300,
  sigma     = 150,
  beta      = 520
  )

# what is this?
inits&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Intercept
## [1] 1300
## 
## $sigma
## [1] 150
## 
## $beta
## [1] 520&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we didn‚Äôt bother setting a starting value for the standard-deviation parameter for the random intercepts. That parameter, then, will just get the &lt;strong&gt;brms&lt;/strong&gt; default. The others will the the start values, as assigned. Now, since we have four chains to assign start values to, a quick and dirty method is to just use the same ones for all four chains. Here‚Äôs how to do that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_of_inits &amp;lt;- list(inits, inits, inits, inits)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our &lt;code&gt;list_of_inits&lt;/code&gt; object is a list into which we have saved four copies of our &lt;code&gt;inits&lt;/code&gt; list. Here‚Äôs how to use those values within &lt;code&gt;brms::brm()&lt;/code&gt;. Just plug them into the &lt;code&gt;inits&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- brm(
  data = dat,
  family = exgaussian(),
  formula = rt ~ 1 + (1 | id),
  cores = 4, seed = 1,
  inits = list_of_inits
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The effective sample sizes are still a little low, but the major pathologies are now gone. Compare the updated traceplot for the intercept to the first one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# adjust fit1
p1 &amp;lt;- p1 +
  geom_trace(subtitle = &amp;quot;fit1 (default settings)&amp;quot;,
             xlab = NULL, xbreaks = NULL)

# fit2
p2 &amp;lt;- ggs(fit2) %&amp;gt;%
  filter(Parameter == &amp;quot;b_Intercept&amp;quot;) %&amp;gt;% 
  mutate(chain = factor(Chain),
         intercept = value) %&amp;gt;% 
  
  ggplot(aes(x = Iteration, y = intercept, color = chain)) +
  geom_trace(subtitle = &amp;quot;fit2 (manual copy/paste inits settings)&amp;quot;) +
  coord_cartesian(ylim = c(1200, 1400))

# combine
p1 / p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Man that looks better! See how all four of our chains started out at 1,300? That‚Äôs because of how we copy/pasted &lt;code&gt;inits&lt;/code&gt; four times within our &lt;code&gt;list_of_inits&lt;/code&gt; object. This is kinda okay, but we can do better.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-3-set-the-initial-values-with-a-custom-function.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model 3: Set the initial values with a custom function.&lt;/h3&gt;
&lt;p&gt;Returning back to MCMC theory, a bit, it‚Äôs generally a better idea to assign each chain its own starting value. Then, if all chains converge into the same part in the parameter space, that provides more convincing evidence they‚Äôre all properly exploring the posterior. To be clear, this isn‚Äôt rigorous evidence. It‚Äôs just better evidence than if we started them all in the same spot.&lt;/p&gt;
&lt;p&gt;One way to give each chain its own starting value would be to manually set them. Here‚Äôs what that would look like if we were only working with two chains.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set the values for the first chain
inits1 &amp;lt;- list(
  Intercept = 1250,
  sigma     = 140,
  beta      = 500
  )

# set new values for the second chain
inits2 &amp;lt;- list(
  Intercept = 1350,
  sigma     = 160,
  beta      = 540
  )

# combine the two lists into a single list
list_of_inits &amp;lt;- list(inits1, inits2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach will work fine, but it‚Äôs tedious, especially if you‚Äôd like to apply it to a large number of parameters. A more programmatic approach would be to use a pseudorandom number-generating function to randomly set the starting values. Since the intercept is an unbounded parameter, the posterior for which will often look Gaussian, the &lt;code&gt;rnorm()&lt;/code&gt; function can be a great choice for selecting its starting values. Since both &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters need to be non-negative, a better choice might be the &lt;code&gt;runif()&lt;/code&gt; or &lt;code&gt;rgamma()&lt;/code&gt; functions. Here we‚Äôll just use &lt;code&gt;runif()&lt;/code&gt; for each.&lt;/p&gt;
&lt;p&gt;Since we‚Äôre talking about using the pseudorandom number generators to pick our values, it would be nice if the results were reproducible. We can do that by working in the &lt;code&gt;set.seed()&lt;/code&gt; function. Finally, it would be really sweet if we had a way to wrap &lt;code&gt;set.seed()&lt;/code&gt; and the various number-generating functions into a single higher-order function. Here‚Äôs one way to make such a function, which I‚Äôm calling &lt;code&gt;set_inits()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set_inits &amp;lt;- function(seed = 1) {
  
  set.seed(seed)
  list(
    Intercept = rnorm(n = 1, mean = 1300, sd = 100),
    sigma     = runif(n = 1, min = 100, max = 200),
    beta      = runif(n = 1, min = 450, max = 550)
  )
  
}

# try it out
set_inits(seed = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Intercept
## [1] 1426.295
## 
## $sigma
## [1] 137.2124
## 
## $beta
## [1] 507.2853&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how we set the parameters within the &lt;code&gt;rnorm()&lt;/code&gt; and &lt;code&gt;runif()&lt;/code&gt; functions to values that seemed reasonable given our model. These values aren‚Äôt magic and you could adjust them to your own needs. Now, here‚Äôs how to use our handy &lt;code&gt;set_inits()&lt;/code&gt; function to choose similar, but distinct, starting values for each of our four chains. We save the results in a higher-order list called &lt;code&gt;my_second_list_of_inits&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_second_list_of_inits &amp;lt;- list(
  # different seed values will return different results
  set_inits(seed = 1),
  set_inits(seed = 2),
  set_inits(seed = 3),
  set_inits(seed = 4)
)

# what have we done?
str(my_second_list_of_inits)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 4
##  $ :List of 3
##   ..$ Intercept: num 1237
##   ..$ sigma    : num 157
##   ..$ beta     : num 541
##  $ :List of 3
##   ..$ Intercept: num 1210
##   ..$ sigma    : num 157
##   ..$ beta     : num 467
##  $ :List of 3
##   ..$ Intercept: num 1204
##   ..$ sigma    : num 138
##   ..$ beta     : num 483
##  $ :List of 3
##   ..$ Intercept: num 1322
##   ..$ sigma    : num 129
##   ..$ beta     : num 478&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now just plug &lt;code&gt;my_second_list_of_inits&lt;/code&gt; into the &lt;code&gt;inits&lt;/code&gt; argument and fit the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;- brm(
  data = dat,
  family = exgaussian(),
  formula = rt ~ 1 + (1 | id),
  cores = 4, seed = 1,
  inits = my_second_list_of_inits
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with &lt;code&gt;fit2&lt;/code&gt;, our &lt;code&gt;fit3&lt;/code&gt; came out okay. Let‚Äôs inspect the intercept parameter with a final trace plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# adjust fit2
p2 &amp;lt;- p2 +
  geom_trace(subtitle = &amp;quot;fit2 (manual copy/paste inits settings)&amp;quot;,
             xlab = NULL, xbreaks = NULL)

# fit3
p3 &amp;lt;- ggs(fit3) %&amp;gt;%
  filter(Parameter == &amp;quot;b_Intercept&amp;quot;) %&amp;gt;% 
  mutate(chain = factor(Chain),
         intercept = value) %&amp;gt;% 
  
  ggplot(aes(x = Iteration, y = intercept, color = chain)) +
  geom_trace(subtitle = &amp;quot;fit3 (inits by a custom function)&amp;quot;) +
  coord_cartesian(ylim = c(1200, 1400))

# combine
p1 / p2 / p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we have visual evidence that even though all four chains started at different places in the parameter space, they all converged into the same area. This still isn‚Äôt fully rigorous evidence our chains are performing properly, but it‚Äôs a major improvement from &lt;code&gt;fit1&lt;/code&gt; and a minor improvement from &lt;code&gt;fit2&lt;/code&gt;. They aren‚Äôt shown here, but the same point holds for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;p&gt;Okay, just for kicks and giggles, let‚Äôs see how well our last model did by way of a posterior predictive check.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bayesplot::color_scheme_set(fk[c(31, 31, 31, 3, 3, 3)])

pp_check(fit3, nsamples = 100) + 
  # we don&amp;#39;t need to see the whole right tail
  coord_cartesian(xlim = c(0, 5000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model could be better, but it‚Äôs moving in the right direction and there don‚Äôt appear to be any major pathologies, like what we saw with &lt;code&gt;fit1&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recap&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If your try to fit a model with MCMC, you may sometimes end up with pathologies, such as divergent transitions, large numbers of transitions, high R-hat values, and/or very low effective sample size estimates.&lt;/li&gt;
&lt;li&gt;Sometimes these pathologies arise when the starting values for your chains are far away from the centers of their posterior densities.&lt;/li&gt;
&lt;li&gt;When using &lt;strong&gt;brms&lt;/strong&gt;, you can solve this problem by setting the starting values with the &lt;code&gt;inits&lt;/code&gt; argument.&lt;/li&gt;
&lt;li&gt;One approach is to manually set the starting values, saving them in a list of lists.&lt;/li&gt;
&lt;li&gt;Another approach is to use the pseudorandom number generators, such as &lt;code&gt;rnorm()&lt;/code&gt; and &lt;code&gt;runif()&lt;/code&gt;, to assign starting values within user-defined ranges.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] lisa_0.1.2      ggmcmc_1.5.1.1  patchwork_1.1.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1  
##  [7] stringr_1.4.0   dplyr_1.0.6     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.2   
## [13] ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6         splines_4.0.4       
##   [6] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17        digest_0.6.27       
##  [11] htmltools_0.5.1.1    rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [16] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [21] colorspace_2.0-0     rvest_0.3.6          haven_2.3.1          xfun_0.23            callr_3.7.0         
##  [26] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [31] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0             pkgbuild_1.2.0      
##  [36] rstan_2.21.2         abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1        GGally_2.1.1        
##  [41] DBI_1.1.0            miniUI_0.1.1.1       xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [46] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3        RColorBrewer_1.1-2  
##  [51] ellipsis_0.3.2       farver_2.1.0         reshape_0.8.8        pkgconfig_2.0.3      loo_2.4.1           
##  [56] sass_0.3.1           dbplyr_2.0.0         utf8_1.2.1           labeling_0.4.2       tidyselect_1.1.1    
##  [61] rlang_0.4.11         reshape2_1.4.4       later_1.2.0          munsell_0.5.0        cellranger_1.1.0    
##  [66] tools_4.0.4          cli_2.5.0            generics_0.1.0       broom_0.7.6          ggridges_0.5.3      
##  [71] evaluate_0.14        fastmap_1.1.0        yaml_2.2.1           processx_3.5.2       knitr_1.33          
##  [76] fs_1.5.0             nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [81] rstudioapi_0.13      compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2    curl_4.3            
##  [86] gamm4_0.2-6          reprex_0.3.0         statmod_1.4.35       bslib_0.2.4          stringi_1.6.2       
##  [91] highr_0.9            ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41     
##  [96] Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0        vctrs_0.3.8         
## [101] pillar_1.6.1         lifecycle_1.0.0      jquerylib_0.1.4      bridgesampling_1.0-0 estimability_1.3    
## [106] httpuv_1.6.0         R6_2.5.0             bookdown_0.22        promises_1.2.0.1     gridExtra_2.3       
## [111] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [116] assertthat_0.2.1     withr_2.4.2          shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33         
## [121] parallel_4.0.4       hms_0.5.3            grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [126] rmarkdown_2.8        shiny_1.6.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-B√ºrkner2021Parameterization&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2021). &lt;em&gt;Parameterization of response distributions in brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1‚Äì28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395‚Äì411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ‚Äô&lt;span&gt;Stan&lt;/span&gt;‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-marinGgmcmcAnalysisMCMC2016&#34; class=&#34;csl-entry&#34;&gt;
Fern√°ndez i Mar√≠n, X. (2016). &lt;span class=&#34;nocase&#34;&gt;ggmcmc&lt;/span&gt;: &lt;span&gt;Analysis&lt;/span&gt; of &lt;span&gt;MCMC&lt;/span&gt; samples and &lt;span&gt;Bayesian&lt;/span&gt; inference. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;70&lt;/em&gt;(9), 1‚Äì20. &lt;a href=&#34;https://doi.org/10.18637/jss.v070.i09&#34;&gt;https://doi.org/10.18637/jss.v070.i09&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-ggmcmc&#34; class=&#34;csl-entry&#34;&gt;
Fern√°ndez i Mar√≠n, X. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;ggmcmc&lt;/span&gt;: &lt;span&gt;Tools&lt;/span&gt; for analyzing &lt;span&gt;MCMC&lt;/span&gt; simulations from &lt;span&gt;Bayesian&lt;/span&gt; inference&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=ggmcmc&#34;&gt;https://CRAN.R-project.org/package=ggmcmc&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp;amp; Rubin, D. B. (2013). &lt;em&gt;Bayesian data analysis&lt;/em&gt; (Third Edition). &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://stat.columbia.edu/~gelman/book/&#34;&gt;https://stat.columbia.edu/~gelman/book/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gillBayesianMethods2015&#34; class=&#34;csl-entry&#34;&gt;
Gill, J. (2015). &lt;em&gt;Bayesian methods: &lt;span&gt;A&lt;/span&gt; social and behavioral sciences approach&lt;/em&gt; (Third Edition). &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Bayesian-Methods-A-Social-and-Behavioral-Sciences-Approach-Third-Edition/Gill/p/book/9781439862483&#34;&gt;https://www.routledge.com/Bayesian-Methods-A-Social-and-Behavioral-Sciences-Approach-Third-Edition/Gill/p/book/9781439862483&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-lisa&#34; class=&#34;csl-entry&#34;&gt;
Littlefield, T. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;lisa&lt;/span&gt;: &lt;span&gt;Color&lt;/span&gt; palettes from color lisa&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=lisa&#34;&gt;https://CRAN.R-project.org/package=lisa&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-patchwork&#34; class=&#34;csl-entry&#34;&gt;
Pedersen, T. L. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;patchwork&lt;/span&gt;: &lt;span&gt;The&lt;/span&gt; composer of plots&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;https://CRAN.R-project.org/package=patchwork&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-standevelopmentteamStanReferenceManual2021&#34; class=&#34;csl-entry&#34;&gt;
Stan Development Team. (2021). &lt;em&gt;Stan reference manual, &lt;span&gt;Version&lt;/span&gt; 2.27&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/docs/2_27/reference-manual/&#34;&gt;https://mc-stan.org/docs/2_27/reference-manual/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ‚Äôtidyverse‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., Fran√ßois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., M√ºller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., ‚Ä¶ Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;There are different ways to parameterize the exGaussian distribution and these differences may involve different ways to express what we‚Äôre calling &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Since our parameterization is based on Paul B√ºrkner‚Äôs work, you might check out the &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/vignettes/brms_families.html#response-time-models&#34;&gt;&lt;em&gt;Response time models&lt;/em&gt;&lt;/a&gt; section in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-B√ºrkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; document, &lt;em&gt;Parameterization of response distributions in brms&lt;/em&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Effect sizes for experimental trials analyzed with multilevel growth models: Two of two</title>
      <link>/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/</link>
      <pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/</guid>
      <description>
&lt;script src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;orientation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Orientation&lt;/h2&gt;
&lt;p&gt;This post is the second and final installment of a two-part series. In the &lt;a href=&#34;https://solomonkurz.netlify.app/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/&#34;&gt;first post&lt;/a&gt;, we explored how one might compute an effect size for two-group experimental data with only &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; time points. In this second post, we fulfill our goal to show how to generalize this framework to experimental data collected over &lt;span class=&#34;math inline&#34;&gt;\(3+\)&lt;/span&gt; time points. The data and overall framework come from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;i-still-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I still make assumptions.&lt;/h3&gt;
&lt;p&gt;As with the &lt;a href=&#34;https://solomonkurz.netlify.app/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/#i-make-assumptions.&#34;&gt;first post&lt;/a&gt;, I make a handful of assumptions about your background knowledge. Though I won‚Äôt spell them out again, here, I should stress that you‚Äôll want to be familiar with multilevel models to get the most out of this post. To brush up, I recommend &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;Raudenbush &amp;amp; Bryk&lt;/a&gt; (&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett&lt;/a&gt; (&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As before, all code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;. Here we load our primary &lt;strong&gt;R&lt;/strong&gt; packages‚Äì&lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;, and the &lt;a href=&#34;http://style.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;‚Äìand adjust the global plotting theme defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)
library(tidybayes)
library(tidyverse)

# adjust the global plotting theme
theme_set(
  theme_linedraw() +
    theme(text = element_text(family = &amp;quot;Times&amp;quot;),
          panel.grid = element_blank(),
          strip.text = element_text(margin = margin(b = 3, t = 3)))
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;Once again, we use the &lt;a href=&#34;https://tibble.tidyverse.org/reference/tribble.html&#34;&gt;tribble&lt;/a&gt; approach to enter the synthetic data Feingold displayed in his Table 1 (p.¬†46).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  tribble(
    ~id, ~tx, ~t1, ~t2, ~t3, ~t4,
    101, -0.5, 3, 5, 5,  7,
    102, -0.5, 4, 4, 6,  6,
    103, -0.5, 4, 5, 7,  8,
    104, -0.5, 5, 6, 6,  8,
    105, -0.5, 5, 6, 7,  8,
    106, -0.5, 5, 7, 7,  7,
    107, -0.5, 5, 6, 8,  8,
    108, -0.5, 6, 6, 7,  9,
    109, -0.5, 6, 8, 9,  10,
    110, -0.5, 7, 7, 8,  9,
    111,  0.5, 3, 5, 7,  9,
    112,  0.5, 4, 7, 9,  11,
    113,  0.5, 4, 6, 8,  11,
    114,  0.5, 5, 7, 9,  10,
    115,  0.5, 5, 6, 9,  11,
    116,  0.5, 5, 7, 10, 10,
    117,  0.5, 5, 8, 8,  11,
    118,  0.5, 6, 7, 9,  12,
    119,  0.5, 6, 9, 11, 13,
    120,  0.5, 7, 8, 10, 12
  ) %&amp;gt;% 
  mutate(`t4-t1`   = t4 - t1,
         condition = ifelse(tx == -0.5, &amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;))

# inspect the first six rows
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##      id    tx    t1    t2    t3    t4 `t4-t1` condition
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    
## 1   101  -0.5     3     5     5     7       4 control  
## 2   102  -0.5     4     4     6     6       2 control  
## 3   103  -0.5     4     5     7     8       4 control  
## 4   104  -0.5     5     6     6     8       3 control  
## 5   105  -0.5     5     6     7     8       3 control  
## 6   106  -0.5     5     7     7     7       2 control&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To reacquaint ourselves with the data, we might make a plot. Last time we plotted a subset of the individual trajectories next to the averages, by treatment group. Here we‚Äôll superimpose all the individual-level trajectories atop the group averages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  pivot_longer(t1:t4) %&amp;gt;% 
  mutate(time      = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double(),
         condition = ifelse(tx &amp;lt; 0, &amp;quot;tx = -0.5 (control)&amp;quot;, &amp;quot;tx = 0.5 (treatment)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = time, y = value)) +
  stat_smooth(aes(color = condition),
              method = &amp;quot;lm&amp;quot;, formula = &amp;#39;y ~ x&amp;#39;,
              se = F, size = 4) +
  geom_line(aes(group = id),
            size = 1/4) +
  scale_color_viridis_d(end = .75, direction = -1, breaks = NULL) +
  facet_wrap(~ condition)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig1-1.png&#34; width=&#34;624&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The thick lines are the group averages and the thinner lines are for the individual participants. Though participants tend to increase in both groups, those in the treatment condition appear to have increased at a more rapid pace. We want a standardized effect size that can capture those differences in a familiar metric. We‚Äôll begin to explain what that will be, next.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;div id=&#34;we-need-a-framework.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need a framework.&lt;/h3&gt;
&lt;p&gt;Traditional analytic strategies, such as ordinary least squares (OLS) regression and the analysis of variance (ANOVA) framework, can work okay with data collected on one or two time points. In his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;, &lt;a href=&#34;#ref-feingoldARegressionFramework2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; work, which is the inspiration for this blog series, Feingold recommended what he called growth-modeling analysis (GMA) for data collected on &lt;span class=&#34;math inline&#34;&gt;\(3+\)&lt;/span&gt; time points. If you‚Äôre not familiar with the term GMA, it‚Äôs a longitudinal version of what others have called hierarchical linear models, mixed-effects models, random-effects models, or multilevel models. For longitudinal data, I‚Äôm fond of the term &lt;em&gt;multilevel growth model&lt;/em&gt;, but you can use whatever term you like. If you‚Äôre interested, Raudenbush and Bryk touched on the historic origins of several of these terms in the first chapter of their &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt; text.&lt;/p&gt;
&lt;p&gt;Though multilevel growth models, GMAs, have become commonplace in many applied areas, it‚Äôs not immediately obvious how to compute standardized effect sizes when one uses them. In his Discussion section, Feingold &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009, p. 49&lt;/a&gt;)&lt;/span&gt; pointed out this topic is missing from many text books and software user‚Äôs guides. For example, though I took five statistics courses in graduate school, one of which even focused on the longitudinal growth model, none of my courses covered how to compute an effect size in a longitudinal growth model and none of my text books covered the topic, either. It‚Äôs hard to expect researchers to use strategies we don‚Äôt bother to teach, which is the reason for this blog series.&lt;/p&gt;
&lt;p&gt;We might walk out the framework with statistical notation. If we say our outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; varies across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; partitcipants and &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; time points, we might use Feingold‚Äôs Raudenbusch-&amp;amp;-Bryk-type notation to express our upcoming statistical model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{align*}
y_{ti} &amp;amp; = \beta_{00} + \beta_{01} (\text{treatment})_i + \beta_{10} (\text{time})_{ti} + \color{darkred}{\beta_{11}} (\text{treatment})_i (\text{time})_{ti} \\
&amp;amp; \;\;\; + [r_{0i} + r_{1i} (\text{time})_{ti} + e_{ti}],
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where variance in &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; is decomposed into the last three terms, &lt;span class=&#34;math inline&#34;&gt;\(r_{0i}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(r_{1i}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(e_{ti}\)&lt;/span&gt;. Here we follow the usual assumption that within-participant variance is normally distributed, &lt;span class=&#34;math inline&#34;&gt;\(e_{ti} \sim \operatorname N(0, \sigma_\epsilon^2)\)&lt;/span&gt;, and the &lt;span class=&#34;math inline&#34;&gt;\(r_{\text{x}i}\)&lt;/span&gt; values follow a bivariate normal distribution,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{bmatrix} r_{0i} \\ r_{1i} \end{bmatrix} \sim \operatorname N \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \tau_{00} &amp;amp; \tau_{01} \\ \tau_{01} &amp;amp; \tau_{11} \end{bmatrix} \right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; terms on the diagonal are the variances and the off-diagonal &lt;span class=&#34;math inline&#34;&gt;\(\tau_{01}\)&lt;/span&gt; is their covariance. We‚Äôll be fitting this model with Bayesian software, which means all parameters will be given prior distributions. But since our goal is to emphasize the effect size and the multilevel framework, I‚Äôm just going to use the &lt;strong&gt;brms&lt;/strong&gt; default settings for the priors and will avoid expressing them in formal statistical notation&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this model, the four &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters are often called the ‚Äúfixed effects,‚Äù or the population parameters. Our focal parameter will be &lt;span class=&#34;math inline&#34;&gt;\(\color{darkred}{\beta_{11}}\)&lt;/span&gt;, which is why we marked it off in red. This parameter is the interaction between time and treatment condition. Put another way, &lt;span class=&#34;math inline&#34;&gt;\(\color{darkred}{\beta_{11}}\)&lt;/span&gt; is the difference in the average rate of change, by treatment. Once we fit our multilevel growth model, we will explore how one might transform the &lt;span class=&#34;math inline&#34;&gt;\(\color{darkred}{\beta_{11}}\)&lt;/span&gt; parameter into our desired effect size.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model.&lt;/h3&gt;
&lt;p&gt;As you‚Äôll learn in any good multilevel text book, multilevel models typically require the data to be in the long format. Here we‚Äôll transform our data into that format and call the results &lt;code&gt;d_long&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wrangle
d_long &amp;lt;-
  d %&amp;gt;% 
  pivot_longer(t1:t4, values_to = &amp;quot;y&amp;quot;) %&amp;gt;% 
  mutate(time = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double()) %&amp;gt;% 
  mutate(time_f = (time * 2) - 5,
         time_c = time - mean(time),
         time0  = time - 1,
         time01 = (time - 1) / 3)

# what have we done?
head(d_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 11
##      id    tx `t4-t1` condition name      y  time time_f time_c time0 time01
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1   101  -0.5       4 control   t1        3     1     -3   -1.5     0  0    
## 2   101  -0.5       4 control   t2        5     2     -1   -0.5     1  0.333
## 3   101  -0.5       4 control   t3        5     3      1    0.5     2  0.667
## 4   101  -0.5       4 control   t4        7     4      3    1.5     3  1    
## 5   102  -0.5       2 control   t1        4     1     -3   -1.5     0  0    
## 6   102  -0.5       2 control   t2        4     2     -1   -0.5     1  0.333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; paper, Feingold mentioned he coded time as a factor which&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;was mean centered by using linear weights (-3, -1, 1, and 3 for T1 through T4, respectively) for a four-level design obtained from a table of orthogonal polynomials (Snedecor &amp;amp; Cochran, 1967) for the within-subjects (Level 1 in HLM terminology) facet of the analysis. (p.¬†47)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can find this version of the time variable in the &lt;code&gt;time_f&lt;/code&gt; column. However, I have no interest in modeling with time coded according to a scheme of orthogonal polynomials. But I do think it makes sense to center time or scale it so the lowest value is zero. You can find those versions of time in the &lt;code&gt;time_c&lt;/code&gt; and &lt;code&gt;time0&lt;/code&gt; columns. The model, below, uses &lt;code&gt;time0&lt;/code&gt;. Although this will change the scale of our model parameters relative to those in Feingold‚Äôs paper, it will have little influence on how we compute the effect size of interest.&lt;/p&gt;
&lt;p&gt;Here‚Äôs how we might fit the multilevel growth model for the two treatment conditions with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d_long,
      family = gaussian,
      y ~ 1 + time0 + tx + time0:tx + (1 + time0 | id),
      cores = 4,
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Review the parameter summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time0 + tx + time0:tx + (1 + time0 | id) 
##    Data: d_long (Number of observations: 80) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 20) 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)            1.09      0.22     0.74     1.60 1.00     1372     2034
## sd(time0)                0.10      0.07     0.01     0.27 1.00     1063     1908
## cor(Intercept,time0)    -0.08      0.51    -0.93     0.91 1.00     4260     2290
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     5.00      0.27     4.47     5.55 1.00     1262     1502
## time0         1.50      0.06     1.38     1.62 1.00     5705     2527
## tx           -0.01      0.53    -1.02     1.05 1.01     1167     1781
## time0:tx      1.00      0.12     0.75     1.25 1.00     3793     2208
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.57      0.06     0.47     0.70 1.00     3132     2986
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything looks fine. If you check them, the trace plots of the chains look good, too&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. If you execute the code below, you‚Äôll see our primary results cohere nicely with the maximum likelihood results from the frequentist &lt;strong&gt;lme4&lt;/strong&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lme4::lmer(data = d_long,
           y ~ 1 + time0 + tx + time0:tx + (1 + time0 | id)) %&amp;gt;% 
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Regardless on whether you focus on the output from &lt;strong&gt;brms&lt;/strong&gt; or &lt;strong&gt;lme4&lt;/strong&gt;, our coefficients will differ a bit from those Feingold reported because of our different scaling of the time variable. But from a high-level perspective, it‚Äôs the same model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unstandardized-effect-size.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unstandardized effect size.&lt;/h3&gt;
&lt;p&gt;Our interest lies in the &lt;code&gt;time0:tx&lt;/code&gt; interaction, which is the unstandardized effect size for the ‚Äúdifference between the means of the slopes of the treatment and the control group‚Äù (p.¬†47). You might also describe this as a difference in differences. Here‚Äôs a focused summary of that coefficient, which Feingold called &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;time0:tx&amp;quot;, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
## 1.0000705 0.1246595 0.7534974 1.2493457&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since there are three units of time between baseline (&lt;code&gt;time0 == 0&lt;/code&gt;) and the final assessment point (&lt;code&gt;time0 == 3&lt;/code&gt;), we can get the difference in pre/post differences by multiplying that &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt; coefficient by &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;time0:tx&amp;quot;, -2] * 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
## 3.000212 2.260492 3.748037&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thinking back to the original wide-formatted &lt;code&gt;d&lt;/code&gt; data, this value is the multilevel growth model version of the difference in change scores (&lt;code&gt;t4-t1&lt;/code&gt;) in the treatment conditions, &lt;span class=&#34;math inline&#34;&gt;\(M_\text{change-T} - M_\text{change-C}\)&lt;/span&gt;. Here compute that value by hand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group-level change score means
m_change_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t4-t1`)) %&amp;gt;% pull()  # 6
m_change_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t4-t1`)) %&amp;gt;% pull()  # 3

# difference in change score means
m_change_t - m_change_c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the reasons we went through the trouble of fitting a multilevel model is so we could accompany that difference in change scores with high-quality 95% intervals. Here they are in a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(fixef(fit1)[, -2] * 3) %&amp;gt;% 
  rownames_to_column(&amp;quot;coefficient&amp;quot;) %&amp;gt;% 
  filter(coefficient == &amp;quot;time0:tx&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = 0)) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_pointrange(fatten = 1) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(&amp;quot;unstandardized difference in change scores&amp;quot;~(beta[1][1])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig2-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The population average could be anywhere from 2.25 to 3.75, but the best guess is it‚Äôs about 3. However, since the metric on this outcome variable is arbitrary (these data were simulated, remember), it‚Äôs hard to interpret how ‚Äúlarge‚Äù this is. A standardized effect size can help.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-to-define-the-standardized-mean-difference-for-the-multilevel-growth-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need to define the standardized mean difference for the multilevel growth model.&lt;/h3&gt;
&lt;p&gt;Based on &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-raudenbushEffectsOfStudyDuration2001&#34; role=&#34;doc-biblioref&#34;&gt;Raudenbush &amp;amp; Liu&lt;/a&gt; (&lt;a href=&#34;#ref-raudenbushEffectsOfStudyDuration2001&#34; role=&#34;doc-biblioref&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt;, Feingold presented two effect-size formulas for our multilevel growth model. The first, which he called &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-change}\)&lt;/span&gt;, is on a completely different scale from any of the effect sizes mentioned in the first post (e.g., &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt;). Importantly, it turns out Raudenbush and Liu recommended their &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-change}\)&lt;/span&gt; formula should be used for power calculations, but not necessarily to convey the magnitude of an effect. Thus we will not consider it further&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Feingold reported the formula for their other effect size was&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_\text{GMA-raw} = \beta_{11}(\text{time}) / SD_\text{raw}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt; in Feingold‚Äôs equation is the multilevel interaction term between time and experimental condition‚Äìwhat we just visualized in a coefficient plot. The &lt;span class=&#34;math inline&#34;&gt;\((\text{time})\)&lt;/span&gt; part in the equation is a stand-in for the quantity of time units from the beginning of the study to the end point. Since our multilevel model used the &lt;code&gt;time0&lt;/code&gt; variable, which was &lt;code&gt;0&lt;/code&gt; at baseline and &lt;code&gt;3&lt;/code&gt; at the final time point, we would enter a 3 into the equation (i.e., &lt;span class=&#34;math inline&#34;&gt;\(3 - 0 = 3\)&lt;/span&gt;). The part of Feingold‚Äôs equation that‚Äôs left somewhat vague is what he meant by the denominator, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw}\)&lt;/span&gt;. On page 47, he used the value of 1.15 in his example. Without any reference to experimental condition in the subscript, one might assume that value is the standard deviation for the criterion across all time points or, perhaps, just at baseline. It turns out that‚Äôs not the case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# standard deviation for the criterion across all time points
d_long %&amp;gt;% 
  summarise(sd = sd(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##      sd
##   &amp;lt;dbl&amp;gt;
## 1  2.22&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# standard deviation for the criterion at baseline
d_long %&amp;gt;% 
  filter(time == 1) %&amp;gt;% 
  summarise(sd = sd(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##      sd
##   &amp;lt;dbl&amp;gt;
## 1  1.12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this particular data set, the value Feingold used is the same as the standard deviation for either of the experimental conditions at baseline.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_raw_pre_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(s = sd(t1)) %&amp;gt;% pull()  # treatment baseline SD
sd_raw_pre_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(s = sd(t1)) %&amp;gt;% pull()  # control baseline SD

sd_raw_pre_c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_raw_pre_t&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But since he didn‚Äôt use a subscript, I suspect Feingold meant to convey a pooled standard deviation, following the equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SD_\text{pooled} = \sqrt{\frac{SD_\text{raw(pre-T)}^2 + SD_\text{raw(pre-C)}^2}{2}},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is a sample version of Cohen‚Äôs original equation 2.3.2 &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988, p. 44&lt;/a&gt;)&lt;/span&gt;. Here‚Äôs how to compute the pooled standard deviation by hand, which we‚Äôll save as &lt;code&gt;sd_raw_pre_p&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_raw_pre_p &amp;lt;- sqrt((sd_raw_pre_c^2 + sd_raw_pre_t^2) / 2)
sd_raw_pre_p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since Feingold‚Äôs synthetic data are a special case where &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-T)} = SD_\text{raw(pre-C)} = SD_\text{pooled}\)&lt;/span&gt;, these distinctions might all seem dull and pedantic. Yet if your real-world data look anything like mine, this won‚Äôt be the case and you‚Äôll need to understand how distinguish between and choose from among these options.&lt;/p&gt;
&lt;p&gt;Another thing to consider is that whereas Feingold‚Äôs synthetic data have the desirable quality where the sample sizes are the same across the experimental conditions (&lt;span class=&#34;math inline&#34;&gt;\(n_\text{T} = n_\text{C} = 10\)&lt;/span&gt;), this won‚Äôt always be the case. If you end up with unbalanced experimental data, you might consider the sample-size weighted pooled standard deviation, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}^*\)&lt;/span&gt;, which I believe has its origins in Hedges‚Äô work &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hedgesDistributionTheoryforGlass1981&#34; role=&#34;doc-biblioref&#34;&gt;1981, p. 110&lt;/a&gt;)&lt;/span&gt;. It follows the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SD_\text{pooled}^* = \sqrt{\frac{(n_\text{T} - 1)SD_\text{raw(pre-T)}^2 + (n_\text{C} - 1)SD_\text{raw(pre-C)}^2}{n_\text{T} + n_\text{C} - 2}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here it is for Feingold‚Äôs data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the sample sizes
n_t &amp;lt;- 10
n_c &amp;lt;- 10

# compute the sample size robust pooled SD
sqrt(((n_t - 1) * sd_raw_pre_c^2 + (n_c - 1) * sd_raw_pre_t^2) / (n_t + n_c - 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, in the special case of these synthetic data, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}^*\)&lt;/span&gt; happens to be the same value as &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}\)&lt;/span&gt;, which is also the same value as &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-T)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt;. This will not always the case with your real-world data. Choose your &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; with care and make sure to report which ever formula you use. Don‚Äôt be coy with your effect-size calculations.&lt;/p&gt;
&lt;p&gt;You may be wondering, though, whether you can use the standard deviations for one of the treatment conditions rather than a variant of the pooled standard deviation. &lt;em&gt;Yes&lt;/em&gt;, you can. I think Cumming &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;, Chapter 11)&lt;/span&gt; did a nice job walking through this issue. For example, if we thought of our control condition as a true benchmark for what we‚Äôd expect at baseline, we could just use &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt; as our standardizer. This is sometimes referred to as a Glass‚Äô &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; or Glass‚Äô &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt;. Whatever you choose and whatever you call it, just make sure to clearly define your standardizing formula for your audience.&lt;/p&gt;
&lt;p&gt;Therefore, if we use &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt; (&lt;code&gt;sd_raw_pre_c&lt;/code&gt;) as our working value, we can compute &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-raw}\)&lt;/span&gt; as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;time0:tx&amp;quot;, 1] * 3 / sd_raw_pre_c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.598259&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within the Bayesian framework, we can get a full posterior distribution for the standardized version of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-raw}\)&lt;/span&gt;, by working directly with all the posterior draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit1) %&amp;gt;% 
  mutate(d = `b_time0:tx` * 3 / sd_raw_pre_p) %&amp;gt;% 
  
  ggplot(aes(x = d, y = 0)) +
  geom_vline(xintercept = 0, linetype = 2) +
  stat_halfeye(.width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(italic(d)[GMA-raw]~(&amp;quot;standardized difference in change scores&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig3-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The population average could be anywhere from 2 to 3.25, but the best guess is it‚Äôs about 2.5. In my field (clinical psychology), this would be considered a very large effect size. Anyway, here are the numeric values for the posterior median and percentile-based 95% interval.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit1) %&amp;gt;% 
  mutate(d = `b_time0:tx` * 3 / sd_raw_pre_p) %&amp;gt;% 
  median_qi(d) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     d .lower .upper .width .point .interval
## 1 2.6   1.96   3.25   0.95 median        qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to compute this is to work with the model formula and the posterior samples from the fixed effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit1) %&amp;gt;% 
  # simplify the output
  select(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  # compute the treatment-level means for pre and post
  mutate(m_pre_t  = b_Intercept + b_time0 * 0 + b_tx *  0.5 + `b_time0:tx`* 0 *  0.5,
         m_pre_c  = b_Intercept + b_time0 * 0 + b_tx * -0.5 + `b_time0:tx`* 0 * -0.5,
         m_post_t = b_Intercept + b_time0 * 3 + b_tx *  0.5 + `b_time0:tx`* 3 *  0.5,
         m_post_c = b_Intercept + b_time0 * 3 + b_tx * -0.5 + `b_time0:tx`* 3 * -0.5) %&amp;gt;% 
  # compute the treatment-level change scores
  mutate(m_change_t = m_post_t - m_pre_t,
         m_change_c = m_post_c - m_pre_c) %&amp;gt;% 
  # compute the difference of differences
  mutate(beta_11 = m_change_t - m_change_c) %&amp;gt;% 
  # compute the multilevel effect size
  mutate(d_GAM_raw = beta_11 / sd_raw_pre_c) %&amp;gt;% 
  # wrangle and summarize
  pivot_longer(m_pre_t:d_GAM_raw) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  mean_qi(value) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 7
##   name       value .lower .upper .width .point .interval
##   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
## 1 beta_11     3      2.26   3.75   0.95 mean   qi       
## 2 d_GAM_raw   2.6    1.96   3.25   0.95 mean   qi       
## 3 m_change_c  3      2.48   3.53   0.95 mean   qi       
## 4 m_change_t  6      5.49   6.53   0.95 mean   qi       
## 5 m_post_c    8.01   7.26   8.8    0.95 mean   qi       
## 6 m_post_t   11     10.2   11.8    0.95 mean   qi       
## 7 m_pre_c     5.01   4.26   5.77   0.95 mean   qi       
## 8 m_pre_t     5      4.27   5.75   0.95 mean   qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how the summary values in the rows for &lt;code&gt;beta_11&lt;/code&gt; and &lt;code&gt;d_GAM_raw&lt;/code&gt; match up with those we computed, above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;you-may-want-options.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;You may want options.&lt;/h3&gt;
&lt;p&gt;Turns out there‚Äôs an other way to compute the standardized mean difference for experimental longitudinal data. You can just fit the model to the standardized data. As with our approach, above, the trick is to make sure you standardized the data with a defensible standardizer. I recommend you default to the pooled standard deviation at baseline (&lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}\)&lt;/span&gt;). To do so, we first compute the weighted mean at baseline.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group-level baseline means
m_raw_pre_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t1`)) %&amp;gt;% pull()
m_raw_pre_c &amp;lt;- filter(d, tx ==  &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t1`)) %&amp;gt;% pull()

# weighted (pooled) baseline mean
m_raw_pre_p &amp;lt;- (m_raw_pre_t * n_t + m_raw_pre_c * n_c) / (n_t + n_c)

m_raw_pre_p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next use the weighted baseline mean and the pooled baseline standard deviation to standardize the data, saving the results as &lt;code&gt;z&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_long &amp;lt;-
  d_long %&amp;gt;% 
  mutate(z = (y - m_raw_pre_p) / sd_raw_pre_p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now just fit a multilevel growth model with our new standardized variable &lt;code&gt;z&lt;/code&gt; as the criterion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;-
  brm(data = d_long,
      family = gaussian,
      z ~ 1 + time0 + tx + time0:tx + (1 + time0 | id),
      cores = 4,
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the parameter summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: z ~ 1 + time0 + tx + time0:tx + (1 + time0 | id) 
##    Data: d_long (Number of observations: 80) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 20) 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)            0.95      0.20     0.63     1.42 1.00     1293     1768
## sd(time0)                0.08      0.06     0.00     0.23 1.00      937     1827
## cor(Intercept,time0)    -0.07      0.51    -0.90     0.91 1.00     4454     2198
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.01      0.24    -0.48     0.45 1.00     1000     1533
## time0         1.30      0.05     1.19     1.41 1.00     5287     2785
## tx           -0.01      0.46    -0.93     0.93 1.00     1069     1714
## time0:tx      0.87      0.11     0.66     1.08 1.00     4651     2963
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.49      0.05     0.41     0.59 1.00     2769     2610
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, our focal parameter is &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit2)[&amp;quot;time0:tx&amp;quot;, -2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate      Q2.5     Q97.5 
## 0.8688574 0.6575292 1.0765760&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But since our data are coded such that baseline is &lt;code&gt;time0 == 0&lt;/code&gt; and the final time point is &lt;code&gt;time0 == 3&lt;/code&gt;, we‚Äôll need to multiply that coefficient by 3 to get the effect size in the pre/post metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit2)[&amp;quot;time0:tx&amp;quot;, -2] * 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
## 2.606572 1.972588 3.229728&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is it, within simulation variance of the effect size from the last section. Let‚Äôs compare them with a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(fixef(fit1)[&amp;quot;time0:tx&amp;quot;, -2] * 3 / sd_raw_pre_p,
      fixef(fit2)[&amp;quot;time0:tx&amp;quot;, -2] * 3) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  mutate(data = c(&amp;quot;unstandardized data&amp;quot;, &amp;quot;standardized data&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = data)) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_pointrange(fatten = 1) +
  labs(x = expression(italic(d)[GMA-raw]~(&amp;quot;standardized difference in change scores&amp;quot;)),
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig4-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yep, they‚Äôre pretty much the same.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sum-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sum up&lt;/h2&gt;
&lt;p&gt;Yes, one can compute a standardized mean difference effect size for experimental data analyzed with a multilevel growth model. The focal parameter is the treatment-time interaction, what we called &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;. The trick is to divide that parameter by the pooled standard deviation at baseline. This will put the effect size, what Feingold called &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-raw}\)&lt;/span&gt;, into a conventional Cohen‚Äôs-&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-type metric. But be mindful that this method may require you to multiply the effect by a number that corrects for how you have scaled the time variable. In the example we worked through, we multiplied by 3.&lt;/p&gt;
&lt;p&gt;As an alternative workflow, you can also fit the model on data that were standardized using the pooled standard deviation at baseline. This will automatically put the &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt; in the effect-size metric. But as with the other method, you still might have to correct for how you scaled the time variable.&lt;/p&gt;
&lt;p&gt;Though we‚Äôre not covering it, here, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldARegressionFramework2013&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldARegressionFramework2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; extended this framework to other contexts. For example, he discussed how to apply it to data with nonlinear trends and to models with other covariates. Just know the foundation is right here:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_\text{GMA-raw} = \beta_{11}(\text{time}) / SD_\text{raw}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0   
##  [8] ggplot2_3.3.3   tidyverse_1.3.0 tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6     
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6         splines_4.0.4       
##   [6] svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17       
##  [11] digest_0.6.27        htmltools_0.5.1.1    rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000    haven_2.3.1         
##  [26] xfun_0.22            callr_3.5.1          crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1     
##  [36] V8_3.4.0             distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1       viridisLite_0.3.0   
##  [46] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16              htmlwidgets_1.5.2   
##  [51] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.1       pkgconfig_2.0.3     
##  [56] loo_2.4.1            farver_2.0.3         dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2      
##  [61] tidyselect_1.1.0     rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [66] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0       broom_0.7.5         
##  [71] ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1        yaml_2.2.1           fs_1.5.0            
##  [76] processx_3.4.5       knitr_1.31           nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [81] xml2_1.3.2           rstudioapi_0.13      compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [86] curl_4.3             gamm4_0.2-6          reprex_0.3.0         statmod_1.4.35       stringi_1.5.3       
##  [91] highr_0.8            ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41     
##  [96] Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
## [101] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [106] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3        codetools_0.2-18    
## [111] boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1    
## [116] withr_2.4.1          shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [126] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1‚Äì28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395‚Äì411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ‚Äô&lt;span&gt;Stan&lt;/span&gt;‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brms2021RM&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt; reference manual, &lt;span&gt;Version&lt;/span&gt; 2.15.0&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/brms.pdf&#34;&gt;https://CRAN.R-project.org/package=brms/brms.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cohenStatisticalPowerAnalysis1988a&#34; class=&#34;csl-entry&#34;&gt;
Cohen, J. (1988). &lt;em&gt;Statistical power analysis for the behavioral sciences&lt;/em&gt;. &lt;span&gt;L. Erlbaum Associates&lt;/span&gt;. &lt;a href=&#34;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&#34;&gt;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummingUnderstandingTheNewStatistics2012&#34; class=&#34;csl-entry&#34;&gt;
Cumming, G. (2012). &lt;em&gt;Understanding the new statistics: &lt;span&gt;Effect&lt;/span&gt; sizes, confidence intervals, and meta-analysis&lt;/em&gt;. &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&#34;&gt;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-feingoldEffectSizeForGMA2009&#34; class=&#34;csl-entry&#34;&gt;
Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(1), 43. &lt;a href=&#34;https://doi.org/10.1037/a0014699&#34;&gt;https://doi.org/10.1037/a0014699&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-feingoldARegressionFramework2013&#34; class=&#34;csl-entry&#34;&gt;
Feingold, A. (2013). A regression framework for effect size assessments in longitudinal modeling of group differences. &lt;em&gt;Review of General Psychology&lt;/em&gt;, &lt;em&gt;17&lt;/em&gt;(1), 111‚Äì121. &lt;a href=&#34;https://doi.org/10.1037/a0030048&#34;&gt;https://doi.org/10.1037/a0030048&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hedgesDistributionTheoryforGlass1981&#34; class=&#34;csl-entry&#34;&gt;
Hedges, L. V. (1981). Distribution theory for &lt;span&gt;Glass&lt;/span&gt;‚Äôs estimator of effect size and related estimators. &lt;em&gt;Journal of Educational Statistics&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(2), 107‚Äì128. &lt;a href=&#34;https://doi.org/10.3102/10769986006002107&#34;&gt;https://doi.org/10.3102/10769986006002107&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hoffmanLongitudinalAnalysisModeling2015&#34; class=&#34;csl-entry&#34;&gt;
Hoffman, L. (2015). &lt;em&gt;Longitudinal analysis: &lt;span&gt;Modeling&lt;/span&gt; within-person fluctuation and change&lt;/em&gt; (1 edition). &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&#34;&gt;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ‚Äôgeoms‚Äô for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-raudenbushHLM2002&#34; class=&#34;csl-entry&#34;&gt;
Raudenbush, S. W., &amp;amp; Bryk, A. S. (2002). &lt;em&gt;Hierarchical linear models: &lt;span&gt;Applications&lt;/span&gt; and data analysis methods&lt;/em&gt; (Second Edition). &lt;span&gt;SAGE Publications, Inc&lt;/span&gt;. &lt;a href=&#34;https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230&#34;&gt;https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-raudenbushEffectsOfStudyDuration2001&#34; class=&#34;csl-entry&#34;&gt;
Raudenbush, S. W., &amp;amp; Liu, X.-F. (2001). Effects of study duration, frequency of observation, and sample size on power in studies of group differences in polynomial change. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(4), 387. &lt;a href=&#34;https://doi.org/10.1037/1082-989X.6.4.387&#34;&gt;https://doi.org/10.1037/1082-989X.6.4.387&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-singerAppliedLongitudinalData2003&#34; class=&#34;csl-entry&#34;&gt;
Singer, J. D., &amp;amp; Willett, J. B. (2003). &lt;em&gt;Applied longitudinal data analysis: &lt;span&gt;Modeling&lt;/span&gt; change and event occurrence&lt;/em&gt;. &lt;span&gt;Oxford University Press, USA&lt;/span&gt;. &lt;a href=&#34;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ‚Äôtidyverse‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., Fran√ßois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., M√ºller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., ‚Ä¶ Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you‚Äôre curious about our priors, fit the models on your computer and then execute &lt;code&gt;fit1$prior&lt;/code&gt;. To learn more about &lt;strong&gt;brms&lt;/strong&gt; default priors, spend some time with the &lt;a href=&#34;https://CRAN.R-project.org/package=brms/brms.pdf&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; reference manual&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brms2021RM&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2021&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;If you‚Äôre not into the whole Bayesian framework I‚Äôm using, you can just ignore the part about trace plots and chains. If you‚Äôre into it, execute &lt;code&gt;plot(fit1)&lt;/code&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Really. If you are interested in communicating your research results to others, do not mess with the &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-change}\)&lt;/span&gt;. It‚Äôs on a totally different metric from the conventional Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and you‚Äôll just end up confusing people.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Effect sizes for experimental trials analyzed with multilevel growth models: One of two</title>
      <link>/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/</link>
      <pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/</guid>
      <description>
&lt;script src=&#34;/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;This post is the first installment of a two-part series. The impetus is a project at work. A colleague had longitudinal data for participants in two experimental groups, which they examined with a multilevel growth model of the kind we‚Äôll explore in the next post. My colleague then summarized the difference in growth for the two conditions with a standardized mean difference they called &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Their effect size looked large, to me, and I was perplexed when I saw the formula they used to compute their version of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. It had been a while since I had to compute an effect size like this, so I dove back into the literature, where I realized Feingold had worked this issue out in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; paper in &lt;a href=&#34;https://www.apa.org/pubs/journals/met&#34;&gt;&lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The purpose of this series is to show how to compute a Cohen‚Äôs-&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; type effect size when you have longitudinal data on &lt;span class=&#34;math inline&#34;&gt;\(3+\)&lt;/span&gt; time points for two experimental groups. In this first post, we‚Äôll warm up with the basics. In the second post, we‚Äôll get down to business. The data and overall framework come from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;This series is an applied tutorial moreso than an introduction. I‚Äôm presuming you have a passing familiarity with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should be familiar with effect sizes, particularly with standardized mean differences. If you need to brush up, consider Cohen‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988&lt;/a&gt;)&lt;/span&gt; authoritative text, or Cummings newer &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; text. Since we‚Äôll be making extensive use of Feingold‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; paper, you should at least save it as a reference. For nice conceptual overview, I also recommend Kelley and Preacher‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kelley2012effect&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; paper, &lt;a href=&#34;https://www3.nd.edu/~kkelley/publications/articles/Kelley_and_Preacher_Psychological_Methods_2012.pdf&#34;&gt;&lt;em&gt;On effect size&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Though it won‚Äôt be important for this first post, you‚Äôll want to be familiar with multilevel regression for the next‚Äìit‚Äôs a major part of why I‚Äôm making this series! For texts that focus on the longitudinal models relevant for the topic, I recommend &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;Raudenbush &amp;amp; Bryk&lt;/a&gt; (&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett&lt;/a&gt; (&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;‚Äìthe one I personally learned on‚Äì; or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To fully befit from the next post, it‚Äôll help if you have a passing familiarity with Bayesian regression (though frequentists will still be able to get the main points). For thorough introductions, I recommend either edition of McElreath‚Äôs text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Kruschke‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text; or Gelman, Hill, and Vehtari‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; text. If you go with McElreath, he has a fine series of freely-available lectures &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. Probably the best place to learn about the &lt;strong&gt;tidyverse&lt;/strong&gt;-style of coding, as well as an introduction to &lt;strong&gt;R&lt;/strong&gt;, is Grolemund and Wickham‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-grolemundDataScience2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; freely-available online text, &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R for data science&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we load our primary &lt;strong&gt;R&lt;/strong&gt; packages and adjust the global plotting theme defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork)

# adjust the global plotting theme
theme_set(
  theme_linedraw() +
    theme(text = element_text(family = &amp;quot;Times&amp;quot;),
          panel.grid = element_blank(),
          strip.text = element_text(margin = margin(b = 3, t = 3)))
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;Happily, Feingold included a working example of synthetic trial data in his paper. You can find the full data set displayed in his Table 1 (p.¬†46). Here we‚Äôll use a &lt;a href=&#34;https://tibble.tidyverse.org/reference/tribble.html&#34;&gt;tribble&lt;/a&gt; approach to enter those data into &lt;strong&gt;R&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  tribble(
    ~id, ~tx, ~t1, ~t2, ~t3, ~t4,
    101, -0.5, 3, 5, 5,  7,
    102, -0.5, 4, 4, 6,  6,
    103, -0.5, 4, 5, 7,  8,
    104, -0.5, 5, 6, 6,  8,
    105, -0.5, 5, 6, 7,  8,
    106, -0.5, 5, 7, 7,  7,
    107, -0.5, 5, 6, 8,  8,
    108, -0.5, 6, 6, 7,  9,
    109, -0.5, 6, 8, 9,  10,
    110, -0.5, 7, 7, 8,  9,
    111,  0.5, 3, 5, 7,  9,
    112,  0.5, 4, 7, 9,  11,
    113,  0.5, 4, 6, 8,  11,
    114,  0.5, 5, 7, 9,  10,
    115,  0.5, 5, 6, 9,  11,
    116,  0.5, 5, 7, 10, 10,
    117,  0.5, 5, 8, 8,  11,
    118,  0.5, 6, 7, 9,  12,
    119,  0.5, 6, 9, 11, 13,
    120,  0.5, 7, 8, 10, 12
  ) %&amp;gt;% 
  mutate(`t4-t1`   = t4 - t1,
         condition = ifelse(tx == -0.5, &amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;))

# inspect the first six rows
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##      id    tx    t1    t2    t3    t4 `t4-t1` condition
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    
## 1   101  -0.5     3     5     5     7       4 control  
## 2   102  -0.5     4     4     6     6       2 control  
## 3   103  -0.5     4     5     7     8       4 control  
## 4   104  -0.5     5     6     6     8       3 control  
## 5   105  -0.5     5     6     7     8       3 control  
## 6   106  -0.5     5     7     7     7       2 control&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These synthetic data are from a hypothetical clinical trial where (&lt;span class=&#34;math inline&#34;&gt;\(N = 20\)&lt;/span&gt;) participants were randomized into a control group (&lt;code&gt;tx == -0.5&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt;) or a treatment group (&lt;code&gt;tx == 0.5&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt;). Their responses on a single outcome variable, &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;, were recorded over four time points, which are recorded in columns &lt;code&gt;t1&lt;/code&gt; through &lt;code&gt;t4&lt;/code&gt;. The simple difference score between the first (&lt;code&gt;t1&lt;/code&gt;) and last time points (&lt;code&gt;t4&lt;/code&gt;) was computed in the &lt;code&gt;t4-t1&lt;/code&gt; column. For good measure, I threw in a nominal &lt;code&gt;condition&lt;/code&gt; variable to help clarify the levels of &lt;code&gt;tx&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To get a sense of the data, it might be helpful to plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# participant-level trajectories
p1 &amp;lt;-
  d %&amp;gt;% 
  pivot_longer(t1:t4) %&amp;gt;% 
  mutate(time      = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double(),
         condition = factor(condition, levels = c(&amp;quot;treatment&amp;quot;, &amp;quot;control&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = time, y = value, color = condition)) +
  geom_line(size = 1) +
  scale_color_viridis_d(end = .75, breaks = NULL) +
  scale_y_continuous(breaks = 0:4 * 3, limits = c(0, 13)) +
  labs(subtitle = &amp;quot;participant-level trajectories&amp;quot;,
       y = &amp;quot;outcome&amp;quot;) +
  facet_wrap(~id) +
  theme(strip.text.x = element_text(margin = margin(b = 0.25, t = 0.25)))

# group average trajectories
p2 &amp;lt;-
  d %&amp;gt;% 
  pivot_longer(t1:t4) %&amp;gt;% 
  mutate(time      = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double(),
         condition = factor(condition, levels = c(&amp;quot;treatment&amp;quot;, &amp;quot;control&amp;quot;))) %&amp;gt;% 
  group_by(time, condition) %&amp;gt;% 
  summarise(mean = mean(value)) %&amp;gt;% 
  
  ggplot(aes(x = time, y = mean, color = condition)) +
  geom_line(size = 2) +
  scale_color_viridis_d(end = .75) +
  scale_y_continuous(breaks = 0:4 * 3, limits = c(0, 13)) +
  labs(subtitle = &amp;quot;group averages&amp;quot;,
       y = &amp;quot;outcome&amp;quot;)

# combine
p1 + p2 + plot_layout(widths = c(5, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;720&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The series of miniature plots on the left shows the trajectory of each participant‚Äôs raw data, over time. The larger plot on the right shows the average value for each of the experimental conditions, over time. Although there is some variation across individuals within experimental conditions, clear trends emerge. The plot on the right shows the experimental conditions had the same average values at baseline (&lt;code&gt;t1&lt;/code&gt;), both conditions tended to increase over time, but the &lt;code&gt;treatment&lt;/code&gt; condition showed larger changes over time, relative to the &lt;code&gt;control&lt;/code&gt; condition.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-do-we-care-about&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What do we care about?&lt;/h2&gt;
&lt;p&gt;There are a lot of questions a clinical researcher might want to ask from data of this kind. If we narrow our focus to causal inference&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; with regards to the treatment conditions, I think there are three fundamental questions we‚Äôd want to ask. They all have to do with change over time:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How much did the participants in the control group change, on average?&lt;/li&gt;
&lt;li&gt;How much did the participants in the treatment group change, on average?&lt;/li&gt;
&lt;li&gt;What was the difference in change in the treatment group versus the control group, on average?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ideally, we‚Äôd like to express our answers to these questions, particularly the third, in terms of a meaningfully defined effect size. That will be the goal of the remainder of this post and the next.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;warm-up-with-just-two-time-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Warm up with just two time points&lt;/h2&gt;
&lt;p&gt;Before we put on our big-kid pants and start fitting longitudinal growth models, I recommend we follow Feingold‚Äôs approach and first focus on how we‚Äôd answer these questions with two-time-point data. If we were to drop the variables &lt;code&gt;t2&lt;/code&gt; and &lt;code&gt;t3&lt;/code&gt;, these data would have the form of a pre/post experiment, which Feingold called an &lt;em&gt;independent-groups pretest‚Äìposttest&lt;/em&gt; design &lt;span class=&#34;citation&#34;&gt;(IGPP&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009, p. 46&lt;/a&gt;)&lt;/span&gt;. A major reason to warm up in this way is because much of the work on effect sizes, from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;Cohen&lt;/a&gt; (&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988&lt;/a&gt;)&lt;/span&gt; and others &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;Cumming, 2012&lt;/a&gt;)&lt;/span&gt;, has been in the context of cross-sectional and pre/post designs, such as the IGPP. Not only have the analytic strategies centered on these simple cases, but the effect sizes designed for these simple cases are the ones most readers are used to interpreting. One of the big points in Feingold‚Äôs paper is we should prefer it when the effect sizes for our longitudinal growth models have clear links to the traditional effect sizes. I am inclined to agree.&lt;/p&gt;
&lt;div id=&#34;data-summaries.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data summaries.&lt;/h3&gt;
&lt;p&gt;To get a sense of the pre/post changes in the two conditions, a fine place to start is with summary statistics. Here we compute the means and standard deviations in the outcome variable for each condition at pre and post. We also throw in the means and standard deviations for the change scores, &lt;code&gt;t4-t1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  pivot_longer(cols = c(t1, t4, `t4-t1`),
               names_to = &amp;quot;variable&amp;quot;) %&amp;gt;% 
  group_by(variable, condition) %&amp;gt;% 
  summarise(mean = mean(value),
            sd   = sd(value))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
## # Groups:   variable [3]
##   variable condition  mean    sd
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 t1       control       5 1.15 
## 2 t1       treatment     5 1.15 
## 3 t4       control       8 1.15 
## 4 t4       treatment    11 1.15 
## 5 t4-t1    control       3 0.816
## 6 t4-t1    treatment     6 0.816&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Feingold displayed most of these statistics in his Table 1 (p.¬†46). Make special note of how consistent the &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; values are. This will become important in the second post. Anyway, now we‚Äôre ready to start defining the effect sizes. We‚Äôll start with the unstandardized kind.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unstandardized-mean-differences.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unstandardized mean differences.&lt;/h3&gt;
&lt;p&gt;The version of Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; we‚Äôre ultimately working up to is a special kind of standardized effect size. Yet not all effect sizes are standardized. In cases where the metric of the dependent variable is inherently meaningful, Pek and Flora &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-pekReportingEffectSizes2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; actually recommend researchers use &lt;em&gt;un&lt;/em&gt;standardized effect sizes. Say we thought the data in this example had values that were inherently meaningful. We could answer the three research questions, above, directly with sample statistics. Here we answer the first two questions by focusing on the means of the change scores, by experimental condition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  group_by(condition) %&amp;gt;% 
  summarise(mean_change = mean(`t4-t1`))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   condition mean_change
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
## 1 control             3
## 2 treatment           6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To answer to our final question, we simply compute the difference between those two change scores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;6 - 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although the average values in both groups changed over time, the participants in the &lt;code&gt;treatment&lt;/code&gt; condition changed 3 units more, on average, than those in the &lt;code&gt;control&lt;/code&gt; condition. Is that difference meaningful? At the moment, it seems hard to say because these data are not actually on an inherently meaningful metric. The whole thing is made up and abstract. We might be better off by using a standardized effect size, instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standardized-mean-differences.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Standardized mean differences.&lt;/h3&gt;
&lt;p&gt;The approach above works great if the outcome variable is inherently meaningful and if we have no interest in comparing these results with studies on different outcome variables. In reality, clinical researchers often use sum scores from self-report questionnaires as their primary outcome variables and these scores take on seemingly arbitrary values. Say you work in depression research. There are numerous questionnaires designed to measure depression &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-friedThe52SymptomsOfMajorDepression2017&#34; role=&#34;doc-biblioref&#34;&gt;Fried, 2017&lt;/a&gt;)&lt;/span&gt; and their sum scores are all scaled differently. The problem is even worse if you‚Äôd like to compare two different kinds of outcomes, such as depression and anxiety. This is where standardized effect sizes come in.&lt;/p&gt;
&lt;p&gt;Since we are focusing on the data from the first and last time points, we can use conventional summary-statistic oriented strategies to compute the &lt;em&gt;standardized&lt;/em&gt; mean differences. In the literature, you‚Äôll often find standardized mean differences referred to as a Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, named after the late &lt;a href=&#34;https://en.wikipedia.org/wiki/Jacob_Cohen_(statistician)&#34;&gt;Jacob Cohen&lt;/a&gt;. I suspect what isn‚Äôt always appreciated is that there are many ways to compute &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and that ‚ÄúCohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;‚Äù can both refer to the general family of standardized mean differences or to a specific kind of standardized mean difference. In addition to Cohen‚Äôs original &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988&lt;/a&gt;)&lt;/span&gt; work, I think Geoff Cumming walked this out nicely in Chapter 11 of his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; text. Here we‚Äôll consider two versions of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; from Feingold‚Äôs paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Two effect sizes can be calculated from an IGPP design, one using the standard deviation of the change scores in the denominator and the other using the standard deviation of the raw scores &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-morrisEstimatingEffectSizes2008&#34; role=&#34;doc-biblioref&#34;&gt;Morris, 2008&lt;/a&gt;; &lt;a href=&#34;#ref-morrisCombiningEffectSizeEstimates2002&#34; role=&#34;doc-biblioref&#34;&gt;Morris &amp;amp; DeShon, 2002&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
d_\text{IGPP-change} &amp;amp; = (M_\text{change-T} / SD_\text{change-T}) - (M_\text{change-C} / SD_\text{change-C}) \\
&amp;amp; = (6/0.82) - (3/0.82) = 3.67,
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{change-T}\)&lt;/span&gt; is the [standard deviation for the change scores]&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; for the treatment group and &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{change-T}\)&lt;/span&gt; is the [standard deviation for the change scores] for the control group. (If homogeneity of variance across conditions is assumed, each of these terms can be replaced by &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{change}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
d_\text{IGPP-raw} &amp;amp; = (M_\text{change-T} / SD_\text{raw(pre-T)}) - (M_\text{change-C} / SD_\text{raw(pre-C)}) \\
&amp;amp; = (6/1.15) - (3/1.15) = 2.60,
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(M_\text{change-T}\)&lt;/span&gt; is the mean of the change scores for the treatment group, &lt;span class=&#34;math inline&#34;&gt;\(M_\text{change-C}\)&lt;/span&gt; is the mean of change scores for the control group, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-T)}\)&lt;/span&gt; is the pretest &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; for the treatment group, and &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt; is the pretest &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; for the control group. (If homogeneity of variance is assumed, each of the last two terms can be replaced by &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw}\)&lt;/span&gt;.) (p.¬†47)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We should practice computing these values by hand. First, we compute the group-level summary statistics and save each value separately for further use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group-level change score means
m_change_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t4-t1`)) %&amp;gt;% pull()  # 6
m_change_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t4-t1`)) %&amp;gt;% pull()  # 3

# group-level change score sds
sd_change_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(s = sd(`t4-t1`)) %&amp;gt;% pull()  # 0.8164966
sd_change_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(s = sd(`t4-t1`)) %&amp;gt;% pull()  # 0.8164966

# group-level baseline sds
sd_raw_pre_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(s = sd(t1)) %&amp;gt;% pull()  # 1.154701
sd_raw_pre_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(s = sd(t1)) %&amp;gt;% pull()  # 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With all those values saved, here‚Äôs how we might use the first equation, above, to compute Feingold‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(m_change_t / sd_change_t) - (m_change_c / sd_change_c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.674235&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a similar way, here‚Äôs how we might the second equation to compute &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(m_change_t / sd_raw_pre_t) - (m_change_c / sd_raw_pre_c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.598076&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In most areas of psychology, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;‚Äôs of this size would seem large. Whether researchers prefer the &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; approach or the &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt; approach, both return effect sizes in the form of a &lt;em&gt;standardized difference of differences&lt;/em&gt;. The primary question is what values to standardize the differences by (i.e., which &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; estimates might we place in the denominators). As discussed by both &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;Cumming&lt;/a&gt; (&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;, researchers are at liberty to make rational decisions on how to standardize their variables, and thus how to compute &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Whatever you decide for your research, just make sure you clarify your choice and your formulas for your audience.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extensions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extensions.&lt;/h3&gt;
&lt;p&gt;This is about as far as we‚Äôre going to go with the IGPP version of Feingold‚Äôs synthetic data. But if you do end up with data of this kind or similar, there are other things to consider. Real brief, here are two:&lt;/p&gt;
&lt;div id=&#34;its-good-to-express-uncertainty.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;It‚Äôs good to express uncertainty.&lt;/h4&gt;
&lt;p&gt;Just like any other statistical estimate, we should express the uncertainty in our effect sizes, somehow. In the seventh edition of the APA &lt;em&gt;Publication Manual&lt;/em&gt;, we read: ‚Äúwhenever possible, provide a confidence interval for each effect size reported to indicate the precision of estimation of the effect size‚Äù &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-apaPublicationManual2020&#34; role=&#34;doc-biblioref&#34;&gt;American Psychological Association, 2020, p. 89&lt;/a&gt;)&lt;/span&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Since the ultimate purpose of this mini blog series is to show how to compute effect sizes for multilevel growth models, I am not going to dive into this issue, here. We‚Äôre just warming up for the main event in the next post. But if you ever need to compute 95% CIs for &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt; based on IGPP data, check out Chapter 11 in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;Cumming&lt;/a&gt; (&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;there-are-many-more-ds.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;There are many more &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;‚Äôs.&lt;/h4&gt;
&lt;p&gt;We‚Äôve already mentioned there are several kinds of Cohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; effect sizes. With Feingold‚Äôs data, we practiced two: &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt;. Feingold built his paper on the foundation of &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-morrisCombiningEffectSizeEstimates2002&#34; role=&#34;doc-biblioref&#34;&gt;Morris &amp;amp; DeShon&lt;/a&gt; (&lt;a href=&#34;#ref-morrisCombiningEffectSizeEstimates2002&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt;, which covered a larger variety of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;‚Äôs suited for cross-sectional and pre/post designs with one or two experimental conditions. Morris and DeShon‚Äôs writing style was accessible and I liked their statistical notation. You might check out their paper and expand your &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; repertoire.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-we-learned-and-whats-soon-to-come&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What we learned and what‚Äôs soon to come&lt;/h2&gt;
&lt;p&gt;In this first post, we learned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Effect sizes for experimental trials analyzed with multilevel growth models aren‚Äôt straightforward.&lt;/li&gt;
&lt;li&gt;Much of the effect size literature is based on simple cross-sectional or two-time-point designs with one or two groups.&lt;/li&gt;
&lt;li&gt;Effect sizes can be standardized or unstandardized.&lt;/li&gt;
&lt;li&gt;‚ÄúCohen‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;‚Äù can refer to either a general class of standardized mean differences, or to a specific standardized mean differences.&lt;/li&gt;
&lt;li&gt;As discussed in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;, the two effect sizes recommended for IGPP designs are &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt;, both of which can be computed with simple summary statistics.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stay tuned for the second post in this series, where we‚Äôll extend these skills to two-group experimental data with more than two time points. The multilevel growth model will make its grand appearance and it‚Äôll just be great!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] patchwork_1.1.1 forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5    
##  [5] purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0   
##  [9] ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.1.0  xfun_0.22         haven_2.3.1       colorspace_2.0-0 
##  [5] vctrs_0.3.6       generics_0.1.0    viridisLite_0.3.0 htmltools_0.5.1.1
##  [9] yaml_2.2.1        utf8_1.1.4        rlang_0.4.10      pillar_1.5.1     
## [13] withr_2.4.1       glue_1.4.2        DBI_1.1.0         dbplyr_2.0.0     
## [17] modelr_0.1.8      readxl_1.3.1      lifecycle_1.0.0   munsell_0.5.0    
## [21] blogdown_1.3      gtable_0.3.0      cellranger_1.1.0  rvest_0.3.6      
## [25] evaluate_0.14     labeling_0.4.2    knitr_1.31        fansi_0.4.2      
## [29] highr_0.8         broom_0.7.5       Rcpp_1.0.6        scales_1.1.1     
## [33] backports_1.2.1   jsonlite_1.7.2    farver_2.0.3      fs_1.5.0         
## [37] hms_0.5.3         digest_0.6.27     stringi_1.5.3     bookdown_0.21    
## [41] grid_4.0.4        cli_2.3.1         tools_4.0.4       magrittr_2.0.1   
## [45] crayon_1.4.1      pkgconfig_2.0.3   ellipsis_0.3.1    xml2_1.3.2       
## [49] reprex_0.3.0      lubridate_1.7.9.2 assertthat_0.2.1  rmarkdown_2.7    
## [53] httr_1.4.2        rstudioapi_0.13   R6_2.5.0          compiler_4.0.4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-apaPublicationManual2020&#34; class=&#34;csl-entry&#34;&gt;
American Psychological Association. (2020). &lt;em&gt;Publication manual of the &lt;span&gt;American Psychological Association&lt;/span&gt;&lt;/em&gt; (Seventh Edition). &lt;span&gt;American Psychological Association&lt;/span&gt;. &lt;a href=&#34;https://apastyle.apa.org/products/publication-manual-7th-edition&#34;&gt;https://apastyle.apa.org/products/publication-manual-7th-edition&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cohenStatisticalPowerAnalysis1988a&#34; class=&#34;csl-entry&#34;&gt;
Cohen, J. (1988). &lt;em&gt;Statistical power analysis for the behavioral sciences&lt;/em&gt;. &lt;span&gt;L. Erlbaum Associates&lt;/span&gt;. &lt;a href=&#34;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&#34;&gt;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummingUnderstandingTheNewStatistics2012&#34; class=&#34;csl-entry&#34;&gt;
Cumming, G. (2012). &lt;em&gt;Understanding the new statistics: &lt;span&gt;Effect&lt;/span&gt; sizes, confidence intervals, and meta-analysis&lt;/em&gt;. &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&#34;&gt;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-feingoldEffectSizeForGMA2009&#34; class=&#34;csl-entry&#34;&gt;
Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(1), 43. &lt;a href=&#34;https://doi.org/10.1037/a0014699&#34;&gt;https://doi.org/10.1037/a0014699&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-friedThe52SymptomsOfMajorDepression2017&#34; class=&#34;csl-entry&#34;&gt;
Fried, E. I. (2017). The 52 symptoms of major depression: &lt;span&gt;Lack&lt;/span&gt; of content overlap among seven common depression scales. &lt;em&gt;Journal of Affective Disorders&lt;/em&gt;, &lt;em&gt;208&lt;/em&gt;, 191‚Äì197. &lt;a href=&#34;https://doi.org/10.1016/j.jad.2016.10.019&#34;&gt;https://doi.org/10.1016/j.jad.2016.10.019&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-grolemundDataScience2017&#34; class=&#34;csl-entry&#34;&gt;
Grolemund, G., &amp;amp; Wickham, H. (2017). &lt;em&gt;R for data science&lt;/em&gt;. &lt;span&gt;O‚ÄôReilly&lt;/span&gt;. &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;https://r4ds.had.co.nz&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hoffmanLongitudinalAnalysisModeling2015&#34; class=&#34;csl-entry&#34;&gt;
Hoffman, L. (2015). &lt;em&gt;Longitudinal analysis: &lt;span&gt;Modeling&lt;/span&gt; within-person fluctuation and change&lt;/em&gt; (1 edition). &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&#34;&gt;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kelley2012effect&#34; class=&#34;csl-entry&#34;&gt;
Kelley, K., &amp;amp; Preacher, K. J. (2012). On effect size. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;17&lt;/em&gt;(2), 137. &lt;a href=&#34;https://doi.org/10.1037/a0028086&#34;&gt;https://doi.org/10.1037/a0028086&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-morrisEstimatingEffectSizes2008&#34; class=&#34;csl-entry&#34;&gt;
Morris, S. B. (2008). Estimating effect sizes from pretest-posttest-control group designs. &lt;em&gt;Organizational Research Methods&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(2), 364‚Äì386. &lt;a href=&#34;https://doi.org/10.1177/1094428106291059&#34;&gt;https://doi.org/10.1177/1094428106291059&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-morrisCombiningEffectSizeEstimates2002&#34; class=&#34;csl-entry&#34;&gt;
Morris, S. B., &amp;amp; DeShon, R. P. (2002). Combining effect size estimates in meta-analysis with repeated measures and independent-groups designs. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;7&lt;/em&gt;(1), 105. &lt;a href=&#34;https://doi.org/10.1037/1082-989X.7.1.105&#34;&gt;https://doi.org/10.1037/1082-989X.7.1.105&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pekReportingEffectSizes2018&#34; class=&#34;csl-entry&#34;&gt;
Pek, J., &amp;amp; Flora, D. B. (2018). Reporting effect sizes in original psychological research: &lt;span&gt;A&lt;/span&gt; discussion and tutorial. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;23&lt;/em&gt;(2), 208. https://doi.org/&lt;a href=&#34;https://doi.apa.org/fulltext/2017-10871-001.html&#34;&gt;https://doi.apa.org/fulltext/2017-10871-001.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-raudenbushHLM2002&#34; class=&#34;csl-entry&#34;&gt;
Raudenbush, S. W., &amp;amp; Bryk, A. S. (2002). &lt;em&gt;Hierarchical linear models: &lt;span&gt;Applications&lt;/span&gt; and data analysis methods&lt;/em&gt; (Second Edition). &lt;span&gt;SAGE Publications, Inc&lt;/span&gt;. &lt;a href=&#34;https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230&#34;&gt;https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-singerAppliedLongitudinalData2003&#34; class=&#34;csl-entry&#34;&gt;
Singer, J. D., &amp;amp; Willett, J. B. (2003). &lt;em&gt;Applied longitudinal data analysis: &lt;span&gt;Modeling&lt;/span&gt; change and event occurrence&lt;/em&gt;. &lt;span&gt;Oxford University Press, USA&lt;/span&gt;. &lt;a href=&#34;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ‚Äôtidyverse‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., Fran√ßois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., M√ºller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., ‚Ä¶ Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you are unfamiliar with causal inference and confused over why causal inference might lead us to limit our focus in this way, check out Chapters 18 through 21 in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;Gelman et al.&lt;/a&gt; (&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I‚Äôm not in love with introducing this new acronym. But if we want to follow along with Feingold, we may as well get used to his term.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;In the original paper, Feingold used the term ‚Äúmean change score‚Äù here as well as a bit later in the sentence. After reading this through several times and working through his examples, I‚Äôm confident these were typos. With technical material of this kind, it‚Äôs hard to avoid a typo or two.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;We Bayesians, of course, can forgive the frequentist bias in the wording of the APA‚Äôs otherwise sound recommendation.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Could you compute Bayesian credible intervals for IGPP effect sizes by adjusting some of the strategies from my earlier blog post, &lt;a href=&#34;Regression%20models%20for%202-timepoint%20non-experimental%20data&#34;&gt;&lt;em&gt;Regression models for 2-timepoint non-experimental data&lt;/em&gt;&lt;/a&gt;? Yes, you could.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Regression models for 2-timepoint non-experimental data</title>
      <link>/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/</link>
      <pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/</guid>
      <description>
&lt;script src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;purpose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Purpose&lt;/h2&gt;
&lt;p&gt;In the contemporary longitudinal data analysis literature, 2-timepoint data (a.k.a. pre/post data) get a bad wrap. Singer and Willett &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003, p. 10&lt;/a&gt;)&lt;/span&gt; described 2-timepoint data as only ‚Äúmarginally better‚Äù than cross-sectional data and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-rogosaGrowthCurveApproach1982&#34; role=&#34;doc-biblioref&#34;&gt;Rogosa et al.&lt;/a&gt; (&lt;a href=&#34;#ref-rogosaGrowthCurveApproach1982&#34; role=&#34;doc-biblioref&#34;&gt;1982&lt;/a&gt;)&lt;/span&gt; give a technical overview on the limitations of 2-timepoint data. Limitations aside, sometimes two timepoints are all you have. In those cases, researchers should have a good sense of which data analysis options they have at their disposal. I recently came across &lt;a href=&#34;https://twitter.com/jwalkrunski&#34;&gt;Jeffrey Walker&lt;/a&gt;‚Äôs free &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-walkerElementsOfStatisticalModeling2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; text, &lt;a href=&#34;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/&#34;&gt;&lt;em&gt;Elements of statistical modeling for experimental biology&lt;/em&gt;&lt;/a&gt;, which contains a &lt;a href=&#34;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/models-for-longitudinal-experiments-pre-post-designs.html&#34;&gt;nice chapter&lt;/a&gt; on 2-timepoint experimental designs. Inspired by his work, this post aims to explore how one might analyze &lt;em&gt;non-experimental&lt;/em&gt; 2-timepoint data within a regression model paradigm.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;In this post, I‚Äôm presuming you are familiar with longitudinal data analysis with conventional and multilevel regression. Though I don‚Äôt emphasize it much, it will also help if you‚Äôre familiar with Bayesian statistics. All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. The statistical models will be fit with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and we‚Äôll also make some use of the &lt;strong&gt;tidybayes&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;patchwork&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-patchwork&#34; role=&#34;doc-biblioref&#34;&gt;Pedersen, 2019&lt;/a&gt;)&lt;/span&gt; packages. If you need to shore up, I list some educational resources at the &lt;a href=&#34;#next-steps&#34;&gt;end of the post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Load the primary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;warm-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Warm-up&lt;/h2&gt;
&lt;p&gt;Before we jump into 2-timepoint data, we‚Äôll first explore how one might analyze a fuller data set of 6 timepoints. We will then reduce the data set to two different 2-timepoint versions for use in the remainder of the post.&lt;/p&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;We will simulate the data based on a conventional multilevel growth model of the kind you can learn about in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett&lt;/a&gt; (&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;, or Kurz &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, Chapter 14)&lt;/span&gt;. We‚Äôll have one criterion variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; which will vary across participants &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and over time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. For simplicity, the systemic change over time will be linear. We might express it in statistical notation&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} + u_{0i} + u_{1i} \text{time}_{ti} \\
\sigma &amp;amp; = \sigma_\epsilon \\
\begin{bmatrix} u_{0i} \\ u_{1i} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf \Sigma \right) \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_1 \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the population-level intercept (initial status) and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the population-level slope (change over time). The &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}\)&lt;/span&gt; terms are the participant-level deviations around the population-level intercept and slope. Those &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; deviations follow a bivariate normal distribution centered on zero (they are deviations, after all) and including a covariance matrix, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt;. As is typical within the &lt;strong&gt;brms&lt;/strong&gt; framework &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2017&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; is decomposed into a matrix of standard deviations (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf S\)&lt;/span&gt;) and a correlation matrix (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf R\)&lt;/span&gt;). Also notice we renamed our original &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameter as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt; to help distinguish it from the multilevel standard deviations in the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S\)&lt;/span&gt; matrix (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt;). In this way, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt; capture differences &lt;em&gt;between&lt;/em&gt; participants in their intercepts and slopes, whereas &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt; captures the differences &lt;em&gt;within&lt;/em&gt; participants over time that occur apart from their linear trajectories.&lt;/p&gt;
&lt;p&gt;To simulate data of this kind, we‚Äôll first set the true values for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0, \beta_1, \sigma_0, \sigma_1, \rho,\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b0     &amp;lt;- 0      # starting point (average intercept)
b1     &amp;lt;- 1      # growth over time (average slope)
sigma0 &amp;lt;- 1      # std dev in intercepts
sigma1 &amp;lt;- 1      # std dev in slopes
rho    &amp;lt;- -.5    # correlation between intercepts and slopes
sigma_e &amp;lt;- 0.75  # std dev within participants&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now combine several of those values to define the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S, \mathbf R,\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; matrices. Then simulate &lt;span class=&#34;math inline&#34;&gt;\(N = 100\)&lt;/span&gt; participant-level intercepts and slopes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu     &amp;lt;- c(b0, b1)          # combine the means in a vector
sigmas &amp;lt;- c(sigma0, sigma1)  # combine the std devs in a vector

s &amp;lt;- diag(sigmas)      # standard deviation matrix
r &amp;lt;- matrix(c(1, rho,  # correlation matrix
             rho, 1), nrow = 2)

# now matrix multiply s and r to get a covariance matrix
sigma &amp;lt;- s %*% r %*% s

# how many participants would you like?
n_id &amp;lt;- 100

# make the simulation reproducible
set.seed(1)

vary_effects &amp;lt;- 
  MASS::mvrnorm(n_id, mu = mu, Sigma = sigma) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  set_names(&amp;quot;intercepts&amp;quot;, &amp;quot;slopes&amp;quot;) %&amp;gt;% 
  mutate(id = 1:n_id) %&amp;gt;% 
  select(id, everything())

head(vary_effects)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id  intercepts     slopes
## 1  1  0.85270825  0.7676584
## 2  2 -0.18009772  1.1379818
## 3  3  1.17913643  0.7317852
## 4  4 -1.46056809  2.3025393
## 5  5  0.04193022  1.6126544
## 6  6 -0.17309717 -0.5941901&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have our random intercepts and slopes, we‚Äôre almost ready to simulate our &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; values. We just need to decide on how many values we‚Äôd like to collect over time and how we‚Äôd like to structure those assessment periods. To keep things simple, I‚Äôm going to specify six evenly-spaced timepoints. The first timepoint will be set to 0, the last timepoint will be set to 1, and the four timepoints in the middle will be the corresponding fractions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many timepoints?
time_points &amp;lt;- 6

d &amp;lt;-
  vary_effects %&amp;gt;% 
  # add in time
  expand(nesting(id, intercepts, slopes),
         time = seq(from = 0, to = 1, length.out = time_points)) %&amp;gt;% 
  # now use the model formula to compute y
  mutate(y = rnorm(n(), mean = intercepts + slopes * time, sd = sigma_e))

head(d, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 5
##       id intercepts slopes  time      y
##    &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1     1      0.853  0.768   0    1.16 
##  2     1      0.853  0.768   0.2  2.27 
##  3     1      0.853  0.768   0.4  2.35 
##  4     1      0.853  0.768   0.6  1.07 
##  5     1      0.853  0.768   0.8 -0.247
##  6     1      0.853  0.768   1    3.49 
##  7     2     -0.180  1.14    0    0.320
##  8     2     -0.180  1.14    0.2  0.453
##  9     2     -0.180  1.14    0.4  0.265
## 10     2     -0.180  1.14    0.6  0.885&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we move on, I should acknowledge that this simulation workflow is heavily influenced by McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, Chapter 14)&lt;/span&gt;. You can find a similar workflow in the &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-debruineUnderstandingMixedEffects2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; preprint by DeBruine and Barr, &lt;a href=&#34;https://psyarxiv.com/xp5cy/&#34;&gt;&lt;em&gt;Understanding mixed effects models through data simulation&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explore-the-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explore the data.&lt;/h3&gt;
&lt;p&gt;Before fitting the model, it might help if we look at what we‚Äôve done. Here‚Äôs a scatter plot of the random intercepts and slopes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set the global plotting theme
theme_set(theme_linedraw() +
            theme(text = element_text(family = &amp;quot;Times&amp;quot;),
                  panel.grid = element_blank()))

p1 &amp;lt;-
  vary_effects %&amp;gt;% 
  ggplot(aes(x = intercepts, y = slopes)) +
  geom_point() +
  stat_ellipse(color = &amp;quot;grey50&amp;quot;)

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The 95%-interval ellipse helps point out the negative correlation between the intercepts and slopes. Here‚Äôs the Pearson‚Äôs correlation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vary_effects %&amp;gt;% 
  summarise(rho = cor(intercepts, slopes))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          rho
## 1 -0.4502206&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It‚Äôs no coincidence that value is very close to our data-generating &lt;code&gt;rho&lt;/code&gt; value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rho&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now check the sample means and standard deviations of our &lt;code&gt;intercepts&lt;/code&gt; and &lt;code&gt;slopes&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vary_effects %&amp;gt;% 
  summarise(b0 = mean(intercepts),
            b1 = mean(slopes),
            sigma0 = sd(intercepts),
            sigma1 = sd(slopes)) %&amp;gt;% 
  mutate_all(round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      b0   b1 sigma0 sigma1
## 1 -0.08 1.11   0.91   0.91&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those aren‚Äôt quite the true data-generating values for &lt;code&gt;b0&lt;/code&gt; through &lt;code&gt;sigma1&lt;/code&gt;, from above. But they‚Äôre pretty decent sample approximations. With only &lt;span class=&#34;math inline&#34;&gt;\(N = 100\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T = 6\)&lt;/span&gt;, this is about as close as we should expect.&lt;/p&gt;
&lt;p&gt;To get a sense of the &lt;code&gt;time&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values, we‚Äôll plot them in two ways. First we‚Äôll plot a random subset from nine of our simulated participants. Then we‚Äôll plot the linear trajectories from all 100 participants, along with the grand mean trajectory.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

p2 &amp;lt;-
  d %&amp;gt;% 
  nest(data = c(intercepts, slopes, time, y)) %&amp;gt;% 
  slice_sample(n = 9) %&amp;gt;% 
  unnest(data) %&amp;gt;% 
  
  ggplot(aes(x = time, y = y)) +
  geom_point() +
  geom_abline(aes(intercept = intercepts, slope = slopes),
              color = &amp;quot;blue&amp;quot;) +
  labs(subtitle = &amp;quot;random subset of 9 participants&amp;quot;) +
  theme(strip.background = element_blank(),
        strip.text = element_blank()) +
  facet_wrap(~slopes)

p3 &amp;lt;-
  d %&amp;gt;% 
  ggplot(aes(x = time, y = y)) +
  geom_point(color = &amp;quot;transparent&amp;quot;) +
  geom_abline(aes(intercept = intercepts, slope = slopes, group = id),
              color = &amp;quot;blue&amp;quot;, size = 1/10, alpha = 1/2) +
  geom_abline(intercept = b0, slope = b1,
              color = &amp;quot;blue&amp;quot;, size = 2) +
  labs(subtitle = &amp;quot;All participant-level trajectories, along\nwith the grand mean&amp;quot;)

# combine
(p2 + p3) &amp;amp;
  scale_x_continuous(breaks = 0:5 / 5, labels = c(0, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, 1)) &amp;amp;
  coord_cartesian(ylim = c(-2.5, 3.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;See how the points in the plots on the left deviate quite a bit from their linear trajectories? That‚Äôs the result of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-multilevel-growth-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the multilevel growth model.&lt;/h3&gt;
&lt;p&gt;Now we‚Äôll use &lt;strong&gt;brms&lt;/strong&gt; to fit the multilevel growth model. If all goes well, we should largely reproduce our data-generating values in the posterior. Before fitting the model, we should consider a few things about the &lt;code&gt;brm()&lt;/code&gt; syntax.&lt;/p&gt;
&lt;p&gt;In this model and in most of the models to follow, we‚Äôre relying on the default &lt;code&gt;brm()&lt;/code&gt; priors. When fitting real-world models, you are much better off going beyond the defaults. However, I will generally deemphasize priors, in this post, to help keep the focus on the conceptual models.&lt;/p&gt;
&lt;p&gt;Note how we set the &lt;code&gt;seed&lt;/code&gt; argument. Though you don‚Äôt need to do this, setting the &lt;code&gt;seed&lt;/code&gt; makes the results more reproducible.&lt;/p&gt;
&lt;p&gt;Also, note the custom settings for &lt;code&gt;iter&lt;/code&gt; and &lt;code&gt;warmup&lt;/code&gt;. Often times, the default settings are fine. But since we‚Äôll be comparing a lot of models, I want to make sure we have enough posterior draws from each to ensure stable estimates.&lt;/p&gt;
&lt;p&gt;Okay, fit the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m0 &amp;lt;-
  brm(data = d,
      y ~ 1 + time + (1 + time | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time + (1 + time | id) 
##    Data: d (Number of observations: 600) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)           0.89      0.09     0.72     1.07 1.00     3663     5989
## sd(time)                0.84      0.15     0.53     1.14 1.00     1782     2936
## cor(Intercept,time)    -0.40      0.15    -0.65    -0.05 1.00     3758     4228
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.13      0.11    -0.33     0.08 1.00     4153     6181
## time          1.16      0.13     0.91     1.42 1.00     6636     6477
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.81      0.03     0.76     0.87 1.00     4706     5720
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A series of plots might help show how well our model captured the data-generating values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# name the parameters with the Greek terms
names &amp;lt;- c(&amp;quot;beta[0]&amp;quot;, &amp;quot;beta[1]&amp;quot;, &amp;quot;sigma[0]&amp;quot;, &amp;quot;sigma[1]&amp;quot;, &amp;quot;rho&amp;quot;, &amp;quot;sigma[epsilon]&amp;quot;)

# for the vertical lines marking off the true values
vline &amp;lt;-
  tibble(name = names,
         true_value = c(b0, b1, sigma0, sigma1, rho, sigma_e))

# wrangle
posterior_samples(m0) %&amp;gt;% 
  select(b_Intercept:sigma) %&amp;gt;% 
  set_names(names) %&amp;gt;% 
  pivot_longer(everything()) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(.width = .95, normalize = &amp;quot;panels&amp;quot;, size = 1/2) +
  geom_vline(data = vline,
             aes(xintercept = true_value),
             size = 1/4, linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_wrap(~name, scales = &amp;quot;free&amp;quot;, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The marginal posterior distribution for all the major summary parameters is summarized by the median (dot) and percentile-based 95% interval (horizontal line). The true values are shown in the dashed vertical lines. Overall, we did okay.&lt;/p&gt;
&lt;p&gt;As fun as this has all been, we‚Äôve just been warming up.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;make-the-2-timepoint-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Make the 2-timepoint data.&lt;/h3&gt;
&lt;p&gt;Before fitting the 2-timepoint longitudinal models, we‚Äôll need to adjust the data, which currently contains values over six timepoints. Since it‚Äôs easy to think of 2-timepoint data in terms of pre and post, we‚Äôll keep the data points for which &lt;code&gt;time == 0&lt;/code&gt; and &lt;code&gt;time == 1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_long &amp;lt;-
  d %&amp;gt;% 
  filter(time == 0 | time == 1) %&amp;gt;% 
  select(-intercepts, -slopes) %&amp;gt;% 
  mutate(`pre/post` = factor(if_else(time == 0, &amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;),
                             levels = c(&amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;))) 

head(small_data_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##      id  time     y `pre/post`
##   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;     
## 1     1     0 1.16  pre       
## 2     1     1 3.49  post      
## 3     2     0 0.320 pre       
## 4     2     1 1.27  post      
## 5     3     0 0.879 pre       
## 6     3     1 0.971 post&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the name implies, the &lt;code&gt;small_data_long&lt;/code&gt; data are still in the long format. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; described this as the &lt;em&gt;stacked format&lt;/em&gt; and Singer and Willett &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt; called this a &lt;em&gt;person-period data set&lt;/em&gt;. Each level of &lt;code&gt;id&lt;/code&gt; has two rows, one for each level of &lt;code&gt;time&lt;/code&gt;, which is an explicit variable. In this formulation, &lt;code&gt;time == 0&lt;/code&gt; is the same as the ‚Äúpre‚Äù timepoint and &lt;code&gt;time == 1&lt;/code&gt; is the same as ‚Äúpost.‚Äù To help clarify that, we added a &lt;code&gt;pre/post&lt;/code&gt; column.&lt;/p&gt;
&lt;p&gt;We‚Äôll need a second variant of this data set, this time in the wide format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide &amp;lt;-
  small_data_long %&amp;gt;% 
  select(-time) %&amp;gt;% 
  pivot_wider(names_from = `pre/post`, values_from = y) %&amp;gt;% 
  mutate(change = post - pre)

head(small_data_wide)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##      id    pre   post change
##   &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1     1  1.16   3.49  2.33  
## 2     2  0.320  1.27  0.953 
## 3     3  0.879  0.971 0.0920
## 4     4 -0.979  0.586 1.57  
## 5     5 -0.825  1.85  2.68  
## 6     6 -0.912 -0.841 0.0715&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With our &lt;code&gt;small_data_wide&lt;/code&gt; data, each level of &lt;code&gt;id&lt;/code&gt; only has one row. The time-structured &lt;code&gt;y&lt;/code&gt; column was broken up into a &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; column, and we no longer have a variable explicitly defining &lt;em&gt;time&lt;/em&gt;. We have a new column, &lt;code&gt;change&lt;/code&gt;, which is the result of subtracting &lt;code&gt;pre&lt;/code&gt; from &lt;code&gt;post&lt;/code&gt;. In her text, Hoffman referred to this type of data structure as the &lt;em&gt;multivariate format&lt;/em&gt; and Singer and Willett called it a &lt;em&gt;person-level data set&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The information is essentially the same in these two data sets, &lt;code&gt;small_data_long&lt;/code&gt; and &lt;code&gt;small_data_wide&lt;/code&gt;. Yet, the models supported by them will provide different insights.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;timepoint-longitudinal-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2-timepoint longitudinal models&lt;/h2&gt;
&lt;p&gt;Before we start fitting and interpreting models, we should prepare ourselves with an overview.&lt;/p&gt;
&lt;div id=&#34;overview.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview.&lt;/h3&gt;
&lt;p&gt;We will consider 20 ways to fit models based on 2-timepoint data. It seems like there multiple ways to categorize these. Here we‚Äôll break them up into four groupings.&lt;/p&gt;
&lt;p&gt;The first four model types will take &lt;code&gt;post&lt;/code&gt; as the criterion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_1 \colon\)&lt;/span&gt; The unconditional post model (&lt;code&gt;post ~ 1&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_2 \colon\)&lt;/span&gt; The simple autoregressive model (&lt;code&gt;post ~ 1 + pre&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_3 \colon\)&lt;/span&gt; The bivariate autoregressive model (&lt;code&gt;bf(post ~ 1 + pre) + bf(pre ~ 1) + set_rescor(rescor = FALSE)&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_4 \colon\)&lt;/span&gt; The bivariate correlational pre/post model (&lt;code&gt;bf(post ~ 1) + bf(pre ~ 1) + set_rescor(rescor = TRUE)&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next four model types will take &lt;code&gt;change&lt;/code&gt; as the criterion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_5 \colon\)&lt;/span&gt; The unconditional change-score model (&lt;code&gt;change ~ 1&lt;/code&gt;) and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_6 \colon\)&lt;/span&gt; The conditional change-score model (&lt;code&gt;change ~ 1 + pre&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_7 \colon\)&lt;/span&gt; The bivariate conditional change-score model (&lt;code&gt;bf(change ~ 1 + pre) + bf(pre ~ 1) + set_rescor(rescor = FALSE)&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_8 \colon\)&lt;/span&gt; The bivariate correlational pre/change model (&lt;code&gt;bf(change ~ 1) + bf(pre ~ 1) + set_rescor(rescor = TRUE)&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next eight model types will take &lt;code&gt;y&lt;/code&gt; as the criterion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_9 \colon\)&lt;/span&gt; The grand-mean model (&lt;code&gt;y ~ 1&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{10} \colon\)&lt;/span&gt; The random-intercept model (&lt;code&gt;y ~ 1 + (1 | id)&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{11} \colon\)&lt;/span&gt; The cross-classified model (&lt;code&gt;y ~ 1 + (1 | id) + (1 | time)&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{12} \colon\)&lt;/span&gt; The simple liner model (&lt;code&gt;y ~ 1 + time&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{13} \colon\)&lt;/span&gt; The liner model with a random intercept (&lt;code&gt;y ~ 1 + time + (1 | id)&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{14} \colon\)&lt;/span&gt; The liner model with a random slope (&lt;code&gt;y ~ 1 + time + (0 + time | id)&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{15} \colon\)&lt;/span&gt; The multilevel growth model with regularizing priors (&lt;code&gt;y ~ 1 + time + (1 + time | id)&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{16} \colon\)&lt;/span&gt; The fixed effects with correlated error model (&lt;code&gt;y ~ 1 + time + ar(time = time, p = 1, gr = id&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final four model types will expand previous ones with robust variance parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{17} \colon\)&lt;/span&gt; The cross-classified model with robust variances for discrete time (&lt;code&gt;bf(y ~ 1 + (1 | id) + (1 | time), sigma ~ 0 + factor(time))&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{18} \colon\)&lt;/span&gt; The simple liner model with robust variance for linear time (&lt;code&gt;bf(y ~ 1 + time, sigma ~ 1 + time)&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{19} \colon\)&lt;/span&gt; The liner model with correlated random intercepts for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; (&lt;code&gt;bf(y ~ 1 + time + (1 |x| id), sigma ~ 1 + (1 |x| id))&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{20} \colon\)&lt;/span&gt; The liner model with a random slope for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and uncorrelated random intercept for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; (&lt;code&gt;bf(y ~ 1 + time + (0 + time | id), sigma ~ 1 + (1 | id))&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As far as these model names go, I‚Äôm making no claim they are canonical. Call them what you want. My goal, here, is to use names that are minimally descriptive and similar to the terms you might find used by other authors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;models-focusing-on-the-second-timepoint-post.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models focusing on the second timepoint, &lt;code&gt;post&lt;/code&gt;.&lt;/h3&gt;
&lt;div id=&#34;mathcal-m_1-colon-the-unconditional-post-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_1 \colon\)&lt;/span&gt; The unconditional post model.&lt;/h4&gt;
&lt;p&gt;We can write the unconditional post model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{post}_i &amp;amp; \sim \operatorname{Normal}(\mu, \sigma) \\
\mu &amp;amp; = \beta_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is both the model intercept and the estimate for the mean value of &lt;code&gt;post&lt;/code&gt;. The focus this model places on &lt;code&gt;post&lt;/code&gt; comes at the cost of any contextual information on what earlier values we might compare &lt;code&gt;post&lt;/code&gt; to. Also, since the only variable in the model is &lt;code&gt;post&lt;/code&gt;, this technically is &lt;em&gt;not&lt;/em&gt; a 2-timepoint model. But given its connection to the models to follow, it‚Äôs worth working through.&lt;/p&gt;
&lt;p&gt;Here‚Äôs how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;-
  brm(data = small_data_wide,
      post ~ 1,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: post ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.02      0.12     0.79     1.25 1.00     6972     5683
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.17      0.08     1.02     1.36 1.00     8412     6599
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might compare those parameters with their sample values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(mean = mean(post),
            sd = sd(post))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##    mean    sd
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1  1.02  1.16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you only care about computing the population estimates for &lt;span class=&#34;math inline&#34;&gt;\(\mu_\text{post}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{post}\)&lt;/span&gt;, this model does a great job. With no other variables in the model, this approach does a poor job telling us about growth processes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_2-colon-the-simple-autoregressive-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_2 \colon\)&lt;/span&gt; The simple autoregressive model.&lt;/h4&gt;
&lt;p&gt;The simple model with the &lt;code&gt;pre&lt;/code&gt; scores predicting &lt;code&gt;post&lt;/code&gt; is a substantial improvement from the previous one. It follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{post}_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 \text{pre}_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected value of &lt;code&gt;post&lt;/code&gt; when &lt;code&gt;pre&lt;/code&gt; is at zero. As with many other regression contexts, centering the predictor &lt;code&gt;pre&lt;/code&gt; at the mean or some other meaningful value can help make &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; more interpretable. Of greater interest is the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficient, which is the expected deviation from &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; for a one-unit increase in &lt;code&gt;pre&lt;/code&gt;. But since &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; are really the same variable &lt;code&gt;y&lt;/code&gt; measured at two timepoints, it might be helpful if we express this model in another way. In perhaps more technical form, the simple model with &lt;code&gt;pre&lt;/code&gt; predicting &lt;code&gt;post&lt;/code&gt; is really an autoregressive model following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \phi y_{t - 1,i},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the criterion &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; varies across persons &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and timepoints &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Here we only have two timepoints, &lt;span class=&#34;math inline&#34;&gt;\(\text{post} = t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{pre} = t - 1\)&lt;/span&gt;. The strength of association between &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{t - 1}\)&lt;/span&gt; captured by the autoregressive parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, which is often expressed in a correlation metric.&lt;/p&gt;
&lt;p&gt;Here‚Äôs how to fit the model with &lt;code&gt;brm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m2 &amp;lt;-
  brm(data = small_data_wide,
      post ~ 1 + pre,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: post ~ 1 + pre 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.05      0.12     0.83     1.28 1.00     8995     6957
## pre           0.24      0.10     0.04     0.43 1.00     9548     6404
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.15      0.08     1.00     1.32 1.00     8407     7109
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let‚Äôs compare the &lt;code&gt;pre&lt;/code&gt; coefficient with the Pearson‚Äôs correlation between &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(correlation = cor(pre, post))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   correlation
##         &amp;lt;dbl&amp;gt;
## 1       0.232&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well look at that. Recall that the &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; parameter is the expected value in &lt;code&gt;post&lt;/code&gt; when the predictor &lt;code&gt;pre&lt;/code&gt; is at zero. Though the sample mean for &lt;code&gt;pre&lt;/code&gt; is very close to zero, it‚Äôs not exactly so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(pre_mean = mean(pre))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   pre_mean
##      &amp;lt;dbl&amp;gt;
## 1   -0.154&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here‚Äôs how to use that information to predict the mean value for &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m2)[1, 1] + fixef(m2)[2, 1] * mean(small_data_wide$pre)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.015851&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get a full posterior summary with aid from &lt;code&gt;fitted()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nd &amp;lt;- tibble(pre = mean(small_data_wide$pre))

fitted(m2, newdata = nd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Estimate Est.Error      Q2.5    Q97.5
## [1,] 1.015851 0.1142147 0.7939083 1.241721&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_3-colon-the-bivariate-autoregressive-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_3 \colon\)&lt;/span&gt; The bivariate autoregressive model.&lt;/h4&gt;
&lt;p&gt;Though the simple autoregressive model gives us a sense of the strength of association between &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt;‚Äìand thus a sense of the stability in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; over time‚Äì, it still lacks an explicit parameter for mean value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(t - 1\)&lt;/span&gt;. Enter the bivariate autoregressive model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{post}_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\text{pre}_i &amp;amp; \sim \operatorname{Normal}(\nu, \tau) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 \text{pre}_i \\
\nu   &amp;amp; = \gamma_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{post}_i\)&lt;/span&gt; is still modeled as a simple linear function of &lt;span class=&#34;math inline&#34;&gt;\(\text{pre}_i\)&lt;/span&gt;, but now we also include an unconditional model for &lt;span class=&#34;math inline&#34;&gt;\(\text{pre}_i\)&lt;/span&gt;. This will give us an explicit comparison for where we started at the outset (&lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;) and where we ended up (&lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;). We can fit this model using the &lt;strong&gt;brms&lt;/strong&gt; multivariate syntax where the two submodels are encased in &lt;code&gt;bf()&lt;/code&gt; statements &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-B√ºrkner2021Multivariate&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2021b&lt;/a&gt;)&lt;/span&gt;. Also, be careful to use &lt;code&gt;set_rescor(rescor = FALSE)&lt;/code&gt; to omit a residual correlation between the two. Their association is already handled with the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m3 &amp;lt;-
  brm(data = small_data_wide,
      bf(post ~ 1 + pre) +
        bf(pre ~ 1) +
        set_rescor(rescor = FALSE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: post ~ 1 + pre 
##          pre ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## post_Intercept     1.05      0.12     0.82     1.28 1.00    10717     7470
## pre_Intercept     -0.15      0.11    -0.38     0.07 1.00    12822     7706
## post_pre           0.24      0.10     0.04     0.43 1.00    11030     7641
## 
## Family Specific Parameters: 
##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_post     1.15      0.08     1.00     1.33 1.00    10955     7694
## sigma_pre      1.16      0.08     1.01     1.33 1.00    13273     7703
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we‚Äôve broken out the multivariate syntax, we might consider a second bivariate model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_4-colon-the-bivariate-correlational-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_4 \colon\)&lt;/span&gt; The bivariate correlational model.&lt;/h4&gt;
&lt;p&gt;The bivariate correlational model follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\begin{bmatrix} \text{post}_i \\ \text{pre}_i \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} \mu \\ \nu \end{bmatrix}, \mathbf \Sigma \right) \\
\mu &amp;amp; = \beta_0 \\
\nu &amp;amp; = \gamma_0 \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma &amp;amp; 0 \\ 0 &amp;amp; \tau \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where means of both &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; are modeled in intercept-only models. However, the association between the two timepoints is captured in the residual correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. Yet because we have no predictors in for either variable, the ‚Äúresidual‚Äù correlation is really just a correlation. We might also gain some insights if we re-express this model in terms of &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{t - 1,i}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\begin{bmatrix} y_{ti} \\ y_{t - 1,i} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} \mu_t \\ \mu_{t - 1} \end{bmatrix}, \mathbf \Sigma \right) \\
\mu_t &amp;amp; = \beta_t \\
\mu_{t - 1} &amp;amp; = \beta_{t - 1} \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_t &amp;amp; 0 \\ 0 &amp;amp; \sigma_{t - 1} \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where what we formerly called an autoregressive coefficient in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_2\)&lt;/span&gt;, we‚Äôre now calling a correlation. Note also that this model freely estimates &lt;span class=&#34;math inline&#34;&gt;\(\sigma_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{t - 1}\)&lt;/span&gt;. In some contexts, these are presumed to be equal. Though we won‚Äôt be imposing that constraint, here, I believe it is possible with the &lt;strong&gt;brms&lt;/strong&gt; &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&#34;&gt;non-linear syntax&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-B√ºrkner2021Non_linear&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2021c&lt;/a&gt;)&lt;/span&gt;. Anyway, here‚Äôs how to fit the model with the &lt;code&gt;brm()&lt;/code&gt; function .&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m4 &amp;lt;-
  brm(data = small_data_wide,
      bf(post ~ 1) +
        bf(pre ~ 1) +
        set_rescor(rescor = TRUE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note out use of the &lt;code&gt;set_rescor(rescor = TRUE)&lt;/code&gt; syntax in the model &lt;code&gt;formula&lt;/code&gt;. This explicitly told &lt;code&gt;brm()&lt;/code&gt; to include the residual correlation. Here‚Äôs the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: post ~ 1 
##          pre ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## post_Intercept     1.02      0.12     0.79     1.24 1.00    10992     8033
## pre_Intercept     -0.15      0.12    -0.39     0.08 1.00    10679     8012
## 
## Family Specific Parameters: 
##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_post     1.18      0.08     1.03     1.36 1.00    11342     7819
## sigma_pre      1.16      0.08     1.02     1.34 1.00    11150     7777
## 
## Residual Correlations: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(post,pre)     0.23      0.09     0.03     0.40 1.00    10074     7292
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the intercept and sigma parameters do a good job capturing the sample statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  pivot_longer(pre:post) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  summarise(mean = mean(value),
            sd = sd(value)) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   name   mean    sd
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 post   1.02  1.16
## 2 pre   -0.15  1.14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new ‚Äòrescor‚Äô line at the bottom of the &lt;code&gt;print()&lt;/code&gt; summary approximates the Pearson‚Äôs correlation of the two variables, much like the autoregressive parameter did two models up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(correlation = cor(pre, post))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   correlation
##         &amp;lt;dbl&amp;gt;
## 1       0.232&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another nice quality of this model is if you subtract &lt;span class=&#34;math inline&#34;&gt;\(\gamma_0\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\beta_t - \beta_{t - 1}\)&lt;/span&gt;), you‚Äôd end up with the posterior mean of the change score.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m4) %&amp;gt;% 
  mutate(change = b_post_Intercept - b_pre_Intercept) %&amp;gt;% 
  summarise(mu_change = mean(change))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mu_change
## 1  1.168898&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keeping that in mind, let‚Äôs switch gears to the first of the change-score models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;change-score-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Change-score models.&lt;/h3&gt;
&lt;p&gt;Instead of modeling &lt;code&gt;post&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt;, we might instead want to focus on the change from &lt;code&gt;pre&lt;/code&gt; to &lt;code&gt;post&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y_{ti} - y_{t - 1,i}\)&lt;/span&gt;. When you subtract &lt;code&gt;pre&lt;/code&gt; from &lt;code&gt;post&lt;/code&gt; in your data set‚Äìlike we did to make the &lt;code&gt;change&lt;/code&gt; variable‚Äì, the product is often referred to as a change score or difference score, &lt;span class=&#34;math inline&#34;&gt;\(y_\Delta\)&lt;/span&gt;. Though they‚Äôre conceptually intuitive and simple to compute, change scores have a long history of criticisms in the methodological literature, particularly around issues of reliability &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-lordStatisticalTheoriesMental1968&#34; role=&#34;doc-biblioref&#34;&gt;Lord &amp;amp; Novick, 1968&lt;/a&gt;; &lt;a href=&#34;#ref-rogosaGrowthCurveApproach1982&#34; role=&#34;doc-biblioref&#34;&gt;Rogosa et al., 1982&lt;/a&gt;; cf. &lt;a href=&#34;#ref-kisbu2013monte&#34; role=&#34;doc-biblioref&#34;&gt;Kisbu-Sakarya et al., 2013&lt;/a&gt;)&lt;/span&gt;. Here we consider four change-score models simply as options.&lt;/p&gt;
&lt;div id=&#34;mathcal-m_5-colon-the-unconditional-change-score-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_5 \colon\)&lt;/span&gt; The unconditional change-score model.&lt;/h4&gt;
&lt;p&gt;We‚Äôve already saved that in our &lt;code&gt;small_data_wide&lt;/code&gt; data as &lt;code&gt;change&lt;/code&gt;. Here‚Äôs what the unconditional change-score model&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; looks like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{change}_i &amp;amp; \sim \operatorname{Normal}(\mu, \sigma) \\
\mu &amp;amp; = \beta_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected value for &lt;span class=&#34;math inline&#34;&gt;\(\text{change}_i\)&lt;/span&gt;. In the terms of the last model, &lt;span class=&#34;math inline&#34;&gt;\(\text{change}_i = \text{post}_i - \text{pre}_i\)&lt;/span&gt; or, in the terms of the simple autoregressive model, &lt;span class=&#34;math inline&#34;&gt;\(y_{\Delta i} = y_{ti} - y_{t - 1,i}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m5 &amp;lt;-
  brm(data = small_data_wide,
      change ~ 1,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: change ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.17      0.14     0.89     1.45 1.00     8075     6240
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.44      0.10     1.26     1.66 1.00     8673     6829
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, this model suggests the average change from &lt;code&gt;pre&lt;/code&gt; to &lt;code&gt;post&lt;/code&gt; was about 1.2 units. We can compute the sample statistics for that in two ways.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(change = mean(change),
            `post - pre` = mean(post - pre))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   change `post - pre`
##    &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1   1.17         1.17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The major deficit in the unconditional change model is that change is disconnected from any reference points. We have no explicit way of knowing what number we changed from or what number we changed to. The next model offers a solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_6-colon-the-conditional-change-score-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_6 \colon\)&lt;/span&gt; The conditional change-score model.&lt;/h4&gt;
&lt;p&gt;Instead of fitting an unconditional model of &lt;code&gt;change&lt;/code&gt;, why not condition on the initial &lt;code&gt;pre&lt;/code&gt; value? We might express this as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{change}_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 \text{pre}_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is now the expected value for &lt;code&gt;change&lt;/code&gt; when &lt;code&gt;pre&lt;/code&gt; is at zero. As with the simple autoregressive model, centering the predictor &lt;code&gt;pre&lt;/code&gt; at the mean or some other meaningful value can help make &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; more interpretable. Perhaps of greater interest, the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficient allows us to predict different levels of &lt;code&gt;change&lt;/code&gt;, conditional in the initial values at &lt;code&gt;pre&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m6 &amp;lt;-
  brm(data = small_data_wide,
      change ~ 1 + pre,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: change ~ 1 + pre 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.05      0.12     0.82     1.28 1.00     9315     7108
## pre          -0.76      0.10    -0.96    -0.57 1.00    10559     7804
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.15      0.08     1.00     1.33 1.00     9745     7268
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the intercept in this model is the expected &lt;code&gt;change&lt;/code&gt; value based on when &lt;code&gt;pre == 0&lt;/code&gt;, it might be easiest to interpret that value when using the mean of &lt;code&gt;pre&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m6)[1, 1] + fixef(m6)[2, 1] * mean(small_data_wide$pre)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.167458&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is the point prediction for the mean of &lt;code&gt;change&lt;/code&gt;. Let‚Äôs compare that to the sample value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(mean = mean(change))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##    mean
##   &amp;lt;dbl&amp;gt;
## 1  1.17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, we can get a fuller summary using the &lt;code&gt;fitted()&lt;/code&gt; method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nd &amp;lt;- tibble(pre = mean(small_data_wide$pre))

fitted(m6, newdata = nd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Estimate Est.Error      Q2.5   Q97.5
## [1,] 1.167458 0.1151598 0.9428394 1.39075&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how the coefficient for &lt;code&gt;pre&lt;/code&gt; is about -0.76. This is a rough analogue of the negative correlation among the intercepts and slopes in the original data-generating model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rho&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It tells us something very similar; participants with higher values at &lt;code&gt;pre&lt;/code&gt; tended to have lower &lt;code&gt;change&lt;/code&gt; values. Much like with the simple autoregressive model, a deficit of this model is there is no explicit parameter for the expected value of &lt;code&gt;pre&lt;/code&gt;, which we can amend by fitting a bivariate model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_7-colon-the-bivariate-conditional-change-score-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_7 \colon\)&lt;/span&gt; The bivariate conditional change-score model.&lt;/h4&gt;
&lt;p&gt;The bivariate conditional change-score model follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{change}_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\text{pre}_i &amp;amp; \sim \operatorname{Normal}(\nu, \tau) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 \text{pre}_i \\
\nu   &amp;amp; = \gamma_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the simple linear model of &lt;span class=&#34;math inline&#34;&gt;\(\text{change}_i\)&lt;/span&gt; conditional on &lt;span class=&#34;math inline&#34;&gt;\(\text{pre}_i\)&lt;/span&gt; is coupled with an unconditional intercept-only model for &lt;span class=&#34;math inline&#34;&gt;\(\text{pre}_i\)&lt;/span&gt;. We can fit this model with &lt;strong&gt;brms&lt;/strong&gt; by way of the multivariate syntax, where the two submodels are encased in &lt;code&gt;bf()&lt;/code&gt; statements and we set &lt;code&gt;set_rescor(rescor = FALSE)&lt;/code&gt; to omit a residual correlation between the two.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m7 &amp;lt;-
  brm(data = small_data_wide,
      bf(change ~ 1 + pre) +
        bf(pre ~ 1) +
        set_rescor(rescor = FALSE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: change ~ 1 + pre 
##          pre ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## change_Intercept     1.05      0.12     0.82     1.28 1.00    11908     7575
## pre_Intercept       -0.16      0.12    -0.39     0.07 1.00    13113     8030
## change_pre          -0.76      0.10    -0.96    -0.57 1.00    12023     7190
## 
## Family Specific Parameters: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_change     1.15      0.08     1.00     1.33 1.00    13064     7696
## sigma_pre        1.16      0.08     1.01     1.33 1.00    11419     7142
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have an intercept for &lt;code&gt;pre&lt;/code&gt;, we can use the model parameters to compute the expected values for &lt;code&gt;pre&lt;/code&gt;, &lt;code&gt;change&lt;/code&gt;, and &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m7) %&amp;gt;% 
  mutate(pre    = b_pre_Intercept,
         change = b_change_Intercept + b_change_pre * b_pre_Intercept) %&amp;gt;% 
  mutate(post = pre + change) %&amp;gt;% 
  pivot_longer(pre:post) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  mean_qi(value) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 7
##   name   value .lower .upper .width .point .interval
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
## 1 change  1.17   0.88   1.45   0.95 mean   qi       
## 2 post    1.01   0.78   1.25   0.95 mean   qi       
## 3 pre    -0.16  -0.39   0.07   0.95 mean   qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The posterior means are in the &lt;code&gt;value&lt;/code&gt; column and the lower- and upper-levels of the percentile-based 95% intervals are in the &lt;code&gt;.lower&lt;/code&gt; and &lt;code&gt;.upper&lt;/code&gt; columns. Now compare those mean estimates with the sample means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  pivot_longer(-id) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  summarise(mean = mean(value)) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   name    mean
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 change  1.17
## 2 post    1.02
## 3 pre    -0.15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As handy as this model is, the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficient might not be in the most intuitive metric. Let‚Äôs reparameterize.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_8-colon-the-bivariate-correlational-prechange-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_8 \colon\)&lt;/span&gt; The bivariate correlational pre/change model.&lt;/h4&gt;
&lt;p&gt;The bivariate correlational pre/change model follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\begin{bmatrix} \text{change}_i \\ \text{pre}_i \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} \mu \\ \nu \end{bmatrix}, \mathbf \Sigma \right) \\
\mu &amp;amp; = \beta_0 \\
\nu &amp;amp; = \gamma_0 \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma &amp;amp; 0 \\ 0 &amp;amp; \tau \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where means of both &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;change&lt;/code&gt; are modeled in intercept-only models and the association between the two is captured by the correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. And again, because we have no predictors in for either variable, the ‚Äúresidual‚Äù correlation is really just a correlation. Here‚Äôs how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m8 &amp;lt;-
  brm(data = small_data_wide,
      bf(change ~ 1) +
        bf(pre ~ 1) +
        set_rescor(rescor = TRUE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: change ~ 1 
##          pre ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## change_Intercept     1.17      0.15     0.88     1.45 1.00     8200     7551
## pre_Intercept       -0.15      0.12    -0.38     0.08 1.00     8160     7194
## 
## Family Specific Parameters: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_change     1.44      0.10     1.26     1.67 1.00     8811     7147
## sigma_pre        1.16      0.08     1.01     1.33 1.00     8996     7487
## 
## Residual Correlations: 
##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(change,pre)    -0.60      0.06    -0.71    -0.46 1.00     8299     7233
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the parameter in the ‚ÄòResidual Correlations‚Äô section of the summary output is a close analogue to our original data-generating &lt;code&gt;rho&lt;/code&gt; parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rho&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those with higher &lt;code&gt;pre&lt;/code&gt; values tended to have lower &lt;code&gt;change&lt;/code&gt; values. We can look at that with a plot of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2 &amp;lt;-
  small_data_wide %&amp;gt;% 
  ggplot(aes(x = pre, y = change)) +
  geom_point() +
  stat_ellipse(color = &amp;quot;grey50&amp;quot;)

(p1 + p2) &amp;amp;
  coord_cartesian(xlim = range(small_data_wide$pre),
                  ylim = range(small_data_wide$change))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we‚Äôve placed the scatter plot of the data-generating &lt;code&gt;id&lt;/code&gt;-level &lt;code&gt;intercepts&lt;/code&gt; and &lt;code&gt;slopes&lt;/code&gt; next to the scatter plot of the &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;change&lt;/code&gt; scores. They are not exactly the same, but the latter are a partial consequence of the former. This is why the correlation parameter in our &lt;code&gt;m8&lt;/code&gt; model closely, but not exactly, resembled the data-generating &lt;code&gt;rho&lt;/code&gt; parameter.&lt;/p&gt;
&lt;p&gt;Okay, let‚Äôs switch gears again.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;models-modeling-the-criterion-y_ti-directly.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models modeling the criterion &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; directly.&lt;/h3&gt;
&lt;p&gt;All of the models focusing on &lt;code&gt;post&lt;/code&gt; or &lt;code&gt;change&lt;/code&gt; used the wide version of the data, &lt;code&gt;small_data_wide&lt;/code&gt;. The remaining models will all take advantage of the long data set, &lt;code&gt;small_data_long&lt;/code&gt;, and take &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; as the criterion. Consequently, most of these models will use some version of the multilevel model.&lt;/p&gt;
&lt;div id=&#34;mathcal-m_9-colon-the-grand-mean-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_9 \colon\)&lt;/span&gt; The grand-mean model.&lt;/h4&gt;
&lt;p&gt;The simplest model we might fit using the long version of the 2-timepoint data, &lt;code&gt;small_data_long&lt;/code&gt;, is what we might call the grand-mean model, or what &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; called the &lt;em&gt;between-person empty model&lt;/em&gt;. It follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu, \sigma) \\
\mu &amp;amp; = \beta_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected value value for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; across all &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; participants and &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; timepoints. We fit this with &lt;code&gt;brm()&lt;/code&gt; much like we fit the unconditional post model and the unconditional change-score model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m9 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 ,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.43      0.09     0.25     0.61 1.00     8486     6236
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.30      0.07     1.18     1.43 1.00     9455     6868
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might check these with the sample statistics for &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_long %&amp;gt;% 
  summarise(mean = mean(y),
            sd = sd(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##    mean    sd
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 0.431  1.29&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though this model did to a good job describing the population values for the mean and standard deviation for &lt;code&gt;y&lt;/code&gt;, it did a terrible job telling us about change in &lt;code&gt;y&lt;/code&gt;, about individual differences in that change, or about anything else of interest we might have as longitudinal researchers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_10-colon-the-random-intercept-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{10} \colon\)&lt;/span&gt; The random-intercept model.&lt;/h4&gt;
&lt;p&gt;Now that we have a grand mean, we might want to ask what kinds of variables would help explain the variation around the grand mean. From a multilevel perspective, the first source of variation of interest will be across participants, which we can express by allowing the mean to vary by participant. This is what Hoffman called the &lt;em&gt;within-person empty model&lt;/em&gt; and the &lt;em&gt;empty means, random intercept model&lt;/em&gt;. It follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  &amp;amp; = \beta_0 + u_{\text{id},i} \\
u_\text{id} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{id}) \\
\sigma &amp;amp; = \sigma_\epsilon,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is now the grand mean among the participant-level means in &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt;. The participant-level deviations from the grand mean are expressed as &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{id},i}\)&lt;/span&gt;, which is normally distributed with a mean at zero (these are &lt;em&gt;deviations&lt;/em&gt;, after all) and a standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{id}\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt; parameter is a mixture of the variation within participants and over time. With &lt;strong&gt;brms&lt;/strong&gt;, we can fit the random-intercept model model like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m10 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + (1 | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .99))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + (1 | id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.23      0.15     0.01     0.56 1.00     2079     4188
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.43      0.09     0.24     0.61 1.00    13669     6976
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.27      0.07     1.14     1.41 1.00     6211     5523
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; intercept returned a very similar estimate for the grand mean, but we now interpret it as the grand mean for the within-person means for &lt;code&gt;y&lt;/code&gt;. The variation in the &lt;code&gt;id&lt;/code&gt;-level deviations around the grand mean, which we called &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{id},i}\)&lt;/span&gt;, is summarized by &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{id}\)&lt;/span&gt; in the ‚ÄòGroup-Level Effects‚Äô section of the summary output.&lt;/p&gt;
&lt;p&gt;Authors of many longitudinal text books &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman, 2015&lt;/a&gt;; &lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett, 2003&lt;/a&gt;)&lt;/span&gt; typically present this model as a way to directly compare the between- and within-person variation in the data by way of the intraclass correlation coefficient (ICC),&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{ICC} = \frac{\text{between-person variance}}{\text{total variance}} = \frac{\sigma_\text{id}^2}{\sigma_\text{id}^2 + \sigma_\epsilon^2}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When using frequentist methods, the ICC is typically expressed with a point estimate. When working with all our posterior draws, we can get full posterior distributions within the Bayesian framework.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m10) %&amp;gt;% 
  mutate(icc = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&amp;gt;% 
  ggplot(aes(x = icc, y = 0)) +
  stat_halfeye(.width = .95) +
  scale_x_continuous(&amp;quot;Intraclass correlation coefficient (ICC)&amp;quot;, 
                     breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The ICC is a proportion, which limits it to the range of zero to one. Here it suggests that 0‚Äì20% of the variation in our data is due to differences &lt;em&gt;between&lt;/em&gt; participants; the remaining variation occurs within them. Given that our data were collected across time, it might make sense to fit a model that explicitly accounts for time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_11-colon-the-cross-classified-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{11} \colon\)&lt;/span&gt; The cross-classified model.&lt;/h4&gt;
&lt;p&gt;A direct extension of the random-intercept model is the cross-classified multilevel model &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2015&lt;/a&gt;, Chapter 12)&lt;/span&gt;, which we might express as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti}   &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} &amp;amp; = \beta_0 + u_{\text{id},i} + u_{\text{time},i} \\
u_\text{id} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{id}) \\
u_\text{time} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{time}) \\
\sigma &amp;amp; = \sigma_\epsilon,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{id}\)&lt;/span&gt; captures systemic differences between participants, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{time}\)&lt;/span&gt; captures systemic variation across the two timepoints, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt; captures the variation within participants over time. Another way to think of this model is as a Bayesian multilevel version of the repeated-measures ANOVA&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, where variance is partitioned into a between level (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{id} \approx \text{SS}_\text{between}\)&lt;/span&gt;), a model level (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{time} \approx \text{SS}_\text{model}\)&lt;/span&gt;), and error (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon \approx \text{SS}_\text{error}\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m11 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + (1 | time) + (1 | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .9999,
                     max_treedepth = 11))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m11)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + (1 | time) + (1 | id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.51      0.16     0.14     0.78 1.00     1519     1545
## 
## ~time (Number of levels: 2) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.67      1.30     0.41     5.00 1.00     5240     7037
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.46      1.07    -1.78     2.81 1.00     4541     4935
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.04      0.08     0.90     1.20 1.00     2313     3750
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The intercept is the grand mean across all measures of &lt;code&gt;y&lt;/code&gt;. The first row in the ‚ÄòGroup-Level Effects‚Äô section is our summary for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{id}\)&lt;/span&gt;, which gives us a sense of the variation between participants in their overall tendencies in the criterion &lt;code&gt;y&lt;/code&gt;. If we use the &lt;code&gt;posterior_samples()&lt;/code&gt;, we can even look at the posteriors for the &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{id},i}\)&lt;/span&gt; parameters, themselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m11) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_id&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = value, y = reorder(name, value))) +
  stat_pointinterval(point_interval = mean_qi, .width = .95, size = 1/6) +
  scale_y_discrete(expression(italic(i)), breaks = NULL) +
  labs(subtitle = expression(sigma[id]~is~the~summary~of~the~variation~across~these),
       x = expression(italic(u)[id][&amp;#39;,&amp;#39;*italic(i)]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;336&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given that each of the &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{id},i}\)&lt;/span&gt; parameters is based primarily on two data points (the two data points per participant), it should be no surprise they are fairly wide. Even a few more measurement occasions within participants will narrow them substantially. If you fit the same model using the original 6-timepoint data, you‚Äôll see the 95% intervals are almost half as wide.&lt;/p&gt;
&lt;p&gt;Perhaps of greater interest are the &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{time},i}\)&lt;/span&gt; parameters. If you combine them with the intercept, you‚Äôll get the model-based expected values at both timepoints.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m11) %&amp;gt;% 
  transmute(pre  = b_Intercept + `r_time[0,Intercept]`,
            post = b_Intercept + `r_time[1,Intercept]`) %&amp;gt;% 
  pivot_longer(everything()) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  mean_qi(value) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   name  value .lower .upper .width .point .interval
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
## 1 post   1.01   0.78   1.24   0.95 mean   qi       
## 2 pre   -0.15  -0.38   0.09   0.95 mean   qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final variance parameter, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{time}\)&lt;/span&gt;, captures the within-participant variation over time. With a model like this, it seems natural to directly compare the magnitudes of the three variance parameters, which answers the question: &lt;em&gt;Where‚Äôs the variance at&lt;/em&gt;? Here we‚Äôll do so with a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m11) %&amp;gt;% 
  select(sd_id__Intercept:sigma) %&amp;gt;% 
  set_names(&amp;quot;sigma[id]&amp;quot;, &amp;quot;sigma[time]&amp;quot;, &amp;quot;sigma[epsilon]&amp;quot;) %&amp;gt;% 
  pivot_longer(everything()) %&amp;gt;% 
  mutate(name = factor(name, levels = c(&amp;quot;sigma[epsilon]&amp;quot;, &amp;quot;sigma[time]&amp;quot;, &amp;quot;sigma[id]&amp;quot;))) %&amp;gt;%
  
  ggplot(aes(x = value, y = name)) +
  tidybayes::stat_halfeye(.width = .95, size = 1, normalize = &amp;quot;xy&amp;quot;) +
  scale_x_continuous(&amp;quot;marginal posterior&amp;quot;, expand = expansion(mult = c(0, 0.05)), breaks = c(0, 1, 2, 5)) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(xlim = c(0, 5.25),
                  ylim = c(1.5, 3.5)) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-48-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At first glance, it might be surprising how wide the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{time}\)&lt;/span&gt; posterior is compared to the other two. Yet recall this parameter is summarizing the standard deviation of only two levels. If you have experience with multilevel models, you‚Äôll know that it can be difficult to estimate a variance parameter with few levels‚Äìtwo levels is the extreme lower limit. This is why we had to fiddle with the &lt;code&gt;adapt_delta&lt;/code&gt; and &lt;code&gt;max_treedepth&lt;/code&gt; parameters within the &lt;code&gt;brm()&lt;/code&gt; function to get the model to sample properly. Though we pulled this off using default priors, don‚Äôt be surprised if you have to use tighter priors when fitting a model like this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_12-colon-the-simple-liner-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{12} \colon\)&lt;/span&gt; The simple liner model.&lt;/h4&gt;
&lt;p&gt;The last three models focused on the grand mean and sources of variance around that grand mean. A more familiar looking approach might be to fit a simple linear model with &lt;code&gt;y&lt;/code&gt; conditional on &lt;code&gt;time&lt;/code&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected value at the first timepoint and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; captures the change in &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; for the final timepoint.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m12 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + time,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.15      0.12    -0.38     0.08 1.00     9459     7344
## time          1.17      0.16     0.85     1.49 1.00     9582     7294
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.16      0.06     1.05     1.28 1.00     9982     7362
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now our &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; parameters are the direct pre/post single-level analogues to the population-level &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; parameters from our original multilevel model based on the full 6-timepoint data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m0) %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Estimate Est.Error  Q2.5 Q97.5
## Intercept    -0.13      0.11 -0.33  0.08
## time          1.16      0.13  0.91  1.42&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The major deficit in this model, which is the reason you‚Äôll see it criticized in the methodological literature, is it ignores how the &lt;code&gt;y&lt;/code&gt; values are nested within levels of &lt;code&gt;id&lt;/code&gt;. The only variance parameter, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, was estimated under the typical assumption that the residuals are all independent of one another. Sure, the model formula accounted for the overall trend in &lt;code&gt;time&lt;/code&gt;, but it ignored the insights revealed from many of the other models the capture between-participant correlations in intercepts and slopes. This means that if you know something about the value of one‚Äôs residual for when &lt;code&gt;time == 0&lt;/code&gt;, you‚Äôll also know something about where to expect their residual for when &lt;code&gt;time == 1&lt;/code&gt;. The two are not independent. As long as we‚Äôre working with the data in the long format, we‚Äôll want to account for this, somehow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_13-colon-the-liner-model-with-a-random-intercept.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{13} \colon\)&lt;/span&gt; The liner model with a random intercept.&lt;/h4&gt;
&lt;p&gt;A natural first step to accounting for how the &lt;code&gt;y&lt;/code&gt; values are nested within levels of &lt;code&gt;id&lt;/code&gt; is to fit a random-intercept model, or what &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; called the &lt;em&gt;fixed linear time, random intercept model&lt;/em&gt;. It follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} &amp;amp; = \beta_{0i} + \beta_1 \text{time}_{ti} \\
\beta_{0i} &amp;amp; = \gamma_0 + u_{0i} \\
u_{0i}  &amp;amp; \sim \operatorname{Normal}(0, \sigma_0) \\
\sigma &amp;amp; = \sigma_\epsilon,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0i}\)&lt;/span&gt; is the intercept and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the time slope. Although this parameterization holds the variation in slopes constant across participants, between-participant variation is at least captured in &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0i}\)&lt;/span&gt;, which is decomposed into a grand mean, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_0\)&lt;/span&gt;, and participant-level deviations around that grand mean, &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt;. Those participant-level deviations are summarized by the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt; parameter. In this model, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt; is a mixture of within-participant variation and between-participant variation in slopes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m13 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + time + (1 | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m13)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time + (1 | id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.52      0.15     0.14     0.78 1.00     1645     1960
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.15      0.12    -0.38     0.08 1.00     9441     7715
## time          1.17      0.15     0.88     1.46 1.00    14821     6993
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.03      0.08     0.90     1.19 1.00     2403     4360
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters are very similar to those from the simple linear model, above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m12) %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Estimate Est.Error  Q2.5 Q97.5
## Intercept    -0.15      0.12 -0.38  0.08
## time          1.17      0.16  0.85  1.49&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But now look at the size of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt;, which suggests substantial differences in staring points. To get a sense of what this means, we‚Äôll plot all 100 participant-level trajectories with a little help from &lt;code&gt;fitted()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nd &amp;lt;- distinct(small_data_long, id, time)

fitted(m13, 
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m13)[1, 1],
              slope = fixef(m13)[2, 1],
              size = 3, color = &amp;quot;blue&amp;quot;) +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = &amp;quot;Random intercepts, fixed slope&amp;quot;,
       y = &amp;quot;y&amp;quot;) +
  coord_cartesian(ylim = c(-1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-53-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The bold blue line in the middle is based on the population-level intercept and slope, whereas the thinner black lines are the participant-level trajectories. To keep from &lt;a href=&#34;https://www.data-to-viz.com/caveat/overplotting.html&#34;&gt;overplotting&lt;/a&gt;, we‚Äôre only showing the posterior means, here. Because we only allowed the intercept to vary across participants, all the slopes are identical. And indeed, look at all the variation we see in the intercepts‚Äìan insight lacking in the simple linear model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_14-colon-the-liner-model-with-a-random-slope.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{14} \colon\)&lt;/span&gt; The liner model with a random slope.&lt;/h4&gt;
&lt;p&gt;The counterpoint to the last model is to allow the time slopes, but not the intercepts, vary across participants:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_{1i} \text{time}_{ti} \\
\beta_{1i} &amp;amp; = \gamma_1 + u_{1i} \\
u_{1i} &amp;amp; \sim \operatorname{Normal}(0, \sigma_1) \\
\sigma   &amp;amp; = \sigma_\epsilon,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the intercept for all participants. Now &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1i}\)&lt;/span&gt; is the population mean for the distribution of slopes, which vary across participants, the standard deviation for which is measured by &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m14 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + time + (0 + time | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .95))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m14)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time + (0 + time | id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(time)     0.33      0.21     0.02     0.75 1.00     2240     4641
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.15      0.11    -0.38     0.08 1.00    20513     7195
## time          1.17      0.16     0.85     1.49 1.00    18079     7133
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.13      0.06     1.01     1.26 1.00     6825     6149
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here‚Äôs random-slopes alternative to the random-intercepts plot from the last section.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(m14, 
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m14)[1, 1],
              slope = fixef(m14)[2, 1],
              size = 3, color = &amp;quot;blue&amp;quot;) +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = &amp;quot;Fixed intercept, random slopes&amp;quot;,
       y = &amp;quot;y&amp;quot;) +
  coord_cartesian(ylim = c(-1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-55-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But why choose between random intercepts or random slopes?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_15-colon-the-multilevel-growth-model-with-regularizing-priors.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{15} \colon\)&lt;/span&gt; The multilevel growth model with regularizing priors.&lt;/h4&gt;
&lt;p&gt;If you were modeling 2-timepoint data with conventional frequentist estimators (e.g., maximum likelihood), you can have random intercepts or random slopes, but you can‚Äôt have both; that would require data from three timepoints or more. But because Bayesian models bring in extra information by way of the priors, you can actually fit a full multilevel growth model with both random intercepts and slopes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti}   &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma_\epsilon ) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} + u_{0i} + u_{1i} \text{time}_{ti} \\
\begin{bmatrix} u_{0i} \\ u_{1i} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf \Sigma \right) \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_1 \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix}.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The trick is you have to go beyond the diffuse &lt;strong&gt;brms&lt;/strong&gt; default settings for the priors for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt;. If you have high-quality information from theory or previous studies, you can base the priors on those. Another approach is to use regularizing priors. Given standardized data, members of the Stan team like either &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}^+(0, 1)\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Student-t}^+(3, 0, 1)\)&lt;/span&gt; for variance parameters (see the &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#generic-prior-for-anything&#34;&gt;Generic prior for anything&lt;/a&gt; section from the &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;&lt;em&gt;Prior choice recommendations&lt;/em&gt; wiki&lt;/a&gt;). In the second edition of his text, McElreath generally favored the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Exponential}(1)\)&lt;/span&gt; prior &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020&lt;/a&gt;)&lt;/span&gt;, which is the approach we‚Äôll experiment with, here. It‚Äôll also help if we use a regularizing prior on the correlation among the intercepts and slopes, &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m15 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + time + (1 + time | id),
      seed = 1,
      prior = prior(exponential(1), class = sd) +
        prior(lkj(4), class = cor),
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .9995))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even with our tighter priors, we still had to adjust the &lt;code&gt;adapt_delta&lt;/code&gt; parameter to improve the quality of the MCMC sampling. Take a look at the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time + (1 + time | id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)           0.52      0.18     0.12     0.84 1.01     1012     1473
## sd(time)                0.31      0.23     0.01     0.88 1.00     1021      876
## cor(Intercept,time)    -0.05      0.33    -0.63     0.59 1.00     4039     6272
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.15      0.11    -0.38     0.07 1.00    10022     8348
## time          1.17      0.15     0.88     1.46 1.00    13870     7175
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.01      0.10     0.80     1.18 1.00     1060      946
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though the posteriors, particularly for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; parameters, are not as precise as with the 6-timepoint data, we now have a model with a summary mirroring the structure of the data-generating model. Yet compared to the data-generating values, the estimates for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; are particularly biased toward zero. Here‚Äôs a look at the trajectories.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(m15,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m15)[1, 1],
              slope = fixef(m15)[2, 1],
              size = 3, color = &amp;quot;blue&amp;quot;) +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = &amp;quot;Random intercepts AND random slopes\n(2-timepoint data)&amp;quot;,
       y = &amp;quot;y&amp;quot;) +
  coord_cartesian(ylim = c(-1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-57-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For comparision, here‚Äôs the plot for the original 6-timepoint model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(m0,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m0)[1, 1],
              slope = fixef(m0)[2, 1],
              size = 3, color = &amp;quot;blue&amp;quot;) +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = &amp;quot;Random intercepts AND random slopes\n(6-timepoint data)&amp;quot;,
       y = &amp;quot;y&amp;quot;) +
  coord_cartesian(ylim = c(-1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-58-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There wasn‚Äôt enough information in the 2-timepoint data set to capture the complexity in the full 6-timepoint data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_16-colon-the-fixed-effects-with-correlated-error-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{16} \colon\)&lt;/span&gt; The fixed effects with correlated error model.&lt;/h4&gt;
&lt;p&gt;Though our Bayesian 2-timepoint version of the full multilevel growth model was exciting, it‚Äôs not generally used in the wild. Even with our tighter regularizing priors, there just wasn‚Äôt enough information in the data to do the model justice. A very different and humbler approach is to combine the simple linear model with the autoregressive model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \mathbf \Sigma) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma &amp;amp; 0 \\ 0 &amp;amp; \sigma \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; captures the correlation between the responses in the two timepoints, &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t - 1\)&lt;/span&gt;, which is an alternative to the way the mixed model from above handles the dependencies (a.k.a &lt;a href=&#34;https://en.wikipedia.org/wiki/Heteroscedasticity&#34;&gt;heteroskedasticity&lt;/a&gt;) inherent in longitudinal data. Note how &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; in this model is defined very differently from the full multilevel growth model from above. To fit this model with &lt;strong&gt;brms&lt;/strong&gt;, we use the &lt;code&gt;ar()&lt;/code&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m16 &amp;lt;-
  brm(data = small_data_long,
      y ~ time + ar(time = time, p = 1, gr = id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m16)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ time + ar(time = time, p = 1, gr = id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Correlation Structures:
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## ar[1]     0.23      0.10     0.04     0.43 1.00     9766     7458
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.15      0.12    -0.38     0.07 1.00    10593     7136
## time          1.17      0.15     0.89     1.46 1.00     9607     6546
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.15      0.06     1.04     1.27 1.00    10604     7665
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This summary suggests that, after you account for the linear trend, the correlation between &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{t - 1,i}\)&lt;/span&gt; is about 0.23. Though we don‚Äôt get &lt;code&gt;id&lt;/code&gt;-specific variance parameters, this model does account for the nonindependence of the data over time. If you scroll back up, notice how similar this is to the correlation from the bivariate correlational model, &lt;code&gt;m4&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-models-with-robust-variance-parameters.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The models with robust variance parameters.&lt;/h3&gt;
&lt;p&gt;In the social sciences, many of our theories and statistical models are comparisons of or changes in group means. Every model in this blog post uses the normal likelihood, which parameterizes the criterion in terms of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Every time we added some kind of linear model, we focused that model around the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. But contemporary Bayesian software allows us to model the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameter, too. Within the &lt;strong&gt;brms&lt;/strong&gt; framework, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-B√ºrkner2021Distributional&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner&lt;/a&gt; (&lt;a href=&#34;#ref-B√ºrkner2021Distributional&#34; role=&#34;doc-biblioref&#34;&gt;2021a&lt;/a&gt;)&lt;/span&gt; calls these &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&#34;&gt;distributional models&lt;/a&gt;. The final four models under consideration all use some form of the distributional modeling syntax to relax unnecessarily restrictive assumptions on the variance parameters. Though this section is not exhaustive, it should give a sense of how flexible this approach can be.&lt;/p&gt;
&lt;div id=&#34;mathcal-m_17-colon-the-cross-classified-model-with-robust-variances-for-discrete-time.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{17} \colon\)&lt;/span&gt; The cross-classified model with robust variances for discrete time.&lt;/h4&gt;
&lt;p&gt;One of the criticisms of the conventional repeated measures ANOVA approach is how it presumes the variances in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are constant over time. However, we can relax that constraint with a model like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} &amp;amp; = \beta_0 + u_{\text{id},i} + u_{\text{time},t} \\
\log (\sigma_{ti}) &amp;amp; = \eta_{\text{time},t} \\
u_\text{id} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{id}) \\
u_\text{time} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{time}),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we are now modeling both parameters in the likelihood, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; AND &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. The second line shows a typical-looking model for &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ti}\)&lt;/span&gt;. All the excitement lies in the third line, which contains the linear model for &lt;span class=&#34;math inline&#34;&gt;\(\log (\sigma_{ti})\)&lt;/span&gt;. The reason we are modeling &lt;span class=&#34;math inline&#34;&gt;\(\log (\sigma_{ti})\)&lt;/span&gt; rather than directly modeling &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is to avoid solutions that predict negative values for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. For this model, it‚Äôs unlikely we‚Äôd run into that problem. But since the &lt;strong&gt;brms&lt;/strong&gt; default is to use the log link anytime we model &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; within the distributional modeling syntax, we‚Äôll just get used to the log link right from the start. If you are unfamiliar with link functions, they‚Äôre widely used within the generalized linear modeling framework. Logistic regression with the logit link and Poisson regression with the log link are two widely-used examples. For more on link functions and the generalized linear model, check out the texts by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-agrestiFoundationsLinearGeneralized2015&#34; role=&#34;doc-biblioref&#34;&gt;Agresti&lt;/a&gt; (&lt;a href=&#34;#ref-agrestiFoundationsLinearGeneralized2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Gelman, Hill, and Vehtari &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;; and McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Anyway, the linear model for &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ti}\)&lt;/span&gt; is exactly the same as with the original cross-classified model, &lt;code&gt;m11&lt;/code&gt;; it includes a grand mean (&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;) and two kinds of deviations around that grand mean (&lt;span class=&#34;math inline&#34;&gt;\(u_{\text{id},i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{time},t}\)&lt;/span&gt;). The model for &lt;span class=&#34;math inline&#34;&gt;\(\log (\sigma_{ti})\)&lt;/span&gt; contains an intercept, which varies across the two levels of time, &lt;span class=&#34;math inline&#34;&gt;\(\eta_{\text{time},t}\)&lt;/span&gt;. Here‚Äôs how to fit the model with &lt;code&gt;brms::brm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m17 &amp;lt;-
  brm(data = small_data_long,
      bf(y ~ 1 + (1 | time) + (1 | id),
         sigma ~ 0 + factor(time)),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .999,
                     max_treedepth = 12))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m17)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: y ~ 1 + (1 | time) + (1 | id) 
##          sigma ~ 0 + factor(time)
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.53      0.15     0.15     0.79 1.00     1376     1194
## 
## ~time (Number of levels: 2) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.66      1.26     0.41     5.00 1.00     5133     6854
## 
## Population-Level Effects: 
##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept             0.45      1.07    -1.78     2.66 1.00     4653     5056
## sigma_factortime0     0.01      0.10    -0.18     0.20 1.00     3068     5577
## sigma_factortime1     0.03      0.10    -0.17     0.22 1.00     2678     4488
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even though we didn‚Äôt explicitly ask to use the log link in our &lt;code&gt;brm()&lt;/code&gt; syntax, you can look at the second line in the &lt;code&gt;print()&lt;/code&gt; output to see that it was automatically used. Though I won‚Äôt explore how to do so, here, one can fit this model without the log link. Anyway, the primary focus in this model is the &lt;code&gt;sigma_factortime0&lt;/code&gt; and &lt;code&gt;sigma_factortime1&lt;/code&gt; lines in the ‚ÄòPopulation-Level Effects‚Äô section of the output. Those are the summaries for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters, conditional on whether &lt;code&gt;time == 0&lt;/code&gt; or &lt;code&gt;time == 1&lt;/code&gt;. Though is might be difficult to evaluate parameters on the log scale, we can simply exponentiate them to convert them back to their natural metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m17)[2:3, c(1, 3:4)] %&amp;gt;% exp()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   Estimate      Q2.5    Q97.5
## sigma_factortime0 1.013928 0.8352872 1.223336
## sigma_factortime1 1.032290 0.8446723 1.247435&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, it looks like the two parameters are largely overlapping. If we work directly with the posterior draws, we can compute a formal difference score and plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m17) %&amp;gt;% 
  mutate(`sigma[time==0]` = exp(b_sigma_factortime0),
         `sigma[time==1]` = exp(b_sigma_factortime1),
         `sigma[time==1]-sigma[time==0]` = exp(b_sigma_factortime1) - exp(b_sigma_factortime0)) %&amp;gt;% 
  pivot_longer(`sigma[time==0]`:`sigma[time==1]-sigma[time==0]`) %&amp;gt;% 
  
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye(.width = .95) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(ylim = c(1.5, 3.1)) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-62-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, it looks like there was little difference between &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{time} = 0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{time} = 1}\)&lt;/span&gt;. This shouldn‚Äôt be a surprise; we simulated that data that way. However, it won‚Äôt always be like this in real-world data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_18-colon-the-simple-liner-model-with-robust-variance-for-linear-time.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{18} \colon\)&lt;/span&gt; The simple liner model with robust variance for linear time.&lt;/h4&gt;
&lt;p&gt;Our cross-classified approach treated &lt;code&gt;time&lt;/code&gt; as a factor. Here we‚Äôll treat it as a continuous variable in the models of both &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. This will be a straight extension of the simple linear model, &lt;code&gt;m12&lt;/code&gt;, following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} \\
\log(\sigma_{ti}) &amp;amp; = \eta_0 + \eta_1 \text{time}_{ti},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected value at the first timepoint and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; captures the change in &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; for the final timepoint. Now &lt;span class=&#34;math inline&#34;&gt;\(\log(\sigma_{ti})\)&lt;/span&gt; has a similar linear model, where &lt;span class=&#34;math inline&#34;&gt;\(\eta_0\)&lt;/span&gt; is the expected log of the standard deviation at the first timepoint and &lt;span class=&#34;math inline&#34;&gt;\(\eta_1\)&lt;/span&gt; captures the change standard deviation for the final timepoint.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m18 &amp;lt;-
  brm(data = small_data_long,
      bf(y ~ 1 + time, 
         sigma ~ 1 + time),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m18)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: y ~ 1 + time 
##          sigma ~ 1 + time
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept          -0.15      0.11    -0.38     0.06 1.00    11620     6673
## sigma_Intercept     0.14      0.07     0.00     0.28 1.00    11090     7442
## time                1.17      0.16     0.85     1.49 1.00    11137     7818
## sigma_time          0.01      0.10    -0.18     0.21 1.00    11157     7620
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even though this model looks very different from the last one, we can wrangle the posterior draws a little to make a similar plot comparing &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; at the two timepoints.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m18) %&amp;gt;% 
  mutate(`sigma[time==0]` = exp(b_sigma_Intercept),
         `sigma[time==1]` = exp(b_sigma_Intercept + b_sigma_time * 1),
         `sigma[time==1]-sigma[time==0]` = exp(b_sigma_Intercept) - exp(b_sigma_Intercept + b_sigma_time)) %&amp;gt;% 
  pivot_longer(`sigma[time==0]`:`sigma[time==1]-sigma[time==0]`) %&amp;gt;% 
  
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye(.width = .95) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(ylim = c(1.5, 3.1)) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-64-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though this model is robust to differences in &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; based on timepoint, it still ignores systemic differences across participants. The next model tackles that that limitation in spades.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_19-colon-the-liner-model-with-correlated-random-intercepts-for-mu-and-sigma.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{19} \colon\)&lt;/span&gt; The liner model with correlated random intercepts for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma.\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Here we return to the multilevel model framework to accommodate participant-level differences for both &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} + u_{0i}  \\
\log(\sigma_{ti}) &amp;amp; = \eta_0  + u_{2i} \\
\begin{bmatrix} u_{0i} \\ u_{2i} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf \Sigma \right) \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_2 \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the grand mean for the intercepts and &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; are the participant-level deviations around that grand mean. &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the time slope, which is invariant across all participants in this model. The &lt;span class=&#34;math inline&#34;&gt;\(\eta_0\)&lt;/span&gt; parameter is the grand mean for the log standard deviations and &lt;span class=&#34;math inline&#34;&gt;\(u_{2i}\)&lt;/span&gt; captures the participant-level deviations around that grand mean. In the fourth line, we learn that &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{2i}\)&lt;/span&gt; are multivariate normal, with a mean vector of two zeros and a variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt;. As is typical within the &lt;strong&gt;brms&lt;/strong&gt; framework, we decompose &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; into a variance matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S\)&lt;/span&gt; and correlation matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R\)&lt;/span&gt;. Of particular interest is the &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; parameter, which captures the correlation in the participant-level intercepts and participant-level standard deviations. Here‚Äôs how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m19 &amp;lt;-
  brm(data = small_data_long,
      bf(y ~ 1 + time + (1 |x| id),
         sigma ~ 1 + (1 |x| id)),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .9))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may have noticed the &lt;code&gt;|x|&lt;/code&gt; parts in the &lt;code&gt;formula&lt;/code&gt; lines for &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt;. What that did was tell &lt;strong&gt;brms&lt;/strong&gt; we wanted those parameters to be correlated. That is, that‚Äôs how we estimated the &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; parameter. There was nothing special about including &lt;code&gt;x&lt;/code&gt; between the vertical lines. We could have used any other character. The important thing is that we used the same character in both. Anyway, here‚Äôs the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m19)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: y ~ 1 + time + (1 | x | id) 
##          sigma ~ 1 + (1 | x | id)
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)                      0.52      0.16     0.14     0.78 1.00     1496     1443
## sd(sigma_Intercept)                0.11      0.08     0.01     0.29 1.00     3032     4589
## cor(Intercept,sigma_Intercept)    -0.34      0.49    -0.97     0.80 1.00     5949     6097
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept          -0.15      0.12    -0.38     0.08 1.00    10102     7923
## sigma_Intercept     0.02      0.08    -0.13     0.17 1.00     2344     3931
## time                1.17      0.15     0.88     1.45 1.00    14868     8009
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once again, &lt;strong&gt;brms&lt;/strong&gt; used the log link for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. If you want to see &lt;span class=&#34;math inline&#34;&gt;\(\eta_0\)&lt;/span&gt; in its natural &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; metric, exponentiate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m19)[&amp;quot;sigma_Intercept&amp;quot;, c(1, 3:4)] %&amp;gt;% exp()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate      Q2.5     Q97.5 
## 1.0171275 0.8759026 1.1849572&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;sd(sigma_Intercept)&lt;/code&gt; row in the ‚ÄòGroup-Level Effects‚Äô section shows the variation in those &lt;span class=&#34;math inline&#34;&gt;\(\log \sigma\)&lt;/span&gt;‚Äôs. It might be easier to appreciate them in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m19) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_id__sigma&amp;quot;)) %&amp;gt;% 
  mutate(sigma_i = exp(sd_id__sigma_Intercept + value)) %&amp;gt;% 
  
  ggplot(aes(x = sigma_i, y = reorder(name, sigma_i))) +
  stat_pointinterval(point_interval = mean_qi, .width = .95, size = 1/6) +
  scale_y_discrete(expression(italic(i)), breaks = NULL) +
  labs(subtitle = expression(sigma[2]~summarizes~the~variation~across~these),
       x = expression(sigma[italic(i)]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-67-1.png&#34; width=&#34;336&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, there was not a lot of variation in &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; across participants. This is because we simulated the data that way. Though it may be hard to model participant-level variances with 2-timepoint data, I have found it comes in handy in real-world data sets based on more measurement occasions.&lt;/p&gt;
&lt;p&gt;Finally, it might be useful to consider our &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; parameter, which suggested a mild negative correlation between the &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{2i}\)&lt;/span&gt; deviations. Here‚Äôs how you might visualize that in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  posterior_samples(m19) %&amp;gt;% select(starts_with(&amp;quot;r_id[&amp;quot;)) %&amp;gt;% set_names(1:100),
  posterior_samples(m19) %&amp;gt;% select(starts_with(&amp;quot;r_id__sigma&amp;quot;) %&amp;gt;% set_names(1:100))
) %&amp;gt;% 
  mutate(iter = rep(1:c(n() / 2), times = 2),
         type = rep(c(&amp;quot;intercept&amp;quot;, &amp;quot;log_sigma&amp;quot;), each = n() / 2)) %&amp;gt;% 
  pivot_longer(-c(iter, type)) %&amp;gt;% 
  pivot_wider(names_from = type, values_from = value) %&amp;gt;% 
  
  ggplot(aes(x = intercept, y = log_sigma, group = name)) +
  stat_ellipse(geom = &amp;quot;polygon&amp;quot;, level = .01, alpha = 1/4) +
  labs(x = expression(italic(u)[0][italic(i)]),
       y = expression(log(italic(u)[2][italic(i)])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-68-1.png&#34; width=&#34;336&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each of the ovals is a 1% ellipse of the bivariate posterior for &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\log(u_{2i})\)&lt;/span&gt;. Notice how using ellipses helps reveal the differences in the between- and within-person patterns.&lt;/p&gt;
&lt;p&gt;This approach where residual variance parameters vary across participants has its origins in the work of &lt;a href=&#34;https://health.uchicago.edu/faculty/donald-hedeker-phd&#34;&gt;Donald Hedeker&lt;/a&gt; and colleagues &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hedekerApplicationMixedeffectsLocation2008&#34; role=&#34;doc-biblioref&#34;&gt;Hedeker et al., 2008&lt;/a&gt;, &lt;a href=&#34;#ref-hedekerModelingWithinsubjectVariance2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;. More recently, &lt;a href=&#34;https://twitter.com/rastlab&#34;&gt;Philippe Rast&lt;/a&gt; and colleagues (particularly graduate student, &lt;a href=&#34;wdonald_1985&#34;&gt;Donald Williams&lt;/a&gt;) have adapted this approach for use within the Stan/&lt;strong&gt;brms&lt;/strong&gt; ecosystem &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-williamsBayesianMultivariateMixedeffects2019a&#34; role=&#34;doc-biblioref&#34;&gt;Williams, Liu, et al., 2019&lt;/a&gt;; &lt;a href=&#34;#ref-williamsSurfaceUnearthingWithinperson2019&#34; role=&#34;doc-biblioref&#34;&gt;Williams, Rouder, et al., 2019&lt;/a&gt;; &lt;a href=&#34;#ref-williamsPuttingIndividualReliability2019&#34; role=&#34;doc-biblioref&#34;&gt;Williams, Martin, et al., 2019&lt;/a&gt;; &lt;a href=&#34;#ref-williamsBayesianNonlinearMixedeffects2019a&#34; role=&#34;doc-biblioref&#34;&gt;Williams, Zimprich, et al., 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_20-colon-the-liner-model-with-a-random-slope-for-mu-and-uncorrelated-random-intercept-for-sigma.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{20} \colon\)&lt;/span&gt; The liner model with a random slope for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and uncorrelated random intercept for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/h4&gt;
&lt;p&gt;Though we can find interesting things when we allow the random components in the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; models, we don‚Äôt have to think of them as covarying. Here we fit an extension of the linear model with a random time slope, where we add an orthogonal random intercept for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} + u_{1i} \\
\log(\sigma_{ti}) &amp;amp; = \eta_0  + u_{2i}  \\
u_{1i} &amp;amp; \sim \operatorname{Normal}(0, \sigma_1) \\
u_{2i} &amp;amp; \sim \operatorname{Normal}(0, \sigma_2),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the two random components, &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{2i}\)&lt;/span&gt;, are now modeled with separate normal distributions, &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, \sigma_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, \sigma_2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m20 &amp;lt;-
  brm(data = small_data_long,
      bf(y ~ 1 + time + (0 + time | id),
         sigma ~ 1 + (1 | id)),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in sharp contrast with our syntax for the previous model, this time we did not employ the &lt;code&gt;|x|&lt;/code&gt; syntax in the &lt;code&gt;formula&lt;/code&gt; lines for &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt;. By omitting the &lt;code&gt;|x|&lt;/code&gt; syntax, we omitted the correlation among those two random effects. Here‚Äôs the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: y ~ 1 + time + (0 + time | id) 
##          sigma ~ 1 + (1 | id)
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(time)                0.33      0.21     0.02     0.75 1.00     2031     4346
## sd(sigma_Intercept)     0.12      0.09     0.00     0.32 1.00     2463     3519
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept          -0.15      0.11    -0.37     0.07 1.00    13451     7317
## sigma_Intercept     0.11      0.06    -0.02     0.23 1.00     4612     4412
## time                1.18      0.16     0.86     1.49 1.00    12600     7054
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2\)&lt;/span&gt; summaries in the ‚ÄòGroup-Level Effects‚Äô section. It‚Äôs hard to compare them directly, because one is based on parameters in the log metric. But we can at least get a sense of what these parameters are summarizing by plotting the bivariate posterior for &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\log(u_{2i})\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  posterior_samples(m20) %&amp;gt;% select(starts_with(&amp;quot;r_id[&amp;quot;)) %&amp;gt;% set_names(1:100),
  posterior_samples(m20) %&amp;gt;% select(starts_with(&amp;quot;r_id__sigma&amp;quot;) %&amp;gt;% set_names(1:100))
) %&amp;gt;% 
  mutate(iter = rep(1:c(n() / 2), times = 2),
         type = rep(c(&amp;quot;slope&amp;quot;, &amp;quot;log_sigma&amp;quot;), each = n() / 2)) %&amp;gt;% 
  pivot_longer(-c(iter, type)) %&amp;gt;% 
  pivot_wider(names_from = type, values_from = value) %&amp;gt;% 
  
  ggplot(aes(x = slope, y = log_sigma, group = name)) +
  stat_ellipse(geom = &amp;quot;polygon&amp;quot;, level = .01, alpha = 1/4) +
  labs(x = expression(italic(u)[1][italic(i)]),
       y = expression(log(italic(u)[2][italic(i)])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-70-1.png&#34; width=&#34;336&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how, this time, the 1% ellipses suggest no clear association between these two dimensions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;As promised, here I recommend some resources for understanding the models in this post.&lt;/p&gt;
&lt;div id=&#34;books-focusing-on-longutidinal-data-analysis.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Books focusing on longutidinal data analysis.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;My introduction to longitudinal data analysis was through Singer and Willett &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, &lt;a href=&#34;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;&lt;em&gt;Applied longitudinal data analysis: Modeling change and event occurrence&lt;/em&gt;&lt;/a&gt;. Their focus was on the multilevel growth model and on survival analysis, primary from a maximum-likelihood frequentist framework. However, they generally avoided 2-timepoint data analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hoffman‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text, &lt;a href=&#34;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&#34;&gt;&lt;em&gt;Longitudinal analysis: Modeling within-person fluctuation and change&lt;/em&gt;&lt;/a&gt; is another thorough introduction to the multilevel growth model, from a frequentist perspective. Hoffman covered 2-timepoint data analysis and variants from the ANOVA family. The text comes with a companion website, &lt;a href=&#34;https://www.pilesofvariance.com/&#34;&gt;https://www.pilesofvariance.com/&lt;/a&gt;, which contains extensive data and code files for reproducing the material.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Newsom‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-newsom2015longitudinal&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text, &lt;a href=&#34;http://www.longitudinalsem.com/&#34;&gt;&lt;em&gt;Longitudinal structural equation modeling: A comprehensive introduction&lt;/em&gt;&lt;/a&gt;, covers longitudinal data analysis from a structural equation modeling (SEM) perspective. Even for those not interested in SEM, his Chapter 4 does a nice job introducing the autoregressive and change-score models. The companion website, &lt;a href=&#34;http://www.longitudinalsem.com/&#34;&gt;http://www.longitudinalsem.com/&lt;/a&gt;, contains data and script files for most of the problems in the text.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;books-introducing-regression.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Books introducing regression.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Gelman, Hill, and Vehtari‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://www.cambridge.org/core/books/regression-and-other-stories/DD20DD6C9057118581076E54E40C372C&#34;&gt;&lt;em&gt;Regression and other stories&lt;/em&gt;&lt;/a&gt; contains a thorough introduction to single-level regression, primarily from a Bayesian framework. The text is not oriented around longitudinal analyses, per se, but it does include several chapters on causal inference. Vehtari hosts a GitHub repo, &lt;a href=&#34;https://github.com/avehtari/ROS-Examples&#34;&gt;https://github.com/avehtari/ROS-Examples&lt;/a&gt;, where you can download the data files and &lt;strong&gt;R&lt;/strong&gt; scripts for many of the examples.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Both editions of McElreath‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;&lt;em&gt;Statistical rethinking: A Bayesian course with examples in R and Stan&lt;/em&gt;&lt;/a&gt; provide a thorough introduction to Bayesian regression, both single-level and multilevel. McElreath also touched on causal inference and included a few examples of longitudinal data analysis. His text includes extensive examples of &lt;strong&gt;R&lt;/strong&gt; code and his website, &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;, contains information about the accompanying statistical software.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] patchwork_1.1.1 tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0  
##  [7] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3  
## [13] tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] splines_4.0.4        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [49] htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [53] ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3      loo_2.4.1           
##  [57] dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2       tidyselect_1.1.0    
##  [61] rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [65] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [69] broom_0.7.5          ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1       
##  [73] yaml_2.2.1           processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [77] nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [81] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [85] gamm4_0.2-6          curl_4.3             reprex_0.3.0         statmod_1.4.35      
##  [89] stringi_1.5.3        highr_0.8            ps_1.6.0             blogdown_1.3        
##  [93] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [97] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1        
## [101] lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [105] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [109] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [113] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [117] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [125] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-agrestiFoundationsLinearGeneralized2015&#34; class=&#34;csl-entry&#34;&gt;
Agresti, A. (2015). &lt;em&gt;Foundations of linear and generalized linear models&lt;/em&gt;. &lt;span&gt;John Wiley &amp;amp; Sons&lt;/span&gt;. &lt;a href=&#34;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&#34;&gt;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-B√ºrkner2021Distributional&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2021a). &lt;em&gt;Estimating distributional models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-B√ºrkner2021Multivariate&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2021b). &lt;em&gt;Estimating multivariate models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-B√ºrkner2021Non_linear&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2021c). &lt;em&gt;Estimating non-linear models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1‚Äì28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395‚Äì411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ‚Äô&lt;span&gt;Stan&lt;/span&gt;‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-debruineUnderstandingMixedEffects2020&#34; class=&#34;csl-entry&#34;&gt;
DeBruine, L. M., &amp;amp; Barr, D. J. (2020). &lt;em&gt;Understanding mixed effects models through data simulation&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1177/2515245920965119&#34;&gt;https://doi.org/10.1177/2515245920965119&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hedekerApplicationMixedeffectsLocation2008&#34; class=&#34;csl-entry&#34;&gt;
Hedeker, D., Mermelstein, R. J., &amp;amp; Demirtas, H. (2008). An application of a mixed-effects location scale model for analysis of ecological momentary assessment (&lt;span&gt;EMA&lt;/span&gt;) data. &lt;em&gt;Biometrics&lt;/em&gt;, &lt;em&gt;64&lt;/em&gt;(2), 627‚Äì634. &lt;a href=&#34;https://doi.org/10.1111/j.1541-0420.2007.00924.x&#34;&gt;https://doi.org/10.1111/j.1541-0420.2007.00924.x&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hedekerModelingWithinsubjectVariance2012&#34; class=&#34;csl-entry&#34;&gt;
Hedeker, D., Mermelstein, R. J., &amp;amp; Demirtas, H. (2012). Modeling between- and within-subject variance in ecological momentary assessment (&lt;span&gt;EMA&lt;/span&gt;) data using mixed-effects location scale models. &lt;em&gt;Statistics in Medicine&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(27). &lt;a href=&#34;https://doi.org/10.1002/sim.5338&#34;&gt;https://doi.org/10.1002/sim.5338&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hoffmanLongitudinalAnalysisModeling2015&#34; class=&#34;csl-entry&#34;&gt;
Hoffman, L. (2015). &lt;em&gt;Longitudinal analysis: &lt;span&gt;Modeling&lt;/span&gt; within-person fluctuation and change&lt;/em&gt; (1 edition). &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&#34;&gt;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ‚Äôgeoms‚Äô for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kisbu2013monte&#34; class=&#34;csl-entry&#34;&gt;
Kisbu-Sakarya, Y., MacKinnon, D. P., &amp;amp; Aiken, L. S. (2013). A &lt;span&gt;Monte Carlo&lt;/span&gt; comparison study of the power of the analysis of covariance, simple difference, and residual change scores in testing two-wave data. &lt;em&gt;Educational and Psychological Measurement&lt;/em&gt;, &lt;em&gt;73&lt;/em&gt;(1), 47‚Äì62. &lt;a href=&#34;https://doi.org/10.1177/0013164412450574&#34;&gt;https://doi.org/10.1177/0013164412450574&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020). &lt;em&gt;Statistical rethinking with brms, Ggplot2, and the tidyverse: &lt;span&gt;Second&lt;/span&gt; edition&lt;/em&gt; (version 0.1.1). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lordStatisticalTheoriesMental1968&#34; class=&#34;csl-entry&#34;&gt;
Lord, F. M., &amp;amp; Novick, M. R. (1968). &lt;em&gt;Statistical theories of mental test scores&lt;/em&gt;. &lt;span&gt;Addison-Wesley&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-newsom2015longitudinal&#34; class=&#34;csl-entry&#34;&gt;
Newsom, J. T. (2015). &lt;em&gt;Longitudinal structural equation modeling: &lt;span&gt;A&lt;/span&gt; comprehensive introduction&lt;/em&gt;. &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;http://www.longitudinalsem.com/&#34;&gt;http://www.longitudinalsem.com/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-patchwork&#34; class=&#34;csl-entry&#34;&gt;
Pedersen, T. L. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;patchwork&lt;/span&gt;: &lt;span&gt;The&lt;/span&gt; composer of plots&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;https://CRAN.R-project.org/package=patchwork&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rogosaGrowthCurveApproach1982&#34; class=&#34;csl-entry&#34;&gt;
Rogosa, D., Brandt, D., &amp;amp; Zimowski, M. (1982). A growth curve approach to the measurement of change. &lt;em&gt;Psychological Bulletin&lt;/em&gt;, &lt;em&gt;92&lt;/em&gt;(3), 726‚Äì748. &lt;a href=&#34;https://doi.org/10.1037/0033-2909.92.3.726&#34;&gt;https://doi.org/10.1037/0033-2909.92.3.726&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-singerAppliedLongitudinalData2003&#34; class=&#34;csl-entry&#34;&gt;
Singer, J. D., &amp;amp; Willett, J. B. (2003). &lt;em&gt;Applied longitudinal data analysis: &lt;span&gt;Modeling&lt;/span&gt; change and event occurrence&lt;/em&gt;. &lt;span&gt;Oxford University Press, USA&lt;/span&gt;. &lt;a href=&#34;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-walkerElementsOfStatisticalModeling2018&#34; class=&#34;csl-entry&#34;&gt;
Walker, J. A. (2018). &lt;em&gt;Elements of statistical modeling for experimental biology&lt;/em&gt; (&#34;2020‚Äì11th‚Äì22&#34; ed.). &lt;a href=&#34;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/&#34;&gt;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ‚Äôtidyverse‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., Fran√ßois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., M√ºller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., ‚Ä¶ Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsBayesianMultivariateMixedeffects2019a&#34; class=&#34;csl-entry&#34;&gt;
Williams, D. R., Liu, S., Martin, S. R., &amp;amp; Rast, P. (2019). &lt;em&gt;Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/4kfjp&#34;&gt;https://doi.org/10.31234/osf.io/4kfjp&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsPuttingIndividualReliability2019&#34; class=&#34;csl-entry&#34;&gt;
Williams, D. R., Martin, S. R., &amp;amp; Rast, P. (2019). &lt;em&gt;Putting the individual into reliability: &lt;span&gt;Bayesian&lt;/span&gt; testing of homogeneous within-person variance in hierarchical models&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/hpq7w&#34;&gt;https://doi.org/10.31234/osf.io/hpq7w&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsSurfaceUnearthingWithinperson2019&#34; class=&#34;csl-entry&#34;&gt;
Williams, D. R., Rouder, J., &amp;amp; Rast, P. (2019). &lt;em&gt;Beneath the surface: &lt;span&gt;Unearthing&lt;/span&gt; within-&lt;span&gt;Person&lt;/span&gt; variability and mean relations with &lt;span&gt;Bayesian&lt;/span&gt; mixed models&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/gwatq&#34;&gt;https://doi.org/10.31234/osf.io/gwatq&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsBayesianNonlinearMixedeffects2019a&#34; class=&#34;csl-entry&#34;&gt;
Williams, D. R., Zimprich, D. R., &amp;amp; Rast, P. (2019). A &lt;span&gt;Bayesian&lt;/span&gt; nonlinear mixed-effects location scale model for learning. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(5), 1968‚Äì1986. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01255-9&#34;&gt;https://doi.org/10.3758/s13428-019-01255-9&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Throughout this post, my statistical notation will be a blend of sensibilities from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett&lt;/a&gt; (&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;, and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Though I‚Äôm no fan of the null-hypothesis significance testing paradigm, it might be helpful to point out if one were to focus on whether zero is within the 95% interval bounds of our &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; parameter, you be viewing this model through the lens of the repeated-measures &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test. For more on that connection, see Chapter 3 in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-newsom2015longitudinal&#34; role=&#34;doc-biblioref&#34;&gt;Newsom&lt;/a&gt; (&lt;a href=&#34;#ref-newsom2015longitudinal&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;There‚Äôs some debate over how to think about the repeated measures ANOVA and what its closest multilevel analogue might be. For a nice collection of perspectives, check out &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1342645143082594304&#34;&gt;this Twitter thread&lt;/a&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Time-varying covariates in longitudinal multilevel models contain state- and trait-level information: This includes binary variables, too</title>
      <link>/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/</guid>
      <description>
&lt;script src=&#34;/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;When you have a time-varying covariate you‚Äôd like to add to a multilevel growth model, it‚Äôs important to break that variable into two. One part of the variable will account for within-person variation. The other part will account for between person variation. Keep reading to learn how you might do so when your time-varying covariate is binary.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;i-assume-things.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I assume things.&lt;/h2&gt;
&lt;p&gt;For this post, I‚Äôm presuming you are familiar with longitudinal multilevel models and vaguely familiar with the basic differences between frequentist and Bayesian statistics. All code in is &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;&lt;strong&gt;R&lt;/strong&gt;&lt;/a&gt;, with a heavy use of the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt;‚Äìwhich you might learn a lot about &lt;a href=&#34;http://r4ds.had.co.nz&#34;&gt;here&lt;/a&gt;, especially &lt;a href=&#34;http://r4ds.had.co.nz/transform.html&#34;&gt;chapter 5&lt;/a&gt;‚Äì, and the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; package&lt;/a&gt; for Bayesian regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;context&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Context&lt;/h2&gt;
&lt;p&gt;In my applied work, one of my collaborators collects longitudinal behavioral data. They are in the habit of analyzing their focal dependent variables (DVs) with variants of the longitudinal multilevel model, which is great. Though they often collect their primary independent variables (IVs) at all time points, they typically default to only using the baseline values for their IVs to predict the random intercepts and slopes of the focal DVs.&lt;/p&gt;
&lt;p&gt;It seems like we‚Äôre making inefficient use of the data. At first I figured we‚Äôd just use the IVs at all time points, which would be treating them as time-varying covariates. But time varying covariates don‚Äôt allow one to predict variation in the random intercepts and slopes, which I and my collaborator would like to do. So while using the IVs at all time points as time-varying covariates makes use of more of the available data, it requires us to trade one substantive focus for another, which seems frustrating.&lt;/p&gt;
&lt;p&gt;After low-key chewing on this for a while, I recalled that it‚Äôs possible to decompose time-varying covariates into measures of traits and states. Consider the simple case where your time-varying covariate, &lt;span class=&#34;math inline&#34;&gt;\(x_{ij}\)&lt;/span&gt; is continuous. In this notation, the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; values vary across persons &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and time points &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. If we compute the person level mean, &lt;span class=&#34;math inline&#34;&gt;\(\overline x_i\)&lt;/span&gt;, that would be a time-invariant covariate and would, conceptually, be a measure of a person‚Äôs trait level for &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Even if you do this, it‚Äôs still okay to include both &lt;span class=&#34;math inline&#34;&gt;\(\overline x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_{ij}\)&lt;/span&gt; in the model equation. The former would be the time-&lt;em&gt;invariant&lt;/em&gt; covariate that might predict the variation in the random intercepts and slopes. The latter would still serve as a time-&lt;em&gt;varying&lt;/em&gt; covariate that might account for the within-person variation in the DV over time.&lt;/p&gt;
&lt;p&gt;There, of course, are technicalities about how one might center &lt;span class=&#34;math inline&#34;&gt;\(\overline x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_{ij}\)&lt;/span&gt; that one should carefully consider for these kinds of models. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.928.9848&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Enders &amp;amp; Tofighi (2007)&lt;/a&gt; covered the issue from a cross-sectional perspective. &lt;a href=&#34;http://www.pilesofvariance.com/index.html&#34;&gt;Hoffman (2015)&lt;/a&gt; covered it from a longitudinal perspective. But in the grand scheme of things, those are small potatoes. The main deal is that I can use our IVs as both time-varying and time-invariant predictors.&lt;/p&gt;
&lt;p&gt;I was pretty excited once I remembered all this.&lt;/p&gt;
&lt;p&gt;But then I realized that some of my collaborator‚Äôs IVs are binary, which initially seemed baffling, to me. Would it be sensible to compute &lt;span class=&#34;math inline&#34;&gt;\(\overline x_i\)&lt;/span&gt; for a binary time-varying covariate? What would that mean for the time-varying version of the variable? So I did what any responsible postdoctoral researcher would do. I posed the issue on Twitter.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Context: multilevel growth models.&lt;br&gt;&lt;br&gt;When you have a continuous time-varying covariate, you can decompose it into two variables, an id-level grand mean and the time-specific deviations from that mean. Is anyone aware of a complimentary approach for binary time-varying covariates?&lt;/p&gt;&amp;mdash; Solomon Kurz (@SolomonKurz) &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1188185892332150789?ref_src=twsrc%5Etfw&#34;&gt;October 26, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;My initial thoughts on the topic were a little confused. I wasn‚Äôt differentiating well between issues about the variance decomposition and centering and I‚Äôm a little embarrassed over that gaff. But I‚Äôm still glad I posed the question to Twitter. My virtual colleagues came through in spades! In particular, I‚Äôd like to give major shout outs to Andrea Howard (&lt;a href=&#34;https://twitter.com/DrAndreaHoward&#34;&gt;@DrAndreaHoward&lt;/a&gt;), Mattan Ben-Shachar (&lt;a href=&#34;https://twitter.com/mattansb&#34;&gt;@mattansb&lt;/a&gt;), and Aidan Wright (&lt;a href=&#34;https://twitter.com/aidangcw&#34;&gt;@aidangcw&lt;/a&gt;), who collectively pointed me to the solution. It was detailed in the references I listed, above: Enders &amp;amp; Tofighi (2007) and Hoffman (2015). Thank you, all!&lt;/p&gt;
&lt;p&gt;Here‚Äôs the deal: Yes, you simply take the person-level means for the binary covariate &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. That will create a vector of time-invariant IVs ranging continuously from 0 to 1. They‚Äôll be in a probability metric and they conceptually index a person‚Äôs probability of endorsing 1 over time. It‚Äôs basically the same as a batting average in baseball. You are at liberty to leave the time-invariant covariate in this metric, or you could center it by standardizing or some other sensible transformation. As for the state version of the IV, &lt;span class=&#34;math inline&#34;&gt;\(x_{ij}\)&lt;/span&gt;, you‚Äôd just leave it exactly as it is. [There are other ways to code binary data, such as effects coding. I‚Äôm not a fan and will not be covering that in detail, here. But yes, you could recode your time-varying binary covariate that way, too.]&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;break-out-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Break out the data&lt;/h2&gt;
&lt;p&gt;We should practice this with some data. I‚Äôve been chipping away at working through Singer and Willett‚Äôs classic (2003) text, &lt;a href=&#34;https://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;&lt;em&gt;Applied longitudinal data analysis: Modeling change and event occurrence&lt;/em&gt;&lt;/a&gt; with &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt; code. You can find the working files in this &lt;a href=&#34;https://github.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse&#34;&gt;GitHub repository&lt;/a&gt;. In chapter 5, Singer and Willett worked through a series of examples with a data set with a continuous DV and a binary IV. Here are those data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

d &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse/master/data/unemployment_pp.csv&amp;quot;)

glimpse(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 674
## Columns: 4
## $ id     &amp;lt;dbl&amp;gt; 103, 103, 103, 641, 641, 641, 741, 846, 846, 846, 937, 937, 111‚Ä¶
## $ months &amp;lt;dbl&amp;gt; 1.149897, 5.946612, 12.911704, 0.788501, 4.862423, 11.827515, 1‚Ä¶
## $ cesd   &amp;lt;dbl&amp;gt; 25, 16, 33, 27, 7, 25, 40, 2, 22, 0, 3, 8, 3, 0, 5, 7, 18, 26, ‚Ä¶
## $ unemp  &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, ‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;set-the-stage-with-descriptive-plots.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set the stage with descriptive plots.&lt;/h3&gt;
&lt;p&gt;The focal DV is &lt;code&gt;cesd&lt;/code&gt;, a continuous variable measuring depression. Singer and Willett (2003):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Each time participants completed the Center for Epidemiologic Studies‚Äô Depression (CES-D) scale (&lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/014662167700100306?casa_token=igspo7W_9SUAAAAA%3AhnRVqiDEM-b6nNh_-8VQ6tx1PukP8nsqyo4yd4m_inspjhH-3aeShEGodUxux8GuInG9AYbP1D2GLA&amp;amp;journalCode=apma&#34;&gt;Radloff, 1977&lt;/a&gt;), which asks them to rate, on a four-point scale, the frequency with which they experience each of the 20 depressive symptoms. The CES-D scores can vary from a low or 0 for someone with no symptoms to a high of 80 for someone in serious distress. (p.¬†161)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here‚Äôs what the &lt;code&gt;cesd&lt;/code&gt; scores look like, collapsing over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_gray() +
            theme(panel.grid = element_blank()))

d %&amp;gt;% 
  ggplot(aes(x = cesd)) +
  geom_histogram(fill = &amp;quot;grey50&amp;quot;, binwidth = 1) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since these are longutdnial data, our fundamental IV is a measure of time. That‚Äôs captured in the &lt;code&gt;months&lt;/code&gt; column. Most participants have data on just three occasions and the &lt;code&gt;months&lt;/code&gt; values range from about 0 to 15.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  ggplot(aes(x = months)) +
  geom_histogram(fill = &amp;quot;grey50&amp;quot;, binwidth = 1) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The main research question we‚Äôll be addressing is: &lt;em&gt;What do participants‚Äô &lt;code&gt;cesd&lt;/code&gt; scores look like over time and to what extent does their employment/unemployment status help explain their depression?&lt;/em&gt; So our substantive IV of interest is &lt;code&gt;unemp&lt;/code&gt;, which is coded 0 = employed and 1 = unemployed. Since participants were recruited from local unemployment offices, everyone started off as &lt;code&gt;unemp == 1&lt;/code&gt;. The values varied after that. Here‚Äôs a look at the data from a random sample of 25 of the participants.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this makes `sample_n()` reproducible
set.seed(5)

# wrangle the data a little
d %&amp;gt;% 
  nest(data = c(months, cesd, unemp)) %&amp;gt;% 
  sample_n(size = 25) %&amp;gt;% 
  unnest(data) %&amp;gt;% 
  mutate(id = str_c(&amp;quot;id: &amp;quot;, id),
         e  = if_else(unemp == 0, &amp;quot;employed&amp;quot;, &amp;quot;unemployed&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = months, y = cesd)) +
  geom_line(aes(group = id),
            size = 1/4) +
  geom_point(aes(color = e),
             size = 7/4) +
  scale_color_manual(NULL, values = c(&amp;quot;blue3&amp;quot;, &amp;quot;red3&amp;quot;)) +
  theme(panel.grid      = element_blank(),
        legend.position = &amp;quot;top&amp;quot;) +
  facet_wrap(~id, nrow = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;embrace-the-hate.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Embrace the hate.&lt;/h3&gt;
&lt;p&gt;To be honest, I kinda hate these data. There are too few measurement occasions within participants for my liking and the assessment schedule just seems bazar. As we‚Äôll see in a bit, these data are also un-ideal to address exactly the kinds of models this blog is centered on.&lt;/p&gt;
&lt;p&gt;Yet it‚Äôs for just these reasons I love these data. Real-world data analysis is ugly. The data are never what you want or expected them to be. So it seems the data we use in our educational materials should be equally terrible.&lt;/p&gt;
&lt;p&gt;Much like we do for our most meaningful relationships, let‚Äôs embrace our hate/love ambivalence for our data with wide-open eyes and tender hearts. üñ§&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;time-to-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Time to model.&lt;/h3&gt;
&lt;p&gt;Following Singer and Willett, we can define our first model using a level-1/level-2 specification. The level-1 model would be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{cesd}_{ij} = \pi_{0i} + \pi_{1i} \text{months}_{ij} + \pi_{2i} \text{unemp}_{ij} + \epsilon_{ij},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\pi_{0i}\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(\pi_{1i}\)&lt;/span&gt; is the effect of &lt;code&gt;months&lt;/code&gt; on &lt;code&gt;cesd&lt;/code&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\pi_{2i}\)&lt;/span&gt; is the effect of &lt;code&gt;unemp&lt;/code&gt; on &lt;code&gt;cesd&lt;/code&gt;. The final term, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij}\)&lt;/span&gt;, is the within-person variation not accounted for by the model‚Äìsometimes called error or residual variance. Our &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij}\)&lt;/span&gt; term follows the usual distribution of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\epsilon_{ij} \sim \operatorname{Normal} (0, \sigma_\epsilon),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which, in words, means that the within-person variance estimates are normally distributed with a mean of zero and a standard deviation that‚Äôs estimated from the data. The corresponding level-2 model follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\pi_{0i} &amp;amp; = \gamma_{00} + \zeta_{0i} \\
\pi_{1i} &amp;amp; = \gamma_{10} + \zeta_{1i} \\
\pi_{2i} &amp;amp; = \gamma_{20},
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; is the grand mean for the intercept, which varies by person, as captured by the level-2 variance term &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0i}\)&lt;/span&gt;. Similarly, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{10}\)&lt;/span&gt; is the grand mean for the effect of &lt;code&gt;months&lt;/code&gt;, which varies by person, as captured by the second level-2 variance term &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{1i}\)&lt;/span&gt;. With this parameterization, it turns out &lt;span class=&#34;math inline&#34;&gt;\(\pi_{2i}\)&lt;/span&gt; does not vary by person and so its &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{20}\)&lt;/span&gt; terms does not get a corresponding level-2 variance coefficient. If we wanted the effects of the time-varying covariate &lt;code&gt;unemp&lt;/code&gt; to vary across individuals, we‚Äôd expand the definition of &lt;span class=&#34;math inline&#34;&gt;\(\pi_{2i}\)&lt;/span&gt; to be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\pi_{2i} = \gamma_{20} + \zeta_{2i}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Within our &lt;strong&gt;brms&lt;/strong&gt; paradigm, the two level-2 variance parameters follow the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\begin{bmatrix} 
\zeta_{0i} \\ \zeta_{1i} \\
\end{bmatrix} &amp;amp; \sim \operatorname{Normal} 
\left ( 
\begin{bmatrix} 0 \\ 0 \end{bmatrix},
\mathbf{D} \mathbf{\Omega} \mathbf{D}&amp;#39;
\right ), \text{where} \\

\mathbf{D}    &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_1 \end{bmatrix} \text{and} \\

\mathbf{\Omega}  &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho_{01} \\ \rho_{01} &amp;amp; 1 \end{bmatrix}.

\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I‚Äôll be using a weakly-regularizing approach for the model priors in this post. I detail how I came to these in the &lt;a href=&#34;https://github.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse/blob/master/05.md&#34;&gt;Chapter 5 file from my GitHub repo&lt;/a&gt;. If you check that file, you‚Äôll see this model is a simplified version of &lt;code&gt;fit10&lt;/code&gt;. Here are our priors:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\gamma_{00}     &amp;amp; \sim \operatorname{Normal}(14.5, 20) \\
\gamma_{10} \text{ and }  \gamma_{20}  &amp;amp; \sim \operatorname{Normal}(0, 10) \\
\sigma_\epsilon, \sigma_0,  \text{ and } \sigma_1 &amp;amp; \sim \operatorname{Student-t} (3, 0, 10) \\
\Omega          &amp;amp; \sim \operatorname{LKJ} (4).
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Feel free to explore different priors on your own. But now we‚Äôre done spelling our our first model, it‚Äôs time to fire up our main statistical package, &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can fit the model with &lt;code&gt;brms::brm()&lt;/code&gt;, like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d, 
      family = gaussian,
      cesd ~ 0 + intercept + months + unemp + (1 + months | id),
      prior = c(prior(normal(14.5, 20), class = b, coef = &amp;quot;intercept&amp;quot;),
                prior(normal(0, 10),    class = b),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(4), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = .95),
      seed = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we explore the results from this model, we should point out that we only included &lt;code&gt;unemp&lt;/code&gt; as a level-1 time-varying predictor. As Hoffman pointed out in her (2015) text, the flaw in this approach is that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;time-varying predictors contain both between-person and within-person information&lt;/em&gt;‚Ä¶&lt;/p&gt;
&lt;p&gt;[Thus,] time-varying predictors will need to be represented by two separate predictors that distinguish their between-person and within-person sources of variance in order to properly distinguish their potential between-person and within-person effects on a longitudinal outcome. (pp.¬†329, 333, &lt;em&gt;emphasis&lt;/em&gt; in the original)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The simplest way to separate the between-person variance in &lt;code&gt;unemp&lt;/code&gt; from the pure within-person variation is to compute a new variable capturing &lt;span class=&#34;math inline&#34;&gt;\(\overline{\text{unemp}}_i\)&lt;/span&gt;, the person-level means for their unemployment status. Here we compute that variable, which we‚Äôll call &lt;code&gt;unemp_id_mu&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;% 
  group_by(id) %&amp;gt;% 
  mutate(unemp_id_mu = mean(unemp)) %&amp;gt;% 
  ungroup()

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##      id months  cesd unemp unemp_id_mu
##   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1   103  1.15     25     1       1    
## 2   103  5.95     16     1       1    
## 3   103 12.9      33     1       1    
## 4   641  0.789    27     1       0.333
## 5   641  4.86      7     0       0.333
## 6   641 11.8      25     0       0.333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because &lt;code&gt;umemp&lt;/code&gt; is binary, &lt;span class=&#34;math inline&#34;&gt;\(\overline{\text{unemp}}_i\)&lt;/span&gt; can only take on values ranging from 0 to 1. Here are the unique values we have for &lt;code&gt;unemp_id_mu&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  distinct(unemp_id_mu)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 1
##   unemp_id_mu
##         &amp;lt;dbl&amp;gt;
## 1       1    
## 2       0.333
## 3       0.667
## 4       0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because each participant‚Äôs &lt;span class=&#34;math inline&#34;&gt;\(\overline{\text{unemp}}_i\)&lt;/span&gt; was based on 3 or fewer measurement occasions, basic algebra limited the variability in our &lt;code&gt;unemp_id_mu&lt;/code&gt; values. You‚Äôll also note that there were no 0s. This, recall, is because participants were recruited at local unemployment offices, leaving all participants with at least one starting value of &lt;code&gt;unemp == 1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We should rehearse how we might interpret the &lt;code&gt;unemp_id_mu&lt;/code&gt; values. First recall they are considered level-2 variables; they are between-participant variables. Since they are averages of binary data, they are in a probability metric. In this instance, they are each participants overall probability of being unemployed‚Äìtheir trait-level propensity toward unemployment. No doubt these values would be more reliable if they were computed from data on a greater number of assessment occasions. But with three measurement occasions, we at least have a sense of stability.&lt;/p&gt;
&lt;p&gt;Since our new &lt;span class=&#34;math inline&#34;&gt;\(\overline{\text{unemp}}_i\)&lt;/span&gt; variable is a level-2 predictor, the level-1 equation for our next model is the same as before:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{cesd}_{ij} = \pi_{0i} + \pi_{1i} \text{months}_{ij} + \pi_{2i} \text{unemp}_{ij} + \epsilon_{ij}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, there are two new terms in our level-2 model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\pi_{0i} &amp;amp; = \gamma_{00} + \gamma_{01} (\overline{\text{unemp}}_i) + \zeta_{0i} \\
\pi_{1i} &amp;amp; = \gamma_{10} + \gamma_{11} (\overline{\text{unemp}}_i) + \zeta_{1i} \\
\pi_{2i} &amp;amp; = \gamma_{20},
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is meant to convey that &lt;span class=&#34;math inline&#34;&gt;\(\overline{\text{unemp}}_i\)&lt;/span&gt; is allowed to explain variability in both initial status on CES-D scores (i.e., the random intercepts) and change in CES-D scores over time (i.e., the random &lt;code&gt;months&lt;/code&gt; slopes). Our variance parameters are all the same:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\epsilon_{ij} &amp;amp; \sim \operatorname{Normal} (0, \sigma_\epsilon) \text{ and} \\
\begin{bmatrix} 
\zeta_{0i} \\ \zeta_{1i} \\
\end{bmatrix} &amp;amp; \sim \operatorname{Normal} 
\left ( 
\begin{bmatrix} 0 \\ 0 \end{bmatrix},
\mathbf{D} \mathbf{\Omega} \mathbf{D}&amp;#39;
\right ), \text{where} \\

\mathbf{D}    &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_1 \end{bmatrix} \text{and} \\

\mathbf{\Omega}  &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho_{01} \\ \rho_{01} &amp;amp; 1 \end{bmatrix}.

\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Our priors also follow the same basic specification as before:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\gamma_{00}     &amp;amp; \sim \operatorname{Normal}(14.5, 20) \\
\gamma_{01}, \gamma_{10}, \gamma_{11}, \text{ and }  \gamma_{20}  &amp;amp; \sim \operatorname{Normal}(0, 10) \\
\sigma_\epsilon, \sigma_0,  \text{ and } \sigma_1 &amp;amp; \sim \operatorname{Student-t} (3, 0, 10) \\
\Omega          &amp;amp; \sim \operatorname{LKJ} (4).
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note, however, that the inclusion of our new level-2 predictor, &lt;span class=&#34;math inline&#34;&gt;\((\overline{\text{unemp}}_i)\)&lt;/span&gt;, changes the meaning of the intercept, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt;. The intercept is now the expected value for a person for whom &lt;code&gt;unemp_id_mu == 0&lt;/code&gt; at the start of the study (i.e., &lt;code&gt;months == 0&lt;/code&gt;). I still think our intercept prior from the first model is fine for this example. But do think carefully about the priors you use in your real-world data analyses.&lt;/p&gt;
&lt;p&gt;Here‚Äôs how to fit the udpdate model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;-
  brm(data = d, 
      family = gaussian,
      cesd ~ 0 + intercept + months + unemp + unemp_id_mu + unemp_id_mu:months + (1 + months | id),
      prior = c(prior(normal(14.5, 20), class = b, coef = &amp;quot;intercept&amp;quot;),
                prior(normal(0, 10),    class = b),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(4), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We should fit one more model before we look at the parameters. If you were paying close attention, above, you may have noticed how it‚Äôs odd that we kept &lt;code&gt;unemp_id_mu&lt;/code&gt; in it‚Äôs natural metric. Sure, it‚Äôs fine in principle‚Äìsensible even‚Äìto use a variable in a probability metric. But in this particular study, none of the participants had a value of &lt;code&gt;unemp_id_mu == 0&lt;/code&gt; because all of them were unemployed at the first time point. Though it is mathematically kosher to fit a model with an intercept based on &lt;code&gt;unemp_id_mu == 0&lt;/code&gt;, it‚Äôs awkward to interpret. So in this case, it makes sense to transform the metric of our level-2 predictor. Perhaps the simplest way is to standardize the variable. That would then give an intercept based on the average &lt;code&gt;unemp_id_mu&lt;/code&gt; value and a &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{01}\)&lt;/span&gt; coefficient that was the expected change in intercept based on a one-standard-deviation higher value in &lt;code&gt;unemp_id_mu&lt;/code&gt;. Let‚Äôs compute that new standardized variable, which we‚Äôll call &lt;code&gt;unemp_id_mu_s&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;% 
  nest(data = c(months:unemp)) %&amp;gt;% 
  mutate(unemp_id_mu_s = (unemp_id_mu - mean(unemp_id_mu)) / sd(unemp_id_mu)) %&amp;gt;% 
  unnest(data)

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##      id unemp_id_mu months  cesd unemp unemp_id_mu_s
##   &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
## 1   103       1      1.15     25     1         0.873
## 2   103       1      5.95     16     1         0.873
## 3   103       1     12.9      33     1         0.873
## 4   641       0.333  0.789    27     1        -1.58 
## 5   641       0.333  4.86      7     0        -1.58 
## 6   641       0.333 11.8      25     0        -1.58&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model formula is the same as before with the exception that we replace &lt;code&gt;unemp_id_mu&lt;/code&gt; with &lt;code&gt;unemp_id_mu_s&lt;/code&gt;. For simplicity, I‚Äôm leaving the priors the way they were.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;-
  brm(data = d, 
      family = gaussian,
      cesd ~ 0 + intercept + months + unemp + unemp_id_mu_s + unemp_id_mu_s:months + (1 + months | id),
      prior = c(prior(normal(14.5, 20), class = b, coef = &amp;quot;intercept&amp;quot;),
                prior(normal(0, 10),    class = b),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(4), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = .9),
      seed = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of examining each of the model summaries one by one, we‚Äôll condense the information into a series of coefficient plots. For simplicity, we‚Äôll restrict our focus to the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the `fit1` summaries
fixef(fit1) %&amp;gt;%
  data.frame() %&amp;gt;%
  rownames_to_column(&amp;quot;par&amp;quot;) %&amp;gt;%
  mutate(fit = &amp;quot;fit1&amp;quot;) %&amp;gt;% 
  bind_rows(
    # add the `fit2` summaries
    fixef(fit2) %&amp;gt;%
      data.frame() %&amp;gt;% 
      rownames_to_column(&amp;quot;par&amp;quot;) %&amp;gt;%
      mutate(fit = &amp;quot;fit2&amp;quot;),
    # add the `fit2` summaries
    fixef(fit3) %&amp;gt;%
      data.frame() %&amp;gt;% 
      rownames_to_column(&amp;quot;par&amp;quot;) %&amp;gt;%
      mutate(fit = &amp;quot;fit3&amp;quot;)
  ) %&amp;gt;% 
  # rename the parameters
  mutate(gamma = case_when(
    par == &amp;quot;intercept&amp;quot;     ~ &amp;quot;gamma[0][0]&amp;quot;,
    par == &amp;quot;months&amp;quot;        ~ &amp;quot;gamma[1][0]&amp;quot;,
    par == &amp;quot;unemp&amp;quot;         ~ &amp;quot;gamma[2][0]&amp;quot;,
    str_detect(par, &amp;quot;:&amp;quot;)   ~ &amp;quot;gamma[1][1]&amp;quot;,
    par == &amp;quot;unemp_id_mu&amp;quot;   ~ &amp;quot;gamma[0][1]&amp;quot;,
    par == &amp;quot;unemp_id_mu_s&amp;quot; ~ &amp;quot;gamma[0][1]&amp;quot;
  )) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = fit, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_pointrange(fatten = 3) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ gamma, nrow = 3, scale = &amp;quot;free_x&amp;quot;, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In case you‚Äôre not familiar with the output from the &lt;code&gt;brms::fixef()&lt;/code&gt; function, each of the parameter estimates are summarized by their posterior means (i.e,. the dots) and percentile-based 95% intervals (i.e., the horizontal lines).&lt;/p&gt;
&lt;p&gt;Recall how earlier I complained that these data weren‚Äôt particularly good for demonstrating this method? Well, here you finally get to see why. Regardless of the model, the estimates didn‚Äôt change much. In these data, the predictive utility of our between-level variable, &lt;code&gt;unemp_id_mu&lt;/code&gt;‚Äìstandardized or not‚Äì, was just about zilch. This is summarized by the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{01}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{11}\)&lt;/span&gt; parameters. Both are centered around zero for both models containing them. Thus adding in an inconsequential level-2 predictor had little effect on its level-1 companion, &lt;code&gt;unemp&lt;/code&gt;, which was expressed by &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{20}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Depressing as these results are, the practice was still worthwhile. Had we not decomposed our time-varying &lt;code&gt;unemp&lt;/code&gt; variable into its within- and between-level components, we would never had known that the trait levels of &lt;code&gt;umemp&lt;/code&gt; were inconsequential for these analyses. Now we know. For these models, all the action for &lt;code&gt;unemp&lt;/code&gt; was at the within-person level.&lt;/p&gt;
&lt;p&gt;This is also the explanation for why we focused on the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;s to the neglect of the variance parameters. Because our &lt;code&gt;unemp_id_mu&lt;/code&gt; variables were poor predictors of the random effects, there was no reason to expect they‚Äôd differ meaningfully across models. And because &lt;code&gt;unemp_id_mu&lt;/code&gt; is only a level-2 predictor, it never had any hope for changing the estimates for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-centering-umemp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What about centering &lt;code&gt;umemp&lt;/code&gt;?&lt;/h3&gt;
&lt;p&gt;If you look through our primary two references for this post, Enders &amp;amp; Tofighi (2007) and Hoffman (2015), you‚Äôll see both works spend a lot of time on discussing how one might center the level-1 versions of the time-varying covariates. If &lt;code&gt;unemp&lt;/code&gt; was a continuous variable, we would have had to contend with that issue, too. But this just isn‚Äôt necessary with binary variables. They have a sensible interpretation when left in the typical 0/1 format. So my recommendation is when you‚Äôre decomposing your binary time-varying covariates, put your focus on meaningfully centering the level-2 version of the variable. Leave the level-1 version alone. However, if you‚Äôre really interested in playing around with alternatives like effects coding, Enders and Tofighi provided several recommendations.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
##  [9] tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         splines_4.0.4        crosstalk_1.1.0.1   
##   [7] TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17       
##  [10] digest_0.6.27        htmltools_0.5.1.1    rsconnect_0.8.16    
##  [13] fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [16] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1          
##  [19] sandwich_3.0-0       prettyunits_1.1.1    colorspace_2.0-0    
##  [22] rvest_0.3.6          haven_2.3.1          xfun_0.22           
##  [25] callr_3.5.1          crayon_1.4.1         jsonlite_1.7.2      
##  [28] lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [31] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1     
##  [34] V8_3.4.0             pkgbuild_1.2.0       rstan_2.21.2        
##  [37] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [40] emo_0.0.0.9000       DBI_1.1.0            miniUI_0.1.1.1      
##  [43] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [46] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2          
##  [49] threejs_0.3.3        ellipsis_0.3.1       pkgconfig_2.0.3     
##  [52] loo_2.4.1            farver_2.0.3         dbplyr_2.0.0        
##  [55] utf8_1.1.4           tidyselect_1.1.0     labeling_0.4.2      
##  [58] rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1       
##  [61] munsell_0.5.0        cellranger_1.1.0     tools_4.0.4         
##  [64] cli_2.3.1            generics_0.1.0       broom_0.7.5         
##  [67] ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1       
##  [70] yaml_2.2.1           processx_3.4.5       knitr_1.31          
##  [73] fs_1.5.0             nlme_3.1-152         mime_0.10           
##  [76] projpred_2.0.2       xml2_1.3.2           compiler_4.0.4      
##  [79] bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [82] curl_4.3             gamm4_0.2-6          reprex_0.3.0        
##  [85] statmod_1.4.35       stringi_1.5.3        highr_0.8           
##  [88] ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6   
##  [91] lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [94] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
##  [97] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0
## [100] estimability_1.3     httpuv_1.5.4         R6_2.5.0            
## [103] bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [106] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0  
## [109] MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1    
## [112] withr_2.4.1          shinystan_2.5.0      multcomp_1.4-16     
## [115] mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [118] grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [121] rmarkdown_2.7        shiny_1.5.0          lubridate_1.7.9.2   
## [124] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Would you like all your posteriors in one plot?</title>
      <link>/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/</guid>
      <description>
&lt;script src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A colleague reached out to me earlier this week with a plotting question. They had fit a series of Bayesian models, all containing a common parameter of interest. They knew how to plot their focal parameter one model at a time, but were stumped on how to combine the plots across models into a seamless whole. It reminded me a bit of this gif&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/Bqn8Z7xdPCFy0/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;which I originally got from &lt;a href=&#34;https://twitter.com/JennyBryan&#34;&gt;Jenny Bryan&lt;/a&gt;‚Äôs great talk, &lt;a href=&#34;https://www.youtube.com/watch?v=4MfUCX_KpdE&#34;&gt;&lt;em&gt;Behind every great plot there‚Äôs a great deal of wrangling&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The goal of this post is to provide solutions. We‚Äôll practice a few different ways you can combine the posterior samples from your Bayesian models into a single plot. As usual, we‚Äôll be fitting our models with &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt;, wrangling with packages from the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt;, and getting a little help from the &lt;a href=&#34;https://mjskay.github.io/tidybayes/index.html&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I make assumptions.&lt;/h2&gt;
&lt;p&gt;For this post, I‚Äôm presuming you are familiar Bayesian regression using &lt;strong&gt;brms.&lt;/strong&gt; I‚Äôm also assuming you‚Äôve coded using some of the foundational functions from the &lt;strong&gt;tidyverse.&lt;/strong&gt; If you‚Äôd like to firm up your foundations a bit, check out these resources.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To learn about Bayesian regression, I recommend the introductory text books by either McElreath (&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;here&lt;/a&gt;) or Kruschke (&lt;a href=&#34;http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/&#34;&gt;here&lt;/a&gt;). Both authors host blogs (&lt;a href=&#34;http://doingbayesiandataanalysis.blogspot.com&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://elevanth.org/blog/&#34;&gt;here&lt;/a&gt;, respectively). If you go with McElreath, do check out his &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;online lectures&lt;/a&gt; and my ebooks where I translated his text to &lt;strong&gt;brms&lt;/strong&gt; and &lt;strong&gt;tidyverse&lt;/strong&gt; code (&lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;here&lt;/a&gt;). I have a similar ebook translation for Kruschke‚Äôs text (&lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For even more &lt;strong&gt;brms&lt;/strong&gt;-related resources, you can find vignettes and documentation &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/index.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;tidyverse&lt;/strong&gt; introductions, your best bets are &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R4DS&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://style.tidyverse.org&#34;&gt;&lt;em&gt;The tidyverse style guide&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;same-parameter-different-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Same parameter, different models&lt;/h2&gt;
&lt;p&gt;Let‚Äôs load our primary statistical packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)
library(tidybayes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate &lt;span class=&#34;math inline&#34;&gt;\(n = 150\)&lt;/span&gt; draws from the standard normal distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 150

set.seed(1)
d &amp;lt;-
  tibble(y = rnorm(n, mean = 0, sd = 1))

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##        y
##    &amp;lt;dbl&amp;gt;
## 1 -0.626
## 2  0.184
## 3 -0.836
## 4  1.60 
## 5  0.330
## 6 -0.820&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we‚Äôll fit three intercept-only models for &lt;code&gt;y&lt;/code&gt;. Each will follow the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i     &amp;amp; \sim \text{Normal} (\mu, \sigma) \\
\mu     &amp;amp; = \beta_0 \\
\beta_0 &amp;amp; \sim \text{Normal} (0, x) \\
\sigma  &amp;amp; \sim \text{Student-t}(3, 0, 10)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the unconditional intercept (i.e., an intercept not conditioned on any predictors). We will be fitting three alternative models. All will have the same prior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{Student-t}(3, 0, 10)\)&lt;/span&gt;, which is the &lt;strong&gt;brms&lt;/strong&gt; default in this case. [If you‚Äôd like to check, use the &lt;code&gt;get_prior()&lt;/code&gt; function.] The only way the models will differ is by their prior on the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;. By model, those priors will be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fit1&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, 10)\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit2&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, 1)\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit3&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, 0.1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So if you were wondering, the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in the &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, x)\)&lt;/span&gt; line, above, was a stand-in for the varying &lt;a href=&#34;https://en.wikipedia.org/wiki/Hyperparameter&#34;&gt;hyperparameter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here we fit the models in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 1,
      prior(normal(0, 10), class = Intercept),
      seed = 1)

fit2 &amp;lt;-
  update(fit1,
         prior = prior(normal(0, 1), class = Intercept),
         seed = 1)

fit3 &amp;lt;-
  update(fit1,
         prior = prior(normal(0, 0.1), class = Intercept),
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Normally we‚Äôd use &lt;code&gt;plot()&lt;/code&gt; to make sure the chains look good and then use something like &lt;code&gt;print()&lt;/code&gt; or &lt;code&gt;posterior_summary()&lt;/code&gt; to summarize the models‚Äô results. I‚Äôve checked and they‚Äôre all fine. For the sake of space, let‚Äôs press forward.&lt;/p&gt;
&lt;p&gt;If you were going to plot the results of an individual fit using something like the &lt;code&gt;tidybayes::stat_halfeye()&lt;/code&gt; function, the next step would be extracting the posterior draws. Here we‚Äôll do so with the &lt;code&gt;brms::posterior_samples()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post1 &amp;lt;- posterior_samples(fit1)
post2 &amp;lt;- posterior_samples(fit2)
post3 &amp;lt;- posterior_samples(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Focusing on &lt;code&gt;fit1&lt;/code&gt;, here‚Äôs how we‚Äôd plot the results for the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this part is unnecessary; it just adjusts some theme defaults to my liking
theme_set(theme_gray() +
            theme(axis.text.y  = element_text(hjust = 0),
                  axis.ticks.y = element_blank(),
                  panel.grid   = element_blank()))

# plot!
post1 %&amp;gt;% 
  ggplot(aes(x = b_Intercept, y = 0)) +
  stat_halfeye() +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But how might we get the posterior draws from all three fits into one plot?&lt;/em&gt; The answer is by somehow combining the posterior draws from each into one data frame. There are many ways to do this. Perhaps the simplest is with the &lt;code&gt;bind_rows()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  bind_rows(
    post1,
    post2,
    post3
  ) %&amp;gt;% 
  mutate(prior = str_c(&amp;quot;normal(0, &amp;quot;, c(10, 1, 0.1), &amp;quot;)&amp;quot;) %&amp;gt;% rep(., each = 4000))

head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_Intercept     sigma      lp__         prior
## 1  0.06440413 0.9408454 -202.2537 normal(0, 10)
## 2  0.02603356 0.9416735 -202.1114 normal(0, 10)
## 3 -0.02122717 0.8967501 -202.0446 normal(0, 10)
## 4  0.02620046 0.9521795 -202.2594 normal(0, 10)
## 5  0.02620046 0.9521795 -202.2594 normal(0, 10)
## 6  0.08025366 0.9101939 -202.1808 normal(0, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;bind_rows()&lt;/code&gt; function worked well, here, because all three post objects had the same number of columns of the same names. So we just stacked them three high. That is, we went from three data objects of 4,000 rows and 3 columns to one data object with 12,000 rows and 3 columns. But with the &lt;code&gt;mutate()&lt;/code&gt; function we did add a fourth column, &lt;code&gt;prior&lt;/code&gt;, that indexed which model each row came from. Now our data are ready, we can plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  ggplot(aes(x = b_Intercept, y = prior)) +
  stat_halfeye()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our plot arrangement made it easy to compare the results of tightening the prior on &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;; the narrower the prior, the narrower the posterior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-if-my-posterior_samples-arent-of-the-same-dimensions-across-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What if my &lt;code&gt;posterior_samples()&lt;/code&gt; aren‚Äôt of the same dimensions across models?&lt;/h2&gt;
&lt;p&gt;For the next examples, we need new data. Here we‚Äôll simulate three predictors‚Äì&lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, and &lt;code&gt;x3&lt;/code&gt;. We then simulate our criterion &lt;code&gt;y&lt;/code&gt; as a linear additive function of those predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
d &amp;lt;-
  tibble(x1 = rnorm(n, mean = 0, sd = 1),
         x2 = rnorm(n, mean = 0, sd = 1),
         x3 = rnorm(n, mean = 0, sd = 1)) %&amp;gt;% 
  mutate(y  = rnorm(n, mean = 0 + x1 * 0 + x2 * 0.2 + x3 * -0.4))

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##       x1      x2     x3      y
##    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 -0.626  0.450   0.894  0.694
## 2  0.184 -0.0186 -1.05  -0.189
## 3 -0.836 -0.318   1.97  -1.61 
## 4  1.60  -0.929  -0.384 -1.59 
## 5  0.330 -1.49    1.65  -2.41 
## 6 -0.820 -1.08    1.51  -0.764&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to work with these data in two ways. For the first example, we‚Äôll fit a series of univariable models following the same basic form, but each with a different predictor. For the second example, we‚Äôll fit a series of multivariable models with various combinations of the predictors. Each requires its own approach.&lt;/p&gt;
&lt;div id=&#34;same-form-different-predictors.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Same form, different predictors.&lt;/h3&gt;
&lt;p&gt;This time we‚Äôre just using the &lt;strong&gt;brms&lt;/strong&gt; default priors. As such, the models all follow the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i     &amp;amp; \sim \text{Normal} (\mu_i, \sigma) \\
\mu_i   &amp;amp; = \beta_0 + \beta_n x_n\\
\beta_0 &amp;amp; \sim \text{Student-t}(3, 0, 10) \\
\sigma  &amp;amp; \sim \text{Student-t}(3, 0, 10)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You may be wondering &lt;em&gt;What about the prior for&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\beta_n\)&lt;/span&gt;&lt;em&gt;?&lt;/em&gt; The &lt;strong&gt;brms&lt;/strong&gt; defaults for those are improper flat priors. We define &lt;span class=&#34;math inline&#34;&gt;\(\beta_n x_n\)&lt;/span&gt; for the next three models as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fit4&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 x_1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit5&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_2 x_2\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit5&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_3 x_3\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let‚Äôs fit the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit4 &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 1 + x1,
      seed = 1)

fit5 &amp;lt;-
  update(fit4,
         newdata = d,
         y ~ 1 + x2,
         seed = 1)

fit6 &amp;lt;-
  update(fit4,
         newdata = d,
         y ~ 1 + x3,
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like before, save the posterior draws for each as separate data frames.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post4 &amp;lt;- posterior_samples(fit4)
post5 &amp;lt;- posterior_samples(fit5)
post6 &amp;lt;- posterior_samples(fit6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, our simple &lt;code&gt;bind_rows()&lt;/code&gt; trick won‚Äôt work well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  post4,
  post5,
  post6
) %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_Intercept        b_x1    sigma      lp__ b_x2 b_x3
## 1 -0.26609646 -0.07795464 1.249694 -242.9716   NA   NA
## 2 -0.11933443 -0.03143494 1.251379 -240.4618   NA   NA
## 3 -0.10952301  0.02739295 1.278072 -241.2102   NA   NA
## 4 -0.08785528 -0.01065453 1.443157 -245.2715   NA   NA
## 5 -0.22020421 -0.16635358 1.185220 -241.7569   NA   NA
## 6  0.02973246 -0.13106488 1.123438 -239.2940   NA   NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We don‚Äôt want separate columns for &lt;code&gt;b_x1&lt;/code&gt;, &lt;code&gt;b_x2&lt;/code&gt;, and &lt;code&gt;b_x3&lt;/code&gt;. We want them all stacked atop one another. One simple solution is a two-step wherein we (1) select the relevant columns from each and bind them together with &lt;code&gt;bind_cols()&lt;/code&gt; and then (2) stack them atop one another with the &lt;code&gt;gather()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  bind_cols(
    post4 %&amp;gt;% select(b_x1),
    post5 %&amp;gt;% select(b_x2),
    post6 %&amp;gt;% select(b_x3)
  ) %&amp;gt;% 
  gather() %&amp;gt;% 
  mutate(predictor = str_remove(key, &amp;quot;b_&amp;quot;))

head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    key       value predictor
## 1 b_x1 -0.07795464        x1
## 2 b_x1 -0.03143494        x1
## 3 b_x1  0.02739295        x1
## 4 b_x1 -0.01065453        x1
## 5 b_x1 -0.16635358        x1
## 6 b_x1 -0.13106488        x1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That &lt;code&gt;mutate()&lt;/code&gt; line at the end wasn‚Äôt necessary, but it will make the plot more attractive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  ggplot(aes(x = value, y = predictor)) +
  stat_halfeye()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;different-combinations-of-predictors-in-different-forms.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Different combinations of predictors in different forms.&lt;/h3&gt;
&lt;p&gt;Now we fit a series of multivariable models. The first three will have combinations of two of the predictors. The final model will have all three. For simplicity, we continue to use the &lt;strong&gt;brms&lt;/strong&gt; default priors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit7 &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 1 + x1 + x2,
      seed = 1)

fit8 &amp;lt;-
  update(fit7,
         newdata = d,
         y ~ 1 + x1 + x3,
         seed = 1)

fit9 &amp;lt;-
  update(fit7,
         newdata = d,
         y ~ 1 + x2 + x3,
         seed = 1)

fit10 &amp;lt;-
  update(fit7,
         newdata = d,
         y ~ 1 + x1 + x2 + x3,
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Individually extract the posterior draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post7  &amp;lt;- posterior_samples(fit7)
post8  &amp;lt;- posterior_samples(fit8)
post9  &amp;lt;- posterior_samples(fit9)
post10 &amp;lt;- posterior_samples(fit10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a look at what happens this time when we use the &lt;code&gt;bind_rows()&lt;/code&gt; approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  bind_rows(
    post7,
    post8,
    post9,
    post10
  ) 

glimpse(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 16,000
## Columns: 6
## $ b_Intercept &amp;lt;dbl&amp;gt; -0.034398871, 0.008116322, 0.109134954, -0.134114504, -0.148230448, 0.04629622‚Ä¶
## $ b_x1        &amp;lt;dbl&amp;gt; -0.018887709, -0.156024614, -0.248414749, 0.057442787, 0.241874229, -0.3504998‚Ä¶
## $ b_x2        &amp;lt;dbl&amp;gt; 0.23847261, 0.27500306, 0.37294396, 0.20640317, 0.15437136, 0.28201317, 0.1538‚Ä¶
## $ sigma       &amp;lt;dbl&amp;gt; 1.250134, 1.065501, 1.029253, 1.220301, 1.206074, 1.114755, 1.180636, 1.266597‚Ä¶
## $ lp__        &amp;lt;dbl&amp;gt; -236.9970, -236.7477, -241.3055, -237.9540, -242.0909, -239.3407, -237.2902, -‚Ä¶
## $ b_x3        &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still have the various data frames stacked atop another, with the data from &lt;code&gt;post7&lt;/code&gt; in the first 4,000 rows. See how the values in the &lt;code&gt;b_x3&lt;/code&gt; column are all missing (i.e., filled with &lt;code&gt;NA&lt;/code&gt; values)? That‚Äôs because &lt;code&gt;fit7&lt;/code&gt; didn‚Äôt contain &lt;code&gt;x3&lt;/code&gt; as a predictor. Similarly, if we were to look at rows 4,001 through 8,000, we‚Äôd see column &lt;code&gt;b_x2&lt;/code&gt; would be the one filled with &lt;code&gt;NA&lt;/code&gt;s. This behavior is a good thing, here. After a little more wrangling, we‚Äôll plot and it should be become clear why. Here‚Äôs the wrangling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  posts %&amp;gt;% 
  select(starts_with(&amp;quot;b_x&amp;quot;)) %&amp;gt;% 
  mutate(contains = rep(c(&amp;quot;&amp;lt;1, 1, 0&amp;gt;&amp;quot;, &amp;quot;&amp;lt;1, 0, 1&amp;gt;&amp;quot;, &amp;quot;&amp;lt;0, 1, 1&amp;gt;&amp;quot;, &amp;quot;&amp;lt;1, 1, 1&amp;gt;&amp;quot;), each = 4000)) %&amp;gt;% 
  gather(key, value, -contains) %&amp;gt;% 
  mutate(coefficient = str_remove(key, &amp;quot;b_x&amp;quot;) %&amp;gt;% str_c(&amp;quot;beta[&amp;quot;, ., &amp;quot;]&amp;quot;))

head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    contains  key       value coefficient
## 1 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.01888771     beta[1]
## 2 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.15602461     beta[1]
## 3 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.24841475     beta[1]
## 4 &amp;lt;1, 1, 0&amp;gt; b_x1  0.05744279     beta[1]
## 5 &amp;lt;1, 1, 0&amp;gt; b_x1  0.24187423     beta[1]
## 6 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.35049990     beta[1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;code&gt;contains&lt;/code&gt; variable, we indexed which fit the draws came from. The 1‚Äôs and 0‚Äôs within the angle brackets indicate which of the three predictors were present within the model with the 1‚Äôs indicating they were and the 0‚Äôs indicating they were not. For example, &lt;code&gt;&amp;lt;1, 1, 0&amp;gt;&lt;/code&gt; in the first row indicated this was the model including &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt;. Importantly, we also added a &lt;code&gt;coefficient&lt;/code&gt; index. This is just a variant of &lt;code&gt;key&lt;/code&gt; that‚Äôll make the strip labels in our plot more attractive. Behold:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  drop_na(value) %&amp;gt;% 
  ggplot(aes(x = value, y = contains)) +
  stat_halfeye() +
  ylab(NULL) +
  facet_wrap(~coefficient, ncol = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hopefully now it‚Äôs clear why it was good to save those cells with the &lt;code&gt;NA&lt;/code&gt;s.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bonus-you-can-streamline-your-workflow.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bonus: You can streamline your workflow.&lt;/h2&gt;
&lt;p&gt;The workflows above are generally fine. But they‚Äôre a little inefficient. If you‚Äôd like to reduce the amount of code you‚Äôre writing and the number of objects you have floating around in your environment, you might consider a more streamlined workflow where you work with your fit objects in bulk. Here we‚Äôll demonstrate a nested tibble approach with the first three fits.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  tibble(name  = str_c(&amp;quot;fit&amp;quot;, 1:3),
         prior = str_c(&amp;quot;normal(0, &amp;quot;, c(10, 1, 0.1), &amp;quot;)&amp;quot;)) %&amp;gt;% 
  mutate(fit = map(name, get)) %&amp;gt;% 
  mutate(post = map(fit, posterior_samples))
  
head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 4
##   name  prior          fit       post                
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;              
## 1 fit1  normal(0, 10)  &amp;lt;brmsfit&amp;gt; &amp;lt;df[,3] [4,000 √ó 3]&amp;gt;
## 2 fit2  normal(0, 1)   &amp;lt;brmsfit&amp;gt; &amp;lt;df[,3] [4,000 √ó 3]&amp;gt;
## 3 fit3  normal(0, 0.1) &amp;lt;brmsfit&amp;gt; &amp;lt;df[,3] [4,000 √ó 3]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a 3-row nested tibble. The first column, &lt;code&gt;name&lt;/code&gt; is just a character vector with the names of the fits. The next column isn‚Äôt necessary, but it nicely explicates the main difference in the models: the prior we used on the intercept. It‚Äôs in the &lt;code&gt;map()&lt;/code&gt; functions within the two &lt;code&gt;mutate()&lt;/code&gt;lines where all the magic happens. With the first, we used the &lt;code&gt;get()&lt;/code&gt; function to snatch up the &lt;strong&gt;brms&lt;/strong&gt; fit objects matching the names in the &lt;code&gt;name&lt;/code&gt; column. In the second, we used the &lt;code&gt;posterior_samples()&lt;/code&gt; function to extract the posterior draws from each of the fits saved in &lt;code&gt;fit&lt;/code&gt;. Do you see how each for in the &lt;code&gt;post&lt;/code&gt; column contains an entire &lt;span class=&#34;math inline&#34;&gt;\(4,000 \times 3\)&lt;/span&gt; data frame? That‚Äôs why we refer to this as a nested tibble. We have data frames compressed within data frames. If you‚Äôd like to access the data within the &lt;code&gt;post&lt;/code&gt; column, just &lt;code&gt;unnest()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  select(-fit) %&amp;gt;% 
  unnest(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12,000 x 5
##    name  prior         b_Intercept sigma  lp__
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 fit1  normal(0, 10)     0.0644  0.941 -202.
##  2 fit1  normal(0, 10)     0.0260  0.942 -202.
##  3 fit1  normal(0, 10)    -0.0212  0.897 -202.
##  4 fit1  normal(0, 10)     0.0262  0.952 -202.
##  5 fit1  normal(0, 10)     0.0262  0.952 -202.
##  6 fit1  normal(0, 10)     0.0803  0.910 -202.
##  7 fit1  normal(0, 10)    -0.00142 0.886 -202.
##  8 fit1  normal(0, 10)     0.0696  0.939 -202.
##  9 fit1  normal(0, 10)    -0.172   0.943 -205.
## 10 fit1  normal(0, 10)     0.0259  0.839 -203.
## # ‚Ä¶ with 11,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After un-nesting, we can remake the plot from above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  select(-fit) %&amp;gt;% 
  unnest(post) %&amp;gt;% 

  ggplot(aes(x = b_Intercept, y = prior)) +
  stat_halfeye()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To learn more about using the &lt;strong&gt;tidyverse&lt;/strong&gt; for iterating and saving the results in nested tibbles, check out &lt;a href=&#34;https://twitter.com/hadleywickham&#34;&gt;Hadley Wickham&lt;/a&gt;‚Äôs great talk, &lt;a href=&#34;https://www.youtube.com/watch?v=rz3_FDVt9eg&#34;&gt;&lt;em&gt;Managing many models&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session information&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5    
##  [7] purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] splines_4.0.4        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [49] htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [53] ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3      loo_2.4.1           
##  [57] dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2       tidyselect_1.1.0    
##  [61] rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [65] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [69] broom_0.7.5          ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1       
##  [73] yaml_2.2.1           processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [77] nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [81] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [85] gamm4_0.2-6          curl_4.3             reprex_0.3.0         statmod_1.4.35      
##  [89] stringi_1.5.3        highr_0.8            ps_1.6.0             blogdown_1.3        
##  [93] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [97] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1        
## [101] lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [105] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [109] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [113] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [117] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [125] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stein&#39;s Paradox and What Partial Pooling Can Do For You</title>
      <link>/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/</link>
      <pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/</guid>
      <description>
&lt;script src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;[edited on January 18, 2021]&lt;/p&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;a href=&#34;https://www.urbandictionary.com/define.php?term=tl%3Bdr&#34;&gt;tl;dr&lt;/a&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Sometimes a mathematical result is strikingly contrary to generally held belief even though an obviously valid proof is given. &lt;a href=&#34;https://en.wikipedia.org/wiki/Charles_M._Stein&#34;&gt;Charles Stein&lt;/a&gt; of Stanford University discovered such a paradox in statistics in 1955. His result undermined a century and a half of work on estimation theory. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 119&lt;/a&gt;)&lt;/span&gt;
The James-Stein estimator leads to better predictions than simple means. Though I don‚Äôt recommend you actually use the James-Stein estimator in applied research, understanding why it works might help clarify why it‚Äôs time social scientists consider &lt;a href=&#34;http://elevanth.org/blog/2017/08/24/multilevel-regression-as-default/&#34;&gt;defaulting to multilevel models&lt;/a&gt; for their work-a-day projects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;the-james-stein-can-help-us-understand-multilevel-models.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The James-Stein can help us understand multilevel models.&lt;/h2&gt;
&lt;p&gt;I recently noticed someone‚ÄîI wish I could recall who‚Äîtweet about Efron and Morris‚Äôs classic paper, &lt;a href=&#34;http://statweb.stanford.edu/~ckirby/brad/other/Article1977.pdf&#34;&gt;&lt;em&gt;Stein‚Äôs paradox in statistics&lt;/em&gt;&lt;/a&gt;. At the time, I was vaguely aware of the paper but hadn‚Äôt taken the chance to read it. The tweet‚Äôs author mentioned how good a read it was. Now I‚Äôve finally given it a look, I concur. I‚Äôm not a sports fan, but I really appreciated their primary example using batting averages from baseball players in 1970. It clarified why partial pooling leads to better estimates than taking simple averages.&lt;/p&gt;
&lt;p&gt;In this post, I‚Äôll walk out Efron and Morris‚Äôs baseball example and then link it to contemporary Bayesian multilevel models.&lt;/p&gt;
&lt;div id=&#34;i-assume-things.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I assume things.&lt;/h3&gt;
&lt;p&gt;For this project, I‚Äôm presuming you are familiar with logistic regression, vaguely familiar with the basic differences between frequentist and Bayesian approaches to fitting regression models, and have heard of multilevel models. All code in is &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;&lt;strong&gt;R&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with a heavy use of the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;, and the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; package&lt;/a&gt; for Bayesian regression &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;B√ºrkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;behold-the-baseball-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Behold the &lt;code&gt;baseball&lt;/code&gt; data.&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Stein‚Äôs paradox concerns the use of observed averages to estimate unobservable quantities. Averaging is the second most basic process in statistics, the first being the simple act of counting. A baseball player who gets seven hits in 20 official times at bat is said to have a batting average of .350. In computing this statistic we are forming an estimate of the payer‚Äôs true batting ability in terms of his observed average rate of success. Asked how well the player will do in his next 100 times at bat, we would probably predict 35 more hits. In traditional statistical theory it can be proved that no other estimation rule is uniformly better than the observed average.&lt;/p&gt;
&lt;p&gt;The paradoxical element in Stein‚Äôs result is that it sometimes contradicts this elementary law of statistical theory. If we have three or more baseball players, and if we are interested in predicting future batting averages for each of them, then there is a procedure that is better than simply extrapolating from the three separate averages‚Ä¶&lt;/p&gt;
&lt;p&gt;As our primary data we shall consider the batting averages of 18 major-league players as they were recorded after their first 45 times at bat in the 1970 season. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 119&lt;/a&gt;)&lt;/span&gt;
Let‚Äôs enter the &lt;code&gt;baseball&lt;/code&gt; data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
baseball &amp;lt;- 
  tibble(player = c(&amp;quot;Clemente&amp;quot;, &amp;quot;F Robinson&amp;quot;, &amp;quot;F Howard&amp;quot;, &amp;quot;Johnstone&amp;quot;, &amp;quot;Berry&amp;quot;, &amp;quot;Spencer&amp;quot;, &amp;quot;Kessinger&amp;quot;, &amp;quot;L Alvarado&amp;quot;, &amp;quot;Santo&amp;quot;, &amp;quot;Swoboda&amp;quot;, &amp;quot;Unser&amp;quot;, &amp;quot;Williams&amp;quot;, &amp;quot;Scott&amp;quot;, &amp;quot;Petrocelli&amp;quot;, &amp;quot;E Rodriguez&amp;quot;, &amp;quot;Campaneris&amp;quot;, &amp;quot;Munson&amp;quot;, &amp;quot;Alvis&amp;quot;),
         hits = c(18:15, 14, 14:12, 11, 11, rep(10, times = 5), 9:7),
         times_at_bat = 45,
         true_ba = c(.346, .298, .276, .222, .273, .27, .263, .21, .269, .23, .264, .256, .303, .264, .226, .286, .316, .2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here‚Äôs what those data look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(baseball)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 18
## Columns: 4
## $ player       &amp;lt;chr&amp;gt; &amp;quot;Clemente&amp;quot;, &amp;quot;F Robinson&amp;quot;, &amp;quot;F Howard&amp;quot;, &amp;quot;Johnstone&amp;quot;, &amp;quot;Berry&amp;quot;, &amp;quot;Spencer&amp;quot;, &amp;quot;Kessi‚Ä¶
## $ hits         &amp;lt;dbl&amp;gt; 18, 17, 16, 15, 14, 14, 13, 12, 11, 11, 10, 10, 10, 10, 10, 9, 8, 7
## $ times_at_bat &amp;lt;dbl&amp;gt; 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45
## $ true_ba      &amp;lt;dbl&amp;gt; 0.346, 0.298, 0.276, 0.222, 0.273, 0.270, 0.263, 0.210, 0.269, 0.230, 0.264, ‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have data from 18 players. The main columns are of the number of &lt;code&gt;hits&lt;/code&gt; for their first 45 &lt;code&gt;times_at_bat&lt;/code&gt;. I got the &lt;code&gt;player&lt;/code&gt;, &lt;code&gt;hits&lt;/code&gt;, and &lt;code&gt;times_at_bat&lt;/code&gt; values directly from the paper. However, Efron and Morris didn‚Äôt include the batting averages for the end of the season in the paper. Happily, I was able to find those values in the &lt;a href=&#34;http://statweb.stanford.edu/~ckirby/brad/LSI/chapter1.pdf&#34;&gt;online&lt;/a&gt; posting of the first chapter of one of Effron‚Äôs books &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronEmpiricalBayesJamesStein2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; . They‚Äôre included in the &lt;code&gt;true_ba&lt;/code&gt; column.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;These were all the players who happened to have batted exactly 45 times the day the data were tabulated. A batting average is defined, of course, simply as the number of hits divided by the number of times at bat; it is always a number between 0 and 1. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 119&lt;/a&gt;)&lt;/span&gt;
I like use a lot of plots to better understand what I‚Äôm doing. Before we start plotting, I should point out the color theme in this project comes from &lt;a href=&#34;https://teamcolorcodes.com/seattle-mariners-color-codes/&#34;&gt;here&lt;/a&gt;. [Haters gonna hate.]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;navy_blue &amp;lt;- &amp;quot;#0C2C56&amp;quot;
nw_green  &amp;lt;- &amp;quot;#005C5C&amp;quot;  
silver    &amp;lt;- &amp;quot;#C4CED4&amp;quot;
theme_set(theme_grey() +
            theme(panel.grid = element_blank(),
                  panel.background = element_rect(fill = silver),
                  strip.background = element_rect(fill = silver)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might use a histogram to get a sense of the &lt;code&gt;hits&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  ggplot(aes(x = hits)) +
  geom_histogram(color = nw_green,
                 fill  = navy_blue,
                 size  = 1/10, binwidth = 1) +
  scale_x_continuous(&amp;quot;hits during the first 45 trials&amp;quot;,
                     breaks = 7:18)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is the distribution of the end-of-the-season batting averages, &lt;code&gt;true_ba&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)
baseball %&amp;gt;% 
  ggplot(aes(x = true_ba, y = 0)) +
  stat_halfeye(point_interval = median_qi, .width = .5,
               color = navy_blue, fill = alpha(nw_green, 2/3)) +
  geom_rug(color = navy_blue, size = 1/3, alpha = 1/2) +
  ggtitle(NULL, 
          subtitle = &amp;quot;The dot and horizontal line are the median and\ninterquartile range, respectively.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;james-stein-will-help-us-achieve-our-goal.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;James-Stein will help us achieve our goal.&lt;/h3&gt;
&lt;p&gt;For each of the 18 players in the data, our goal is to the best job possible to use the data for their first 45 times at bat (i.e., &lt;code&gt;hits&lt;/code&gt; and &lt;code&gt;times_at_bat&lt;/code&gt;) to predict their batting averages at the end of the season (i.e., &lt;code&gt;true_ba&lt;/code&gt;). Before Charles Stein, the conventional reasoning was their initial batting averages (i.e., &lt;code&gt;hits / times_at_bat&lt;/code&gt;) are the best way to do this. It turns out that would be na√Øve. To see why, let&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) = the batting average for the first 45 times at bat,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y_bar&lt;/code&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\overline y\)&lt;/span&gt;) = the grand mean for the first 45 times at bat,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;) = shrinking factor,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;z&lt;/code&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;) = James-Stein estimate, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;true_ba&lt;/code&gt; (i.e., &lt;code&gt;theta&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) = the batting average at the end of the season.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The first step in applying Stein‚Äôs method is to determine the average of the averages. Obviously this grand average, which we give the symbol &lt;span class=&#34;math inline&#34;&gt;\(\overline y\)&lt;/span&gt;, must also lie between 0 and 1. The essential process in Stein‚Äôs method is the ‚Äúshrinking‚Äù of all the individual averages toward this grand average. If a player‚Äôs hitting record is better than the grand average, then it must be reduced; if he is not hitting as well as the grand average, then his hitting record must be increased. The resulting shrunken value for each player we designate &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 119&lt;/a&gt;)&lt;/span&gt;
As such, the James-Stein estimator is&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[z = \overline y + c(y - \overline y),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, in the paper, &lt;span class=&#34;math inline&#34;&gt;\(c = .212\)&lt;/span&gt;. Let‚Äôs get some of those values into the &lt;code&gt;baseball&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
  baseball &amp;lt;-
  baseball %&amp;gt;% 
  mutate(y = hits / times_at_bat) %&amp;gt;% 
  mutate(y_bar = mean(y),
         c     = .212) %&amp;gt;% 
  mutate(z     = y_bar + c * (y - y_bar),
         theta = true_ba)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18 x 9
##    player       hits times_at_bat true_ba     y y_bar     c     z theta
##    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Clemente       18           45   0.346 0.4   0.265 0.212 0.294 0.346
##  2 F Robinson     17           45   0.298 0.378 0.265 0.212 0.289 0.298
##  3 F Howard       16           45   0.276 0.356 0.265 0.212 0.285 0.276
##  4 Johnstone      15           45   0.222 0.333 0.265 0.212 0.280 0.222
##  5 Berry          14           45   0.273 0.311 0.265 0.212 0.275 0.273
##  6 Spencer        14           45   0.27  0.311 0.265 0.212 0.275 0.27 
##  7 Kessinger      13           45   0.263 0.289 0.265 0.212 0.270 0.263
##  8 L Alvarado     12           45   0.21  0.267 0.265 0.212 0.266 0.21 
##  9 Santo          11           45   0.269 0.244 0.265 0.212 0.261 0.269
## 10 Swoboda        11           45   0.23  0.244 0.265 0.212 0.261 0.23 
## 11 Unser          10           45   0.264 0.222 0.265 0.212 0.256 0.264
## 12 Williams       10           45   0.256 0.222 0.265 0.212 0.256 0.256
## 13 Scott          10           45   0.303 0.222 0.265 0.212 0.256 0.303
## 14 Petrocelli     10           45   0.264 0.222 0.265 0.212 0.256 0.264
## 15 E Rodriguez    10           45   0.226 0.222 0.265 0.212 0.256 0.226
## 16 Campaneris      9           45   0.286 0.2   0.265 0.212 0.252 0.286
## 17 Munson          8           45   0.316 0.178 0.265 0.212 0.247 0.316
## 18 Alvis           7           45   0.2   0.156 0.265 0.212 0.242 0.2&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Which set of values, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, is the better indicator of batting ability for the 18 players in our example? In order to answer that question in a precise way one would have to know the ‚Äútrue batting ability‚Äù of each player. This true average we shall designate &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; (the Greek letter theta). Actually it is an unknowable quantity, an abstraction representing the probability that a player will get a hit on any given time at bat. Although &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is unobservable, we have a good approximation to it: the subsequent performance of the batters. It is sufficient to consider just the remainder of the 1970 season, which includes about nine times as much data as the preliminary averages were based on. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 119&lt;/a&gt;)&lt;/span&gt;
Now we have both &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; in the data, let‚Äôs compare their distributions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  pivot_longer(cols = c(y, z)) %&amp;gt;% 
  mutate(label = ifelse(name == &amp;quot;z&amp;quot;, 
                        &amp;quot;the James-Stein estimate&amp;quot;, 
                        &amp;quot;early-season batting average&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = value, y = label)) +
  geom_vline(xintercept = 0.2654321, linetype = 2,
             color = &amp;quot;white&amp;quot;) +
  stat_halfeye(point_interval = median_qi, .width = .5,
               color = navy_blue, fill = alpha(nw_green, 2/3),
               height = 4) +
  labs(x = &amp;quot;batting average&amp;quot;, y = NULL) +
  coord_cartesian(ylim = c(1.25, 5.25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As implied in the formula, the James-Stein estimates are substantially shrunken towards the grand mean, &lt;code&gt;y_bar&lt;/code&gt;. To get a sense of which estimate is better, we can subtract the estimate from &lt;code&gt;theta&lt;/code&gt;, the end of the season batting average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball &amp;lt;-
  baseball %&amp;gt;% 
  mutate(y_error = theta - y,
         z_error = theta - z)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since &lt;code&gt;y_error&lt;/code&gt; and &lt;code&gt;y_error&lt;/code&gt; are error distributions, we prefer values to be as close to zero as possible. Let‚Äôs take a look.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  pivot_longer(y_error:z_error) %&amp;gt;% 
  
  ggplot(aes(x = value, y = name)) +
  geom_vline(xintercept = 0, linetype = 2,
             color = &amp;quot;white&amp;quot;) +
  stat_halfeye(point_interval = median_qi, .width = .5,
               color = navy_blue, fill = alpha(nw_green, 2/3),
               height = 2.5) +
  labs(x = NULL, y = NULL) +
  coord_cartesian(ylim = c(1.25, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The James-Stein errors (i.e., &lt;code&gt;z_error&lt;/code&gt;) are more concentrated toward zero. In the paper, we read: ‚ÄúOne method of evaluating the two estimates is by simply counting their successes and failures. For 16 of the 18 players the James-Stein estimator &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is closer than the observed average &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to the ‚Äòtrue,‚Äô or seasonal, average &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;‚Äù (pp.¬†119‚Äì121). We can compute that with a little &lt;code&gt;ifelse()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  transmute(closer_to_theta = ifelse(abs(y_error) - abs(z_error) == 0, &amp;quot;equal&amp;quot;,
                                     ifelse(abs(y_error) - abs(z_error) &amp;gt; 0, &amp;quot;z&amp;quot;, &amp;quot;y&amp;quot;))) %&amp;gt;% 
  count(closer_to_theta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   closer_to_theta     n
##   &amp;lt;chr&amp;gt;           &amp;lt;int&amp;gt;
## 1 y                   2
## 2 z                  16&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;A more quantitative way of comparing the two techniques is through the total squared error of estimation‚Ä¶ The observed averages &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; have a total squared error of .077, whereas the squared error of the James-Stein estimators is only .022. By this comparison, then, Stein‚Äôs method is 3.5 times as accurate. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 121&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  pivot_longer(y_error:z_error) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  summarise(total_squared_error = sum(value * value))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   name    total_squared_error
##   &amp;lt;chr&amp;gt;                 &amp;lt;dbl&amp;gt;
## 1 y_error              0.0755
## 2 z_error              0.0214&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get the 3.5 value with simple division.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.07548795 / 0.02137602&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.531431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So it does indeed turn out that shrinking each player‚Äôs initial estimate toward the grand mean of those initial estimates does a better job of predicting their end-of-the-season batting averages than using their individual batting averages. To get a sense of what this looks like, let‚Äôs make our own version of the figure on page 121.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  baseball %&amp;gt;% 
    select(y, z, theta, player) %&amp;gt;% 
    gather(key, value, -player) %&amp;gt;% 
    mutate(time = ifelse(key == &amp;quot;theta&amp;quot;, &amp;quot;theta&amp;quot;, &amp;quot;estimate&amp;quot;)),
  baseball %&amp;gt;% 
    select(player, theta) %&amp;gt;% 
    rename(value = theta) %&amp;gt;% 
    mutate(key  = &amp;quot;theta&amp;quot;, 
           time = &amp;quot;theta&amp;quot;)
) %&amp;gt;% 
  mutate(facet = rep(c(&amp;quot;estimate = y&amp;quot;, &amp;quot;estimate = z&amp;quot;), each = n() / 4) %&amp;gt;% rep(., times = 2)) %&amp;gt;% 
  
  ggplot(aes(x = time, y = value, group = player)) +
  geom_hline(yintercept = 0.2654321, linetype = 2,
             color = &amp;quot;white&amp;quot;) +
  geom_line(alpha = 1/2,
            color = nw_green) +
  geom_point(alpha = 1/2,
             color = navy_blue) +
  labs(x = NULL,
       y = &amp;quot;batting average&amp;quot;) +
  theme(axis.ticks.x = element_blank()) +
  facet_wrap(~facet)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The James-Stein estimator works because of its shrinkage, and the shrinkage factor is called &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. Though in the first parts of the paper, Efron and Morris just told us &lt;span class=&#34;math inline&#34;&gt;\(c = .212\)&lt;/span&gt;, they gave the actual formula for &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; a little later on. If you let &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; be the number of means (i.e., the number of clusters), then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[c = 1 - \frac{(k - 3)\sigma^2}{\sum (y - \overline y)^2}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The difficulty of that formula is we don‚Äôt know the value for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. It‚Äôs not the sample variance of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; (i.e., &lt;code&gt;var(y)&lt;/code&gt;). An &lt;a href=&#34;https://stats.stackexchange.com/questions/5727/james-stein-estimator-how-did-efron-and-morris-calculate-sigma2-in-shrinkag&#34;&gt;answer to this stackexchange question&lt;/a&gt; helped clarify Efron and Morris were using the formula for the standard error of the estimate,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sqrt{\hat p(1 - \hat p) / n},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which, in the variance metric, is simply&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat p(1 - \hat p) / n.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Following along, we can compute &lt;code&gt;sigma_squared&lt;/code&gt; like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sigma_squared &amp;lt;- mean(baseball$y) * (1 - mean(baseball$y))  / 45)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004332842&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can reproduce the &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; value from the paper.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  select(player, y:c) %&amp;gt;% 
  mutate(squared_deviation = (y - y_bar)^2) %&amp;gt;%
  summarise(c_by_hand = 1 - ((n() - 3) * sigma_squared / sum(squared_deviation)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   c_by_hand
##       &amp;lt;dbl&amp;gt;
## 1     0.212&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-go-bayesian.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let‚Äôs go Bayesian.&lt;/h2&gt;
&lt;p&gt;This has been fun. But I don‚Äôt recommend you actually use the James-Stein estimator in your research.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The James-Stein estimator is not the only one that is known to be better than the sample averages‚Ä¶&lt;/p&gt;
&lt;p&gt;The search for new estimators continues. Recent efforts [in the 1970s, that is] have been concentrated on achieving results like those obtained with Stein‚Äôs method for problems involving distributions other than the normal distribution. Several lines of work, including Stein‚Äôs and Robbins‚Äô and more formal &lt;em&gt;Bayesian methods&lt;/em&gt; seem to be converging on a powerful general theory of parameter estimation. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 127&lt;/a&gt;, &lt;em&gt;emphasis&lt;/em&gt; added)&lt;/span&gt;
The James-Stein estimator is not Bayesian, but it is a precursor to the kind of analyses we now do with Bayesian multilevel models, which pool cluster-level means toward a grand mean. To get a sense of this, we‚Äôll fit a couple models. First, let‚Äôs load the &lt;strong&gt;brms&lt;/strong&gt; package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I typically work with the linear regression paradigm. If we were to analyze the &lt;code&gt;baseball&lt;/code&gt; data, we‚Äôd use an aggregated binomial mode, which is a particular kind of logistic regression. You can learn more about it &lt;a href=&#34;https://www.youtube.com/watch?v=DyrUkqK9Tj4&amp;amp;t=1581s&amp;amp;frags=pl%2Cwn&#34;&gt;here&lt;/a&gt;. If we wanted a model that corresponded to the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; estimates, above, we‚Äôd use &lt;code&gt;hits&lt;/code&gt; as the criterion and allow each player to get his own &lt;em&gt;separate&lt;/em&gt; estimate. Since we‚Äôre working within the Bayesian paradigm, we also need to assign priors. In this case, we‚Äôll use a weakly-regularizing &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, 1.5)\)&lt;/span&gt; on the intercepts. See &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;this wiki&lt;/a&gt; for more on weakly-regularizing priors.&lt;/p&gt;
&lt;p&gt;Here‚Äôs the code to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_y &amp;lt;-
  brm(data = baseball, 
      family = binomial,
      hits | trials(45) ~ 0 + player,
      prior(normal(0, 1.5), class = b),
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you were curious, that model followed the statistical formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{hits}_i &amp;amp; \sim \operatorname{Binomial} (n = 45, p_i) \\
\operatorname{logit}(p_i) &amp;amp; = \alpha_\text{player} \\
\alpha_\text{player}      &amp;amp; \sim \operatorname{Normal}(0, 1.5),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; is the probability of player &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_\text{player}\)&lt;/span&gt; is a vector of &lt;span class=&#34;math inline&#34;&gt;\(\text{player}\)&lt;/span&gt;-specific intercepts from within the logistic regression model, and each of those intercepts are given a &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, 1.5)\)&lt;/span&gt; prior on the log-odds scale. (If this is all new and confusing, don‚Äôt worry. I‚Äôll recommended some resources at the end of this post.)&lt;/p&gt;
&lt;p&gt;For our analogue to the James-Stein estimate &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, we‚Äôll fit the multilevel version of that last model. While each player still gets his own estimate, those estimates are now partially-pooled toward the grand mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_z &amp;lt;-
  brm(data = baseball, 
      family = binomial,
      hits | trials(45) ~ 1 + (1 | player),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = sd)),
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That model followed the statistical formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{hits}_i &amp;amp; \sim \operatorname{Binomial}(n = 45, p_i) \\
\operatorname{logit}(p_i) &amp;amp; = \alpha + \alpha_\text{player} \\
\alpha               &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\ 
\alpha_\text{player} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{player}) \\
\sigma_\text{player} &amp;amp; \sim \operatorname{HalfNormal}(0, 1.5),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is the grand mean among the &lt;span class=&#34;math inline&#34;&gt;\(\text{player}\)&lt;/span&gt;-specific intercepts, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_\text{player}\)&lt;/span&gt; is the vector of &lt;span class=&#34;math inline&#34;&gt;\(\text{player}\)&lt;/span&gt;-specific deviations from the grand mean, which are Normally distributed with a mean of zero and a standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{player}\)&lt;/span&gt;, which is estimated from the data.&lt;/p&gt;
&lt;p&gt;Here are the model summaries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_y$fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: c40df95ccb9776814804fa2fdc850e74.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##                      mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b_playerAlvis       -1.62    0.00 0.39  -2.42  -1.86  -1.61  -1.35  -0.89  8659 1.00
## b_playerBerry       -0.77    0.00 0.31  -1.40  -0.98  -0.77  -0.56  -0.19  8866 1.00
## b_playerCampaneris  -1.35    0.00 0.36  -2.08  -1.58  -1.33  -1.11  -0.67  9745 1.00
## b_playerClemente    -0.40    0.00 0.31  -1.02  -0.60  -0.40  -0.21   0.20 11477 1.00
## b_playerERodriguez  -1.22    0.00 0.35  -1.92  -1.45  -1.20  -0.99  -0.57  8931 1.00
## b_playerFHoward     -0.58    0.00 0.31  -1.20  -0.78  -0.57  -0.37   0.01  8294 1.00
## b_playerFRobinson   -0.49    0.00 0.30  -1.09  -0.70  -0.49  -0.28   0.09  8854 1.00
## b_playerJohnstone   -0.68    0.00 0.32  -1.31  -0.89  -0.67  -0.46  -0.09  7975 1.00
## b_playerKessinger   -0.89    0.00 0.33  -1.54  -1.11  -0.88  -0.66  -0.26  8191 1.00
## b_playerLAlvarado   -0.99    0.00 0.32  -1.64  -1.20  -0.98  -0.76  -0.39  8147 1.00
## b_playerMunson      -1.48    0.00 0.37  -2.25  -1.72  -1.47  -1.23  -0.78  9573 1.00
## b_playerPetrocelli  -1.22    0.00 0.36  -1.95  -1.45  -1.21  -0.97  -0.55  9342 1.00
## b_playerSanto       -1.09    0.00 0.32  -1.75  -1.31  -1.09  -0.88  -0.49  8616 1.00
## b_playerScott       -1.22    0.00 0.35  -1.92  -1.44  -1.20  -0.98  -0.58 10266 1.00
## b_playerSpencer     -0.77    0.00 0.32  -1.42  -0.98  -0.77  -0.56  -0.16  9060 1.00
## b_playerSwoboda     -1.10    0.00 0.33  -1.79  -1.31  -1.09  -0.87  -0.47  9435 1.00
## b_playerUnser       -1.21    0.00 0.34  -1.89  -1.44  -1.21  -0.97  -0.57  8065 1.00
## b_playerWilliams    -1.22    0.00 0.36  -1.94  -1.46  -1.21  -0.98  -0.56  7829 1.00
## lp__               -73.41    0.08 3.01 -80.13 -75.25 -73.08 -71.26 -68.43  1285 1.01
## 
## Samples were drawn using NUTS(diag_e) at Wed Apr 21 16:39:19 2021.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_z$fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: 668801c962bccfca26059b8ea6970ac4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##                                   mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b_Intercept                      -1.02    0.00 0.09  -1.20  -1.08  -1.02  -0.96  -0.83  3915    1
## sd_player__Intercept              0.17    0.00 0.11   0.01   0.08   0.15   0.24   0.42  1542    1
## r_player[Alvis,Intercept]        -0.12    0.00 0.19  -0.62  -0.21  -0.06   0.00   0.14  2305    1
## r_player[Berry,Intercept]         0.04    0.00 0.16  -0.27  -0.04   0.02   0.12   0.42  4742    1
## r_player[Campaneris,Intercept]   -0.07    0.00 0.17  -0.51  -0.14  -0.04   0.02   0.22  3840    1
## r_player[Clemente,Intercept]      0.14    0.00 0.19  -0.13   0.00   0.08   0.23   0.62  2526    1
## r_player[E.Rodriguez,Intercept]  -0.05    0.00 0.16  -0.43  -0.13  -0.02   0.03   0.24  3733    1
## r_player[F.Howard,Intercept]      0.09    0.00 0.17  -0.19  -0.01   0.05   0.17   0.53  3200    1
## r_player[F.Robinson,Intercept]    0.11    0.00 0.18  -0.15   0.00   0.07   0.20   0.55  3344    1
## r_player[Johnstone,Intercept]     0.07    0.00 0.17  -0.23  -0.02   0.03   0.14   0.49  3796    1
## r_player[Kessinger,Intercept]     0.02    0.00 0.16  -0.30  -0.06   0.01   0.09   0.37  4818    1
## r_player[L.Alvarado,Intercept]    0.00    0.00 0.15  -0.34  -0.07   0.00   0.07   0.32  4687    1
## r_player[Munson,Intercept]       -0.10    0.00 0.18  -0.56  -0.19  -0.05   0.01   0.18  3442    1
## r_player[Petrocelli,Intercept]   -0.05    0.00 0.16  -0.44  -0.13  -0.02   0.03   0.26  5131    1
## r_player[Santo,Intercept]        -0.03    0.00 0.16  -0.39  -0.10  -0.01   0.05   0.30  4883    1
## r_player[Scott,Intercept]        -0.05    0.00 0.17  -0.45  -0.12  -0.02   0.04   0.26  4392    1
## r_player[Spencer,Intercept]       0.05    0.00 0.16  -0.24  -0.03   0.02   0.13   0.42  4149    1
## r_player[Swoboda,Intercept]      -0.02    0.00 0.16  -0.38  -0.10  -0.01   0.05   0.30  5432    1
## r_player[Unser,Intercept]        -0.04    0.00 0.17  -0.45  -0.12  -0.02   0.04   0.26  4419    1
## r_player[Williams,Intercept]     -0.05    0.00 0.16  -0.42  -0.12  -0.02   0.04   0.27  5162    1
## lp__                            -73.87    0.12 4.09 -82.44 -76.44 -73.79 -71.06 -66.48  1112    1
## 
## Samples were drawn using NUTS(diag_e) at Wed Apr 21 16:39:41 2021.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you‚Äôre new to aggregated binomial or logistic regression, those estimates might be confusing. For technical reasons‚Äìsee &lt;a href=&#34;https://www.youtube.com/watch?v=DyrUkqK9Tj4&amp;amp;t=1430s&amp;amp;frags=pl%2Cwn&#34;&gt;here&lt;/a&gt;‚Äì, they‚Äôre in the log-odds metric. But we can use the &lt;code&gt;brms::inv_logit_scaled()&lt;/code&gt; function to convert them back to a probability metric. &lt;em&gt;Why would we want a probability metric?&lt;/em&gt;, you might ask. As it turns out, batting average is in a probability metric, too. So you might also think of the &lt;code&gt;inv_logit_scaled()&lt;/code&gt; function as turning the model results into a batting-average metric. For example, if we wanted to get the estimated batting average for E. Rodriguez based on the &lt;code&gt;y_fit&lt;/code&gt; model (i.e., the model corresponding to the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; estimator), we might do something like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit_y)[&amp;quot;playerERodriguez&amp;quot;, 1] %&amp;gt;% 
  inv_logit_scaled()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2282195&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To double check the model returned a sensible estimate, here‚Äôs the corresponding &lt;code&gt;y&lt;/code&gt; value from the &lt;code&gt;baseball&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  filter(player == &amp;quot;E Rodriguez&amp;quot;) %&amp;gt;% 
  select(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##       y
##   &amp;lt;dbl&amp;gt;
## 1 0.222&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It‚Äôs a little off, but in the right ballpark. Here is the corresponding estimate from the multilevel model, &lt;code&gt;fit_z&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(fit_z)$player[&amp;quot;E Rodriguez&amp;quot;, 1, ] %&amp;gt;% inv_logit_scaled()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2555493&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And indeed that‚Äôs pretty close to the &lt;code&gt;z&lt;/code&gt; value from the &lt;code&gt;baseball&lt;/code&gt; data, too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  filter(player == &amp;quot;E Rodriguez&amp;quot;) %&amp;gt;% 
  select(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##       z
##   &amp;lt;dbl&amp;gt;
## 1 0.256&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now we have these too competing ways to model the data of the first 45 times at bat, let‚Äôs see how well their estimates predict the &lt;code&gt;true_ba&lt;/code&gt; values. We‚Äôll do so with a couple plots. This first one is of the single-level model which did not pool the batting averages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get the `fitted()` draws and wrangle a bit
f_y &amp;lt;-
  baseball %&amp;gt;% 
  distinct(player) %&amp;gt;% 
  add_fitted_draws(fit_y, dpar = &amp;quot;mu&amp;quot;) %&amp;gt;% 
  left_join(baseball %&amp;gt;% 
              select(player, true_ba))
  
# save the plot
p1 &amp;lt;-
  f_y %&amp;gt;% 
  ggplot(aes(x = mu, y = reorder(player, true_ba))) +
  geom_vline(xintercept = mean(baseball$true_ba), color = &amp;quot;white&amp;quot;) +
  stat_interval(.width = .95, alpha = 1/3, color = nw_green) +
  stat_interval(.width = .50, alpha = 1/3, color = nw_green) +
  geom_point(data = baseball,
             aes(x = true_ba),
             size = 2, alpha = 3/4,
             color = navy_blue) +
  labs(subtitle = &amp;quot;fit_y, the no pooling model&amp;quot;,
       x = &amp;quot;batting average&amp;quot;, 
       y = NULL) +
  coord_cartesian(xlim = c(0, .6)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note our use of some handy convenience functions (i.e., &lt;code&gt;add_fitted_draws()&lt;/code&gt; and &lt;code&gt;stat_interval()&lt;/code&gt;) from the &lt;a href=&#34;https://github.com/mjskay/tidybayes&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This second plot is almost the same as the previous one, but this time based on the partial-pooling multilevel model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f_z &amp;lt;-
  baseball %&amp;gt;% 
  distinct(player) %&amp;gt;% 
  add_fitted_draws(fit_z, dpar = &amp;quot;mu&amp;quot;) %&amp;gt;% 
  left_join(baseball %&amp;gt;% 
              select(player, true_ba))
p2 &amp;lt;-
  f_z %&amp;gt;% 
  ggplot(aes(x = mu, y = reorder(player, true_ba))) +
  geom_vline(xintercept = mean(baseball$true_ba), color = &amp;quot;white&amp;quot;) +
  stat_interval(.width = .95, alpha = 1/3, color = nw_green) +
  stat_interval(.width = .50, alpha = 1/3, color = nw_green) +
  geom_point(data = baseball,
             aes(x = true_ba),
             size = 2, alpha = 3/4,
             color = navy_blue) +
  labs(subtitle = &amp;quot;fit_z, the multilevel pooling model&amp;quot;,
       x = &amp;quot;batting average&amp;quot;, 
       y = NULL) +
  coord_cartesian(xlim = c(0, .6)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we join them together with &lt;strong&gt;patchwork&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-patchwork&#34; role=&#34;doc-biblioref&#34;&gt;Pedersen, 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)
p1 | p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In both panels, the end-of-the-season batting averages (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) are the blue dots. The model-implied estimates are depicted by 95% and 50% interval bands (i.e., the lighter and darker green horizontal lines, respectively). The white line in the background marks off the mean of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Although neither model was perfect, the multilevel model, our analogue to the James-Stein estimates, yielded predictions that appear both more valid and more precise.&lt;/p&gt;
&lt;p&gt;We might also compare the models by their prediction errors. Here we‚Äôll subtract the end-of-the-season batting averages from the model estimates. But unlike with &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt; estimates, above, our &lt;code&gt;fit_y&lt;/code&gt; and &lt;code&gt;fit_z&lt;/code&gt; models yielded entire posterior distributions. Therefore, we‚Äôll express our prediction errors in terms of error distributions, rather than single values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# save the `fit_y` plot
p3 &amp;lt;-
  f_y %&amp;gt;% 
  # the error distribution is just the model-implied values minus 
  # the true end-of-season values
  mutate(error = mu - true_ba)  %&amp;gt;% 
  ggplot(aes(x = error, y = reorder(player, true_ba))) +
  geom_vline(xintercept = c(0, -.2, .2), size = c(1/2, 1/4, 1/4), 
             linetype = c(1, 3, 3), color = &amp;quot;white&amp;quot;) +
  stat_halfeye(point_interval = mean_qi, .width = .95,
               color = navy_blue, fill = alpha(nw_green, 2/3)) +
  coord_cartesian(xlim = c(-.35, .35)) +
  labs(subtitle = &amp;quot;fit_y, the no pooling model&amp;quot;,
       x = &amp;quot;error&amp;quot;, 
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(hjust = .5))
# save the `fit_z` plot
p4 &amp;lt;-
  f_z %&amp;gt;%   
  mutate(error = mu - true_ba)  %&amp;gt;% 
  
  ggplot(aes(x = error, y = reorder(player, true_ba))) +
  geom_vline(xintercept = c(0, -.2, .2), size = c(1/2, 1/4, 1/4), 
             linetype = c(1, 3, 3), color = &amp;quot;white&amp;quot;) +
  stat_halfeye(point_interval = mean_qi, .width = .95,
               color = navy_blue, fill = alpha(nw_green, 2/3)) +
  coord_cartesian(xlim = c(-.35, .35)) +
  labs(subtitle = &amp;quot;fit_z, the multilevel pooling model&amp;quot;,
       x = &amp;quot;error&amp;quot;, 
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(hjust = .5))
# now combine the two and behold
p3 | p4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For consistency, I‚Äôve ordered the players along the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis the same as above. In both panels, we see the prediction error distribution for each player in green and then summarize those distributions in terms of their means and percentile-based 95% intervals. Since these are error distributions, we prefer them to be as close to zero as possible. Although neither model made perfect predictions, the overall errors in the multilevel model were clearly smaller. Much like with the James-Stein estimator, the partial pooling of the multilevel model made for better end-of-the-season estimates.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The paradoxical [consequence of Bayesian multilevel models] is that [they can contradict] this elementary law of statistical theory. If we have [two] or more baseball players, and if we are interested in predicting future batting averages for each of them, then [the Bayesian multilevel model can be better] than simply extrapolating from [the] separate averages. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977&lt;/a&gt;.)&lt;/span&gt;, p.¬†119]
This is another example of how the &lt;a href=&#34;https://en.wikipedia.org/wiki/KISS_principle&#34;&gt;KISS principle&lt;/a&gt; isn‚Äôt always the best bet with data analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;If you‚Äôre new to logistic regression, multilevel models or Bayesian statistics, I recommend any of the following texts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;either edition of McElreath‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;&lt;em&gt;Statistical rethinking&lt;/em&gt;&lt;/a&gt;, both editions for which I have &lt;strong&gt;brms&lt;/strong&gt; translations for &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingBrms2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;, &lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2020&#34; role=&#34;doc-biblioref&#34;&gt;2020c&lt;/a&gt;)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Kruschke‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;&lt;em&gt;Doing Bayesian data analysis&lt;/em&gt;&lt;/a&gt;, for which I have a &lt;strong&gt;brms&lt;/strong&gt; translation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020b&lt;/a&gt;)&lt;/span&gt;; or&lt;/li&gt;
&lt;li&gt;Gelman and Hill‚Äôs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanDataAnalysisUsing2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://stat.columbia.edu/~gelman/arm/&#34;&gt;&lt;em&gt;Data analysis using regression and multilevel/hierarchical models&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you choose &lt;em&gt;Statistical rethinking&lt;/em&gt;, do check out &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;these great lectures&lt;/a&gt; on the text.&lt;/p&gt;
&lt;p&gt;Also, don‚Äôt miss the provocative &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-davis-stoberWhenAreSample2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; preprint by Davis-Stober, Dana and Rouder, &lt;a href=&#34;https://osf.io/2ukxj/&#34;&gt;&lt;em&gt;When are sample means meaningful? The role of modern estimation in psychological science&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] patchwork_1.1.1 brms_2.15.0     Rcpp_1.0.6      tidybayes_2.3.1 forcats_0.5.1   stringr_1.4.0  
##  [7] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3  
## [13] tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] splines_4.0.4        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   sandwich_3.0-0       xts_0.12.1          
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] viridisLite_0.3.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3       
##  [53] arrayhelpers_1.1-0   ellipsis_0.3.1       pkgconfig_2.0.3      loo_2.4.1           
##  [57] farver_2.0.3         dbplyr_2.0.0         utf8_1.1.4           tidyselect_1.1.0    
##  [61] labeling_0.4.2       rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1       
##  [65] munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [69] generics_0.1.0       broom_0.7.5          ggridges_0.5.2       evaluate_0.14       
##  [73] fastmap_1.0.1        yaml_2.2.1           processx_3.4.5       knitr_1.31          
##  [77] fs_1.5.0             nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [81] xml2_1.3.2           compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [85] rstudioapi_0.13      gamm4_0.2-6          curl_4.3             reprex_0.3.0        
##  [89] statmod_1.4.35       stringi_1.5.3        highr_0.8            ps_1.6.0            
##  [93] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2        
##  [97] nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
## [101] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3    
## [105] httpuv_1.5.4         R6_2.5.0             bookdown_0.21        promises_1.1.1      
## [109] gridExtra_2.3        codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0  
## [113] MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1     withr_2.4.1         
## [117] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [125] rmarkdown_2.7        shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3     
## [129] dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1‚Äì28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395‚Äì411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
B√ºrkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ‚Äô&lt;span&gt;Stan&lt;/span&gt;‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-davis-stoberWhenAreSample2017&#34; class=&#34;csl-entry&#34;&gt;
Davis-Stober, C., Dana, J., &amp;amp; Rouder, J. (2017). &lt;em&gt;When are sample means meaningful? &lt;span&gt;The&lt;/span&gt; role of modern estimation in psychological science&lt;/em&gt;. &lt;span&gt;OSF Preprints&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.31219/osf.io/2ukxj&#34;&gt;https://doi.org/10.31219/osf.io/2ukxj&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-efronEmpiricalBayesJamesStein2010&#34; class=&#34;csl-entry&#34;&gt;
Efron, B. (2010). Empirical &lt;span&gt;Bayes&lt;/span&gt; and the &lt;span&gt;James&lt;/span&gt;-&lt;span&gt;Stein&lt;/span&gt; estimator. In &lt;em&gt;Large-scale inference: &lt;span&gt;Empirical Bayes&lt;/span&gt; methods for estimation, testing, and prediction&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://statweb.stanford.edu/~ckirby/brad/LSI/monograph_CUP.pdf&#34;&gt;https://statweb.stanford.edu/~ckirby/brad/LSI/monograph_CUP.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-efronSteinParadoxStatistics1977&#34; class=&#34;csl-entry&#34;&gt;
Efron, B., &amp;amp; Morris, C. (1977). Stein‚Äôs paradox in statistics. &lt;em&gt;Scientific American&lt;/em&gt;, &lt;em&gt;236&lt;/em&gt;(5), 119‚Äì127. &lt;a href=&#34;https://doi.org/10.1038/scientificamerican0577-119&#34;&gt;https://doi.org/10.1038/scientificamerican0577-119&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanDataAnalysisUsing2006&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., &amp;amp; Hill, J. (2006). &lt;em&gt;Data analysis using regression and multilevel/hierarchical models&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9780511790942&#34;&gt;https://doi.org/10.1017/CBO9780511790942&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ‚Äôgeoms‚Äô for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingBrms2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Statistical rethinking with brms, &lt;span class=&#34;nocase&#34;&gt;ggplot2&lt;/span&gt;, and the tidyverse&lt;/em&gt; (version 1.2.0). &lt;a href=&#34;https://doi.org/10.5281/zenodo.3693202&#34;&gt;https://doi.org/10.5281/zenodo.3693202&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzDoingBayesianData2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis in brms and the tidyverse&lt;/em&gt; (version 0.3.0). &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020c). &lt;em&gt;Statistical rethinking with brms, Ggplot2, and the tidyverse: &lt;span&gt;Second&lt;/span&gt; edition&lt;/em&gt; (version 0.1.1). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-patchwork&#34; class=&#34;csl-entry&#34;&gt;
Pedersen, T. L. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;patchwork&lt;/span&gt;: &lt;span&gt;The&lt;/span&gt; composer of plots&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;https://CRAN.R-project.org/package=patchwork&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ‚Äôtidyverse‚Äô&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., Fran√ßois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., M√ºller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., ‚Ä¶ Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

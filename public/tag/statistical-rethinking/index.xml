<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistical Rethinking | Fahim Ahmad</title>
    <link>/tag/statistical-rethinking/</link>
      <atom:link href="/tag/statistical-rethinking/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistical Rethinking</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Fahim Ahmad (2020)</copyright><lastBuildDate>Wed, 09 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Statistical Rethinking</title>
      <link>/tag/statistical-rethinking/</link>
    </image>
    
    <item>
      <title>Multilevel models and the index-variable approach</title>
      <link>/post/2020-12-09-multilevel-models-and-the-index-variable-approach/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-12-09-multilevel-models-and-the-index-variable-approach/</guid>
      <description>
&lt;script src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The set-up&lt;/h2&gt;
&lt;p&gt;PhD candidate Huaiyu Liu recently reached out with a question about how to analyze clustered data. Liu’s basic setup was an experiment with four conditions. The dependent variable was binary, where success = 1, fail = 0. Each participant completed multiple trials under each of the four conditions. The catch was Liu wanted to model those four conditions with a multilevel model using the index-variable approach McElreath advocated for in the second edition of his text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Like any good question, this one got my gears turning. Thanks, Liu! The purpose of this post will be to show how to model data like this two different ways.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;In this post, I’m presuming you are familiar with Bayesian multilevel models and with logistic regression. All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. The statistical models will be fit with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. We’ll also make a little use of the &lt;strong&gt;tidybayes&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;rethinking&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-rethinking&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020b&lt;/a&gt;)&lt;/span&gt; packages. If you need to shore up, I list some educational resources at the &lt;a href=&#34;#next-steps&#34;&gt;end of the post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Load the primary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)
library(tidybayes)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for Liu’s question had the same basic structure as the &lt;code&gt;chimpanzees&lt;/code&gt; data from the &lt;strong&gt;rethinking&lt;/strong&gt; package. Happily, it’s also the case that Liu wanted to fit a model that was very similar to model &lt;code&gt;m14.3&lt;/code&gt; from Chapter 14 of McElreath’s text. Here we’ll load the data and wrangle a little.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(chimpanzees, package = &amp;quot;rethinking&amp;quot;)
d &amp;lt;- chimpanzees
rm(chimpanzees)

# wrangle
d &amp;lt;-
  d %&amp;gt;% 
  mutate(actor = factor(actor),
         treatment = factor(1 + prosoc_left + 2 * condition),
         # this will come in handy, later
         labels    = factor(treatment,
                            levels = 1:4,
                            labels = c(&amp;quot;r/n&amp;quot;, &amp;quot;l/n&amp;quot;, &amp;quot;r/p&amp;quot;, &amp;quot;l/p&amp;quot;)))

glimpse(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 504
## Columns: 10
## $ actor        &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ recipient    &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ condition    &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ block        &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5,…
## $ trial        &amp;lt;int&amp;gt; 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 4…
## $ prosoc_left  &amp;lt;int&amp;gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,…
## $ chose_prosoc &amp;lt;int&amp;gt; 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,…
## $ pulled_left  &amp;lt;int&amp;gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,…
## $ treatment    &amp;lt;fct&amp;gt; 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1,…
## $ labels       &amp;lt;fct&amp;gt; r/n, r/n, l/n, r/n, l/n, l/n, l/n, l/n, r/n, r/n, r/n, l/n, r/n, l/n, r/n, l/…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The focal variable will be &lt;code&gt;pulled_left&lt;/code&gt;, which is binary and coded yes = 1, no = 0. We have four experimental conditions, which are indexed &lt;code&gt;1&lt;/code&gt; through &lt;code&gt;4&lt;/code&gt; in the &lt;code&gt;treatment&lt;/code&gt; variable. The shorthand labels for those conditions are saved as &lt;code&gt;labels&lt;/code&gt;. These data are simple in that there are only seven participants, who are indexed in the &lt;code&gt;actor&lt;/code&gt; column.&lt;/p&gt;
&lt;p&gt;Within the generalized linear model framework, we typically model binary variables with binomial likelihood&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. When you use the conventional link function, you can call this &lt;em&gt;logistic regression&lt;/em&gt;. When you have a binary variable, the parameter of interest is the probability of a 1 in your criterion variable. When you want a quick sample statistic, you can estimate those probabilities with the mean. To get a sense of the data, here are the sample probabilities &lt;code&gt;pulled_left == 1&lt;/code&gt; for each of our seven participants, by the four levels of &lt;code&gt;treatment&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  mutate(treatment = str_c(&amp;quot;treatment &amp;quot;, treatment)) %&amp;gt;% 
  group_by(actor, treatment) %&amp;gt;% 
  summarise(p = mean(pulled_left) %&amp;gt;% round(digits = 2)) %&amp;gt;% 
  pivot_wider(values_from = p, names_from = treatment) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;actor&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models&lt;/h2&gt;
&lt;p&gt;We are going to analyze these data two kinds of multilevel models. The first way is the direct analogue to McElreath’s model &lt;code&gt;m14.3&lt;/code&gt;; it’ll be a multilevel model using the index-variable approach for the population-level intercepts. The second way is a multilevel Bayesian alternative to the ANOVA, based on Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text.&lt;/p&gt;
&lt;p&gt;However, some readers might benefit from a review of what I even mean by the “index-variable” approach. This approach is uncommon in my field of clinical psychology, for example. So before we get down to business, we’ll clear that up by contrasting it with the widely-used dummy-variable approach.&lt;/p&gt;
&lt;div id=&#34;warm-up-with-the-simple-index-variable-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Warm-up with the simple index-variable model.&lt;/h3&gt;
&lt;p&gt;Let’s forget the multilevel model for a moment. One of the more popular ways to use a categorical predictor variable is with the dummy-variable approach. Say we wanted to predict our criterion variable &lt;code&gt;pulled_left&lt;/code&gt; with &lt;code&gt;treatment&lt;/code&gt;, which is a four-category nominal variable. If we denote the number of categories &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;, &lt;code&gt;treatment&lt;/code&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(K = 4\)&lt;/span&gt; nominal variable. The dummy-variable approach would be to break &lt;code&gt;treatment&lt;/code&gt; into &lt;span class=&#34;math inline&#34;&gt;\(K - 1\)&lt;/span&gt; binary variables, which we’d simultaneously enter into the model. Say we broke &lt;code&gt;treatment&lt;/code&gt; into three dummies with the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;% 
  mutate(d2 = if_else(treatment == 2, 1, 0),
         d3 = if_else(treatment == 3, 1, 0),
         d4 = if_else(treatment == 4, 1, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dummy variables &lt;code&gt;d2&lt;/code&gt;, &lt;code&gt;d3&lt;/code&gt;, and &lt;code&gt;d4&lt;/code&gt; would capture the four levels of &lt;code&gt;treatment&lt;/code&gt; like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  distinct(treatment, d2, d3, d4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   treatment d2 d3 d4
## 1         1  0  0  0
## 2         2  1  0  0
## 3         3  0  1  0
## 4         4  0  0  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;d2 == 1&lt;/code&gt; only when &lt;code&gt;treatment == 2&lt;/code&gt;. Similarly, &lt;code&gt;d3 == 1&lt;/code&gt; only when &lt;code&gt;treatment == 3&lt;/code&gt; and &lt;code&gt;d4 == 1&lt;/code&gt; only when &lt;code&gt;treatment == 4&lt;/code&gt;. When &lt;code&gt;treatment == 1&lt;/code&gt;, all three dummies are &lt;code&gt;0&lt;/code&gt;, which makes &lt;code&gt;treatment == 1&lt;/code&gt; the reference category.&lt;/p&gt;
&lt;p&gt;You can write out the statistical model using these &lt;span class=&#34;math inline&#34;&gt;\(K - 1\)&lt;/span&gt; dummies as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \beta_0 + \beta_1 \text{d2}_i + \beta_2 \text{d3}_i + \beta_3 \text{d4}_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is both the “intercept” and the expected value for the first level of &lt;code&gt;treatment&lt;/code&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the expected change in value, relative to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, for the second level of &lt;code&gt;treatment&lt;/code&gt;. In the same way, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; are changes relative to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; for the third and fourth levels of &lt;code&gt;treatment&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;The index-variable approach takes a different stance. Rather than dividing &lt;code&gt;treatment&lt;/code&gt; into dummies, one simply allows each level of &lt;code&gt;treatment&lt;/code&gt; to have its own intercept. You can write that in statistical notation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \gamma_{\text{treatment}[i]},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\text{treatment}[i]\)&lt;/span&gt; subscript indicates the different levels of &lt;code&gt;treatment&lt;/code&gt;, which vary across cases &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, each get their own &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; parameter. Because &lt;code&gt;treatment&lt;/code&gt; has four levels, we end up with four &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;’s: &lt;span class=&#34;math inline&#34;&gt;\(\gamma_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_3\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_4\)&lt;/span&gt;. When you model intercepts in this way, none of the levels of &lt;code&gt;treatment&lt;/code&gt; end up as the reference category and none of the other levels of &lt;code&gt;treatment&lt;/code&gt; are parameterized in terms of deviations from the reference category. Each intercept is estimated in its own terms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick note on notation&lt;/strong&gt;: There’s nothing special about using the letter &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; for our index variable. We could just as easily have used &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;, or whatever. The only reason I’m using &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;, here, is because that’s what McElreath used for his model &lt;code&gt;m14.3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you’d like more practice with dummy variables, McElreath lectured on them &lt;a href=&#34;https://www.youtube.com/watch?v=e0tO64mtYMU&amp;amp;feature=youtu.be&amp;amp;t=3360&#34;&gt;here&lt;/a&gt;. If you’d like to hear McElreath walk out index variables a bit more, you can find that lecture &lt;a href=&#34;https://youtu.be/l_7yIUqWBmE?t=83&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mcelreaths-approach.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;McElreath’s approach.&lt;/h3&gt;
&lt;p&gt;Okay, now we’re up to speed on what Liu meant by wanting to fit a model with the index-variable approach, let’s see what that looks like in a multilevel model.&lt;/p&gt;
&lt;div id=&#34;the-statistical-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The statistical model.&lt;/h4&gt;
&lt;p&gt;Here’s how we might express McElreath’s index-variable approach to these data in statistical notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \gamma_{\text{treatment}[i]} + \alpha_{\text{actor}[i], \text{treatment}[i]} \\
\gamma_j &amp;amp; \sim \operatorname{Normal}(0, 1), \;\;\; \text{for } j = 1, \dots, 4 \\
\begin{bmatrix} \alpha_{j, 1} \\ \alpha_{j, 2} \\ \alpha_{j, 3} \\ \alpha_{j, 4} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \begin{pmatrix} \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \mathbf \Sigma_\text{actor} \end{pmatrix} \\
\mathbf \Sigma_\text{actor} &amp;amp; = \mathbf{S_\alpha R_\alpha S_\alpha} \\
\sigma_{\alpha, [1]}, \dots, \sigma_{\alpha, [4]} &amp;amp; \sim \operatorname{Exponential}(1) \\
\mathbf R_\alpha &amp;amp; \sim \operatorname{LKJ}(2).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this model, we have four population-level intercepts, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_1, \dots, \gamma_4\)&lt;/span&gt;, one for each of the four levels of &lt;code&gt;treatment&lt;/code&gt;. This is one of the critical features required by Liu’s question. &lt;code&gt;actor&lt;/code&gt; is our higher-level grouping variable. The third line spells out the priors for those four &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;’s. Though they all get the same prior in this model, you could use different priors for each, if you wanted.&lt;/p&gt;
&lt;p&gt;Going back to the second line, the term &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{actor}[i], \text{treatment}[i]}\)&lt;/span&gt; is meant to convey that each of the &lt;code&gt;treatment&lt;/code&gt; effects can vary by &lt;code&gt;actor&lt;/code&gt;. We can–and should–do this because each of our participants experienced each of the four levels of &lt;code&gt;treatment&lt;/code&gt; many times. The fourth line containing the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{MVNormal}(\cdot)\)&lt;/span&gt; operator might look intimidating. The vector on the left is just a way to list those four &lt;code&gt;actor&lt;/code&gt;-level deviations we just mentioned. We’ll be treating them much the same way you might treat a random intercept and slope in a multilevel growth model. That is, we presume they follow a multivariate normal distribution. Since these are all deviations, the 4-dimensional mean vector in our multivariate normal distribution contains four zeros. The spread around those zeros are controlled by the variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_\text{actor}\)&lt;/span&gt;. In the next line, we learn that &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_\text{actor}\)&lt;/span&gt; can be decomposed into two terms, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S_\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. It may not yet be clear by the notation, but &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S_\alpha\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; diagonal matrix of standard deviations,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf S_\alpha = \begin{bmatrix} \sigma_{\alpha, [1]} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_{\alpha, [2]} &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; \sigma_{\alpha, [3]} &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \sigma_{\alpha, [4]} \end{bmatrix}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a similar way, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; correlation matrix,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf R_\alpha = \begin{bmatrix} 1 &amp;amp; \rho_{\alpha, [1, 2]} &amp;amp; \rho_{\alpha, [1, 3]} &amp;amp; \rho_{\alpha, [1, 4]} \\ \rho_{\alpha, [2, 1]} &amp;amp; 1 &amp;amp; \rho_{\alpha, [2, 3]} &amp;amp; \rho_{\alpha, [2, 4]} \\ \rho_{\alpha, [3, 1]} &amp;amp; \rho_{\alpha, [3, 2]} &amp;amp; 1 &amp;amp; \rho_{\alpha, [3, 4]} \\ \rho_{\alpha, [4, 1]} &amp;amp; \rho_{\alpha, [4, 2]} &amp;amp; \rho_{\alpha, [4, 3]} &amp;amp; 1 \end{bmatrix}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As we see in the sixth line, all the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\alpha\)&lt;/span&gt; parameters have individual &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Exponential}(1)\)&lt;/span&gt; priors. The final line shows the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt; matrix has the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{LKJ}(2)\)&lt;/span&gt; prior. Though you could certainly use different priors, here we’re sticking close to those McElreath used in his text.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fit the model.&lt;/h4&gt;
&lt;p&gt;Though the statistical model might look intimidating, we can fit it pretty easily with &lt;code&gt;brms::brm()&lt;/code&gt;. We’ll call this &lt;code&gt;fit1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- 
  brm(data = d, 
      family = binomial,
      pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor),
      prior = c(prior(normal(0, 1), class = b),
                prior(exponential(1), class = sd),
                prior(lkj(2), class = cor)),
      cores = 4, seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From a syntax perspective, the important parts were the two occurrences of &lt;code&gt;0 + treatment&lt;/code&gt; in the model &lt;code&gt;formula&lt;/code&gt; line. The first occurrence was how we told &lt;strong&gt;brms&lt;/strong&gt; we wanted our population-level intercept to be indexed by the four levels of &lt;code&gt;treatment&lt;/code&gt;. The second occurrence was where we told &lt;strong&gt;brms&lt;/strong&gt; we wanted those to vary across our seven levels of &lt;code&gt;actor&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor) 
##    Data: d (Number of observations: 504) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~actor (Number of levels: 7) 
##                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(treatment1)                 1.36      0.48     0.69     2.52 1.00     1979     2494
## sd(treatment2)                 0.90      0.40     0.34     1.89 1.00     2188     2451
## sd(treatment3)                 1.84      0.56     1.00     3.15 1.00     2999     3036
## sd(treatment4)                 1.55      0.60     0.73     2.97 1.00     2550     2518
## cor(treatment1,treatment2)     0.42      0.28    -0.21     0.87 1.00     2511     2461
## cor(treatment1,treatment3)     0.52      0.25    -0.07     0.90 1.00     2313     2619
## cor(treatment2,treatment3)     0.48      0.27    -0.12     0.89 1.00     2989     3261
## cor(treatment1,treatment4)     0.44      0.27    -0.17     0.86 1.00     2515     3034
## cor(treatment2,treatment4)     0.44      0.28    -0.17     0.87 1.00     3205     3217
## cor(treatment3,treatment4)     0.57      0.24     0.00     0.92 1.00     3165     3234
## 
## Population-Level Effects: 
##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## treatment1     0.23      0.46    -0.66     1.16 1.00     1871     2504
## treatment2     0.66      0.36    -0.06     1.39 1.00     2780     2687
## treatment3    -0.02      0.56    -1.14     1.07 1.00     3017     2966
## treatment4     0.69      0.51    -0.32     1.72 1.00     3006     2575
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look at the lower level of the output, the four levels in the ‘Population-Level Effects’ section are the four levels of &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{\text{treatment}[i]}\)&lt;/span&gt; from our statistical formula. If you look above at the ‘Group-Level Effects’ section, the four lines beginning with “sd” correspond to our four &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\alpha, [1]}, \dots, \sigma_{\alpha, [4]}\)&lt;/span&gt; parameters. The correlations among those are depicted in the six rows beginning with “cor,” which correspond to the elements within the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;p&gt;It might help if we visualized the model in a plot. Here are the results depicted in a streamlined version of McElreath’s Figure 14.7 &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020a, p. 452&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# for annotation
text &amp;lt;-
  distinct(d, labels) %&amp;gt;% 
  mutate(actor = &amp;quot;actor[1]&amp;quot;,
         prop  = c(.07, .8, .08, .795))

# define the new data
nd &amp;lt;-
  d %&amp;gt;% 
  distinct(actor, condition, labels, prosoc_left, treatment)

# get the fitted draws
fitted(fit1,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  mutate(actor     = str_c(&amp;quot;actor[&amp;quot;, actor, &amp;quot;]&amp;quot;),
         condition = factor(condition)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = labels)) +
  geom_hline(yintercept = .5, color = &amp;quot;white&amp;quot;, linetype = 2) +
  # posterior predictions
  geom_line(aes(y = Estimate, group = prosoc_left),
            size = 3/4) +
  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, shape = condition),
                  fill = &amp;quot;transparent&amp;quot;, fatten = 10, size = 1/3, show.legend = F) + 
  # annotation for the conditions
  geom_text(data = text,
            aes(y = prop, label = labels), 
            size = 3) +
  scale_shape_manual(values = c(21, 19)) +
  scale_x_discrete(NULL, breaks = NULL) +
  scale_y_continuous(&amp;quot;proportion left lever&amp;quot;, breaks = 0:2 / 2, labels = c(&amp;quot;0&amp;quot;, &amp;quot;.5&amp;quot;, &amp;quot;1&amp;quot;)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~actor, nrow = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig1-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s an alternative version, this time faceting by treatment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit1,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  # add the gamma summaries
  left_join(
    tibble(treatment = as.character(1:4),
       gamma = inv_logit_scaled(fixef(fit1)[, 1])),
    by = &amp;quot;treatment&amp;quot;
  )  %&amp;gt;% 
  mutate(treatment = str_c(&amp;quot;treatment[&amp;quot;, treatment, &amp;quot;]&amp;quot;)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = reorder(actor, Estimate), y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(aes(yintercept = gamma),
             color = &amp;quot;white&amp;quot;) +
  geom_pointrange(size = 1/3) +
  scale_x_discrete(breaks = NULL) +
  labs(x = &amp;quot;actor, rank orderred by their average probability&amp;quot;,
       y = &amp;quot;probability of pulling the lever&amp;quot;) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~treatment, nrow = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig2-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The horizontal white lines mark off the posterior means for the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{\text{treatment}[i]}\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kruschkes-approach.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kruschke’s approach.&lt;/h3&gt;
&lt;p&gt;One way to think about our &lt;code&gt;pulled_left&lt;/code&gt; data is they are grouped by two factors. The first factor is the experimental condition, &lt;code&gt;treatment&lt;/code&gt;. The second factor is participant, &lt;code&gt;actor&lt;/code&gt;. Now imagine you arrange the number of times &lt;code&gt;pulled_left == 1&lt;/code&gt; within the cells of a &lt;span class=&#34;math inline&#34;&gt;\(2 \times 2\)&lt;/span&gt; contingency table where the four levels of the &lt;code&gt;treatment&lt;/code&gt; factor are in the rows and the seven levels of &lt;code&gt;actor&lt;/code&gt; are in the columns. Here’s what that might look like in a tile plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  group_by(actor, treatment) %&amp;gt;% 
  summarise(count = sum(pulled_left)) %&amp;gt;% 
  mutate(treatment = factor(treatment, levels = 4:1)) %&amp;gt;% 
  
  ggplot(aes(x = actor, y = treatment, fill = count, label = count)) +
  geom_tile() +
  geom_text(aes(color = count &amp;gt; 6)) +
  scale_color_viridis_d(option = &amp;quot;E&amp;quot;, direction = -1, breaks = NULL) +
  scale_fill_viridis_c(option = &amp;quot;E&amp;quot;, limits = c(0, 18), breaks = NULL) +
  scale_x_discrete(position = &amp;quot;top&amp;quot;, expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  theme(axis.ticks = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig3-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this arrangement, we can model &lt;span class=&#34;math inline&#34;&gt;\(\text{left_pull}_i \sim \operatorname{Binomial}(n_i = 1, p_i)\)&lt;/span&gt;, with three hierarchical grouping factors. The first will be &lt;code&gt;actor&lt;/code&gt;, the second will be &lt;code&gt;treatment&lt;/code&gt;, and the third will be their interaction. Kruschke gave a general depiction of this kind of statistical model in Figure 20.2&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; of his text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke, 2015, p. 588&lt;/a&gt;)&lt;/span&gt;. However, I generally prefer expressing my models using statistical notation similar to McElreath. Though I’m not exactly sure how McElreath would express a model like this, here’s my best attempt using his style of notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \gamma + \alpha_{\text{actor}[i]} + \alpha_{\text{treatment}[i]} + \alpha_{\text{actor}[i] \times \text{treatment}[i]} \\
\gamma &amp;amp; \sim \operatorname{Normal}(0, 1) \\
\alpha_\text{actor}  &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{actor}) \\
\alpha_\text{treatment}  &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{treatment}) \\
\alpha_{\text{actor} \times \text{treatment}} &amp;amp; \sim \operatorname{Normal}(0, \sigma_{\text{actor} \times \text{treatment}}) \\
\sigma_\text{actor} &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_\text{treatment} &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_{\text{actor} \times \text{treatment}} &amp;amp; \sim \operatorname{Exponential}(1).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is our overall intercept and the three &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{&amp;lt;group&amp;gt;}[i]}\)&lt;/span&gt; terms are our multilevel deviations around that overall intercept. Notice that because &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; nas no &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; index, we are not technically using the index variable approach we discussed earlier in this post. But we are still indexing the four levels of &lt;code&gt;treatment&lt;/code&gt; by way of higher-level deviations depicted by the &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{treatment}[i]}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{actor}[i] \times \text{treatment}[i]}\)&lt;/span&gt; parameters in the second line. In contrast to our first model based on McElreath’s work, notice our three &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{&amp;lt;group&amp;gt;}[i]}\)&lt;/span&gt; term are all modeled as &lt;em&gt;univariate&lt;/em&gt; normal. This makes this model an extension of the cross-classified model.&lt;/p&gt;
&lt;div id=&#34;fit-the-second-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fit the second model.&lt;/h4&gt;
&lt;p&gt;Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;. We’ll call it &lt;code&gt;fit2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- 
  brm(data = d, 
      family = binomial,
      pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | treatment) + (1 | actor:treatment),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(exponential(1), class = sd)),
      cores = 4, seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | treatment) + (1 | actor:treatment) 
##    Data: d (Number of observations: 504) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~actor (Number of levels: 7) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     2.00      0.66     1.07     3.68 1.00     1270     1894
## 
## ~actor:treatment (Number of levels: 28) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.25      0.18     0.01     0.70 1.00     1296     1810
## 
## ~treatment (Number of levels: 4) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.53      0.36     0.07     1.46 1.00     1144     1032
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.46      0.63    -0.81     1.71 1.00      989     1969
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a model like this, a natural first question is: &lt;em&gt;Where is the variance at?&lt;/em&gt; We can answer that by comparing the three lines in the output from the ‘Group-Level Effects’ section. It might be easier if we plotted the posteriors for those &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{&amp;lt;group&amp;gt;}\)&lt;/span&gt; parameters, instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

posterior_samples(fit2) %&amp;gt;% 
  select(starts_with(&amp;quot;sd&amp;quot;)) %&amp;gt;% 
  set_names(str_c(&amp;quot;sigma[&amp;quot;, c(&amp;quot;actor&amp;quot;, &amp;quot;actor~X~treatment&amp;quot;, &amp;quot;treatment&amp;quot;), &amp;quot;]&amp;quot;)) %&amp;gt;% 
  pivot_longer(everything()) %&amp;gt;% 
  mutate(name = factor(name,
                       levels = str_c(&amp;quot;sigma[&amp;quot;, c(&amp;quot;actor~X~treatment&amp;quot;, &amp;quot;treatment&amp;quot;, &amp;quot;actor&amp;quot;), &amp;quot;]&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = value, y = name)) +
  stat_halfeye(.width = .95, size = 1/2) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  xlab(&amp;quot;marginal posterior (log-odds scale)&amp;quot;) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig4-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like most of the action was between the seven actors. But there was some variation among the four levels of &lt;code&gt;treatment&lt;/code&gt; and even the interaction between the two factors wasn’t completely pushed against zero.&lt;/p&gt;
&lt;p&gt;Okay, here’s an alternative version of the first plot from &lt;code&gt;fit1&lt;/code&gt;, above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit2,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  mutate(actor     = str_c(&amp;quot;actor[&amp;quot;, actor, &amp;quot;]&amp;quot;),
         condition = factor(condition)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = labels)) +
  geom_hline(yintercept = .5, color = &amp;quot;white&amp;quot;, linetype = 2) +
  # posterior predictions
  geom_line(aes(y = Estimate, group = prosoc_left),
            size = 3/4) +
  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, shape = condition),
                  fill = &amp;quot;transparent&amp;quot;, fatten = 10, size = 1/3, show.legend = F) + 
  scale_shape_manual(values = c(21, 19)) +
  scale_x_discrete(NULL, breaks = NULL) +
  scale_y_continuous(&amp;quot;proportion left lever&amp;quot;, limits = 0:1,
                     breaks = 0:2 / 2, labels = c(&amp;quot;0&amp;quot;, &amp;quot;.5&amp;quot;, &amp;quot;1&amp;quot;)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~actor, nrow = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig5-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The two models made similar predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-not-make-the-horse-race-official&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why not make the horse race official?&lt;/h3&gt;
&lt;p&gt;Just for kicks and giggles, we’ll compare the two models with the LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- add_criterion(fit1, criterion = &amp;quot;loo&amp;quot;)
fit2 &amp;lt;- add_criterion(fit2, criterion = &amp;quot;loo&amp;quot;)

# LOO differences
loo_compare(fit1, fit2) %&amp;gt;% print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic
## fit2    0.0       0.0  -266.8      9.5        12.2    0.6    533.7   19.1  
## fit1   -4.5       3.0  -271.3      9.7        19.3    1.2    542.6   19.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# LOO weights
model_weights(fit1, fit2, weights = &amp;quot;loo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       fit1       fit2 
## 0.01111874 0.98888126&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like there’s a little bit of an edge for the Kruschke’s multilevel ANOVA model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-whats-the-difference-anyway&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;But what’s the difference, anyway?&lt;/h3&gt;
&lt;p&gt;Rather than attempt to chose one model based on information criteria, we might back up and focus on the conceptual differences between the two models.&lt;/p&gt;
&lt;p&gt;Our first model, based on McElreath’s index-variable approach, explicitly emphasized the four levels of &lt;code&gt;treatment&lt;/code&gt;. Each one got its own &lt;span class=&#34;math inline&#34;&gt;\(\gamma_j\)&lt;/span&gt;. By modeling those &lt;span class=&#34;math inline&#34;&gt;\(\gamma_j\)&lt;/span&gt;’s with the multivariate normal distribution, we also got an explicit accounting of the &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; correlation structure for those parameters.&lt;/p&gt;
&lt;p&gt;Our second model, based on Kruschke’s multilevel ANOVA approach, took a more general perspective. By modeling &lt;code&gt;actor&lt;/code&gt;, &lt;code&gt;treatment&lt;/code&gt; and their interaction as higher-level grouping factors, &lt;code&gt;fit2&lt;/code&gt; conceptualized both participants and experimental conditions as coming from populations of potential participants and conditions, respectively. No longer are those four &lt;code&gt;treatment&lt;/code&gt; levels inherently special. They’re just the four we happen to have in this iteration of the experiment. Were we to run the experiment again, after all, we might want to alter them a little. The &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{treatment}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{actor} \times \text{treatment}}\)&lt;/span&gt; parameters can help give us a sense of how much variation we’d expect among other similar experimental conditions.&lt;/p&gt;
&lt;p&gt;Since I’m not a chimpanzee researcher, I’m in no position to say which perspective is better for these data. At a predictive level, the models perform similarly. But if I were a researcher wanting to analyze these data or others with a similar structure, I’d want to think clearly about what kinds of points I’d want to make to my target audience. Would I want to make focused points about the four levels of &lt;code&gt;treatment&lt;/code&gt;, or would it make sense to generalize from those four levels to other similar conditions? Each model has its rhetorical strengths and weaknesses.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;If you’re new to the Bayesian multilevel model, I recommend the introductory text by either McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020a&lt;/a&gt;)&lt;/span&gt; or Kruschke &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke, 2015&lt;/a&gt;)&lt;/span&gt;. I have ebook versions of both wherein I translated their code into the &lt;strong&gt;tidyverse&lt;/strong&gt; style and fit their models with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;, &lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2020&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt;. Both McElreath and Kruschke have blogs (&lt;a href=&#34;https://elevanth.org/blog/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://doingbayesiandataanalysis.blogspot.com/&#34;&gt;here&lt;/a&gt;). Also, though it doesn’t cover the multilevel model, you can get a lot of practice with Bayesian regression with the new book by Gelman, Hill, and Vehtari &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. And for more hot Bayesian regression talk, you always have the Stan forums, which even have a &lt;a href=&#34;https://discourse.mc-stan.org/c/interfaces/brms/36&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; section&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5    
##  [7] purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] splines_4.0.4        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] viridisLite_0.3.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3       
##  [53] arrayhelpers_1.1-0   ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3     
##  [57] loo_2.4.1            dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2      
##  [61] tidyselect_1.1.0     rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1       
##  [65] munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [69] generics_0.1.0       broom_0.7.5          ggridges_0.5.2       evaluate_0.14       
##  [73] fastmap_1.0.1        yaml_2.2.1           processx_3.4.5       knitr_1.31          
##  [77] fs_1.5.0             nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [81] xml2_1.3.2           compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [85] rstudioapi_0.13      gamm4_0.2-6          curl_4.3             reprex_0.3.0        
##  [89] statmod_1.4.35       stringi_1.5.3        highr_0.8            ps_1.6.0            
##  [93] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2        
##  [97] nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
## [101] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3    
## [105] httpuv_1.5.4         R6_2.5.0             bookdown_0.21        promises_1.1.1      
## [109] gridExtra_2.3        codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0  
## [113] MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1     withr_2.4.1         
## [117] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [125] rmarkdown_2.7        shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3     
## [129] dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-Bürkner2021Parameterization&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Parameterization of response distributions in brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzDoingBayesianData2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis in brms and the tidyverse&lt;/em&gt; (version 0.3.0). &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Statistical rethinking with brms, Ggplot2, and the tidyverse: &lt;span&gt;Second&lt;/span&gt; edition&lt;/em&gt; (version 0.1.1). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020a). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rethinking&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;rethinking&lt;/span&gt; &lt;span&gt;R&lt;/span&gt; package&lt;/em&gt;. &lt;a href=&#34;https://xcelab.net/rm/software/&#34;&gt;https://xcelab.net/rm/software/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Given the data are coded 0/1, one could also use the Bernoulli likelihood &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2021&lt;/a&gt;, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html#binary-and-count-data-models&#34; role=&#34;doc-biblioref&#34;&gt;&lt;em&gt;Binary and count data models&lt;/em&gt;&lt;/a&gt;)&lt;/span&gt;. I’m just partial to the binomial.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This is the typical parameterization for multilevel models fit with &lt;strong&gt;brms&lt;/strong&gt;. Though he used different notation, Bürkner spelled this all out in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; overview paper, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_overview.pdf&#34;&gt;&lt;em&gt;brms: An R package for Bayesian multilevel models using Stan&lt;/em&gt;&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The careful reader might notice that the models Kruschke focused on in Chapter 20 were all based on the Gaussian likelihood. So in the most technical sense, the model in Figure 20.2 is not a perfect match to our &lt;code&gt;fit2&lt;/code&gt;. I’m hoping my readers might look past those details to see the more general point. For more practice, &lt;a href=&#34;https://bookdown.org/content/3686/count-predicted-variable.html#example-hair-eye-go-again&#34;&gt;Section 24.2&lt;/a&gt; and &lt;a href=&#34;https://bookdown.org/content/3686/count-predicted-variable.html#example-interaction-contrasts-shrinkage-and-omnibus-test&#34;&gt;Section 24.3&lt;/a&gt; of my translation of Kruschke’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;)&lt;/span&gt; show variants of this model type using the Poisson likelihood. In Section &lt;a href=&#34;https://bookdown.org/content/3686/count-predicted-variable.html#log-linear-models-for-contingency-tables-bonus-alternative-parameterization&#34;&gt;24.4&lt;/a&gt; you can even find a variant using the aggregated binomial likelihood.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian meta-analysis in brms-II</title>
      <link>/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/</guid>
      <description>
&lt;script src=&#34;/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://bookdown.org/content/3890/missing-data-and-other-opportunities.html#summary-bonus-meta-analysis&#34;&gt;Section 14.3&lt;/a&gt; of my &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingBrms2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; translation of the first edition of McElreath’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;em&gt;Statistical rethinking&lt;/em&gt;, I included a bonus section covering Bayesian meta-analysis. For my &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2020&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt; translation of the second edition of the text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020&lt;/a&gt;)&lt;/span&gt;, I’d like to include another section on the topic, but from a different perspective. The first time around, we focused on standardized mean differences. This time, I’d like to tackle odds ratios and, while we’re at it, give a little bit of a plug for open science practices.&lt;/p&gt;
&lt;p&gt;The purpose of this post is to present a rough draft of the section. I intend to tack this section onto the end of Chapter 15 (&lt;em&gt;Missing Data and Other Opportunities&lt;/em&gt;), which covers measurement error. If you have any constrictive criticisms, please pass them along either in the &lt;a href=&#34;https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/issues&#34;&gt;GitHub issues for the ebook&lt;/a&gt; or on &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1317854064839958531&#34;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here’s the rough draft:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-bonus-bayesian-meta-analysis-with-odds-ratios&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;del&gt;Summary&lt;/del&gt; Bonus: Bayesian meta-analysis with odds ratios&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these packages and setting alterations will already have been 
# opened and made before this section
library(tidyverse)
library(brms)
library(ggdark)
library(viridis)
library(broom)
library(tidybayes)

theme_set(
  dark_theme_bw() +
    theme(legend.position = &amp;quot;none&amp;quot;,
          panel.grid = element_blank())
  )

# to reset the default ggplot2 theme to its default parameters,
# execute `ggplot2::theme_set(theme_gray())` and `ggdark::invert_geom_defaults()`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use &lt;code&gt;brms::brm()&lt;/code&gt; to fit Bayesian meta-analyses, too.&lt;/p&gt;
&lt;p&gt;Before we proceed, I should acknowledge that this section is heavily influenced by &lt;a href=&#34;https://mvuorre.github.io/#about&#34;&gt;Matti Vourre&lt;/a&gt;’s great blog post, &lt;a href=&#34;https://mvuorre.github.io/post/2016/09/29/meta-analysis-is-a-special-case-of-bayesian-multilevel-modeling/&#34;&gt;&lt;em&gt;Meta-analysis is a special case of Bayesian multilevel modeling&lt;/em&gt;&lt;/a&gt;. Since neither editions of McElreath’s text directly address meta-analyses, we’ll also have to borrow a bit from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelman2013bayesian&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://stat.columbia.edu/~gelman/book/&#34;&gt;&lt;em&gt;Bayesian data analysis, Third edition&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;how-do-meta-analyses-fit-into-the-picture&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How do meta-analyses fit into the picture?&lt;/h3&gt;
&lt;p&gt;Let Gelman and colleagues introduce the topic:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Discussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected &lt;em&gt;a priori&lt;/em&gt; to have predictable effects favoring one study over another…. this third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis…&lt;/p&gt;
&lt;p&gt;The first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125–126, &lt;em&gt;emphasis&lt;/em&gt; in the original)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The basic version of a Bayesian meta-analysis follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_j \sim \operatorname{Normal}(\theta_j, \sigma_j),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; = the point estimate for the effect size of a single study, &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, which is presumed to have been a draw from a Normal distribution centered on &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;. The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. The standard error from study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is specified &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j\)&lt;/span&gt;, which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j\)&lt;/span&gt;, here. Those values we take directly from the original studies.&lt;/p&gt;
&lt;p&gt;Building on the model, we further presume that study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is itself just one draw from a population of related studies, each of which have their own effect sizes. As such, we presume &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; itself has a distribution following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_j \sim \operatorname{Normal}(\mu, \tau),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the meta-analytic effect (i.e., the population mean) and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; is the variation around that mean, what you might also think of as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\tau\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-some-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need some data.&lt;/h3&gt;
&lt;p&gt;Our data in this section come from the second large-scale replication project by the Many Labs team &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kleinManyLabsInvestigating2018&#34; role=&#34;doc-biblioref&#34;&gt;Klein et al., 2018&lt;/a&gt;)&lt;/span&gt;. Of the 28 studies replicated in the study, we will focus on the replication of the trolley experiment from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hauserDissociationMoralJudgments2007&#34; role=&#34;doc-biblioref&#34;&gt;Hauser et al.&lt;/a&gt; (&lt;a href=&#34;#ref-hauserDissociationMoralJudgments2007&#34; role=&#34;doc-biblioref&#34;&gt;2007&lt;/a&gt;)&lt;/span&gt;. Here’s how the study was described by Klein and colleagues:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;According to the principle of double effect, an act that harms other people is more morally permissible if the act is a foreseen side effect rather than the means to the greater good. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hauserDissociationMoralJudgments2007&#34; role=&#34;doc-biblioref&#34;&gt;Hauser et al.&lt;/a&gt; (&lt;a href=&#34;#ref-hauserDissociationMoralJudgments2007&#34; role=&#34;doc-biblioref&#34;&gt;2007&lt;/a&gt;)&lt;/span&gt; compared participants’ reactions to two scenarios to test whether their judgments followed this principle. In the &lt;em&gt;foreseen-side-effect&lt;/em&gt; scenario, a person on an out-of-control train changed the train’s trajectory so that the train killed one person instead of five. In the &lt;em&gt;greater-good&lt;/em&gt; scenario, a person pushed a fat man in front of a train, killing him, to save five people. Whereas &lt;span class=&#34;math inline&#34;&gt;\(89\%\)&lt;/span&gt; of participants judged the action in the foreseen-side-effect scenario as permissible &lt;span class=&#34;math inline&#34;&gt;\((95 \% \; \text{CI} = [87\%, 91\%]),\)&lt;/span&gt; only &lt;span class=&#34;math inline&#34;&gt;\(11\%\)&lt;/span&gt; of participants in the greater-good scenario judged it as permissible &lt;span class=&#34;math inline&#34;&gt;\((95 \% \; \text{CI} = [9\%, 13\%])\)&lt;/span&gt;. The difference between the percentages was significant&lt;span class=&#34;math inline&#34;&gt;\(, \chi^2(1, N = 2,646) = 1,615.96,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .001,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(w = .78,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(d = 2.50,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(95 \% \; \text{CI} = [2.22, 2.86]\)&lt;/span&gt;. Thus, the results provided evidence for the principle of double effect. (p. 459, &lt;em&gt;emphasis&lt;/em&gt; in the original)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can find supporting materials for the replication project on the Open Science Framework at &lt;a href=&#34;https://osf.io/8cd4r/&#34;&gt;https://osf.io/8cd4r/&lt;/a&gt;. The relevant subset of the data for the replication of Hauser et al. come from the &lt;code&gt;Trolley Dilemma 1 (Hauser et al., 2007)&lt;/code&gt; folder within the &lt;code&gt;OSFdata.zip&lt;/code&gt; (&lt;a href=&#34;https://osf.io/ag2pd/&#34;&gt;https://osf.io/ag2pd/&lt;/a&gt;). I’ve downloaded the file and saved it on GitHub.&lt;/p&gt;
&lt;p&gt;Here we load the data and call it &lt;code&gt;h&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h &amp;lt;- 
  readr::read_csv(&amp;quot;https://raw.githubusercontent.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/master/data/Hauser_1_study_by_order_all_CLEAN_CASE.csv&amp;quot;)

h &amp;lt;- 
  h %&amp;gt;% 
  mutate(y   = ifelse(variable == &amp;quot;Yes&amp;quot;, 1, 0),
         loc = factor(Location,
                      levels = distinct(h, Location) %&amp;gt;% pull(Location),
                      labels = 1:59))

glimpse(h)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 6,842
## Columns: 29
## $ uID              &amp;lt;dbl&amp;gt; 65, 68, 102, 126, 145, 263, 267, 298, 309, 318, 350, 356, 376, 431, 438, …
## $ variable         &amp;lt;chr&amp;gt; &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;…
## $ factor           &amp;lt;chr&amp;gt; &amp;quot;SideEffect&amp;quot;, &amp;quot;SideEffect&amp;quot;, &amp;quot;SideEffect&amp;quot;, &amp;quot;SideEffect&amp;quot;, &amp;quot;SideEffect&amp;quot;, &amp;quot;Si…
## $ .id              &amp;lt;chr&amp;gt; &amp;quot;ML2_Slate1_Brazil__Portuguese_execution_illegal_r.csv&amp;quot;, &amp;quot;ML2_Slate1_Braz…
## $ source           &amp;lt;chr&amp;gt; &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;ubc&amp;quot;, …
## $ haus1.1          &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1…
## $ haus1.1t_1       &amp;lt;dbl&amp;gt; 39.054, 36.792, 56.493, 21.908, 25.635, 50.633, 58.661, 50.137, 51.717, 2…
## $ haus2.1          &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ haus2.1t_1       &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ Source.Global    &amp;lt;chr&amp;gt; &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;ubc&amp;quot;, …
## $ Source.Primary   &amp;lt;chr&amp;gt; &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;ubc&amp;quot;, …
## $ Source.Secondary &amp;lt;chr&amp;gt; &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;ubc&amp;quot;, …
## $ Country          &amp;lt;chr&amp;gt; &amp;quot;Brazil&amp;quot;, &amp;quot;Brazil&amp;quot;, &amp;quot;Brazil&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Ca…
## $ Location         &amp;lt;chr&amp;gt; &amp;quot;Social and Work Psychology Department, University of Brasilia, DF, Brazi…
## $ Language         &amp;lt;chr&amp;gt; &amp;quot;Portuguese&amp;quot;, &amp;quot;Portuguese&amp;quot;, &amp;quot;Portuguese&amp;quot;, &amp;quot;English&amp;quot;, &amp;quot;English&amp;quot;, &amp;quot;English&amp;quot;…
## $ Weird            &amp;lt;dbl&amp;gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ Execution        &amp;lt;chr&amp;gt; &amp;quot;illegal&amp;quot;, &amp;quot;illegal&amp;quot;, &amp;quot;illegal&amp;quot;, &amp;quot;illegal&amp;quot;, &amp;quot;illegal&amp;quot;, &amp;quot;illegal&amp;quot;, &amp;quot;illega…
## $ SubjectPool      &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;,…
## $ Setting          &amp;lt;chr&amp;gt; &amp;quot;In a classroom&amp;quot;, &amp;quot;In a classroom&amp;quot;, &amp;quot;In a classroom&amp;quot;, &amp;quot;In a lab&amp;quot;, &amp;quot;In a l…
## $ Tablet           &amp;lt;chr&amp;gt; &amp;quot;Computers&amp;quot;, &amp;quot;Computers&amp;quot;, &amp;quot;Computers&amp;quot;, &amp;quot;Computers&amp;quot;, &amp;quot;Computers&amp;quot;, &amp;quot;Compute…
## $ Pencil           &amp;lt;chr&amp;gt; &amp;quot;No, the whole study was on the computer (except maybe consent/debriefing…
## $ StudyOrderN      &amp;lt;chr&amp;gt; &amp;quot;Hauser|Ross.Slate1|Rottenstrich|Graham|Kay|Inbar|Anderson|VanLange|Huang…
## $ IDiffOrderN      &amp;lt;chr&amp;gt; &amp;quot;ID: Global self-esteem SISE|ID: Mood|ID: Subjective wellbeing|ID: Disgus…
## $ study.order      &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ analysis.type    &amp;lt;chr&amp;gt; &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;…
## $ subset           &amp;lt;chr&amp;gt; &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;al…
## $ case.include     &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…
## $ y                &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1…
## $ loc              &amp;lt;fct&amp;gt; 1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The total sample size is &lt;span class=&#34;math inline&#34;&gt;\(N = 6,842\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h %&amp;gt;% 
  distinct(uID) %&amp;gt;% 
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##       n
##   &amp;lt;int&amp;gt;
## 1  6842&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All cases are to be included.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h %&amp;gt;% 
  count(case.include)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   case.include     n
##   &amp;lt;lgl&amp;gt;        &amp;lt;int&amp;gt;
## 1 TRUE          6842&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data were collected in 59 locations with sample sizes ranging from 34 to 325.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h %&amp;gt;% 
  count(Location) %&amp;gt;% 
  arrange(desc(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 59 x 2
##    Location                                                                                        n
##    &amp;lt;chr&amp;gt;                                                                                       &amp;lt;int&amp;gt;
##  1 University of Toronto, Scarborough                                                            325
##  2 MTurk India Workers                                                                           308
##  3 MTurk US Workers                                                                              304
##  4 University of Illinois at Urbana-Champaign, Champaign, IL                                     198
##  5 Eotvos Lorand University, in Budapest, Hungary                                                180
##  6 Department of Social Psychology, Tilburg University, P.O. Box 90153, Tilburg, 5000 LE, Net…   173
##  7 Department of Psychology, San Diego State University, San Diego, CA 92182                     171
##  8 Department of Psychology, Pennsylvania State University Abington, Abington, PA 19001          166
##  9 American University of Sharjah, United Arab Emirates                                          162
## 10 University of British Columbia, Vancouver, Canada                                             147
## # … with 49 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;our-effect-size-will-be-an-odds-ratio.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Our effect size will be an odds ratio.&lt;/h3&gt;
&lt;p&gt;Here’s how Klein and colleagues summarized their primary results:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the aggregate replication sample &lt;span class=&#34;math inline&#34;&gt;\((N = 6,842\)&lt;/span&gt; after removing participants who responded in less than &lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt; s&lt;span class=&#34;math inline&#34;&gt;\(), 71\%\)&lt;/span&gt; of participants judged the action in the foreseen-side-effect scenario as permissible, but only &lt;span class=&#34;math inline&#34;&gt;\(17\%\)&lt;/span&gt; of participants in the greater-good scenario judged it as permissible. The difference between the percentages was significant, &lt;span class=&#34;math inline&#34;&gt;\(p = 2.2 \text e^{-16},\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\text{OR} = 11.54,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(d = 1.35,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(95\% \; \text{CI} = [1.28, 1.41]\)&lt;/span&gt;. The replication results were consistent with the double-effect hypothesis, and the effect was about half the magnitude of the original &lt;span class=&#34;math inline&#34;&gt;\((d = 1.35,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(95\% \; \text{CI} = [1.28, 1.41],\)&lt;/span&gt; vs. original &lt;span class=&#34;math inline&#34;&gt;\(d = 2.50)\)&lt;/span&gt;. (p. 459)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here is the breakdown of the outcome and primary experimental condition, which will confirm the two empirical percentages mentioned, above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h %&amp;gt;% 
  count(variable, factor) %&amp;gt;% 
  group_by(factor) %&amp;gt;% 
  mutate(percent = 100 * n / sum(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
## # Groups:   factor [2]
##   variable factor          n percent
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 No       GreaterGood  2781    82.8
## 2 No       SideEffect   1026    29.4
## 3 Yes      GreaterGood   577    17.2
## 4 Yes      SideEffect   2458    70.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though the authors presented their overall effect size with a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value, an odds-ratio (OR), and a Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; (i.e., a kind of standardized mean difference), we will focus on the OR. The primary data are binomial counts, which are well-handled with logistic regression. When you perform a logistic regression where a control condition is compared with some experimental condition, the difference between those conditions may be expressed as an OR. To get a sense of what that is, we’ll first practice fitting a logistic regression model with the frequentist &lt;code&gt;glm()&lt;/code&gt; function. Here are the results based on the subset of data from the first location.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm0 &amp;lt;- glm(y ~ factor, family = binomial(logit), data = h %&amp;gt;% filter(loc == 1))

summary(glm0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ factor, family = binomial(logit), data = h %&amp;gt;% 
##     filter(loc == 1))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5227  -0.6231  -0.6231   0.8677   1.8626  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)       -1.5404     0.3673  -4.194 2.74e-05 ***
## factorSideEffect   2.3232     0.4754   4.887 1.02e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 139.47  on 101  degrees of freedom
## Residual deviance: 110.98  on 100  degrees of freedom
## AIC: 114.98
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like with &lt;strong&gt;brms&lt;/strong&gt;, the base-&lt;strong&gt;R&lt;/strong&gt; &lt;code&gt;glm()&lt;/code&gt; function returns the results of a logistic regression model in the log-odds metric. The intercept is the log-odds probability of selecting &lt;em&gt;yes&lt;/em&gt; in the study for participants in the &lt;code&gt;GreaterGood&lt;/code&gt; condition. The ‘factorSideEffect’ parameter is the difference in log-odds probability for participants in the &lt;code&gt;SideEffect&lt;/code&gt; condition. Here’s what happens when you exponentiate that coefficient.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(glm0)[2] %&amp;gt;% exp()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## factorSideEffect 
##         10.20833&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That, my friends, is an odds ratio (OR). &lt;strong&gt;Odds ratios are simply exponentiated logistic regression coefficients&lt;/strong&gt;. The implication of this particular OR is that those in the &lt;code&gt;SideEffect&lt;/code&gt; condition have about 10 times the odds of selecting &lt;em&gt;yes&lt;/em&gt; compared to those in the &lt;code&gt;GreaterGood&lt;/code&gt; condition. In the case of this subset of the data, that’s 18% yeses versus 69%, which seems like a large difference, to me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h %&amp;gt;% 
  filter(loc == 1) %&amp;gt;% 
  count(variable, factor) %&amp;gt;% 
  group_by(factor) %&amp;gt;% 
  mutate(percent = 100 * n / sum(n)) %&amp;gt;% 
  filter(variable == &amp;quot;Yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
## # Groups:   factor [2]
##   variable factor          n percent
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Yes      GreaterGood     9    17.6
## 2 Yes      SideEffect     35    68.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;log-odds-odds-ratios-and-modeling-effect-sizes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Log-odds, odds ratios, and modeling effect sizes.&lt;/h3&gt;
&lt;p&gt;Though it’s common for researchers to express their effect sizes as odds ratios, we don’t want to work directly with odds ratios in a meta-analysis. &lt;em&gt;Why?&lt;/em&gt; Well, think back on why we model binomial data with the logit link. The logit link transforms a bounded &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt; parameter space into an unbounded parameter space ranging from negative to positive infinity. For us Bayesians, it also provides a context in which our &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters are approximately Gaussian. However, when we exponentiate those approximately Gaussian log-odds coefficients, the resulting odds ratios aren’t so Gaussian any more. This is why, even if our ultimate goal is to express a meta-analytic effect as an OR, we want to work with effect sizes in the log-odds metric. It allows us to use the Bayesian meta-analytic framework outlined by Gelman et al, above,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
y_j      &amp;amp; \sim \operatorname{Normal}(\theta_j, \sigma_j) \\
\theta_j &amp;amp; \sim \operatorname{Normal}(\mu, \tau),
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; is the point estimate in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;th study still in the log-odds scale. After fitting the model, we can then exponentiate the meta-analytic parameter &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; into the OR metric.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-the-study-specific-effect-sizes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compute the study-specific effect sizes.&lt;/h3&gt;
&lt;p&gt;Our &lt;code&gt;h&lt;/code&gt; data from the Klein et al replication study includes the un-aggregated data from all of the study locations combined. Before we compute our meta-analysis, we’ll need to compute the study-specific effect sizes and standard errors. Here we do so within a nested tibble.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glms &amp;lt;-
  h %&amp;gt;% 
  select(loc, y, factor) %&amp;gt;% 
  nest(data = c(y, factor)) %&amp;gt;% 
  mutate(glm = map(data, ~update(glm0, data = .))) %&amp;gt;% 
  mutate(coef = map(glm, tidy)) %&amp;gt;% 
  select(-data, -glm) %&amp;gt;% 
  unnest(coef) %&amp;gt;% 
  filter(term == &amp;quot;factorSideEffect&amp;quot;)

# what did we do?
glms %&amp;gt;% 
  mutate_if(is.double, round, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 59 x 6
##    loc   term             estimate std.error statistic p.value
##    &amp;lt;fct&amp;gt; &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 1     factorSideEffect     2.32     0.475      4.89       0
##  2 2     factorSideEffect     3.64     0.644      5.64       0
##  3 3     factorSideEffect     2.37     0.399      5.96       0
##  4 4     factorSideEffect     2.24     0.263      8.54       0
##  5 5     factorSideEffect     2.02     0.505      4.00       0
##  6 6     factorSideEffect     2.49     0.571      4.36       0
##  7 7     factorSideEffect     2.53     0.658      3.84       0
##  8 8     factorSideEffect     1.78     0.459      3.87       0
##  9 9     factorSideEffect     1.81     0.378      4.79       0
## 10 10    factorSideEffect     2.37     0.495      4.79       0
## # … with 49 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;estimate&lt;/code&gt; column we have all the &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; values and &lt;code&gt;std.error&lt;/code&gt; contains the corresponding &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j\)&lt;/span&gt; values. Here they are in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color &amp;lt;- viridis_pal(option = &amp;quot;C&amp;quot;)(7)[5]

glms %&amp;gt;% 
  ggplot(aes(x = std.error, y = estimate)) +
  geom_point(color = color) +
  labs(x = expression(sigma[italic(j)]~(&amp;quot;log-odds&amp;quot;)),
       y = expression(italic(y[j])~(&amp;quot;log-odds&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-bayesian-meta-analysis.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the Bayesian meta-analysis.&lt;/h3&gt;
&lt;p&gt;Now are data are ready, we can express our first Bayesian meta-analysis with the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\text{estimate}_j &amp;amp; \sim \operatorname{Normal}(\theta_j, \; \text{std.error}_j) \\
\theta_j   &amp;amp; \sim \operatorname{Normal}(\mu, \tau) \\
\mu        &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\tau       &amp;amp; \sim \operatorname{Exponential}(1),
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the last two lines spell out our priors. As we learned in &lt;a href=&#34;https://bookdown.org/content/4857/god-spiked-the-integers.html#binomial-regression&#34;&gt;Section 11.1&lt;/a&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, 1.5)\)&lt;/span&gt; prior in the log-odds space is just about flat on the probability space. If you wanted to be more conservative, consider something like &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, 1)\)&lt;/span&gt;. Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;me0 &amp;lt;- 
  brm(data = glms, 
      family = gaussian,
      estimate | se(std.error) ~ 1 + (1 | loc),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(exponential(1), class = sd)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;se()&lt;/code&gt; is one of the &lt;strong&gt;brms&lt;/strong&gt; helper functions designed to provide additional information about the criterion variable. Here it informs &lt;code&gt;brm()&lt;/code&gt; that each &lt;code&gt;estimate&lt;/code&gt; value has an associated measurement error defined in the &lt;code&gt;std.error&lt;/code&gt; column. Unlike the &lt;code&gt;mi()&lt;/code&gt; function, which we used earlier in the chapter to accommodate measurement error and the Bayesian imputation of missing data, the &lt;code&gt;se()&lt;/code&gt; function is specially designed to handle meta-analyses. &lt;code&gt;se()&lt;/code&gt; contains a &lt;code&gt;sigma&lt;/code&gt; argument which is set to &lt;code&gt;FALSE&lt;/code&gt; by default. This will return a model with no estimate for sigma, which is what we want. The uncertainty around the &lt;code&gt;estimate&lt;/code&gt;-value for each study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; has already been encoded in the data as &lt;code&gt;std.error&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s look at the model results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(me0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: estimate | se(std.error) ~ 1 + (1 | loc) 
##    Data: glms (Number of observations: 59) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~loc (Number of levels: 59) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.43      0.09     0.26     0.62 1.00     1956     2389
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.55      0.09     2.38     2.72 1.00     3443     2631
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our estimate for heterogeneity across studies, &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, is about 0.4, suggesting modest differences across the studies. The meta-analytic effect, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, is about 2.5. Both, recall, are in the log-odds metric. Here we exponentiate &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; to get our odds ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(me0) %&amp;gt;% exp()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Estimate Est.Error     Q2.5    Q97.5
## Intercept 12.79272  1.091829 10.85899 15.25431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look back up to the results reported by Klein and colleagues, you’ll see this is rather close to their OR estimate of 11.54.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-bayesian-muiltilevel-alternative.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the Bayesian muiltilevel alternative.&lt;/h3&gt;
&lt;p&gt;We said earlier that meta-analysis is just a special case of the multilevel model, applied to summary data. We typically perform meta-analyses on data summaries because historically it has not been the norm among researchers to make their data publicly available. So effect size summaries were the best we typically had for aggregating study results. However, times are changing (e.g., &lt;a href=&#34;https://www.apa.org/monitor/2017/11/trends-open-science.aspx&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://www.blog.google/products/search/making-it-easier-discover-datasets/&#34;&gt;here&lt;/a&gt;). In this case, Klein and colleagues engaged in open-science practices and reported all their data. Thus we can just directly fit the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\text{y}_{ij} &amp;amp; \sim \operatorname{Binomial}(n = 1, p_{ij}) \\
\operatorname{logit}(p_{ij}) &amp;amp; \sim \alpha + \beta \text{factor}_{ij} + u_{\alpha j} + u_{\beta j} \text{factor}_{ij} \\

\begin{bmatrix} u_{\alpha j} \\ u_{\beta j} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \begin{pmatrix} \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf{SRS} \end{pmatrix} \\

\mathbf S &amp;amp; = \begin{bmatrix} \sigma_\alpha &amp;amp; 0 \\ 0 &amp;amp; \sigma_\beta \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 0 &amp;amp; \rho_{\alpha \beta} \\ \rho_{\beta \alpha} &amp;amp; 0 \end{bmatrix} \\

\alpha &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\beta  &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\sigma_\alpha &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_\beta  &amp;amp; \sim \operatorname{Exponential}(1) \\
\mathbf R &amp;amp; \sim \operatorname{LKJ}(2),
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the criterion variable, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, is nested in &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; participants within &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; locations. The &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter is analogous to the meta-analytic effect (&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\beta\)&lt;/span&gt; is analogous to the expression of heterogeneity in the meta-analytic effect (&lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;). Here is how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;me1 &amp;lt;- 
  brm(data = h, 
      family = binomial,
      y | trials(1) ~ 0 + Intercept + factor + (1 + factor | loc),
      prior = c(prior(normal(0, 1.5), class = b),
                prior(exponential(1), class = sd),
                prior(lkj(2), class = cor)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results for the focal parameters are very similar to those from &lt;code&gt;me0&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(me1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: y | trials(1) ~ 0 + Intercept + factor + (1 + factor | loc) 
##    Data: h (Number of observations: 6842) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~loc (Number of levels: 59) 
##                                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)                       0.42      0.07     0.30     0.57 1.00     2120     2870
## sd(factorSideEffect)                0.48      0.09     0.32     0.66 1.01     1107     2010
## cor(Intercept,factorSideEffect)    -0.31      0.19    -0.62     0.08 1.00     1487     2276
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept           -1.66      0.08    -1.82    -1.52 1.00     2012     2675
## factorSideEffect     2.57      0.09     2.39     2.76 1.00     2044     2623
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the multilevel version of the effect size as an odds ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(me1)[2, -2] %&amp;gt;% exp()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
## 13.02704 10.93772 15.73129&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we compare the study specific effect sizes, &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;, by our two modeling approaches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color &amp;lt;- viridis_pal(option = &amp;quot;C&amp;quot;)(7)[3]

# how many levels are there?
n_loc &amp;lt;- distinct(h, loc) %&amp;gt;% count() %&amp;gt;% pull(n)

# rank by meta-analysis
ranks &amp;lt;-
  tibble(Estimate = coef(me0)$loc[, 1, &amp;quot;Intercept&amp;quot;],
         index    = 1:n_loc) %&amp;gt;% 
  arrange(Estimate) %&amp;gt;% 
  mutate(rank = 1:n_loc)

rbind(coef(me0)$loc[, , &amp;quot;Intercept&amp;quot;],
      coef(me1)$loc[, , &amp;quot;factorSideEffect&amp;quot;]) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  mutate(index = rep(1:n_loc, times = 2),
         type  = rep(c(&amp;quot;meta-analysis&amp;quot;, &amp;quot;multilevel model&amp;quot;), each = n_loc)) %&amp;gt;% 
  left_join(select(ranks, -Estimate), 
            by = &amp;quot;index&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +
  geom_pointrange(fatten = 1, color = color) +
  scale_x_continuous(expression(log-odds~effect~size~(theta[italic(j)])), limits = c(0, 4.5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~type)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results are very similar. You might be curious how to show these results in a more conventional looking forest plot where the names of the groups (typically studies) for the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; values are listed on the left, the point estimate and 95% interval summaries are listed on the right, and the summary for the population level effect, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, is listed beneath all all the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;’s. That’ll require some prep work. First we’ll need to reformat the location names. I’ll save the results in an object called &lt;code&gt;labs&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labs &amp;lt;-
  h %&amp;gt;% 
  mutate(lab = case_when(
    Location == &amp;quot;Social and Work Psychology Department, University of Brasilia, DF, Brazil&amp;quot; ~ &amp;quot;University of Brasilia&amp;quot;,
    Location == &amp;quot;Wilfrid Laurier University, Waterloo, Ontario, Canada&amp;quot; ~ &amp;quot;Wilfrid Laurier University&amp;quot;,
    Location == &amp;quot;University of British Columbia, Vancouver, Canada&amp;quot; ~ &amp;quot;University of British Columbia&amp;quot;,
    Location == &amp;quot;University of Toronto, Scarborough&amp;quot; ~ &amp;quot;University of Toronto&amp;quot;,
    Location == &amp;quot;Division of Social Science, The Hong Kong University of Science and Technology, Hong Kong, China&amp;quot; ~ &amp;quot;Hong Kong University of Science and Technology&amp;quot;,
    Location == &amp;quot;Chinese Academy of Science, Beijing, China&amp;quot; ~ &amp;quot;Chinese Academy of Science&amp;quot;,
    Location == &amp;quot;Shanghai International Studies University, SISU Intercultural Institute, Shanghai, China&amp;quot; ~ &amp;quot;Shanghai International Studies University&amp;quot;,
    Location == &amp;quot;Guangdong Literature &amp;amp; Art Vocational College, Guangzhou, China&amp;quot; ~ &amp;quot;Guangdong Literature &amp;amp; Art Vocational College&amp;quot;,
    Location == &amp;quot;The University of J. E. Purkyně, Ústí nad Labem, Czech Republic&amp;quot; ~ &amp;quot;The University of J. E. Purkyně&amp;quot;,
    Location == &amp;quot;University of Leuven, Belgium&amp;quot; ~ &amp;quot;University of Leuven&amp;quot;,
    Location == &amp;quot;Department of Experimental and Applied Psychology, VU Amsterdam, 1081BT, Amsterdam, The Netherlands&amp;quot; ~ &amp;quot;VU Amsterdam&amp;quot;,
    Location == &amp;quot;Department of Social Psychology, Tilburg University, P.O. Box 90153, Tilburg, 5000 LE, Netherlands&amp;quot; ~ &amp;quot;Department of Social Psychology, Tilburg University&amp;quot;,
    Location == &amp;quot;Eindhoven University of Technology, Eindhoven, Netherlands&amp;quot; ~ &amp;quot;Eindhoven University of Technology&amp;quot;,
    Location == &amp;quot;Department of Communication and Information Sciences, P.O. Box 90153, Tilburg, 5000 LE, Netherlands&amp;quot; ~ &amp;quot;Department of Communication and Information Sciences, Tilburg University&amp;quot;,
    Location == &amp;quot;University of Navarra, Spain&amp;quot; ~ &amp;quot;University of Navarra&amp;quot;,
    Location == &amp;quot;University of Lausanne, Switzerland&amp;quot; ~ &amp;quot;University of Lausanne&amp;quot;,
    Location == &amp;quot;Université de Poitiers, France&amp;quot; ~ &amp;quot;Université de Poitiers&amp;quot;,
    Location == &amp;quot;Eotvos Lorand University, in Budapest, Hungary&amp;quot; ~ &amp;quot;Eotvos Lorand University&amp;quot;,
    Location == &amp;quot;MTurk India Workers&amp;quot; ~ &amp;quot;MTurk India Workers&amp;quot;,
    Location == &amp;quot;University of Winchester, Winchester, Hampshire, England&amp;quot; ~ &amp;quot;University of Winchester&amp;quot;,
    Location == &amp;quot;Doshisha University, Kyoto, Japan&amp;quot; ~ &amp;quot;Doshisha University&amp;quot;,
    Location == &amp;quot;Victoria University of Wellington, New Zealand&amp;quot; ~ &amp;quot;Victoria University of Wellington&amp;quot;,
    Location == &amp;quot;University of Social Sciences and Humanities, Wroclaw, Poland&amp;quot; ~ &amp;quot;University of Social Sciences and Humanities&amp;quot;,
    Location == &amp;quot;Department of Psychology, SWPS University of Social Sciences and Humanities Campus Sopot, Sopot, Poland&amp;quot; ~ &amp;quot;SWPS University of Social Sciences and Humanities Campus Sopot&amp;quot;,
    Location == &amp;quot;badania.net&amp;quot; ~ &amp;quot;badania.net&amp;quot;,
    Location == &amp;quot;Universidade do Porto, Portugal&amp;quot; ~ &amp;quot;Universidade do Porto&amp;quot;,
    Location == &amp;quot;University of Belgrade, Belgrade, Serbia&amp;quot; ~ &amp;quot;University of Belgrade&amp;quot;,
    Location == &amp;quot;University of Johannesburg, Johanneburg, South Africa&amp;quot; ~ &amp;quot;University of Johannesburg&amp;quot;,
    Location == &amp;quot;Santiago, Chile&amp;quot; ~ &amp;quot;Santiago, Chile&amp;quot;,
    Location == &amp;quot;Universidad de Costa Rica, Costa Rica&amp;quot; ~ &amp;quot;Universidad de Costa Rica&amp;quot;,
    Location == &amp;quot;National Autonomous University of Mexico in Mexico City&amp;quot; ~ &amp;quot;National Autonomous University of Mexico&amp;quot;,
    Location == &amp;quot;University of the Republic, Montevideo, Uruguay&amp;quot; ~ &amp;quot;University of the Republic&amp;quot;,
    Location == &amp;quot;Lund University, Lund, Sweden&amp;quot; ~ &amp;quot;Lund University&amp;quot;,
    Location == &amp;quot;Academia Sinica, Taiwan National Taiwan Normal University, Taiwan&amp;quot; ~ &amp;quot;Taiwan National Taiwan Normal University&amp;quot;,
    Location == &amp;quot;Bilgi University, Istanbul, Turkey&amp;quot; ~ &amp;quot;Bilgi University&amp;quot;,
    Location == &amp;quot;Koç University, Istanbul, Turkey&amp;quot; ~ &amp;quot;Koç University&amp;quot;,
    Location == &amp;quot;American University of Sharjah, United Arab Emirates&amp;quot; ~ &amp;quot;American University of Sharjah&amp;quot;,
    Location == &amp;quot;University of Hawaii, Honolulu, HI&amp;quot; ~ &amp;quot;University of Hawaii&amp;quot;,
    Location == &amp;quot;Social Science and Policy Studies Department, Worcester Polytechnic Institute, Worcester, MA 01609&amp;quot; ~ &amp;quot;Worcester Polytechnic Institute&amp;quot;,
    Location == &amp;quot;Department of Psychology, Washington and Lee University, Lexington, VA 24450&amp;quot; ~ &amp;quot;Washington and Lee University&amp;quot;,
    Location == &amp;quot;Department of Psychology, San Diego State University, San Diego, CA 92182&amp;quot; ~ &amp;quot;San Diego State University&amp;quot;,
    Location == &amp;quot;Tufts&amp;quot; ~ &amp;quot;Tufts&amp;quot;,
    Location == &amp;quot;University of Florida, Florida&amp;quot; ~ &amp;quot;University of Florida&amp;quot;,
    Location == &amp;quot;University of Illinois at Urbana-Champaign, Champaign, IL&amp;quot; ~ &amp;quot;University of Illinois at Urbana-Champaign&amp;quot;,
    Location == &amp;quot;Pacific Lutheran University, Tacoma, WA&amp;quot; ~ &amp;quot;Pacific Lutheran University&amp;quot;,
    Location == &amp;quot;University of Virginia, VA&amp;quot; ~ &amp;quot;University of Virginia&amp;quot;,
    Location == &amp;quot;Marian University, Indianapolis, IN&amp;quot; ~ &amp;quot;Marian University&amp;quot;,
    Location == &amp;quot;Department of Psychology, Ithaca College, Ithaca, NY 14850&amp;quot; ~ &amp;quot;Ithaca College&amp;quot;,
    Location == &amp;quot;University of Michigan&amp;quot; ~ &amp;quot;University of Michigan&amp;quot;,
    Location == &amp;quot;Department of Psychology, Pennsylvania State University Abington, Abington, PA 19001&amp;quot; ~ &amp;quot;Pennsylvania State University Abington&amp;quot;,
    Location == &amp;quot;Department of Psychology, Texas A&amp;amp;M University, College Station, TX 77843&amp;quot; ~ &amp;quot;Texas A&amp;amp;M University&amp;quot;,
    Location == &amp;quot;William Paterson University, Wayne, NJ&amp;quot; ~ &amp;quot;William Paterson University&amp;quot;,
    Location == &amp;quot;Department of Cognitive Science, Occidental College, Los Angeles, CA&amp;quot; ~ &amp;quot;Occidental College&amp;quot;,
    Location == &amp;quot;The Pennsylvania State University&amp;quot; ~ &amp;quot;The Pennsylvania State University&amp;quot;,
    Location == &amp;quot;MTurk US Workers&amp;quot; ~ &amp;quot;MTurk US Workers&amp;quot;,
    Location == &amp;quot;University of Graz AND the Universty of Vienna&amp;quot; ~ &amp;quot;University of Graz and the Universty of Vienna&amp;quot;,
    Location == &amp;quot;University of Potsdam, Germany&amp;quot; ~ &amp;quot;University of Potsdam&amp;quot;,
    Location == &amp;quot;Open University of Hong Kong&amp;quot; ~ &amp;quot;Open University of Hong Kong&amp;quot;,
    Location == &amp;quot;Concepción, Chile&amp;quot; ~ &amp;quot;Concepción&amp;quot;
  )) %&amp;gt;% 
  distinct(loc, lab)

# what is this?
labs %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 59
## Columns: 2
## $ loc &amp;lt;fct&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,…
## $ lab &amp;lt;chr&amp;gt; &amp;quot;University of Brasilia&amp;quot;, &amp;quot;Wilfrid Laurier University&amp;quot;, &amp;quot;University of British Columbi…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ll do some tricky wrangling with the output from &lt;code&gt;coef()&lt;/code&gt; and &lt;code&gt;fixef()&lt;/code&gt; to arrange the odds ratio summaries for the population average and the location-specific results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this will help us format the labels on the secondary y-axis
my_format &amp;lt;- function(number) {
  formatC(number, digits = 2, format = &amp;quot;f&amp;quot;)
}

# grab the theta_j summaries
groups &amp;lt;-
  coef(me1)$loc[, , &amp;quot;factorSideEffect&amp;quot;] %&amp;gt;% 
  data.frame() %&amp;gt;% 
  mutate(loc = distinct(h, loc) %&amp;gt;% pull()) %&amp;gt;% 
  arrange(Estimate)

# grat the mu summary
average &amp;lt;-
  fixef(me1) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  slice(2) %&amp;gt;% 
  mutate(loc = &amp;quot;Average&amp;quot;)

# combine and wrangle
post &amp;lt;-
  bind_rows(groups, average) %&amp;gt;% 
  mutate(rank     = c(1:59, 0),
         Estimate = exp(Estimate),
         Q2.5     = exp(Q2.5),
         Q97.5    = exp(Q97.5)) %&amp;gt;% 
  left_join(labs, by = &amp;quot;loc&amp;quot;) %&amp;gt;% 
  arrange(rank) %&amp;gt;% 
  mutate(label   = ifelse(is.na(lab), &amp;quot;POPULATION AVERAGE&amp;quot;, lab),
         summary = str_c(my_format(Estimate), &amp;quot; [&amp;quot;, my_format(Q2.5), &amp;quot;, &amp;quot;, my_format(Q97.5), &amp;quot;]&amp;quot;))

# what have we done?
post %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 60
## Columns: 9
## $ Estimate  &amp;lt;dbl&amp;gt; 13.027040, 5.994537, 7.225509, 7.894728, 7.896201, 7.989348, 8.158148, 8.425675,…
## $ Est.Error &amp;lt;dbl&amp;gt; 0.09183827, 0.23712115, 0.35418533, 0.32549107, 0.35978096, 0.23125168, 0.343829…
## $ Q2.5      &amp;lt;dbl&amp;gt; 10.937724, 3.752456, 3.537577, 4.170577, 3.898701, 5.147465, 4.109451, 4.488119,…
## $ Q97.5     &amp;lt;dbl&amp;gt; 15.731289, 9.501053, 14.080042, 15.016368, 15.701244, 12.588834, 16.272517, 15.8…
## $ loc       &amp;lt;chr&amp;gt; &amp;quot;Average&amp;quot;, &amp;quot;19&amp;quot;, &amp;quot;38&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;32&amp;quot;, &amp;quot;55&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;34&amp;quot;, &amp;quot;22&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;58&amp;quot;, &amp;quot;24&amp;quot;, &amp;quot;…
## $ rank      &amp;lt;dbl&amp;gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22…
## $ lab       &amp;lt;chr&amp;gt; NA, &amp;quot;MTurk India Workers&amp;quot;, &amp;quot;University of Hawaii&amp;quot;, &amp;quot;Guangdong Literature &amp;amp; Art V…
## $ label     &amp;lt;chr&amp;gt; &amp;quot;POPULATION AVERAGE&amp;quot;, &amp;quot;MTurk India Workers&amp;quot;, &amp;quot;University of Hawaii&amp;quot;, &amp;quot;Guangdong …
## $ summary   &amp;lt;chr&amp;gt; &amp;quot;13.03 [10.94, 15.73]&amp;quot;, &amp;quot;5.99 [3.75, 9.50]&amp;quot;, &amp;quot;7.23 [3.54, 14.08]&amp;quot;, &amp;quot;7.89 [4.17, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s our custom forest plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +
  geom_interval(aes(color = label == &amp;quot;POPULATION AVERAGE&amp;quot;),
                size = 1/2) +
  geom_point(aes(size = 1 - Est.Error, color = label == &amp;quot;POPULATION AVERAGE&amp;quot;),
             shape = 15) +
  scale_color_viridis_d(option = &amp;quot;C&amp;quot;, begin = .33, end = .67) +
  scale_size_continuous(range = c(1, 3.5)) +
  scale_x_continuous(&amp;quot;odds ratio&amp;quot;, breaks = 1:6 * 10, expand = expansion(mult = c(0.005, 0.005))) +
  scale_y_continuous(NULL, breaks = 0:59, limits = c(-1, 60), expand = c(0, 0),
                     labels = pull(post, label),
                     sec.axis = dup_axis(labels = pull(post, summary))) +
  theme(text = element_text(family = &amp;quot;Times&amp;quot;),
        axis.text.y = element_text(hjust = 0, color = &amp;quot;white&amp;quot;, size = 7),
        axis.text.y.right = element_text(hjust = 1, size = 7),
        axis.ticks.y = element_blank(),
        panel.background = element_rect(fill = &amp;quot;grey8&amp;quot;),
        panel.border = element_rect(color = &amp;quot;transparent&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You may have noticed this plot is based on the results of our multilevel model, &lt;code&gt;me1&lt;/code&gt;. We could have done the same basic thing with the results from the more conventional meta-analysis model, &lt;code&gt;me0&lt;/code&gt;, too.&lt;/p&gt;
&lt;p&gt;I’m not aware this it typical in random effect meta-analyses, but it might be useful to further clarify the meaning of the two primary parameters, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;. Like with the forest plot, above, we could examine these with either &lt;code&gt;me0&lt;/code&gt; or &lt;code&gt;me1&lt;/code&gt;. For kicks, we’ll use &lt;code&gt;me0&lt;/code&gt; (the conventional Bayesian meta-analysis). In the output from &lt;code&gt;posterior_samples(me0)&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; are in the columns named &lt;code&gt;b_Intercept&lt;/code&gt; and &lt;code&gt;sd_loc__Intercept&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(me0)

post %&amp;gt;% 
  select(b_Intercept:sd_loc__Intercept) %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_Intercept sd_loc__Intercept
## 1    2.378526         0.4688289
## 2    2.562858         0.4555103
## 3    2.435846         0.3252279
## 4    2.658129         0.3895584
## 5    2.451356         0.3583352
## 6    2.672061         0.5595212&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you scroll back above, you’ll see our random effect meta-analysis explicitly presumed our empirical effect-size estimates &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; are approximations of the true effect sizes &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;, which are themselves normally distributed in the population of possible effect sizes from similar studies: &lt;span class=&#34;math inline&#34;&gt;\(\theta_j \sim \operatorname{Normal}(\mu, \tau)\)&lt;/span&gt;. Why not use our posterior samples to simulate draws from &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(\mu, \tau)\)&lt;/span&gt; to get a sense of what this distribution might look like? Recall that the parameters are in the log-odds metric. We’ll present the distribution in that metric and as odds ratios.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color &amp;lt;- viridis_pal(option = &amp;quot;C&amp;quot;)(7)[6]
set.seed(15)

post %&amp;gt;% 
  transmute(lo = rnorm(n(), mean = b_Intercept, sd = sd_loc__Intercept),
            or = rnorm(n(), mean = b_Intercept, sd = sd_loc__Intercept) %&amp;gt;% exp()) %&amp;gt;% 
  slice(1:1e3) %&amp;gt;% 
  pivot_longer(lo:or, values_to = &amp;quot;effect size&amp;quot;) %&amp;gt;% 
  mutate(name = factor(name, labels = c(&amp;quot;log-odds&amp;quot;, &amp;quot;odds ratio&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = `effect size`, y = 0)) +
  geom_dots(color = color, fill = color) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(Normal(mu*&amp;#39;, &amp;#39;*tau))) +
  theme(text = element_text(family = &amp;quot;Times&amp;quot;),
        strip.background = element_rect(color = &amp;quot;transparent&amp;quot;)) +
  facet_wrap(~name, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both panels show 1,000 draws, each of which is depicted by a single dot. If we were to run this experiment 1,000 times and compute the effect size separately for each one, this is what we’d expect those distributions of effect sizes to look like. Seems like there’s a lot of variation in there, eh? The next time you observe your fellow scientists debating over whether a study replicated or not, keep these distributions in mind. Once you start thinking about distributions, replication becomes a tricky notion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parting-thoughts.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parting thoughts.&lt;/h3&gt;
&lt;p&gt;There are other things you might do with these data. For example, you might inspect how much the effect size varies between those from WEIRD and non-WEIRD countries. You might also model the data as clustered by &lt;code&gt;Language&lt;/code&gt; rather than by &lt;code&gt;Location&lt;/code&gt;. But I think we’ve gone far enough to get you started.&lt;/p&gt;
&lt;p&gt;If you’d like to learn more about these methods, do check out Vourre’s &lt;a href=&#34;https://mvuorre.github.io/post/2016/09/29/meta-analysis-is-a-special-case-of-bayesian-multilevel-modeling/&#34;&gt;&lt;em&gt;Meta-analysis is a special case of Bayesian multilevel modeling&lt;/em&gt;&lt;/a&gt;. You might also read Williams, Rast, and Bürkner’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-williamsBayesianMetaanalysisWeakly2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; manuscript, &lt;a href=&#34;https://psyarxiv.com/7tbrm/&#34;&gt;&lt;em&gt;Bayesian meta-analysis with weakly informative prior distributions&lt;/em&gt;&lt;/a&gt;. For an alternative workflow, consider the &lt;a href=&#34;https://github.com/wwiecek/baggr&#34;&gt;&lt;strong&gt;baggr&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-baggr&#34; role=&#34;doc-biblioref&#34;&gt;Wiecek &amp;amp; Meager, 2020&lt;/a&gt;)&lt;/span&gt;, which is designed to fit hierarchical Bayesian meta-analyses with Stan under the hood.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1   broom_0.7.5       viridis_0.5.1     viridisLite_0.3.0 ggdark_0.2.1     
##  [6] brms_2.15.0       Rcpp_1.0.6        forcats_0.5.1     stringr_1.4.0     dplyr_1.0.5      
## [11] purrr_0.3.4       readr_1.4.0       tidyr_1.1.3       tibble_3.1.0      ggplot2_3.3.3    
## [16] tidyverse_1.3.0  
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] svUnit_1.0.3         splines_4.0.4        crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [49] htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [53] ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3      loo_2.4.1           
##  [57] dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2       tidyselect_1.1.0    
##  [61] rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [65] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [69] ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1        yaml_2.2.1          
##  [73] processx_3.4.5       knitr_1.31           fs_1.5.0             nlme_3.1-152        
##  [77] mime_0.10            projpred_2.0.2       xml2_1.3.2           compiler_4.0.4      
##  [81] bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13      gamm4_0.2-6         
##  [85] curl_4.3             reprex_0.3.0         statmod_1.4.35       stringi_1.5.3       
##  [89] highr_0.8            ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6   
##  [93] lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1        
##  [97] shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1         lifecycle_1.0.0     
## [101] bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4         R6_2.5.0            
## [105] bookdown_0.21        promises_1.1.1       gridExtra_2.3        codetools_0.2-18    
## [109] boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [113] assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0      multcomp_1.4-16     
## [117] mgcv_1.8-33          parallel_4.0.4       hms_0.5.3            grid_4.0.4          
## [121] coda_0.19-4          minqa_1.2.4          rmarkdown_2.7        shiny_1.5.0         
## [125] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp;amp; Rubin, D. B. (2013). &lt;em&gt;Bayesian data analysis&lt;/em&gt; (Third Edition). &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://stat.columbia.edu/~gelman/book/&#34;&gt;https://stat.columbia.edu/~gelman/book/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hauserDissociationMoralJudgments2007&#34; class=&#34;csl-entry&#34;&gt;
Hauser, M., Cushman, F., Young, L., Jin, R. K.-X., &amp;amp; Mikhail, J. (2007). A dissociation between moral judgments and justifications. &lt;em&gt;Mind &amp;amp; Language&lt;/em&gt;, &lt;em&gt;22&lt;/em&gt;(1), 1–21. &lt;a href=&#34;https://doi.org/10.1111/j.1468-0017.2006.00297.x&#34;&gt;https://doi.org/10.1111/j.1468-0017.2006.00297.x&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kleinManyLabsInvestigating2018&#34; class=&#34;csl-entry&#34;&gt;
Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A. (2018). Many &lt;span&gt;Labs&lt;/span&gt; 2: &lt;span&gt;Investigating&lt;/span&gt; variation in replicability across samples and settings. &lt;em&gt;Advances in Methods and Practices in Psychological Science&lt;/em&gt;, &lt;em&gt;1&lt;/em&gt;(4), 443–490. &lt;a href=&#34;https://doi.org/10.1177/2515245918810225&#34;&gt;https://doi.org/10.1177/2515245918810225&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingBrms2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Statistical rethinking with brms, &lt;span class=&#34;nocase&#34;&gt;ggplot2&lt;/span&gt;, and the tidyverse&lt;/em&gt; (version 1.2.0). &lt;a href=&#34;https://doi.org/10.5281/zenodo.3693202&#34;&gt;https://doi.org/10.5281/zenodo.3693202&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Statistical rethinking with brms, Ggplot2, and the tidyverse: &lt;span&gt;Second&lt;/span&gt; edition&lt;/em&gt; (version 0.1.1). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-baggr&#34; class=&#34;csl-entry&#34;&gt;
Wiecek, W., &amp;amp; Meager, R. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;baggr&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; aggregate treatment effects&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=baggr&#34;&gt;https://CRAN.R-project.org/package=baggr&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsBayesianMetaanalysisWeakly2018&#34; class=&#34;csl-entry&#34;&gt;
Williams, D. R., Rast, P., &amp;amp; Bürkner, P.-C. (2018). &lt;em&gt;Bayesian meta-analysis with weakly informative prior distributions&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/7tbrm&#34;&gt;https://doi.org/10.31234/osf.io/7tbrm&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian meta-analysis in brms</title>
      <link>/post/bayesian-meta-analysis/</link>
      <pubDate>Sun, 14 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/post/bayesian-meta-analysis/</guid>
      <description>
&lt;script src=&#34;/post/bayesian-meta-analysis/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;[edited Apr 21, 2021]&lt;/p&gt;
&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble&lt;/h2&gt;
&lt;p&gt;I released the first &lt;a href=&#34;https://bookdown.org&#34;&gt;bookdown&lt;/a&gt; version of my &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt; with brms, ggplot2, and the tidyverse&lt;/a&gt; project a couple weeks ago. I consider it the 0.9.0 version&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. I wanted a little time to step back from the project before giving it a final edit for the first major edition. I also wanted to give others a little time to take a look and suggest edits, which some thankfully have.&lt;/p&gt;
&lt;p&gt;Now some time has passed, it’s become clear I’d like to add a bonus section on Bayesian meta-analysis. IMO, this is a natural extension of the hierarchical models McElreath introduced in chapter’s 12 and 13 of &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;his text&lt;/a&gt; and of the measurement-error models he introduced in chapter 14. So the purpose of this post is to present a rough draft of how I’d like to introduce fitting meta-analyses with Bürkner’s great brms package.&lt;/p&gt;
&lt;p&gt;I intend to tack this section onto the end of chapter 14. If you have any &lt;a href=&#34;https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse/issues&#34;&gt;constrictive criticisms, please pass them along&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here’s the rough draft (which I updated on 2018-11-12):&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rough-draft-meta-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rough draft: Meta-analysis&lt;/h2&gt;
&lt;p&gt;If your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use &lt;code&gt;brms::brm()&lt;/code&gt; to fit Bayesian meta-analyses, too.&lt;/p&gt;
&lt;p&gt;Before we proceed, I should acknowledge that this section is heavily influenced by Matti Vourre’s great blog post, &lt;a href=&#34;https://mvuorre.github.io/blog/posts/2016-09-29-bayesian-meta-analysis/&#34;&gt;&lt;em&gt;Meta-analysis is a special case of Bayesian multilevel modeling&lt;/em&gt;&lt;/a&gt;. And since McElreath’s text doesn’t directly address meta-analyses, we’ll take further inspiration from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;&lt;em&gt;Bayesian data analysis, Third edition&lt;/em&gt;&lt;/a&gt;. We’ll let Gelman and colleagues introduce the topic:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Discussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected &lt;em&gt;a priori&lt;/em&gt; to have predictable effects favoring one study over another.… This third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis…&lt;/p&gt;
&lt;p&gt;The first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125–126, &lt;em&gt;emphasis&lt;/em&gt; in the original)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The basic version of a Bayesian meta-analysis follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i \sim \text{Normal}(\theta_i, \sigma_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; = the point estimate for the effect size of a single study, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, which is presumed to have been a draw from a Normal distribution centered on &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;. The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. Which we do. The standard error from study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is specified &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt;, which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt;, here. Those values we take directly from the original studies.&lt;/p&gt;
&lt;p&gt;Building on the model, we further presume that study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is itself just one draw from a population of related studies, each of which have their own effect sizes. As such. we presume &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; itself has a distribution following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_i \sim \text{Normal} (\mu, \tau)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the meta-analytic effect (i.e., the population mean) and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; is the variation around that mean, what you might also think of as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\tau\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Since there’s no example of a meta-analysis in the text, we’ll have to look elsewhere. We’ll focus on Gershoff and Grogan-Kaylor’s (2016) paper, &lt;a href=&#34;https://pdfs.semanticscholar.org/0d03/a2e9f085f0a268b4c0a52f5ac31c17a3e5f3.pdf&#34;&gt;&lt;em&gt;Spanking and Child Outcomes: Old Controversies and New Meta-Analyses&lt;/em&gt;&lt;/a&gt;. From their introduction, we read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Around the world, most children (80%) are spanked or otherwise physically punished by their parents (&lt;a href=&#34;https://www.unicef.org/publications/index_74865.html&#34;&gt;UNICEF, 2014&lt;/a&gt;). The question of whether parents should spank their children to correct misbehaviors sits at a nexus of arguments from ethical, religious, and human rights perspectives both in the U.S. and around the world (&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/cdep.12038&#34;&gt;Gershoff, 2013&lt;/a&gt;). Several hundred studies have been conducted on the associations between parents’ use of spanking or physical punishment and children’s behavioral, emotional, cognitive, and physical outcomes, making spanking one of the most studied aspects of parenting. What has been learned from these hundreds of studies? (p. 453)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our goal will be to learn Bayesian meta-analysis by answering part of that question. I’ve transcribed the values directly from Gershoff and Grogan-Kaylor’s paper and saved them as a file called &lt;code&gt;spank.xlsx&lt;/code&gt;.
You can find the data in &lt;a href=&#34;https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse&#34;&gt;this project’s GitHub repository&lt;/a&gt;. Let’s load them and &lt;code&gt;glimpse()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spank &amp;lt;- readxl::read_excel(&amp;quot;spank.xlsx&amp;quot;)

library(tidyverse)
glimpse(spank)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 111
## Columns: 8
## $ study   &amp;lt;chr&amp;gt; &amp;quot;Bean and Roberts (1981)&amp;quot;, &amp;quot;Day and Roberts (1983)&amp;quot;, &amp;quot;Minton, …
## $ year    &amp;lt;dbl&amp;gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, 2005, 19…
## $ outcome &amp;lt;chr&amp;gt; &amp;quot;Immediate defiance&amp;quot;, &amp;quot;Immediate defiance&amp;quot;, &amp;quot;Immediate defianc…
## $ between &amp;lt;dbl&amp;gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,…
## $ within  &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,…
## $ d       &amp;lt;dbl&amp;gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14, -0.18,…
## $ ll      &amp;lt;dbl&amp;gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, -0.42, -…
## $ ul      &amp;lt;dbl&amp;gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, 0.13, 2.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this paper, the effect size of interest is a &lt;em&gt;Cohen’s d&lt;/em&gt;, derived from the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d = \frac{\mu_\text{treatment} - \mu_\text{comparison}}{\sigma_{pooled}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_{pooled} = \sqrt{\frac{((n_1 - 1) \sigma_1^2) + ((n_2 - 1) \sigma_2^2)}{n_1 + n_2 -2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To help make the equation for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; clearer for our example, we might re-express it as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d = \frac{\mu_\text{spanked} - \mu_\text{not spanked}}{\sigma_{pooled}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;McElreath didn’t really focus on effect sizes in his text. If you need a refresher, you might check out Kelley and Preacher’s &lt;a href=&#34;https://www.researchgate.net/profile/Ken_Kelley/publication/270757972_On_Effect_Size/links/0046351b0cd48217ce000000/On-Effect-Size.pdf&#34;&gt;&lt;em&gt;On effect size&lt;/em&gt;&lt;/a&gt;. But in words, &lt;em&gt;Cohen’s d&lt;/em&gt; is a standardized mean difference between two groups.&lt;/p&gt;
&lt;p&gt;So if you look back up at the results of &lt;code&gt;glimpse(spank)&lt;/code&gt;, you’ll notice the column &lt;code&gt;d&lt;/code&gt;, which is indeed a vector of &lt;em&gt;Cohen’s d&lt;/em&gt; effect sizes. The last two columns, &lt;code&gt;ll&lt;/code&gt; and &lt;code&gt;ul&lt;/code&gt; are the lower and upper limits of the associated 95% frequentist confidence intervals. But we don’t want confidence intervals for our &lt;code&gt;d&lt;/code&gt;-values; we want their standard errors. Fortunately, we can compute those with the following formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SE = \frac{\text{upper limit } – \text{lower limit}}{3.92}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here it is in code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spank &amp;lt;-
  spank %&amp;gt;% 
  mutate(se = (ul - ll) / 3.92)

glimpse(spank)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 111
## Columns: 9
## $ study   &amp;lt;chr&amp;gt; &amp;quot;Bean and Roberts (1981)&amp;quot;, &amp;quot;Day and Roberts (1983)&amp;quot;, &amp;quot;Minton, …
## $ year    &amp;lt;dbl&amp;gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, 2005, 19…
## $ outcome &amp;lt;chr&amp;gt; &amp;quot;Immediate defiance&amp;quot;, &amp;quot;Immediate defiance&amp;quot;, &amp;quot;Immediate defianc…
## $ between &amp;lt;dbl&amp;gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,…
## $ within  &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,…
## $ d       &amp;lt;dbl&amp;gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14, -0.18,…
## $ ll      &amp;lt;dbl&amp;gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, -0.42, -…
## $ ul      &amp;lt;dbl&amp;gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, 0.13, 2.…
## $ se      &amp;lt;dbl&amp;gt; 0.52040816, 0.71683673, 0.21683673, 0.47193878, 0.47193878, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now our data are ready, we can express our first Bayesian meta-analysis with the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\text{d}_i &amp;amp; \sim &amp;amp; \text{Normal}(\theta_i, \sigma_i = \text{se}_i) \\
\theta_i &amp;amp; \sim &amp;amp; \text{Normal} (\mu, \tau) \\
\mu &amp;amp; \sim &amp;amp; \text{Normal} (0, 1) \\
\tau &amp;amp; \sim &amp;amp; \text{HalfCauchy} (0, 1)
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The last two lines, of course, spell out our priors. In psychology, it’s pretty rare to see &lt;em&gt;Cohen’s d&lt;/em&gt;-values greater than the absolute value of &lt;span class=&#34;math inline&#34;&gt;\(\pm 1\)&lt;/span&gt;. So in the absence of more specific domain knowledge–which I don’t have–, it seems like &lt;span class=&#34;math inline&#34;&gt;\(\text{Normal} (0, 1)\)&lt;/span&gt; is a reasonable place to start. And just like McElreath used &lt;span class=&#34;math inline&#34;&gt;\(\text{HalfCauchy} (0, 1)\)&lt;/span&gt; as the default prior for the group-level standard deviations, &lt;a href=&#34;https://psyarxiv.com/7tbrm/&#34;&gt;it makes sense to use it here&lt;/a&gt; for our meta-analytic &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; parameter.&lt;/p&gt;
&lt;p&gt;Let’s load brms.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the code for the first model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b14.5 &amp;lt;- 
  brm(data = spank, family = gaussian,
      d | se(se) ~ 1 + (1 | study),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(cauchy(0, 1), class = sd)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing you might notice is our &lt;code&gt;se(se)&lt;/code&gt; function excluded the &lt;code&gt;sigma&lt;/code&gt; argument. If you recall from section 14.1, we specified &lt;code&gt;sigma = T&lt;/code&gt; in our measurement-error models. The brms default is that within &lt;code&gt;se()&lt;/code&gt;, &lt;code&gt;sigma = FALSE&lt;/code&gt;. As such, we have no estimate for sigma the way we would if we were doing this analysis with the raw data from the studies. Hopefully this makes sense. The uncertainty around the &lt;code&gt;d&lt;/code&gt;-value for each study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; has already been encoded in the data as &lt;code&gt;se&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This brings us to another point. We typically perform meta-analyses on data summaries. In my field and perhaps in yours, this is due to the historical accident that it has not been the norm among researchers to make their data publicly available. So effect size summaries were the best we typically had. However, times are changing (e.g., &lt;a href=&#34;https://www.apa.org/monitor/2017/11/trends-open-science.aspx&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://www.blog.google/products/search/making-it-easier-discover-datasets/&#34;&gt;here&lt;/a&gt;). If the raw data from all the studies for your meta-analysis are available, you can just fit a multilevel model in which the data are nested in the studies. Heck, you could even allow the studies to vary by &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; by taking the &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html#a-simple-distributional-model&#34;&gt;distributional modeling approach&lt;/a&gt; and specify something like &lt;code&gt;sigma ~ 0 + study&lt;/code&gt; or even &lt;code&gt;sigma ~ 1 + (1 | study)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But enough technical talk. Let’s look at the model results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(b14.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: d | se(se) ~ 1 + (1 | study) 
##    Data: spank (Number of observations: 111) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~study (Number of levels: 76) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.26      0.03     0.21     0.33 1.01      754     1582
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.38      0.04     0.31     0.45 1.00      605     1021
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, in our simple Bayesian meta-analysis, we have a population &lt;em&gt;Cohen’s d&lt;/em&gt; of about 0.38. Our estimate for &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, 0.26, suggests we have quite a bit of between-study variability. One question you might ask is: &lt;em&gt;What exactly are these&lt;/em&gt; Cohen’s d&lt;em&gt;s measuring, anyways?&lt;/em&gt; We’ve encoded that in the &lt;code&gt;outcome&lt;/code&gt; vector of the &lt;code&gt;spank&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spank %&amp;gt;% 
  distinct(outcome) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;outcome&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Immediate defiance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Low moral internalization&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child aggression&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child antisocial behavior&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child externalizing behavior problems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child internalizing behavior problems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child mental health problems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child alcohol or substance abuse&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Negative parent–child relationship&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Impaired cognitive ability&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Low self-esteem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Low self-regulation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Victim of physical abuse&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Adult antisocial behavior&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Adult mental health problems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Adult alcohol or substance abuse&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Adult support for physical punishment&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are a few things to note. First, with the possible exception of &lt;code&gt;Adult support for physical punishment&lt;/code&gt;, all of the outcomes are negative. We prefer conditions associated with lower values for things like &lt;code&gt;Child aggression&lt;/code&gt; and &lt;code&gt;Adult mental health problems&lt;/code&gt;. Second, the way the data are coded, larger effect sizes are interpreted as more negative outcomes associated with children having been spanked. That is, our analysis suggests spanking children is associated with worse outcomes. What might not be immediately apparent is that even though there are 111 cases in the data, there are only 76 distinct studies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spank %&amp;gt;% 
  distinct(study) %&amp;gt;% 
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##       n
##   &amp;lt;int&amp;gt;
## 1    76&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, some studies have multiple outcomes. In order to better accommodate the &lt;code&gt;study&lt;/code&gt;- and &lt;code&gt;outcome&lt;/code&gt;-level variances, let’s fit a cross-classified Bayesian meta-analysis reminiscent of the cross-classified chimp model from Chapter 13.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b14.6 &amp;lt;- 
  brm(data = spank, family = gaussian,
      d | se(se) ~ 1 + (1 | study) + (1 | outcome),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(cauchy(0, 1), class = sd)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(b14.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: d | se(se) ~ 1 + (1 | study) + (1 | outcome) 
##    Data: spank (Number of observations: 111) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~outcome (Number of levels: 17) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.08      0.03     0.04     0.14 1.00     1018     1756
## 
## ~study (Number of levels: 76) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.25      0.03     0.20     0.31 1.00      827     1571
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.36      0.04     0.28     0.43 1.01      664     1562
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have two &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; parameters. We might plot them to get a sense of where the variance is at.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(b14.6) %&amp;gt;% 
  select(starts_with(&amp;quot;sd&amp;quot;)) %&amp;gt;% 
  gather(key, tau) %&amp;gt;% 
  mutate(key = str_remove(key, &amp;quot;sd_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;__Intercept&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = tau, fill = key)) +
  geom_density(color = &amp;quot;transparent&amp;quot;, alpha = 2/3) +
  scale_fill_viridis_d(NULL, end = .85) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(tau)) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-meta-analysis/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So at this point, the big story is there’s more variability between the studies than there is the outcomes. But I still want to get a sense of the individual outcomes. Here we’ll use &lt;code&gt;tidybayes::stat_halfeye()&lt;/code&gt; to help us make our version of a &lt;a href=&#34;https://cran.r-project.org/web/packages/forestplot/vignettes/forestplot.html&#34;&gt;forest plot&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load tidybayes
library(tidybayes)

b14.6 %&amp;gt;%
  spread_draws(b_Intercept, r_outcome[outcome,]) %&amp;gt;%
  # add the grand mean to the group-specific deviations
  mutate(mu = b_Intercept + r_outcome) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(outcome = str_replace_all(outcome, &amp;quot;[.]&amp;quot;, &amp;quot; &amp;quot;)) %&amp;gt;% 

  # plot
  ggplot(aes(x = mu, y = reorder(outcome, mu))) +
  geom_vline(xintercept = fixef(b14.6)[1, 1], color = &amp;quot;white&amp;quot;, size = 1) +
  geom_vline(xintercept = fixef(b14.6)[1, 3:4], color = &amp;quot;white&amp;quot;, linetype = 2) +
  stat_halfeye(.width = .95, size = 2/3) +
  labs(x = expression(italic(&amp;quot;Cohen&amp;#39;s d&amp;quot;)),
       y = NULL) +
  theme(panel.grid   = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-meta-analysis/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The solid and dashed vertical white lines in the background mark off the grand mean (i.e., the meta-analytic effect) and its 95% intervals. But anyway, there’s not a lot of variability across the outcomes. Let’s go one step further with the model. Doubling back to Gelman and colleagues, we read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When assuming exchangeability we assume there are no important covariates that might form the basis of a more complex model, and this assumption (perhaps misguidedly) is widely adopted in meta-analysis. What if other information (in addition to the data &lt;span class=&#34;math inline&#34;&gt;\((n, y)\)&lt;/span&gt;) is available to distinguish among the &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; studies in a meta-analysis, so that an exchangeable model is inappropriate? In this situation, we can expand the framework of the model to be exchangeable in the observed data and covariates, for example using a hierarchical regression model. (p. 126)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One important covariate Gershoff and Grogan-Kaylor addressed in their meta-analysis was the type of study. The 76 papers they based their meta-analysis on contained both between- and within-participants designs. In the &lt;code&gt;spank&lt;/code&gt; data, we’ve dummy coded that information with the &lt;code&gt;between&lt;/code&gt; and &lt;code&gt;within&lt;/code&gt; vectors. Both are dummy variables and &lt;code&gt;within&lt;/code&gt; = 1 - &lt;code&gt;between&lt;/code&gt;. Here are the counts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spank %&amp;gt;% 
  count(between)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   between     n
##     &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1       0    71
## 2       1    40&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I use dummies in my models, I prefer to have the majority group stand as the reference category. As such, I typically name those variables by the minority group. In this case, most occasions are based on within-participant designs. Thus, we’ll go ahead and add the &lt;code&gt;between&lt;/code&gt; variable to the model. While we’re at it, we’ll practice using the &lt;code&gt;0 + intercept&lt;/code&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b14.7 &amp;lt;- 
  brm(data = spank, family = gaussian,
      d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome),
      prior = c(prior(normal(0, 1), class = b),
                prior(cauchy(0, 1), class = sd)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(b14.7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome) 
##    Data: spank (Number of observations: 111) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~outcome (Number of levels: 17) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.08      0.03     0.04     0.14 1.00     1490     2304
## 
## ~study (Number of levels: 76) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.25      0.03     0.20     0.31 1.01      820     2203
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## intercept     0.38      0.05     0.29     0.48 1.01      925     1662
## between      -0.07      0.07    -0.21     0.07 1.00      891     1645
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a closer look at &lt;code&gt;b_between&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(b14.7) %&amp;gt;% 
  
  ggplot(aes(x = b_between, y = 0)) +
  stat_halfeye(point_interval = median_qi, .width = c(.5, .95)) +
  labs(x = &amp;quot;Overall difference for between- vs within-participant designs&amp;quot;,
       y = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid   = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-meta-analysis/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;432&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That difference isn’t as large I’d expect it to be. But then again, I’m no spanking researcher. So what do I know?&lt;/p&gt;
&lt;p&gt;There are other things you might do with these data. For example, you might check for trends by year or, as the authors did in their manuscript, distinguish among different severities of corporal punishment. But I think we’ve gone far enough to get you started.&lt;/p&gt;
&lt;p&gt;If you’d like to learn more about these methods, do check out Vourre’s &lt;a href=&#34;https://mvuorre.github.io/blog/posts/2016-09-29-bayesian-meta-analysis/&#34;&gt;&lt;em&gt;Meta-analysis is a special case of Bayesian multilevel modeling&lt;/em&gt;&lt;/a&gt;. From his blog, you’ll learn additional tricks, like making a more traditional-looking forest plot with the &lt;code&gt;brmstools::forest()&lt;/code&gt; function and how our Bayesian brms method compares with frequentist meta-analyses via the &lt;a href=&#34;https://CRAN.R-project.org/package=metafor&#34;&gt;metafor package&lt;/a&gt;. You might also check out Williams, Rast, and Bürkner’s manuscript, &lt;a href=&#34;https://psyarxiv.com/7tbrm/&#34;&gt;&lt;em&gt;Bayesian Meta-Analysis with Weakly Informative Prior Distributions&lt;/em&gt;&lt;/a&gt; to give you an empirical justification for using a half-Cauchy prior for your meta-analysis &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1  
##  [5] stringr_1.4.0   dplyr_1.0.5     purrr_0.3.4     readr_1.4.0    
##  [9] tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.3         splines_4.0.4       
##   [7] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0  
##  [19] xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [22] colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1         
##  [28] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2          
##  [34] gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2        
##  [40] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [43] DBI_1.1.0            miniUI_0.1.1.1       viridisLite_0.3.0   
##  [46] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2          
##  [52] threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.1      
##  [55] farver_2.0.3         pkgconfig_2.0.3      loo_2.4.1           
##  [58] dbplyr_2.0.0         utf8_1.1.4           tidyselect_1.1.0    
##  [61] labeling_0.4.2       rlang_0.4.10         reshape2_1.4.4      
##  [64] later_1.1.0.1        munsell_0.5.0        cellranger_1.1.0    
##  [67] tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [70] broom_0.7.5          ggridges_0.5.2       evaluate_0.14       
##  [73] fastmap_1.0.1        yaml_2.2.1           processx_3.4.5      
##  [76] knitr_1.31           fs_1.5.0             nlme_3.1-152        
##  [79] mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [82] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [85] rstudioapi_0.13      gamm4_0.2-6          curl_4.3            
##  [88] reprex_0.3.0         statmod_1.4.35       stringi_1.5.3       
##  [91] highr_0.8            ps_1.6.0             blogdown_1.3        
##  [94] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2        
##  [97] nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0       
## [100] vctrs_0.3.6          pillar_1.5.1         lifecycle_1.0.0     
## [103] bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [106] R6_2.5.0             bookdown_0.21        promises_1.1.1      
## [109] gridExtra_2.3        codetools_0.2-18     boot_1.3-26         
## [112] colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [115] assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [118] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4         
## [124] minqa_1.2.4          rmarkdown_2.7        shiny_1.5.0         
## [127] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;At the time of this revision (2021-04-21), this ebook is now in &lt;a href=&#34;https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse/releases/tag/1.2.0&#34;&gt;version 1.2.0&lt;/a&gt;. The revision of this post includes fixes to a couple code breaks and a few updated hyperlinks. If you’d like to see the current version of this meta-analysis material, you can find it &lt;a href=&#34;https://bookdown.org/content/3890/missing-data-and-other-opportunities.html#summary-bonus-meta-analysis&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>bookdown, My Process</title>
      <link>/post/how-bookdown/</link>
      <pubDate>Thu, 04 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/post/how-bookdown/</guid>
      <description>
&lt;script src=&#34;/post/how-bookdown/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;I just self-published a book-length version of my project &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt; with brms, ggplot2, and the tidyverse&lt;/a&gt;. By using Yihui Xie’s &lt;a href=&#34;https://bookdown.org&#34;&gt;bookdown package&lt;/a&gt;, I was able to do it for free. If you’ve never heard of it, bookdown enables &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html#what-is-r&#34;&gt;R&lt;/a&gt; users to write books and other long-form articles with &lt;a href=&#34;https://rmarkdown.rstudio.com&#34;&gt;R Markdown&lt;/a&gt;. You can save your bookdown products in a variety of formats (e.g., PDF, HTML) and publish them in several ways, too. The purpose of this post is to give readers a sense of how I used bookdown to make my project. I propose there are three fundamental skill sets you need basic fluency in before playing with bookdown. Those three are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R and R Studio,&lt;/li&gt;
&lt;li&gt;Scripts and R Markdown files, and&lt;/li&gt;
&lt;li&gt;Git and GitHub.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;start-with-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Start with &lt;a href=&#34;https://cran.r-project.org&#34;&gt;R&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;First things first. Since bookdown is a package for use in the R environment, you’re going to have to use R. If you’re unfamiliar with it, &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html&#34;&gt;R is a freely-available programming language particularly well-suited for data analysis&lt;/a&gt;. If you’ve not used R before, learning how to self-publish books is a great incentive to start learning. But unless you already have a background in programming, I think bookdown is poorly-suited for novices. R newbies should check out Roger Peng’s &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/&#34;&gt;&lt;em&gt;R Programming for Data Science&lt;/em&gt;&lt;/a&gt; or Grolemund and Wickham’s &lt;a href=&#34;http://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R for Data Science&lt;/em&gt;&lt;/a&gt;. Both are freely available online and, as it would turn out, made with bookdown. Also, new users should be aware that although you can interact with R directly, there are a variety of other ways to interface with R. I recommend using &lt;a href=&#34;https://www.rstudio.com&#34;&gt;R Studio&lt;/a&gt;. You can find some nice reasons, &lt;a href=&#34;https://www.theanalysisfactor.com/the-advantages-of-rstudio/&#34;&gt;here&lt;/a&gt;. For basic instructions on how to install R and R Studio, you might start &lt;a href=&#34;http://r4ds.had.co.nz/introduction.html#prerequisites&#34;&gt;here&lt;/a&gt;. And if you prefer video tutorials to help you with the installation, just do a simple search in your favorite video-sharing website and several should pop up.&lt;/p&gt;
&lt;p&gt;Personally, I started using R—via R Studio—during the 2015/2016 winter break before taking a spring semester statistics course based around an R package. [In case you’re curious, it was a structural equation modeling course based around a &lt;a href=&#34;https://blogs.baylor.edu/rlatentvariable/&#34;&gt;text by Beaujean&lt;/a&gt; which featured the &lt;a href=&#34;http://lavaan.ugent.be&#34;&gt;lavaan package&lt;/a&gt;]. At the time, I was already familiar with structural equation modeling, so the course was a nice opportunity to learn R. In addition, I was concurrently enrolled in a course on multilevel modeling based on &lt;a href=&#34;http://gseacademic.harvard.edu/alda/&#34;&gt;Singer and Willet’s classic text&lt;/a&gt;. The professor of that course primarily used SAS to teach the material, but he was flexible and allowed me to do the work with R, instead. So that was my introduction to R–a semester of immersion in &lt;a href=&#34;https://twitter.com/search?q=%23rstats&amp;amp;src=typd&#34;&gt;#rstats&lt;/a&gt;. Here are some other &lt;a href=&#34;https://www.r-bloggers.com/the-5-most-effective-ways-to-learn-r/&#34;&gt;tips on how to learn R&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bookdown-uses-markdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;bookdown uses &lt;a href=&#34;https://daringfireball.net/projects/markdown/syntax&#34;&gt;Markdown&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you work with R through R Studio, you can do a handful of things through dropdowns. But really, if you’re going to be using R, you’re going to be coding. As it turns out, there are a variety of ways to code in R. One of the most basic ways is via the &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/200404846-Working-in-the-Console&#34;&gt;console&lt;/a&gt;, which I’m not going to cover in any detail.&lt;/p&gt;
&lt;p&gt;The console is fine for quick operations, but you’re going to want to do most of your coding in some kind of a script. R Studio allows users to save and execute code in script files, which you can learn more about &lt;a href=&#34;http://r4ds.had.co.nz/workflow-scripts.html&#34;&gt;here&lt;/a&gt;. Basic script files are nice in that they allow you to both save and annotate your code.&lt;/p&gt;
&lt;p&gt;However, the annotation options in R Studio script files are limited. After using R Studio scripts for about a year, I learned about &lt;a href=&#34;https://rmarkdown.rstudio.com/r_notebooks&#34;&gt;R Notebooks&lt;/a&gt;. These are special files that allow you to intermingle your R code with prose and the results of the code. R Notebooks also allow users to transform the working documents into professional-looking reports in various formats (e.g., PDF, HTML). And unlike the primitive annotation options with simple script files, R Notebooks use Markdown to allow users to format their prose with things like headers, italicized font, insert hyperlinks, and even embed images. So &lt;a href=&#34;https://daringfireball.net/projects/markdown/&#34;&gt;Markdown&lt;/a&gt;, then, is a simple language that allows for many of those functions.&lt;/p&gt;
&lt;p&gt;Within the R Studio environment, you can use Markdown with two basic file types: &lt;a href=&#34;https://rmarkdown.rstudio.com/lesson-1.html&#34;&gt;R Markdown&lt;/a&gt; files and &lt;a href=&#34;https://rmarkdown.rstudio.com/r_notebooks&#34;&gt;R Notebook&lt;/a&gt; files. R Notebook files are just special kinds of R Markdown files that have, IMO, a better interface. That is, R Notebooks are the newer nicer version of R Markdown files. The main point here is that when I say “bookdown uses Markdown”, I’m pointing out that one of the important skills you’ll want to develop before making content with bookdown is how to use Markdown within R Studio. It’s not terribly complicated to learn, and you can get an overview of the basics &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;http://r4ds.had.co.nz/r-markdown.html&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://bookdown.org/yihui/bookdown/markdown-syntax.html&#34;&gt;here&lt;/a&gt;, or an exhaustive treatment &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you’re a novice, it’ll take you a few days, weeks, or months to get a firm grasp of R. Not so with R Markdown files. You’ll have the basics of those down in an afternoon. That said, I had been an R Notebook user for more than a year before trying my hand at bookdown.&lt;/p&gt;
&lt;p&gt;The first big edition of my &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt; with brms, ggplot2, and the tidyverse&lt;/a&gt; project came in the form of R Notebook files and their HTML counterparts stored in one of my projects on the &lt;a href=&#34;https://osf.io/?gclid=EAIaIQobChMI2ZDP9svj3QIVQ7nACh2rYQIHEAAYASAAEgL7avD_BwE&#34;&gt;Open Science Framework&lt;/a&gt;. I don’t update it very often, but you can still find it &lt;a href=&#34;https://osf.io/97t6w/&#34;&gt;here&lt;/a&gt;. If you’re not familiar with it, the OSF is a &lt;a href=&#34;https://osf.io/4znzp/wiki/home/&#34;&gt;“free, open source web application that connects and supports the research workflow, enabling scientists to increase the efficiency and effectiveness of their research.”&lt;/a&gt; In addition to their wiki, you might check out some of their &lt;a href=&#34;https://cos.io/our-services/training-services/cos-training-tutorials/&#34;&gt;video tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;youll-need-github-too&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;You’ll need &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;, too&lt;/h2&gt;
&lt;p&gt;I’m actually not sure whether you need to know how to use Git and GitHub to use bookdown. In his authoritative book, &lt;a href=&#34;https://bookdown.org/yihui/bookdown/&#34;&gt;&lt;em&gt;bookdown: Authoring Books and Technical Documents with R Markdown&lt;/em&gt;&lt;/a&gt;, Yihui Xie mentioned GitHub in every chapter. If you go to your favorite video-sharing website to look for instructional videos on bookdown, you’ll see the instructors take GitHub as a given, too. If you’re stubborn and have enough ingenuity, you might find a way to successfully use bookdown without GitHub, but you may as well go with the flow on this one.&lt;/p&gt;
&lt;p&gt;If you’ve never heard of it before, &lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&#34;&gt;Git is a system for version control&lt;/a&gt;. By version control, I mean a system by which you can keep track of changes to your code, over time. Even if you don’t have a background in programming, consider a scenario where you had to keep track of many versions of a writing project, perhaps saving your files as &lt;code&gt;first_draft.docx&lt;/code&gt;, &lt;code&gt;second_draft.docx&lt;/code&gt;, &lt;code&gt;final_draft.docx&lt;/code&gt;, &lt;code&gt;final_draft_2.docx&lt;/code&gt;… This was your own make-shift attempt at version control for writing. I’ve seen a lot of introductory material recommend Git and GitHub by leading with version control. And indeed, they do serve that purpose. But IMO, leading with version control is a rhetorical mistake when talking to non-programmers. I haven’t found Git and GitHub the most intuitive and if version control was the only benefit, they wouldn’t be worth the effort. But there are other good reasons to learn.&lt;/p&gt;
&lt;p&gt;IMO, the best reason to learn Git and GitHub is because they allow you to make your work publicly available. When you just use Git, the work stays on your computer. But GitHub allows you to save your files online, too. This makes it easy for others to review them and give you feedback. GitHub also allows you to save things like data files online. So if you’re a working scientist, Git and GitHub might allow you to make a site—a repository—to house the de-identified data and statistical code for one of your projects. It’s another way to do open science. In addition, you can repurpose GitHub to work as blog or an analytic portfolio. And if you’d like to use bookdown, Git and GitHub will be a part of how you manage the files for your projects and make your work more accessible to others.&lt;/p&gt;
&lt;p&gt;If you’re new to all this, you could probably blindly follow along with the steps in Yihui Xie’s bookdown &lt;a href=&#34;https://bookdown.org/yihui/bookdown/&#34;&gt;manual&lt;/a&gt; or any of the online video tutorials. But I suspect that’d be pretty confusing. Before attempting a bookdown project, spend some time getting comfortable with Git and GitHub, first. The best introduction to the topic I’ve seen is Jenny Bryan’s &lt;a href=&#34;http://happygitwithr.com&#34;&gt;&lt;em&gt;Happy Git and GitHub for the useR&lt;/em&gt;&lt;/a&gt;, which, you guessed it, is also freely available and powered by bookdown.&lt;/p&gt;
&lt;p&gt;As I hinted, I found Git and GitHub baffling, at first. I checked out a few online video tutorials, but found them of little help. It really was Bryan’s &lt;a href=&#34;http://happygitwithr.com&#34;&gt;book&lt;/a&gt; that finally got me going. And I’m glad I did. I’ve been slowly working with GitHub for about a year—here’s &lt;a href=&#34;https://github.com/ASKurz&#34;&gt;my profile&lt;/a&gt;—and my first major project was putting together the files for the individual chapters in the &lt;a href=&#34;https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse&#34;&gt;Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse&lt;/a&gt; project. They originally lived as R Notebook files, eventually rendered in a &lt;a href=&#34;https://rmarkdown.rstudio.com/github_document_format.html&#34;&gt;GitHub-friendly .md file format&lt;/a&gt;. After a while, I started playing around with &lt;code&gt;README&lt;/code&gt;-only projects, which are basically a poor man’s GitHub version of blog posts (e.g., check out &lt;a href=&#34;https://github.com/ASKurz/James-Stein-and-Bayesian-partial-pooling&#34;&gt;this one&lt;/a&gt;). For me, and probably for your future bookdown projects, the most important GitHub skills to learn are commits, pushes, and forkes.&lt;/p&gt;
&lt;p&gt;I’d fooled around with GitHub a tiny bit before launching my &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt; with brms, ggplot2, and the tidyverse&lt;/a&gt; project on the OSF. But it was confusing and after an hour or two of trying to make sense of it, I gave up and just figured the OSF would be good enough. After folks started noticing the project, I got a few comments that it’d be more accessible on GitHub. That was what finally influenced me to buckle down learn it in earnest. I’m still a little clunky with it, but I’m functional enough to do things like &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;make this blog&lt;/a&gt;. With a little patience and practice, you can get there, too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;let-yihui-xie-guide-you&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let Yihui Xie guide you&lt;/h2&gt;
&lt;p&gt;So far we’ve covered&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R and R Studio&lt;/li&gt;
&lt;li&gt;Scripts and R Markdown files&lt;/li&gt;
&lt;li&gt;Git and GitHub&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You don’t have become an expert, but you’ll need to become roughly fluent in all three to make good use of bookdown. Basically, if you are able to load data into R, document a rudimentary analysis in an R Notebook file, and then share the project in a non-embarrassing way in GitHub, you’re ready to use bookdown.&lt;/p&gt;
&lt;p&gt;I’ve already mentioned it, but the authoritative work on bookdown is Yihui Xie’s &lt;a href=&#34;https://bookdown.org/yihui/bookdown/&#34;&gt;&lt;em&gt;bookdown: Authoring Books and Technical Documents with R Markdown&lt;/em&gt;&lt;/a&gt;. Yihui Xie, of course, is the author of the package. It’s probably best to just start there, going bit by bit. He also gave an RStudio webinar, &lt;a href=&#34;https://www.youtube.com/watch?v=dVqVscgwSpw&amp;amp;t=12s&#34;&gt;&lt;em&gt;Authoring Books with R Markdown&lt;/em&gt;&lt;/a&gt;, which I found to be a helpful supplement.&lt;/p&gt;
&lt;p&gt;The complete version of my &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt; with brms, ggplot2, and the tidyverse&lt;/a&gt; project has 15 chapters and several preamble sections. Almost all the chapters files include a lot of computationally-intensive code, with the simulations for chapter 6 taking multiple hours to compute. I do not recommend starting off with a project like that, at least not all at once. If you follow along with Yihui Xie’s guide, you’ll practice stitching together simple files, first. After learning those basics, I then picked up other helpful tricks, like &lt;a href=&#34;https://bookdown.org/yihui/bookdown/preview-a-chapter.html#&#34;&gt;caching analyses&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although I didn’t use these resources while I was learning bookdown, you might also benefit from checking out&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sean Kross’s &lt;a href=&#34;http://seankross.com/2016/11/17/How-to-Start-a-Bookdown-Book.html&#34;&gt;&lt;em&gt;How to Start a Bookdown Book&lt;/em&gt;&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Karl Broman’s &lt;a href=&#34;https://kbroman.org/blog/2017/05/31/omg-bookdown/&#34;&gt;&lt;em&gt;omg, bookdown!&lt;/em&gt;&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Rachael Lappan’s &lt;a href=&#34;https://rachaellappan.github.io/bookdown/&#34;&gt;&lt;em&gt;Using Bookdown for tidy documentation&lt;/em&gt;&lt;/a&gt;, or&lt;/li&gt;
&lt;li&gt;Pablo Casas’s &lt;a href=&#34;https://blog.datascienceheroes.com/how-to-self-publish-a-book/&#34;&gt;&lt;em&gt;How to self-publish a book: A handy list of resources&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

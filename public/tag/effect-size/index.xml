<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>effect size | Fahim Ahmad</title>
    <link>/tag/effect-size/</link>
      <atom:link href="/tag/effect-size/index.xml" rel="self" type="application/rss+xml" />
    <description>effect size</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Fahim Ahmad (2020)</copyright><lastBuildDate>Tue, 27 Jul 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>effect size</title>
      <link>/tag/effect-size/</link>
    </image>
    
    <item>
      <title>One-step Bayesian imputation when you have dropout in your RCT</title>
      <link>/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/</link>
      <pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/</guid>
      <description>
&lt;script src=&#34;/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble&lt;/h2&gt;
&lt;p&gt;Suppose you’ve got data from a randomized controlled trial (RCT) where participants received either treatment or control. Further suppose you only collected data at two time points, pre- and post-treatment. Even in the best of scenarios, you’ll probably have some dropout in those post-treatment data. To get the full benefit of your data, you can use one-step Bayesian imputation when you compute your effect sizes. In this post, I’ll show you how.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;For this post, I’m presuming you have a passing familiarity with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should be familiar with effect sizes, particularly with standardized mean differences. If you need to brush up, consider Cohen’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988&lt;/a&gt;)&lt;/span&gt; authoritative text, or Cummings newer &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; text. For nice conceptual overview, I also recommend Kelley and Preacher’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kelley2012effect&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; paper, &lt;a href=&#34;https://www3.nd.edu/~kkelley/publications/articles/Kelley_and_Preacher_Psychological_Methods_2012.pdf&#34;&gt;&lt;em&gt;On effect size&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You should be familiar with Bayesian regression. For thorough introductions, I recommend either edition of McElreath’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text; or Gelman, Hill, and Vehtari’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; text. If you go with McElreath, he has a fine series of freely-available lectures &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Though we won’t be diving deep into it, here, you’ll want to have some familiarity with contemporary missing data theory. You can find brief overviews in the texts by McElreath and Gelman et al, above. For a deeper dive, I recommend &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-enders2010applied&#34; role=&#34;doc-biblioref&#34;&gt;Enders&lt;/a&gt; (&lt;a href=&#34;#ref-enders2010applied&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-little2019statistical&#34; role=&#34;doc-biblioref&#34;&gt;Little &amp;amp; Rubin&lt;/a&gt; (&lt;a href=&#34;#ref-little2019statistical&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. Also, heads up: &lt;a href=&#34;https://twitter.com/AmandaKMontoya/status/1341936335301406722&#34;&gt;word on the street&lt;/a&gt; is Enders is working on a second edition of his book.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;. Data wrangling and plotting were done with help from the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt; and &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;. The data were simulated with help from the &lt;a href=&#34;https://github.com/debruine/faux&#34;&gt;&lt;strong&gt;faux&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-faux&#34; role=&#34;doc-biblioref&#34;&gt;DeBruine, 2021&lt;/a&gt;)&lt;/span&gt; and the Bayesian models were fit using &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we load our primary &lt;strong&gt;R&lt;/strong&gt; packages and adjust the global plotting theme defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(faux)
library(tidybayes)
library(brms)

# adjust the global plotting theme
theme_set(
  theme_tidybayes() +
    theme(axis.line.x = element_blank(),
          axis.line.y = element_blank(),
          panel.border = element_rect(color = &amp;quot;grey85&amp;quot;, size = 1, fill = NA))
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;For this post, we’ll be simulating our data with help from the handy &lt;code&gt;faux::rnorm_multi()&lt;/code&gt; function. To start out, we’ll make two data sets, one for treatment (&lt;code&gt;d_treatment&lt;/code&gt;) and one for control (&lt;code&gt;d_control&lt;/code&gt;). Each will contain outcomes at &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; treatment, with the population parameters for both conditions at &lt;code&gt;pre&lt;/code&gt; being &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(5, 1)\)&lt;/span&gt;. Whereas those parameters stay the same at &lt;code&gt;post&lt;/code&gt; for those in the control condition, the population parameters for those in the treatment condition will raise at &lt;code&gt;post&lt;/code&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(5.7, 1)\)&lt;/span&gt;. Notice that not only did their mean value increase, but their standard deviation increased a bit, too, which is not uncommon in treatment data. Importantly, the correlation between &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; is &lt;span class=&#34;math inline&#34;&gt;\(.75\)&lt;/span&gt; for both conditions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many per group?
n &amp;lt;- 100

set.seed(1)

d_treatment &amp;lt;- rnorm_multi(
  n = n,
  mu = c(5, 5.7),
  sd = c(1, 1.1), 
  r = .75, 
  varnames = list(&amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;)
)

d_control &amp;lt;- rnorm_multi(
  n = n,
  mu = c(5, 5),
  sd = c(1, 1), 
  r = .75, 
  varnames = list(&amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we combine the two data sets and make an explicit &lt;code&gt;tx&lt;/code&gt; variable to distinguish the conditions. Then we simulate missingness in the &lt;code&gt;post&lt;/code&gt; variable in two steps: We use the &lt;code&gt;rbinom()&lt;/code&gt; function to simulate whether a case will be missing and then use a little &lt;code&gt;ifelse()&lt;/code&gt; to make a &lt;code&gt;post_observed&lt;/code&gt; variable that is the same as &lt;code&gt;post&lt;/code&gt; except that the vales are missing in every row for which &lt;code&gt;missing == 1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

d &amp;lt;- bind_rows(
  d_control,
  d_treatment
) %&amp;gt;% 
  mutate(tx = rep(c(&amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;), each = n)) %&amp;gt;% 
  mutate(tx = factor(tx, levels = c(&amp;quot;treatment&amp;quot;, &amp;quot;control&amp;quot;))) %&amp;gt;% 
  mutate(missing = rbinom(n(), size = 1, prob = 0.3)) %&amp;gt;%
  mutate(post_observed = ifelse(missing == 1, NA, post))

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        pre     post      tx missing post_observed
## 1 5.066999 5.698922 control       0      5.698922
## 2 6.950072 6.209520 control       0      6.209520
## 3 5.787145 7.181091 control       0      7.181091
## 4 4.826099 4.554830 control       1            NA
## 5 2.277529 3.447187 control       0      3.447187
## 6 6.801701 7.870996 control       1            NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get a sense for the data, here’s a scatter plot of &lt;code&gt;pre&lt;/code&gt; versus &lt;code&gt;post&lt;/code&gt;, by &lt;code&gt;tx&lt;/code&gt; and &lt;code&gt;missing&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  mutate(missing = factor(missing,
                          levels = 1:0,
                          labels = c(&amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = pre, y = post, color = missing, shape = missing)) +
  geom_point() +
  scale_color_viridis_d(option = &amp;quot;D&amp;quot;, begin = .35, end = .75, direction = -1) +
  scale_shape_manual(values = 17:16) +
  coord_equal(xlim = c(1.5, 8.5),
              ylim = c(1.5, 8.5)) +
  facet_wrap(~ tx)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/index_files/figure-html/geom_point-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that high correlation between &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; in the shapes of the data clouds. To look at the data in another way, here are a few summary statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;p&amp;quot;), names_to = &amp;quot;time&amp;quot;) %&amp;gt;% 
  mutate(time = factor(time, levels = c(&amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;, &amp;quot;post_observed&amp;quot;))) %&amp;gt;% 
  group_by(tx, time) %&amp;gt;% 
  summarise(mean = mean(value, na.rm = T),
            sd = sd(value, na.rm = T),
            min = min(value, na.rm = T),
            max = max(value, na.rm = T)) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
## # Groups:   tx [2]
##   tx        time           mean    sd   min   max
##   &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 treatment pre            5.11  0.91  3.22  7.24
## 2 treatment post           5.8   0.99  3.16  8.35
## 3 treatment post_observed  5.74  1.05  3.16  8.35
## 4 control   pre            5.01  0.99  2.28  7.5 
## 5 control   post           5.05  1.06  1.74  7.87
## 6 control   post_observed  5.1   1.04  1.74  7.45&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistical models&lt;/h2&gt;
&lt;p&gt;We’ll be analyzing the RCT data in two ways. First, we’ll fit a model on the version of the data with no missingness in &lt;code&gt;post&lt;/code&gt;. That will be our benchmark. Then we’ll practice fitting the model with one-step Bayesian imputation and the &lt;code&gt;post_observed&lt;/code&gt; variable. Once we’ve fit and evaluated our models, we’ll then walk out how to compute the effect sizes.&lt;/p&gt;
&lt;div id=&#34;fit-the-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the models.&lt;/h3&gt;
&lt;p&gt;There are a lot of ways to analyze pre/post RCT data. To get a sense of the various strategies, see &lt;a href=&#34;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/models-for-longitudinal-experiments-pre-post-designs.html&#34;&gt;this chapter&lt;/a&gt; in Jeffrey Walker’s free &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-walkerElementsOfStatisticalModeling2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; text and my &lt;a href=&#34;https://solomonkurz.netlify.app/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/&#34;&gt;complimentary blog post&lt;/a&gt; on pre/post non-experimental data. In this post, we’ll be taking the multivariate approach where we simultaneously model &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; as bivariate normal, such that both the mean and standard deviation parameters for both vary depending on the experimental condition (&lt;code&gt;tx&lt;/code&gt;). Importantly, the correlation between &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; is captured in the correlation between the two residual standard deviation parameters.&lt;/p&gt;
&lt;p&gt;Here’s how to fit the model to the full data with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- brm(
  data = d,
  family = gaussian,
  bf(pre ~ 0 + tx, sigma ~ 0 + tx) +
    bf(post ~ 0 + tx, sigma ~ 0 + tx) +
    set_rescor(rescor = TRUE),
  prior = c(prior(normal(5, 1), class = b, resp = pre),
            prior(normal(5, 1), class = b, resp = post),
            prior(normal(log(1), 1), class = b, dpar = sigma, resp = pre),
            prior(normal(log(1), 1), class = b, dpar = sigma, resp = post),
            prior(lkj(2), class = rescor)),
  cores = 4,
  seed = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The priors in this post follow the weakly-regularizing approach McElreath advocated for in the second edition of this text. Also note that because we are allowing the residual standard deviation parameter to vary by &lt;code&gt;tx&lt;/code&gt;, the &lt;strong&gt;brms&lt;/strong&gt; default is to use the log link, which will become important for interpretation and post processing. Here’s the parameter summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = log
##          mu = identity; sigma = log 
## Formula: pre ~ 0 + tx 
##          sigma ~ 0 + tx
##          post ~ 0 + tx 
##          sigma ~ 0 + tx
##    Data: d (Number of observations: 200) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## pre_txtreatment            5.00      0.10     4.81     5.19 1.00     2802     3091
## pre_txcontrol              5.11      0.09     4.93     5.29 1.00     3300     2885
## sigma_pre_txtreatment     -0.03      0.06    -0.15     0.09 1.00     3700     3273
## sigma_pre_txcontrol       -0.08      0.07    -0.21     0.06 1.00     3536     3105
## post_txtreatment           5.74      0.11     5.52     5.96 1.00     3027     2940
## post_txcontrol             5.08      0.09     4.90     5.26 1.00     3305     2758
## sigma_post_txtreatment     0.14      0.06     0.02     0.27 1.00     3555     3230
## sigma_post_txcontrol      -0.08      0.07    -0.21     0.05 1.00     3607     2764
## 
## Residual Correlations: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(pre,post)     0.73      0.03     0.67     0.79 1.00     3475     3305
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After exponentiating the standard deviations, all the parameter summaries look close to the data generating values from our &lt;code&gt;faux::rnorm_multi()&lt;/code&gt; code. This, however, is all just a warm-up. Our goal was to use one-step Bayesian imputation for when we have missing data at post-intervention time point. From a syntax perspective, that involves a few minor changes to our &lt;code&gt;fit1&lt;/code&gt; code. First, we replace the &lt;code&gt;post&lt;/code&gt; variable with &lt;code&gt;post_observed&lt;/code&gt;, which had about &lt;span class=&#34;math inline&#34;&gt;\(30\%\)&lt;/span&gt; of the values missing. In a similar way, we have to adjust a few of the &lt;code&gt;resp&lt;/code&gt; arguments within the &lt;code&gt;prior()&lt;/code&gt; statements. Finally and most crucially, we have to include the &lt;code&gt;| mi()&lt;/code&gt; syntax when defining the linear model for &lt;code&gt;post_observed&lt;/code&gt;. Otherwise, &lt;strong&gt;brms&lt;/strong&gt; will simply drop all the cases with missingness on &lt;code&gt;post_observed&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here’s the code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- brm(
  data = d,
  family = gaussian,
  bf(pre ~ 0 + tx,
     sigma ~ 0 + tx) +
    # notice the changes in this line
    bf(post_observed | mi() ~ 0 + tx,
       sigma ~ 0 + tx) +
    set_rescor(rescor = TRUE),
  prior = c(prior(normal(5, 1), class = b, resp = pre),
            # notice the changes in the `resp` argument
            prior(normal(5, 1), class = b, resp = postobserved),
            prior(normal(log(1), 1), class = b, dpar = sigma, resp = pre),
            # notice the changes in the `resp` argument
            prior(normal(log(1), 1), class = b, dpar = sigma, resp = postobserved),
            prior(lkj(2), class = rescor)),
  cores = 4,
  seed = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of looking at the &lt;code&gt;print()&lt;/code&gt; output, it might be more informative if we compare the results of our two models in a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the parameter names
parameters &amp;lt;- c(
  &amp;quot;mu[treatment]^pre&amp;quot;, &amp;quot;mu[control]^pre&amp;quot;, &amp;quot;sigma[treatment]^pre&amp;quot;, &amp;quot;sigma[control]^pre&amp;quot;,
  &amp;quot;mu[treatment]^post&amp;quot;, &amp;quot;mu[control]^post&amp;quot;, &amp;quot;sigma[treatment]^post&amp;quot;, &amp;quot;sigma[control]^post&amp;quot;, 
  &amp;quot;rho&amp;quot;
  )

# define the deisred order for the parameter names
levels &amp;lt;- c(
  &amp;quot;mu[treatment]^pre&amp;quot;, &amp;quot;mu[control]^pre&amp;quot;, &amp;quot;mu[treatment]^post&amp;quot;, &amp;quot;mu[control]^post&amp;quot;, 
  &amp;quot;sigma[treatment]^pre&amp;quot;, &amp;quot;sigma[control]^pre&amp;quot;, &amp;quot;sigma[treatment]^post&amp;quot;, &amp;quot;sigma[control]^post&amp;quot;, 
  &amp;quot;rho&amp;quot;
  )

# combine the posterior summaries for the two models
rbind(
  posterior_summary(fit1)[1:9, -2],
  posterior_summary(fit2)[1:9, -2]
  ) %&amp;gt;% 
  # wrangle
  data.frame() %&amp;gt;% 
  mutate(data = rep(c(&amp;quot;complete data&amp;quot;, &amp;quot;30% missing&amp;quot;), each = n() / 2),
         par  = rep(parameters, times = 2)) %&amp;gt;% 
  mutate(par      = factor(par, levels = levels),
         Estimate = ifelse(str_detect(par, &amp;quot;sigma&amp;quot;), exp(Estimate), Estimate),
         Q2.5     = ifelse(str_detect(par, &amp;quot;sigma&amp;quot;), exp(Q2.5), Q2.5),
         Q97.5    = ifelse(str_detect(par, &amp;quot;sigma&amp;quot;), exp(Q97.5), Q97.5)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = data)) +
  geom_pointrange(fatten = 1.1) +
  labs(x = &amp;quot;marginal posterior&amp;quot;,
       y = NULL) +
  xlim(0, NA) +
  facet_wrap(~ par, labeller = label_parsed, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/index_files/figure-html/coefficient_plot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since the &lt;code&gt;post_observed&lt;/code&gt; data were missing completely at random (MCAR&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;), it should be no surprise the coefficients are nearly the same between the two models. This, however, will not always (ever?) be the case with your real-world RCT data. Friends, don’t let your friends drop cases or carry the last value forward. Use the &lt;strong&gt;brms&lt;/strong&gt; &lt;code&gt;mi()&lt;/code&gt; syntax, instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;effect-sizes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Effect sizes.&lt;/h3&gt;
&lt;p&gt;At this point, you may be wondering why I didn’t use the familiar dummy-variable approach in either of the models and you might be further wondering why I bothered to allow the standard deviation parameters to vary. One of the sub-goals of this post is to show how to compute the model output into standardized effect sizes. My go-to standardized effect size is good old Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, of which there are many variations. In the case of our pre/post RCT with two conditions, we actually have three &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;’s of interest:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the standardized mean difference for the treatment condition (which we hope is large),&lt;/li&gt;
&lt;li&gt;the standardized mean difference for the control condition (which we hope is near zero), and&lt;/li&gt;
&lt;li&gt;the difference in those first two standardized mean differences (which we also hope is large).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As with all standardized mean differences, it’s a big deal to choose a good value to standardize with. With data like ours, a good default choice is the pooled standard deviation between the two conditions at baseline, which we might define as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_p^\text{pre} = \sqrt{\frac{ \left (\sigma_\text{treatment}^\text{pre} \right )^2 + \left (\sigma_\text{control}^\text{pre} \right)^2}{2}},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the notation is admittedly a little verbose. My hope, however, is this notation will make it easier to map the terms onto the model parameters from above. Anyway, with our definition of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_p^\text{pre}\)&lt;/span&gt; in hand, we’re in good shape to define our three effect sizes of interest as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
d_\text{treatment} &amp;amp; = \frac{\mu_\text{treatment}^\text{post} - \mu_\text{treatment}^\text{pre}}{\sigma_p^\text{pre}}, \\
d_\text{control}   &amp;amp; = \frac{\mu_\text{control}^\text{post} - \mu_\text{control}^\text{pre}}{\sigma_p^\text{pre}}, \; \text{and} \\
d_\text{treatment - control} &amp;amp; = \frac{\left ( \mu_\text{treatment}^\text{post} - \mu_\text{treatment}^\text{pre} \right ) - \left ( \mu_\text{control}^\text{post} - \mu_\text{control}^\text{pre} \right )}{\sigma_p^\text{pre}} \\
&amp;amp; = \left (\frac{\mu_\text{treatment}^\text{post} - \mu_\text{treatment}^\text{pre}}{\sigma_p^\text{pre}} \right ) - \left ( \frac{\mu_\text{control}^\text{post} - \mu_\text{control}^\text{pre}}{\sigma_p^\text{pre}} \right ) \\
&amp;amp; = \left ( d_\text{treatment} \right ) - \left ( d_\text{control} \right ).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The reason we analyzed the RCT data with a bivariate model with varying means and standard deviations was because the parameter values from that model correspond directly with the various &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; terms in the equations for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_p^\text{pre}\)&lt;/span&gt; and our three &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;’s. This insight comes from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke&lt;/a&gt; (&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;, particularly Section 16.3. For a walk-through of that section with a &lt;strong&gt;brms&lt;/strong&gt; + &lt;strong&gt;tidyverse&lt;/strong&gt; workflow, see &lt;a href=&#34;https://bookdown.org/content/3686/metric-predicted-variable-on-one-or-two-groups.html#two-groups&#34;&gt;this section&lt;/a&gt; of my ebook translation of his text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2021&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;)&lt;/span&gt;. The approach we’re taking, here, is a direct bivariate generalization of the material in Kruschke’s text.&lt;/p&gt;
&lt;p&gt;Okay, here’s how to work with the posterior samples from our missing-data model, &lt;code&gt;fit2&lt;/code&gt;, to compute those effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(fit2) %&amp;gt;% 
  # exponentiate the log sd parameters
  mutate(`sigma[treatment]^pre` = exp(b_sigma_pre_txtreatment),
         `sigma[control]^pre`   = exp(b_sigma_pre_txcontrol)) %&amp;gt;% 
  # pooled standard deviation (at pre)
  mutate(`sigma[italic(p)]^pre` = sqrt((`sigma[treatment]^pre`^2 + `sigma[control]^pre`^2) / 2)) %&amp;gt;% 
  # within-condition pre/post effect sizes
  mutate(`italic(d)[treatment]` = (b_postobserved_txtreatment - b_pre_txtreatment) / `sigma[italic(p)]^pre`,
         `italic(d)[control]`   = (b_postobserved_txcontrol   - b_pre_txcontrol)   / `sigma[italic(p)]^pre`) %&amp;gt;%
  # between-condition effect size (i.e., difference in differences)
  mutate(`italic(d)[&amp;#39;treatment - control&amp;#39;]` = `italic(d)[treatment]` - `italic(d)[control]`) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now inspect the posteriors for our three &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;’s and the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_p^\text{pre}\)&lt;/span&gt; in a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels &amp;lt;- c(
  &amp;quot;sigma[italic(p)]^pre&amp;quot;, &amp;quot;italic(d)[&amp;#39;treatment - control&amp;#39;]&amp;quot;, 
  &amp;quot;italic(d)[control]&amp;quot;, &amp;quot;italic(d)[treatment]&amp;quot;
  )

post %&amp;gt;% 
  # wrangle
  pivot_longer(`sigma[italic(p)]^pre`:`italic(d)[&amp;#39;treatment - control&amp;#39;]`) %&amp;gt;% 
  mutate(parameter = factor(name, levels = levels)) %&amp;gt;% 
  
  # plot
ggplot(aes(x = value, y = parameter)) +
  stat_pointinterval(.width = .95, size = 1/2) +
  scale_y_discrete(labels = ggplot2:::parse_safe) +
  labs(x = &amp;quot;marginal posterior&amp;quot;,
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/index_files/figure-html/d_plot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you look back at the data-generating values from above, our effect sizes are about where we’d hope them to be.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-what-about-that-one-step-imputation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;But what about that one-step imputation?&lt;/h3&gt;
&lt;p&gt;From a practical standpoint, one-step Bayesian imputation is a lot like full-information maximum likelihood or multiple imputation–it’s a way to use all of your data that allows you to avoid the biases that come with older methods such as mean imputation or last observation carried forward. In short, one-step Bayesian imputation fits a joint model that expresses both the uncertainty in the model parameters and the uncertainty in the missing data. When we use MCMC methods, the uncertainty in our model parameters is expressed in the samples from the posterior. We worked with those with our &lt;code&gt;posterior_samples()&lt;/code&gt; code, above. In the same way, one-step Bayesian imputation with MCMC also gives us posterior samples for the missing data, too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit2) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of space, I’m not going to show the results of the code block, above. If you were to execute it yourself, you’d see there were a bunch of &lt;code&gt;Ymi_postobserved[i]&lt;/code&gt; columns. Those columns contain the posterior samples for the missing values. The &lt;code&gt;i&lt;/code&gt; part of their names indexes the row number from the original data which had the missing &lt;code&gt;post_observed&lt;/code&gt; value. Just like with the posterior samples of our parameters, we can examine the posterior samples for our missing data with plots, summaries, and so on. Here instead of using the &lt;code&gt;posterior_samples()&lt;/code&gt; output, we’ll use the &lt;code&gt;posterior_summary()&lt;/code&gt; function, instead. This will summarize each parameter and imputed value by its mean, standard deviation, and percentile-based 95% interval. After a little wrangling, we’ll display the results in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_summary(fit2) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  rownames_to_column(&amp;quot;par&amp;quot;) %&amp;gt;% 
  # isolate the imputed values
  filter(str_detect(par, &amp;quot;Ymi&amp;quot;)) %&amp;gt;% 
  mutate(row = str_extract(par, &amp;quot;\\d+&amp;quot;) %&amp;gt;% as.integer()) %&amp;gt;% 
  # join the original data
  left_join(
    d %&amp;gt;% mutate(row = 1:n()),
    by = &amp;quot;row&amp;quot;
  ) %&amp;gt;% 

  # plot!
  ggplot(aes(x = pre, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = tx)) +
  geom_pointrange(fatten = 1, size = 1/4) +
  scale_color_viridis_d(NULL, option = &amp;quot;F&amp;quot;, begin = .2, end = .6, direction = -1) +
  ylab(&amp;quot;post (imputed)&amp;quot;) +
  coord_equal(xlim = c(1.5, 8.5),
              ylim = c(1.5, 8.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We ended up with something of a mash-up of a scatter plot and a coefficient plot. The &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis shows the summaries for the imputed values, summarized by their posterior means (dots) and 95% intervals (vertical lines). In the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis, we’ve connected them with their original &lt;code&gt;pre&lt;/code&gt; values. Notice the strong correlation between the two axes. That’s the consequence of fitting a bivariate model where &lt;code&gt;pre&lt;/code&gt; has a residual correlation with &lt;code&gt;post_observed&lt;/code&gt;. That original data-generating value, recall, was &lt;span class=&#34;math inline&#34;&gt;\(.75\)&lt;/span&gt;. Here’s the summary of the residual correlation from &lt;code&gt;fit2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_summary(fit2)[&amp;quot;rescor__pre__postobserved&amp;quot;, ] %&amp;gt;% 
  round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
##      0.71      0.04      0.63      0.78&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using language perhaps more familiar to those from a structural equation modeling background, the &lt;code&gt;pre&lt;/code&gt; values acted like a missing data covariate for the missing &lt;code&gt;post_observed&lt;/code&gt; values. Had that residual correlation been lower, the relation in the two axes of our plot would have been less impressive, too. Anyway, the point is that one-step Bayesian imputation gives users a nice way to explore the missing data assumptions they’ve imposed in their models, which I think is pretty cool.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;would-you-like-more&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Would you like more?&lt;/h2&gt;
&lt;p&gt;To my knowledge, the introductory material on applied missing data analysis seems awash with full-information maximum likelihood and multiple imputation. One-step Bayesian imputation methods haven’t made it into the mainstream, yet. McElreath covered the one-step approach in both editions of his text and since the way he covered the material was quite different in the two editions, I really recommend you check out both &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;. My ebook translations of McElreath’s texts covered that material from a &lt;strong&gt;brms&lt;/strong&gt; + &lt;strong&gt;tidyverse&lt;/strong&gt; perspective &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2021&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2021&lt;/a&gt;, &lt;a href=&#34;#ref-kurzStatisticalRethinkingBrms2020&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt;. Otherwise, you should check out Bürkner’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021HandleMissingValues&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; vignette, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html&#34;&gt;&lt;em&gt;Handle missing values with brms&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are aware of any other applied text books covering one-step Bayesian imputation, please drop a comment on this tweet.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New blog up!&lt;a href=&#34;https://t.co/Drxd9jrdp1&#34;&gt;https://t.co/Drxd9jrdp1&lt;/a&gt;&lt;br&gt;&lt;br&gt;This time we explore how to handle missing data in a 2-timepoint RCT with the one-step Bayesian imputation approach. It’s slick and simple and another reason to love &lt;a href=&#34;https://twitter.com/hashtag/brms?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#brms&lt;/a&gt;.&lt;/p&gt;&amp;mdash; Solomon Kurz (@SolomonKurz) &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1420434272142434304?ref_src=twsrc%5Etfw&#34;&gt;July 28, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] brms_2.15.0     Rcpp_1.0.6      tidybayes_3.0.0 faux_1.0.0      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.6    
##  [8] purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.2    ggplot2_3.3.5   tidyverse_1.3.1
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6         svUnit_1.0.3        
##   [6] splines_4.0.4        crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17       
##  [11] digest_0.6.27        htmltools_0.5.1.1    rsconnect_0.8.16     fansi_0.4.2          checkmate_2.0.0     
##  [16] magrittr_2.0.1       modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1          
##  [21] sandwich_3.0-0       prettyunits_1.1.1    colorspace_2.0-0     rvest_1.0.1          ggdist_3.0.0        
##  [26] haven_2.3.1          xfun_0.23            callr_3.7.0          crayon_1.4.1         jsonlite_1.7.2      
##  [31] lme4_1.1-25          survival_3.2-10      zoo_1.8-8            glue_1.4.2           gtable_0.3.0        
##  [36] emmeans_1.5.2-1      V8_3.4.0             distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2        
##  [41] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [46] viridisLite_0.4.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [51] htmlwidgets_1.5.3    httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0   posterior_1.0.1     
##  [56] ellipsis_0.3.2       farver_2.1.0         pkgconfig_2.0.3      loo_2.4.1            sass_0.3.1          
##  [61] dbplyr_2.1.1         utf8_1.2.1           labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.11        
##  [66] reshape2_1.4.4       later_1.2.0          munsell_0.5.0        cellranger_1.1.0     tools_4.0.4         
##  [71] cli_3.0.1            generics_0.1.0       broom_0.7.6          ggridges_0.5.3       evaluate_0.14       
##  [76] fastmap_1.1.0        yaml_2.2.1           fs_1.5.0             processx_3.5.2       knitr_1.33          
##  [81] nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2           compiler_4.0.4      
##  [86] bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13      curl_4.3             gamm4_0.2-6         
##  [91] reprex_2.0.0         statmod_1.4.35       bslib_0.2.4          stringi_1.6.2        highr_0.9           
##  [96] ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2        
## [101] nloptr_1.2.2.2       markdown_1.1         tensorA_0.36.2       shinyjs_2.0.0        vctrs_0.3.8         
## [106] pillar_1.6.1         lifecycle_1.0.0      jquerylib_0.1.4      bridgesampling_1.0-0 estimability_1.3    
## [111] httpuv_1.6.0         R6_2.5.0             bookdown_0.22        promises_1.2.0.1     gridExtra_2.3       
## [116] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [121] assertthat_0.2.1     withr_2.4.2          shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33         
## [126] parallel_4.0.4       hms_1.1.0            grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [131] rmarkdown_2.8        shiny_1.6.0          lubridate_1.7.10     base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-Bürkner2021HandleMissingValues&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Handle missing values with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cohenStatisticalPowerAnalysis1988a&#34; class=&#34;csl-entry&#34;&gt;
Cohen, J. (1988). &lt;em&gt;Statistical power analysis for the behavioral sciences&lt;/em&gt;. &lt;span&gt;L. Erlbaum Associates&lt;/span&gt;. &lt;a href=&#34;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&#34;&gt;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummingUnderstandingTheNewStatistics2012&#34; class=&#34;csl-entry&#34;&gt;
Cumming, G. (2012). &lt;em&gt;Understanding the new statistics: &lt;span&gt;Effect&lt;/span&gt; sizes, confidence intervals, and meta-analysis&lt;/em&gt;. &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&#34;&gt;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-faux&#34; class=&#34;csl-entry&#34;&gt;
DeBruine, L. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;faux&lt;/span&gt;: &lt;span&gt;Simulation&lt;/span&gt; for factorial designs&lt;/em&gt; [Manual]. &lt;a href=&#34;https://github.com/debruine/faux&#34;&gt;https://github.com/debruine/faux&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-enders2010applied&#34; class=&#34;csl-entry&#34;&gt;
Enders, C. K. (2010). &lt;em&gt;Applied missing data analysis&lt;/em&gt;. &lt;span&gt;Guilford press&lt;/span&gt;. &lt;a href=&#34;http://www.appliedmissingdata.com/&#34;&gt;http://www.appliedmissingdata.com/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kelley2012effect&#34; class=&#34;csl-entry&#34;&gt;
Kelley, K., &amp;amp; Preacher, K. J. (2012). On effect size. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;17&lt;/em&gt;(2), 137. &lt;a href=&#34;https://doi.org/10.1037/a0028086&#34;&gt;https://doi.org/10.1037/a0028086&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2021&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2021). &lt;em&gt;Statistical rethinking with brms, &lt;span class=&#34;nocase&#34;&gt;ggplot2&lt;/span&gt;, and the tidyverse: &lt;span&gt;Second Edition&lt;/span&gt;&lt;/em&gt; (version 0.2.0). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzDoingBayesianData2021&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis in brms and the tidyverse&lt;/em&gt; (version 0.4.0). &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingBrms2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Statistical rethinking with brms, &lt;span class=&#34;nocase&#34;&gt;ggplot2&lt;/span&gt;, and the tidyverse&lt;/em&gt; (version 1.2.0). &lt;a href=&#34;https://doi.org/10.5281/zenodo.3693202&#34;&gt;https://doi.org/10.5281/zenodo.3693202&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34; class=&#34;csl-entry&#34;&gt;
Little, R. J., &amp;amp; Rubin, D. B. (2019). &lt;em&gt;Statistical analysis with missing data&lt;/em&gt; (third, Vol. 793). &lt;span&gt;John Wiley &amp;amp; Sons&lt;/span&gt;. &lt;a href=&#34;https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798&#34;&gt;https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-walkerElementsOfStatisticalModeling2018&#34; class=&#34;csl-entry&#34;&gt;
Walker, J. A. (2018). &lt;em&gt;Elements of statistical modeling for experimental biology&lt;/em&gt; (&#34;2020–11th–22&#34; ed.). &lt;a href=&#34;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/&#34;&gt;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;As has been noted by others &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020&lt;/a&gt;)&lt;/span&gt; missing-data jargon is generally awful. I’m so sorry you have to contend with acronyms like MCAR, MAR (missing at random) and MNAR (missing not at random), but that’s just the way it is. If you’re not sure about the difference between the three, do consider spending some time with one of the missing data texts I recommended, above.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Effect sizes for experimental trials analyzed with multilevel growth models: Two of two</title>
      <link>/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/</link>
      <pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/</guid>
      <description>
&lt;script src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;orientation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Orientation&lt;/h2&gt;
&lt;p&gt;This post is the second and final installment of a two-part series. In the &lt;a href=&#34;https://solomonkurz.netlify.app/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/&#34;&gt;first post&lt;/a&gt;, we explored how one might compute an effect size for two-group experimental data with only &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; time points. In this second post, we fulfill our goal to show how to generalize this framework to experimental data collected over &lt;span class=&#34;math inline&#34;&gt;\(3+\)&lt;/span&gt; time points. The data and overall framework come from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;i-still-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I still make assumptions.&lt;/h3&gt;
&lt;p&gt;As with the &lt;a href=&#34;https://solomonkurz.netlify.app/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/#i-make-assumptions.&#34;&gt;first post&lt;/a&gt;, I make a handful of assumptions about your background knowledge. Though I won’t spell them out again, here, I should stress that you’ll want to be familiar with multilevel models to get the most out of this post. To brush up, I recommend &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;Raudenbush &amp;amp; Bryk&lt;/a&gt; (&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett&lt;/a&gt; (&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As before, all code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;. Here we load our primary &lt;strong&gt;R&lt;/strong&gt; packages–&lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;, and the &lt;a href=&#34;http://style.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;–and adjust the global plotting theme defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)
library(tidybayes)
library(tidyverse)

# adjust the global plotting theme
theme_set(
  theme_linedraw() +
    theme(text = element_text(family = &amp;quot;Times&amp;quot;),
          panel.grid = element_blank(),
          strip.text = element_text(margin = margin(b = 3, t = 3)))
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;Once again, we use the &lt;a href=&#34;https://tibble.tidyverse.org/reference/tribble.html&#34;&gt;tribble&lt;/a&gt; approach to enter the synthetic data Feingold displayed in his Table 1 (p. 46).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  tribble(
    ~id, ~tx, ~t1, ~t2, ~t3, ~t4,
    101, -0.5, 3, 5, 5,  7,
    102, -0.5, 4, 4, 6,  6,
    103, -0.5, 4, 5, 7,  8,
    104, -0.5, 5, 6, 6,  8,
    105, -0.5, 5, 6, 7,  8,
    106, -0.5, 5, 7, 7,  7,
    107, -0.5, 5, 6, 8,  8,
    108, -0.5, 6, 6, 7,  9,
    109, -0.5, 6, 8, 9,  10,
    110, -0.5, 7, 7, 8,  9,
    111,  0.5, 3, 5, 7,  9,
    112,  0.5, 4, 7, 9,  11,
    113,  0.5, 4, 6, 8,  11,
    114,  0.5, 5, 7, 9,  10,
    115,  0.5, 5, 6, 9,  11,
    116,  0.5, 5, 7, 10, 10,
    117,  0.5, 5, 8, 8,  11,
    118,  0.5, 6, 7, 9,  12,
    119,  0.5, 6, 9, 11, 13,
    120,  0.5, 7, 8, 10, 12
  ) %&amp;gt;% 
  mutate(`t4-t1`   = t4 - t1,
         condition = ifelse(tx == -0.5, &amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;))

# inspect the first six rows
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##      id    tx    t1    t2    t3    t4 `t4-t1` condition
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    
## 1   101  -0.5     3     5     5     7       4 control  
## 2   102  -0.5     4     4     6     6       2 control  
## 3   103  -0.5     4     5     7     8       4 control  
## 4   104  -0.5     5     6     6     8       3 control  
## 5   105  -0.5     5     6     7     8       3 control  
## 6   106  -0.5     5     7     7     7       2 control&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To reacquaint ourselves with the data, we might make a plot. Last time we plotted a subset of the individual trajectories next to the averages, by treatment group. Here we’ll superimpose all the individual-level trajectories atop the group averages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  pivot_longer(t1:t4) %&amp;gt;% 
  mutate(time      = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double(),
         condition = ifelse(tx &amp;lt; 0, &amp;quot;tx = -0.5 (control)&amp;quot;, &amp;quot;tx = 0.5 (treatment)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = time, y = value)) +
  stat_smooth(aes(color = condition),
              method = &amp;quot;lm&amp;quot;, formula = &amp;#39;y ~ x&amp;#39;,
              se = F, size = 4) +
  geom_line(aes(group = id),
            size = 1/4) +
  scale_color_viridis_d(end = .75, direction = -1, breaks = NULL) +
  facet_wrap(~ condition)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig1-1.png&#34; width=&#34;624&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The thick lines are the group averages and the thinner lines are for the individual participants. Though participants tend to increase in both groups, those in the treatment condition appear to have increased at a more rapid pace. We want a standardized effect size that can capture those differences in a familiar metric. We’ll begin to explain what that will be, next.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;div id=&#34;we-need-a-framework.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need a framework.&lt;/h3&gt;
&lt;p&gt;Traditional analytic strategies, such as ordinary least squares (OLS) regression and the analysis of variance (ANOVA) framework, can work okay with data collected on one or two time points. In his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;, &lt;a href=&#34;#ref-feingoldARegressionFramework2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; work, which is the inspiration for this blog series, Feingold recommended what he called growth-modeling analysis (GMA) for data collected on &lt;span class=&#34;math inline&#34;&gt;\(3+\)&lt;/span&gt; time points. If you’re not familiar with the term GMA, it’s a longitudinal version of what others have called hierarchical linear models, mixed-effects models, random-effects models, or multilevel models. For longitudinal data, I’m fond of the term &lt;em&gt;multilevel growth model&lt;/em&gt;, but you can use whatever term you like. If you’re interested, Raudenbush and Bryk touched on the historic origins of several of these terms in the first chapter of their &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt; text.&lt;/p&gt;
&lt;p&gt;Though multilevel growth models, GMAs, have become commonplace in many applied areas, it’s not immediately obvious how to compute standardized effect sizes when one uses them. In his Discussion section, Feingold &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009, p. 49&lt;/a&gt;)&lt;/span&gt; pointed out this topic is missing from many text books and software user’s guides. For example, though I took five statistics courses in graduate school, one of which even focused on the longitudinal growth model, none of my courses covered how to compute an effect size in a longitudinal growth model and none of my text books covered the topic, either. It’s hard to expect researchers to use strategies we don’t bother to teach, which is the reason for this blog series.&lt;/p&gt;
&lt;p&gt;We might walk out the framework with statistical notation. If we say our outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; varies across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; partitcipants and &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; time points, we might use Feingold’s Raudenbusch-&amp;amp;-Bryk-type notation to express our upcoming statistical model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{align*}
y_{ti} &amp;amp; = \beta_{00} + \beta_{01} (\text{treatment})_i + \beta_{10} (\text{time})_{ti} + \color{darkred}{\beta_{11}} (\text{treatment})_i (\text{time})_{ti} \\
&amp;amp; \;\;\; + [r_{0i} + r_{1i} (\text{time})_{ti} + e_{ti}],
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where variance in &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; is decomposed into the last three terms, &lt;span class=&#34;math inline&#34;&gt;\(r_{0i}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(r_{1i}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(e_{ti}\)&lt;/span&gt;. Here we follow the usual assumption that within-participant variance is normally distributed, &lt;span class=&#34;math inline&#34;&gt;\(e_{ti} \sim \operatorname N(0, \sigma_\epsilon^2)\)&lt;/span&gt;, and the &lt;span class=&#34;math inline&#34;&gt;\(r_{\text{x}i}\)&lt;/span&gt; values follow a bivariate normal distribution,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{bmatrix} r_{0i} \\ r_{1i} \end{bmatrix} \sim \operatorname N \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \tau_{00} &amp;amp; \tau_{01} \\ \tau_{01} &amp;amp; \tau_{11} \end{bmatrix} \right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; terms on the diagonal are the variances and the off-diagonal &lt;span class=&#34;math inline&#34;&gt;\(\tau_{01}\)&lt;/span&gt; is their covariance. We’ll be fitting this model with Bayesian software, which means all parameters will be given prior distributions. But since our goal is to emphasize the effect size and the multilevel framework, I’m just going to use the &lt;strong&gt;brms&lt;/strong&gt; default settings for the priors and will avoid expressing them in formal statistical notation&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this model, the four &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters are often called the “fixed effects,” or the population parameters. Our focal parameter will be &lt;span class=&#34;math inline&#34;&gt;\(\color{darkred}{\beta_{11}}\)&lt;/span&gt;, which is why we marked it off in red. This parameter is the interaction between time and treatment condition. Put another way, &lt;span class=&#34;math inline&#34;&gt;\(\color{darkred}{\beta_{11}}\)&lt;/span&gt; is the difference in the average rate of change, by treatment. Once we fit our multilevel growth model, we will explore how one might transform the &lt;span class=&#34;math inline&#34;&gt;\(\color{darkred}{\beta_{11}}\)&lt;/span&gt; parameter into our desired effect size.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model.&lt;/h3&gt;
&lt;p&gt;As you’ll learn in any good multilevel text book, multilevel models typically require the data to be in the long format. Here we’ll transform our data into that format and call the results &lt;code&gt;d_long&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wrangle
d_long &amp;lt;-
  d %&amp;gt;% 
  pivot_longer(t1:t4, values_to = &amp;quot;y&amp;quot;) %&amp;gt;% 
  mutate(time = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double()) %&amp;gt;% 
  mutate(time_f = (time * 2) - 5,
         time_c = time - mean(time),
         time0  = time - 1,
         time01 = (time - 1) / 3)

# what have we done?
head(d_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 11
##      id    tx `t4-t1` condition name      y  time time_f time_c time0 time01
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1   101  -0.5       4 control   t1        3     1     -3   -1.5     0  0    
## 2   101  -0.5       4 control   t2        5     2     -1   -0.5     1  0.333
## 3   101  -0.5       4 control   t3        5     3      1    0.5     2  0.667
## 4   101  -0.5       4 control   t4        7     4      3    1.5     3  1    
## 5   102  -0.5       2 control   t1        4     1     -3   -1.5     0  0    
## 6   102  -0.5       2 control   t2        4     2     -1   -0.5     1  0.333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; paper, Feingold mentioned he coded time as a factor which&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;was mean centered by using linear weights (-3, -1, 1, and 3 for T1 through T4, respectively) for a four-level design obtained from a table of orthogonal polynomials (Snedecor &amp;amp; Cochran, 1967) for the within-subjects (Level 1 in HLM terminology) facet of the analysis. (p. 47)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can find this version of the time variable in the &lt;code&gt;time_f&lt;/code&gt; column. However, I have no interest in modeling with time coded according to a scheme of orthogonal polynomials. But I do think it makes sense to center time or scale it so the lowest value is zero. You can find those versions of time in the &lt;code&gt;time_c&lt;/code&gt; and &lt;code&gt;time0&lt;/code&gt; columns. The model, below, uses &lt;code&gt;time0&lt;/code&gt;. Although this will change the scale of our model parameters relative to those in Feingold’s paper, it will have little influence on how we compute the effect size of interest.&lt;/p&gt;
&lt;p&gt;Here’s how we might fit the multilevel growth model for the two treatment conditions with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d_long,
      family = gaussian,
      y ~ 1 + time0 + tx + time0:tx + (1 + time0 | id),
      cores = 4,
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Review the parameter summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time0 + tx + time0:tx + (1 + time0 | id) 
##    Data: d_long (Number of observations: 80) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 20) 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)            1.09      0.22     0.74     1.60 1.00     1372     2034
## sd(time0)                0.10      0.07     0.01     0.27 1.00     1063     1908
## cor(Intercept,time0)    -0.08      0.51    -0.93     0.91 1.00     4260     2290
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     5.00      0.27     4.47     5.55 1.00     1262     1502
## time0         1.50      0.06     1.38     1.62 1.00     5705     2527
## tx           -0.01      0.53    -1.02     1.05 1.01     1167     1781
## time0:tx      1.00      0.12     0.75     1.25 1.00     3793     2208
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.57      0.06     0.47     0.70 1.00     3132     2986
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything looks fine. If you check them, the trace plots of the chains look good, too&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. If you execute the code below, you’ll see our primary results cohere nicely with the maximum likelihood results from the frequentist &lt;strong&gt;lme4&lt;/strong&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lme4::lmer(data = d_long,
           y ~ 1 + time0 + tx + time0:tx + (1 + time0 | id)) %&amp;gt;% 
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Regardless on whether you focus on the output from &lt;strong&gt;brms&lt;/strong&gt; or &lt;strong&gt;lme4&lt;/strong&gt;, our coefficients will differ a bit from those Feingold reported because of our different scaling of the time variable. But from a high-level perspective, it’s the same model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unstandardized-effect-size.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unstandardized effect size.&lt;/h3&gt;
&lt;p&gt;Our interest lies in the &lt;code&gt;time0:tx&lt;/code&gt; interaction, which is the unstandardized effect size for the “difference between the means of the slopes of the treatment and the control group” (p. 47). You might also describe this as a difference in differences. Here’s a focused summary of that coefficient, which Feingold called &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;time0:tx&amp;quot;, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
## 1.0000705 0.1246595 0.7534974 1.2493457&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since there are three units of time between baseline (&lt;code&gt;time0 == 0&lt;/code&gt;) and the final assessment point (&lt;code&gt;time0 == 3&lt;/code&gt;), we can get the difference in pre/post differences by multiplying that &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt; coefficient by &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;time0:tx&amp;quot;, -2] * 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
## 3.000212 2.260492 3.748037&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thinking back to the original wide-formatted &lt;code&gt;d&lt;/code&gt; data, this value is the multilevel growth model version of the difference in change scores (&lt;code&gt;t4-t1&lt;/code&gt;) in the treatment conditions, &lt;span class=&#34;math inline&#34;&gt;\(M_\text{change-T} - M_\text{change-C}\)&lt;/span&gt;. Here compute that value by hand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group-level change score means
m_change_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t4-t1`)) %&amp;gt;% pull()  # 6
m_change_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t4-t1`)) %&amp;gt;% pull()  # 3

# difference in change score means
m_change_t - m_change_c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the reasons we went through the trouble of fitting a multilevel model is so we could accompany that difference in change scores with high-quality 95% intervals. Here they are in a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(fixef(fit1)[, -2] * 3) %&amp;gt;% 
  rownames_to_column(&amp;quot;coefficient&amp;quot;) %&amp;gt;% 
  filter(coefficient == &amp;quot;time0:tx&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = 0)) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_pointrange(fatten = 1) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(&amp;quot;unstandardized difference in change scores&amp;quot;~(beta[1][1])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig2-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The population average could be anywhere from 2.25 to 3.75, but the best guess is it’s about 3. However, since the metric on this outcome variable is arbitrary (these data were simulated, remember), it’s hard to interpret how “large” this is. A standardized effect size can help.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-to-define-the-standardized-mean-difference-for-the-multilevel-growth-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need to define the standardized mean difference for the multilevel growth model.&lt;/h3&gt;
&lt;p&gt;Based on &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-raudenbushEffectsOfStudyDuration2001&#34; role=&#34;doc-biblioref&#34;&gt;Raudenbush &amp;amp; Liu&lt;/a&gt; (&lt;a href=&#34;#ref-raudenbushEffectsOfStudyDuration2001&#34; role=&#34;doc-biblioref&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt;, Feingold presented two effect-size formulas for our multilevel growth model. The first, which he called &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-change}\)&lt;/span&gt;, is on a completely different scale from any of the effect sizes mentioned in the first post (e.g., &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt;). Importantly, it turns out Raudenbush and Liu recommended their &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-change}\)&lt;/span&gt; formula should be used for power calculations, but not necessarily to convey the magnitude of an effect. Thus we will not consider it further&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Feingold reported the formula for their other effect size was&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_\text{GMA-raw} = \beta_{11}(\text{time}) / SD_\text{raw}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt; in Feingold’s equation is the multilevel interaction term between time and experimental condition–what we just visualized in a coefficient plot. The &lt;span class=&#34;math inline&#34;&gt;\((\text{time})\)&lt;/span&gt; part in the equation is a stand-in for the quantity of time units from the beginning of the study to the end point. Since our multilevel model used the &lt;code&gt;time0&lt;/code&gt; variable, which was &lt;code&gt;0&lt;/code&gt; at baseline and &lt;code&gt;3&lt;/code&gt; at the final time point, we would enter a 3 into the equation (i.e., &lt;span class=&#34;math inline&#34;&gt;\(3 - 0 = 3\)&lt;/span&gt;). The part of Feingold’s equation that’s left somewhat vague is what he meant by the denominator, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw}\)&lt;/span&gt;. On page 47, he used the value of 1.15 in his example. Without any reference to experimental condition in the subscript, one might assume that value is the standard deviation for the criterion across all time points or, perhaps, just at baseline. It turns out that’s not the case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# standard deviation for the criterion across all time points
d_long %&amp;gt;% 
  summarise(sd = sd(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##      sd
##   &amp;lt;dbl&amp;gt;
## 1  2.22&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# standard deviation for the criterion at baseline
d_long %&amp;gt;% 
  filter(time == 1) %&amp;gt;% 
  summarise(sd = sd(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##      sd
##   &amp;lt;dbl&amp;gt;
## 1  1.12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this particular data set, the value Feingold used is the same as the standard deviation for either of the experimental conditions at baseline.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_raw_pre_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(s = sd(t1)) %&amp;gt;% pull()  # treatment baseline SD
sd_raw_pre_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(s = sd(t1)) %&amp;gt;% pull()  # control baseline SD

sd_raw_pre_c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_raw_pre_t&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But since he didn’t use a subscript, I suspect Feingold meant to convey a pooled standard deviation, following the equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SD_\text{pooled} = \sqrt{\frac{SD_\text{raw(pre-T)}^2 + SD_\text{raw(pre-C)}^2}{2}},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is a sample version of Cohen’s original equation 2.3.2 &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988, p. 44&lt;/a&gt;)&lt;/span&gt;. Here’s how to compute the pooled standard deviation by hand, which we’ll save as &lt;code&gt;sd_raw_pre_p&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_raw_pre_p &amp;lt;- sqrt((sd_raw_pre_c^2 + sd_raw_pre_t^2) / 2)
sd_raw_pre_p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since Feingold’s synthetic data are a special case where &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-T)} = SD_\text{raw(pre-C)} = SD_\text{pooled}\)&lt;/span&gt;, these distinctions might all seem dull and pedantic. Yet if your real-world data look anything like mine, this won’t be the case and you’ll need to understand how distinguish between and choose from among these options.&lt;/p&gt;
&lt;p&gt;Another thing to consider is that whereas Feingold’s synthetic data have the desirable quality where the sample sizes are the same across the experimental conditions (&lt;span class=&#34;math inline&#34;&gt;\(n_\text{T} = n_\text{C} = 10\)&lt;/span&gt;), this won’t always be the case. If you end up with unbalanced experimental data, you might consider the sample-size weighted pooled standard deviation, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}^*\)&lt;/span&gt;, which I believe has its origins in Hedges’ work &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hedgesDistributionTheoryforGlass1981&#34; role=&#34;doc-biblioref&#34;&gt;1981, p. 110&lt;/a&gt;)&lt;/span&gt;. It follows the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SD_\text{pooled}^* = \sqrt{\frac{(n_\text{T} - 1)SD_\text{raw(pre-T)}^2 + (n_\text{C} - 1)SD_\text{raw(pre-C)}^2}{n_\text{T} + n_\text{C} - 2}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here it is for Feingold’s data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the sample sizes
n_t &amp;lt;- 10
n_c &amp;lt;- 10

# compute the sample size robust pooled SD
sqrt(((n_t - 1) * sd_raw_pre_c^2 + (n_c - 1) * sd_raw_pre_t^2) / (n_t + n_c - 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, in the special case of these synthetic data, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}^*\)&lt;/span&gt; happens to be the same value as &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}\)&lt;/span&gt;, which is also the same value as &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-T)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt;. This will not always the case with your real-world data. Choose your &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; with care and make sure to report which ever formula you use. Don’t be coy with your effect-size calculations.&lt;/p&gt;
&lt;p&gt;You may be wondering, though, whether you can use the standard deviations for one of the treatment conditions rather than a variant of the pooled standard deviation. &lt;em&gt;Yes&lt;/em&gt;, you can. I think Cumming &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;, Chapter 11)&lt;/span&gt; did a nice job walking through this issue. For example, if we thought of our control condition as a true benchmark for what we’d expect at baseline, we could just use &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt; as our standardizer. This is sometimes referred to as a Glass’ &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; or Glass’ &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt;. Whatever you choose and whatever you call it, just make sure to clearly define your standardizing formula for your audience.&lt;/p&gt;
&lt;p&gt;Therefore, if we use &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt; (&lt;code&gt;sd_raw_pre_c&lt;/code&gt;) as our working value, we can compute &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-raw}\)&lt;/span&gt; as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;time0:tx&amp;quot;, 1] * 3 / sd_raw_pre_c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.598259&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within the Bayesian framework, we can get a full posterior distribution for the standardized version of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-raw}\)&lt;/span&gt;, by working directly with all the posterior draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit1) %&amp;gt;% 
  mutate(d = `b_time0:tx` * 3 / sd_raw_pre_p) %&amp;gt;% 
  
  ggplot(aes(x = d, y = 0)) +
  geom_vline(xintercept = 0, linetype = 2) +
  stat_halfeye(.width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(italic(d)[GMA-raw]~(&amp;quot;standardized difference in change scores&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig3-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The population average could be anywhere from 2 to 3.25, but the best guess is it’s about 2.5. In my field (clinical psychology), this would be considered a very large effect size. Anyway, here are the numeric values for the posterior median and percentile-based 95% interval.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit1) %&amp;gt;% 
  mutate(d = `b_time0:tx` * 3 / sd_raw_pre_p) %&amp;gt;% 
  median_qi(d) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     d .lower .upper .width .point .interval
## 1 2.6   1.96   3.25   0.95 median        qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to compute this is to work with the model formula and the posterior samples from the fixed effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit1) %&amp;gt;% 
  # simplify the output
  select(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  # compute the treatment-level means for pre and post
  mutate(m_pre_t  = b_Intercept + b_time0 * 0 + b_tx *  0.5 + `b_time0:tx`* 0 *  0.5,
         m_pre_c  = b_Intercept + b_time0 * 0 + b_tx * -0.5 + `b_time0:tx`* 0 * -0.5,
         m_post_t = b_Intercept + b_time0 * 3 + b_tx *  0.5 + `b_time0:tx`* 3 *  0.5,
         m_post_c = b_Intercept + b_time0 * 3 + b_tx * -0.5 + `b_time0:tx`* 3 * -0.5) %&amp;gt;% 
  # compute the treatment-level change scores
  mutate(m_change_t = m_post_t - m_pre_t,
         m_change_c = m_post_c - m_pre_c) %&amp;gt;% 
  # compute the difference of differences
  mutate(beta_11 = m_change_t - m_change_c) %&amp;gt;% 
  # compute the multilevel effect size
  mutate(d_GAM_raw = beta_11 / sd_raw_pre_c) %&amp;gt;% 
  # wrangle and summarize
  pivot_longer(m_pre_t:d_GAM_raw) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  mean_qi(value) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 7
##   name       value .lower .upper .width .point .interval
##   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
## 1 beta_11     3      2.26   3.75   0.95 mean   qi       
## 2 d_GAM_raw   2.6    1.96   3.25   0.95 mean   qi       
## 3 m_change_c  3      2.48   3.53   0.95 mean   qi       
## 4 m_change_t  6      5.49   6.53   0.95 mean   qi       
## 5 m_post_c    8.01   7.26   8.8    0.95 mean   qi       
## 6 m_post_t   11     10.2   11.8    0.95 mean   qi       
## 7 m_pre_c     5.01   4.26   5.77   0.95 mean   qi       
## 8 m_pre_t     5      4.27   5.75   0.95 mean   qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how the summary values in the rows for &lt;code&gt;beta_11&lt;/code&gt; and &lt;code&gt;d_GAM_raw&lt;/code&gt; match up with those we computed, above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;you-may-want-options.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;You may want options.&lt;/h3&gt;
&lt;p&gt;Turns out there’s an other way to compute the standardized mean difference for experimental longitudinal data. You can just fit the model to the standardized data. As with our approach, above, the trick is to make sure you standardized the data with a defensible standardizer. I recommend you default to the pooled standard deviation at baseline (&lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}\)&lt;/span&gt;). To do so, we first compute the weighted mean at baseline.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group-level baseline means
m_raw_pre_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t1`)) %&amp;gt;% pull()
m_raw_pre_c &amp;lt;- filter(d, tx ==  &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t1`)) %&amp;gt;% pull()

# weighted (pooled) baseline mean
m_raw_pre_p &amp;lt;- (m_raw_pre_t * n_t + m_raw_pre_c * n_c) / (n_t + n_c)

m_raw_pre_p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next use the weighted baseline mean and the pooled baseline standard deviation to standardize the data, saving the results as &lt;code&gt;z&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_long &amp;lt;-
  d_long %&amp;gt;% 
  mutate(z = (y - m_raw_pre_p) / sd_raw_pre_p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now just fit a multilevel growth model with our new standardized variable &lt;code&gt;z&lt;/code&gt; as the criterion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;-
  brm(data = d_long,
      family = gaussian,
      z ~ 1 + time0 + tx + time0:tx + (1 + time0 | id),
      cores = 4,
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the parameter summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: z ~ 1 + time0 + tx + time0:tx + (1 + time0 | id) 
##    Data: d_long (Number of observations: 80) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 20) 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)            0.95      0.20     0.63     1.42 1.00     1293     1768
## sd(time0)                0.08      0.06     0.00     0.23 1.00      937     1827
## cor(Intercept,time0)    -0.07      0.51    -0.90     0.91 1.00     4454     2198
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.01      0.24    -0.48     0.45 1.00     1000     1533
## time0         1.30      0.05     1.19     1.41 1.00     5287     2785
## tx           -0.01      0.46    -0.93     0.93 1.00     1069     1714
## time0:tx      0.87      0.11     0.66     1.08 1.00     4651     2963
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.49      0.05     0.41     0.59 1.00     2769     2610
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, our focal parameter is &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit2)[&amp;quot;time0:tx&amp;quot;, -2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate      Q2.5     Q97.5 
## 0.8688574 0.6575292 1.0765760&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But since our data are coded such that baseline is &lt;code&gt;time0 == 0&lt;/code&gt; and the final time point is &lt;code&gt;time0 == 3&lt;/code&gt;, we’ll need to multiply that coefficient by 3 to get the effect size in the pre/post metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit2)[&amp;quot;time0:tx&amp;quot;, -2] * 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
## 2.606572 1.972588 3.229728&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is it, within simulation variance of the effect size from the last section. Let’s compare them with a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(fixef(fit1)[&amp;quot;time0:tx&amp;quot;, -2] * 3 / sd_raw_pre_p,
      fixef(fit2)[&amp;quot;time0:tx&amp;quot;, -2] * 3) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  mutate(data = c(&amp;quot;unstandardized data&amp;quot;, &amp;quot;standardized data&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = data)) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_pointrange(fatten = 1) +
  labs(x = expression(italic(d)[GMA-raw]~(&amp;quot;standardized difference in change scores&amp;quot;)),
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig4-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yep, they’re pretty much the same.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sum-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sum up&lt;/h2&gt;
&lt;p&gt;Yes, one can compute a standardized mean difference effect size for experimental data analyzed with a multilevel growth model. The focal parameter is the treatment-time interaction, what we called &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;. The trick is to divide that parameter by the pooled standard deviation at baseline. This will put the effect size, what Feingold called &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-raw}\)&lt;/span&gt;, into a conventional Cohen’s-&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-type metric. But be mindful that this method may require you to multiply the effect by a number that corrects for how you have scaled the time variable. In the example we worked through, we multiplied by 3.&lt;/p&gt;
&lt;p&gt;As an alternative workflow, you can also fit the model on data that were standardized using the pooled standard deviation at baseline. This will automatically put the &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt; in the effect-size metric. But as with the other method, you still might have to correct for how you scaled the time variable.&lt;/p&gt;
&lt;p&gt;Though we’re not covering it, here, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldARegressionFramework2013&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldARegressionFramework2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; extended this framework to other contexts. For example, he discussed how to apply it to data with nonlinear trends and to models with other covariates. Just know the foundation is right here:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_\text{GMA-raw} = \beta_{11}(\text{time}) / SD_\text{raw}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0   
##  [8] ggplot2_3.3.3   tidyverse_1.3.0 tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6     
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6         splines_4.0.4       
##   [6] svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17       
##  [11] digest_0.6.27        htmltools_0.5.1.1    rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000    haven_2.3.1         
##  [26] xfun_0.22            callr_3.5.1          crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1     
##  [36] V8_3.4.0             distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1       viridisLite_0.3.0   
##  [46] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16              htmlwidgets_1.5.2   
##  [51] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.1       pkgconfig_2.0.3     
##  [56] loo_2.4.1            farver_2.0.3         dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2      
##  [61] tidyselect_1.1.0     rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [66] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0       broom_0.7.5         
##  [71] ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1        yaml_2.2.1           fs_1.5.0            
##  [76] processx_3.4.5       knitr_1.31           nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [81] xml2_1.3.2           rstudioapi_0.13      compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [86] curl_4.3             gamm4_0.2-6          reprex_0.3.0         statmod_1.4.35       stringi_1.5.3       
##  [91] highr_0.8            ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41     
##  [96] Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
## [101] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [106] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3        codetools_0.2-18    
## [111] boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1    
## [116] withr_2.4.1          shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [126] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brms2021RM&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt; reference manual, &lt;span&gt;Version&lt;/span&gt; 2.15.0&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/brms.pdf&#34;&gt;https://CRAN.R-project.org/package=brms/brms.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cohenStatisticalPowerAnalysis1988a&#34; class=&#34;csl-entry&#34;&gt;
Cohen, J. (1988). &lt;em&gt;Statistical power analysis for the behavioral sciences&lt;/em&gt;. &lt;span&gt;L. Erlbaum Associates&lt;/span&gt;. &lt;a href=&#34;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&#34;&gt;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummingUnderstandingTheNewStatistics2012&#34; class=&#34;csl-entry&#34;&gt;
Cumming, G. (2012). &lt;em&gt;Understanding the new statistics: &lt;span&gt;Effect&lt;/span&gt; sizes, confidence intervals, and meta-analysis&lt;/em&gt;. &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&#34;&gt;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-feingoldEffectSizeForGMA2009&#34; class=&#34;csl-entry&#34;&gt;
Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(1), 43. &lt;a href=&#34;https://doi.org/10.1037/a0014699&#34;&gt;https://doi.org/10.1037/a0014699&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-feingoldARegressionFramework2013&#34; class=&#34;csl-entry&#34;&gt;
Feingold, A. (2013). A regression framework for effect size assessments in longitudinal modeling of group differences. &lt;em&gt;Review of General Psychology&lt;/em&gt;, &lt;em&gt;17&lt;/em&gt;(1), 111–121. &lt;a href=&#34;https://doi.org/10.1037/a0030048&#34;&gt;https://doi.org/10.1037/a0030048&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hedgesDistributionTheoryforGlass1981&#34; class=&#34;csl-entry&#34;&gt;
Hedges, L. V. (1981). Distribution theory for &lt;span&gt;Glass&lt;/span&gt;’s estimator of effect size and related estimators. &lt;em&gt;Journal of Educational Statistics&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(2), 107–128. &lt;a href=&#34;https://doi.org/10.3102/10769986006002107&#34;&gt;https://doi.org/10.3102/10769986006002107&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hoffmanLongitudinalAnalysisModeling2015&#34; class=&#34;csl-entry&#34;&gt;
Hoffman, L. (2015). &lt;em&gt;Longitudinal analysis: &lt;span&gt;Modeling&lt;/span&gt; within-person fluctuation and change&lt;/em&gt; (1 edition). &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&#34;&gt;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-raudenbushHLM2002&#34; class=&#34;csl-entry&#34;&gt;
Raudenbush, S. W., &amp;amp; Bryk, A. S. (2002). &lt;em&gt;Hierarchical linear models: &lt;span&gt;Applications&lt;/span&gt; and data analysis methods&lt;/em&gt; (Second Edition). &lt;span&gt;SAGE Publications, Inc&lt;/span&gt;. &lt;a href=&#34;https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230&#34;&gt;https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-raudenbushEffectsOfStudyDuration2001&#34; class=&#34;csl-entry&#34;&gt;
Raudenbush, S. W., &amp;amp; Liu, X.-F. (2001). Effects of study duration, frequency of observation, and sample size on power in studies of group differences in polynomial change. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(4), 387. &lt;a href=&#34;https://doi.org/10.1037/1082-989X.6.4.387&#34;&gt;https://doi.org/10.1037/1082-989X.6.4.387&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-singerAppliedLongitudinalData2003&#34; class=&#34;csl-entry&#34;&gt;
Singer, J. D., &amp;amp; Willett, J. B. (2003). &lt;em&gt;Applied longitudinal data analysis: &lt;span&gt;Modeling&lt;/span&gt; change and event occurrence&lt;/em&gt;. &lt;span&gt;Oxford University Press, USA&lt;/span&gt;. &lt;a href=&#34;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you’re curious about our priors, fit the models on your computer and then execute &lt;code&gt;fit1$prior&lt;/code&gt;. To learn more about &lt;strong&gt;brms&lt;/strong&gt; default priors, spend some time with the &lt;a href=&#34;https://CRAN.R-project.org/package=brms/brms.pdf&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; reference manual&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brms2021RM&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2021&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;If you’re not into the whole Bayesian framework I’m using, you can just ignore the part about trace plots and chains. If you’re into it, execute &lt;code&gt;plot(fit1)&lt;/code&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Really. If you are interested in communicating your research results to others, do not mess with the &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-change}\)&lt;/span&gt;. It’s on a totally different metric from the conventional Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and you’ll just end up confusing people.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Effect sizes for experimental trials analyzed with multilevel growth models: One of two</title>
      <link>/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/</link>
      <pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/</guid>
      <description>
&lt;script src=&#34;/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;This post is the first installment of a two-part series. The impetus is a project at work. A colleague had longitudinal data for participants in two experimental groups, which they examined with a multilevel growth model of the kind we’ll explore in the next post. My colleague then summarized the difference in growth for the two conditions with a standardized mean difference they called &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Their effect size looked large, to me, and I was perplexed when I saw the formula they used to compute their version of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. It had been a while since I had to compute an effect size like this, so I dove back into the literature, where I realized Feingold had worked this issue out in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; paper in &lt;a href=&#34;https://www.apa.org/pubs/journals/met&#34;&gt;&lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The purpose of this series is to show how to compute a Cohen’s-&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; type effect size when you have longitudinal data on &lt;span class=&#34;math inline&#34;&gt;\(3+\)&lt;/span&gt; time points for two experimental groups. In this first post, we’ll warm up with the basics. In the second post, we’ll get down to business. The data and overall framework come from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;This series is an applied tutorial moreso than an introduction. I’m presuming you have a passing familiarity with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should be familiar with effect sizes, particularly with standardized mean differences. If you need to brush up, consider Cohen’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988&lt;/a&gt;)&lt;/span&gt; authoritative text, or Cummings newer &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; text. Since we’ll be making extensive use of Feingold’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; paper, you should at least save it as a reference. For nice conceptual overview, I also recommend Kelley and Preacher’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kelley2012effect&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; paper, &lt;a href=&#34;https://www3.nd.edu/~kkelley/publications/articles/Kelley_and_Preacher_Psychological_Methods_2012.pdf&#34;&gt;&lt;em&gt;On effect size&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Though it won’t be important for this first post, you’ll want to be familiar with multilevel regression for the next–it’s a major part of why I’m making this series! For texts that focus on the longitudinal models relevant for the topic, I recommend &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;Raudenbush &amp;amp; Bryk&lt;/a&gt; (&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett&lt;/a&gt; (&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;–the one I personally learned on–; or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To fully befit from the next post, it’ll help if you have a passing familiarity with Bayesian regression (though frequentists will still be able to get the main points). For thorough introductions, I recommend either edition of McElreath’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text; or Gelman, Hill, and Vehtari’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; text. If you go with McElreath, he has a fine series of freely-available lectures &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. Probably the best place to learn about the &lt;strong&gt;tidyverse&lt;/strong&gt;-style of coding, as well as an introduction to &lt;strong&gt;R&lt;/strong&gt;, is Grolemund and Wickham’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-grolemundDataScience2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; freely-available online text, &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R for data science&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we load our primary &lt;strong&gt;R&lt;/strong&gt; packages and adjust the global plotting theme defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork)

# adjust the global plotting theme
theme_set(
  theme_linedraw() +
    theme(text = element_text(family = &amp;quot;Times&amp;quot;),
          panel.grid = element_blank(),
          strip.text = element_text(margin = margin(b = 3, t = 3)))
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;Happily, Feingold included a working example of synthetic trial data in his paper. You can find the full data set displayed in his Table 1 (p. 46). Here we’ll use a &lt;a href=&#34;https://tibble.tidyverse.org/reference/tribble.html&#34;&gt;tribble&lt;/a&gt; approach to enter those data into &lt;strong&gt;R&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  tribble(
    ~id, ~tx, ~t1, ~t2, ~t3, ~t4,
    101, -0.5, 3, 5, 5,  7,
    102, -0.5, 4, 4, 6,  6,
    103, -0.5, 4, 5, 7,  8,
    104, -0.5, 5, 6, 6,  8,
    105, -0.5, 5, 6, 7,  8,
    106, -0.5, 5, 7, 7,  7,
    107, -0.5, 5, 6, 8,  8,
    108, -0.5, 6, 6, 7,  9,
    109, -0.5, 6, 8, 9,  10,
    110, -0.5, 7, 7, 8,  9,
    111,  0.5, 3, 5, 7,  9,
    112,  0.5, 4, 7, 9,  11,
    113,  0.5, 4, 6, 8,  11,
    114,  0.5, 5, 7, 9,  10,
    115,  0.5, 5, 6, 9,  11,
    116,  0.5, 5, 7, 10, 10,
    117,  0.5, 5, 8, 8,  11,
    118,  0.5, 6, 7, 9,  12,
    119,  0.5, 6, 9, 11, 13,
    120,  0.5, 7, 8, 10, 12
  ) %&amp;gt;% 
  mutate(`t4-t1`   = t4 - t1,
         condition = ifelse(tx == -0.5, &amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;))

# inspect the first six rows
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##      id    tx    t1    t2    t3    t4 `t4-t1` condition
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    
## 1   101  -0.5     3     5     5     7       4 control  
## 2   102  -0.5     4     4     6     6       2 control  
## 3   103  -0.5     4     5     7     8       4 control  
## 4   104  -0.5     5     6     6     8       3 control  
## 5   105  -0.5     5     6     7     8       3 control  
## 6   106  -0.5     5     7     7     7       2 control&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These synthetic data are from a hypothetical clinical trial where (&lt;span class=&#34;math inline&#34;&gt;\(N = 20\)&lt;/span&gt;) participants were randomized into a control group (&lt;code&gt;tx == -0.5&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt;) or a treatment group (&lt;code&gt;tx == 0.5&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt;). Their responses on a single outcome variable, &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;, were recorded over four time points, which are recorded in columns &lt;code&gt;t1&lt;/code&gt; through &lt;code&gt;t4&lt;/code&gt;. The simple difference score between the first (&lt;code&gt;t1&lt;/code&gt;) and last time points (&lt;code&gt;t4&lt;/code&gt;) was computed in the &lt;code&gt;t4-t1&lt;/code&gt; column. For good measure, I threw in a nominal &lt;code&gt;condition&lt;/code&gt; variable to help clarify the levels of &lt;code&gt;tx&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To get a sense of the data, it might be helpful to plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# participant-level trajectories
p1 &amp;lt;-
  d %&amp;gt;% 
  pivot_longer(t1:t4) %&amp;gt;% 
  mutate(time      = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double(),
         condition = factor(condition, levels = c(&amp;quot;treatment&amp;quot;, &amp;quot;control&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = time, y = value, color = condition)) +
  geom_line(size = 1) +
  scale_color_viridis_d(end = .75, breaks = NULL) +
  scale_y_continuous(breaks = 0:4 * 3, limits = c(0, 13)) +
  labs(subtitle = &amp;quot;participant-level trajectories&amp;quot;,
       y = &amp;quot;outcome&amp;quot;) +
  facet_wrap(~id) +
  theme(strip.text.x = element_text(margin = margin(b = 0.25, t = 0.25)))

# group average trajectories
p2 &amp;lt;-
  d %&amp;gt;% 
  pivot_longer(t1:t4) %&amp;gt;% 
  mutate(time      = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double(),
         condition = factor(condition, levels = c(&amp;quot;treatment&amp;quot;, &amp;quot;control&amp;quot;))) %&amp;gt;% 
  group_by(time, condition) %&amp;gt;% 
  summarise(mean = mean(value)) %&amp;gt;% 
  
  ggplot(aes(x = time, y = mean, color = condition)) +
  geom_line(size = 2) +
  scale_color_viridis_d(end = .75) +
  scale_y_continuous(breaks = 0:4 * 3, limits = c(0, 13)) +
  labs(subtitle = &amp;quot;group averages&amp;quot;,
       y = &amp;quot;outcome&amp;quot;)

# combine
p1 + p2 + plot_layout(widths = c(5, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;720&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The series of miniature plots on the left shows the trajectory of each participant’s raw data, over time. The larger plot on the right shows the average value for each of the experimental conditions, over time. Although there is some variation across individuals within experimental conditions, clear trends emerge. The plot on the right shows the experimental conditions had the same average values at baseline (&lt;code&gt;t1&lt;/code&gt;), both conditions tended to increase over time, but the &lt;code&gt;treatment&lt;/code&gt; condition showed larger changes over time, relative to the &lt;code&gt;control&lt;/code&gt; condition.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-do-we-care-about&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What do we care about?&lt;/h2&gt;
&lt;p&gt;There are a lot of questions a clinical researcher might want to ask from data of this kind. If we narrow our focus to causal inference&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; with regards to the treatment conditions, I think there are three fundamental questions we’d want to ask. They all have to do with change over time:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How much did the participants in the control group change, on average?&lt;/li&gt;
&lt;li&gt;How much did the participants in the treatment group change, on average?&lt;/li&gt;
&lt;li&gt;What was the difference in change in the treatment group versus the control group, on average?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ideally, we’d like to express our answers to these questions, particularly the third, in terms of a meaningfully defined effect size. That will be the goal of the remainder of this post and the next.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;warm-up-with-just-two-time-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Warm up with just two time points&lt;/h2&gt;
&lt;p&gt;Before we put on our big-kid pants and start fitting longitudinal growth models, I recommend we follow Feingold’s approach and first focus on how we’d answer these questions with two-time-point data. If we were to drop the variables &lt;code&gt;t2&lt;/code&gt; and &lt;code&gt;t3&lt;/code&gt;, these data would have the form of a pre/post experiment, which Feingold called an &lt;em&gt;independent-groups pretest–posttest&lt;/em&gt; design &lt;span class=&#34;citation&#34;&gt;(IGPP&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009, p. 46&lt;/a&gt;)&lt;/span&gt;. A major reason to warm up in this way is because much of the work on effect sizes, from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;Cohen&lt;/a&gt; (&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988&lt;/a&gt;)&lt;/span&gt; and others &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;Cumming, 2012&lt;/a&gt;)&lt;/span&gt;, has been in the context of cross-sectional and pre/post designs, such as the IGPP. Not only have the analytic strategies centered on these simple cases, but the effect sizes designed for these simple cases are the ones most readers are used to interpreting. One of the big points in Feingold’s paper is we should prefer it when the effect sizes for our longitudinal growth models have clear links to the traditional effect sizes. I am inclined to agree.&lt;/p&gt;
&lt;div id=&#34;data-summaries.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data summaries.&lt;/h3&gt;
&lt;p&gt;To get a sense of the pre/post changes in the two conditions, a fine place to start is with summary statistics. Here we compute the means and standard deviations in the outcome variable for each condition at pre and post. We also throw in the means and standard deviations for the change scores, &lt;code&gt;t4-t1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  pivot_longer(cols = c(t1, t4, `t4-t1`),
               names_to = &amp;quot;variable&amp;quot;) %&amp;gt;% 
  group_by(variable, condition) %&amp;gt;% 
  summarise(mean = mean(value),
            sd   = sd(value))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
## # Groups:   variable [3]
##   variable condition  mean    sd
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 t1       control       5 1.15 
## 2 t1       treatment     5 1.15 
## 3 t4       control       8 1.15 
## 4 t4       treatment    11 1.15 
## 5 t4-t1    control       3 0.816
## 6 t4-t1    treatment     6 0.816&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Feingold displayed most of these statistics in his Table 1 (p. 46). Make special note of how consistent the &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; values are. This will become important in the second post. Anyway, now we’re ready to start defining the effect sizes. We’ll start with the unstandardized kind.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unstandardized-mean-differences.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unstandardized mean differences.&lt;/h3&gt;
&lt;p&gt;The version of Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; we’re ultimately working up to is a special kind of standardized effect size. Yet not all effect sizes are standardized. In cases where the metric of the dependent variable is inherently meaningful, Pek and Flora &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-pekReportingEffectSizes2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; actually recommend researchers use &lt;em&gt;un&lt;/em&gt;standardized effect sizes. Say we thought the data in this example had values that were inherently meaningful. We could answer the three research questions, above, directly with sample statistics. Here we answer the first two questions by focusing on the means of the change scores, by experimental condition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  group_by(condition) %&amp;gt;% 
  summarise(mean_change = mean(`t4-t1`))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   condition mean_change
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
## 1 control             3
## 2 treatment           6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To answer to our final question, we simply compute the difference between those two change scores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;6 - 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although the average values in both groups changed over time, the participants in the &lt;code&gt;treatment&lt;/code&gt; condition changed 3 units more, on average, than those in the &lt;code&gt;control&lt;/code&gt; condition. Is that difference meaningful? At the moment, it seems hard to say because these data are not actually on an inherently meaningful metric. The whole thing is made up and abstract. We might be better off by using a standardized effect size, instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standardized-mean-differences.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Standardized mean differences.&lt;/h3&gt;
&lt;p&gt;The approach above works great if the outcome variable is inherently meaningful and if we have no interest in comparing these results with studies on different outcome variables. In reality, clinical researchers often use sum scores from self-report questionnaires as their primary outcome variables and these scores take on seemingly arbitrary values. Say you work in depression research. There are numerous questionnaires designed to measure depression &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-friedThe52SymptomsOfMajorDepression2017&#34; role=&#34;doc-biblioref&#34;&gt;Fried, 2017&lt;/a&gt;)&lt;/span&gt; and their sum scores are all scaled differently. The problem is even worse if you’d like to compare two different kinds of outcomes, such as depression and anxiety. This is where standardized effect sizes come in.&lt;/p&gt;
&lt;p&gt;Since we are focusing on the data from the first and last time points, we can use conventional summary-statistic oriented strategies to compute the &lt;em&gt;standardized&lt;/em&gt; mean differences. In the literature, you’ll often find standardized mean differences referred to as a Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, named after the late &lt;a href=&#34;https://en.wikipedia.org/wiki/Jacob_Cohen_(statistician)&#34;&gt;Jacob Cohen&lt;/a&gt;. I suspect what isn’t always appreciated is that there are many ways to compute &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and that “Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;” can both refer to the general family of standardized mean differences or to a specific kind of standardized mean difference. In addition to Cohen’s original &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988&lt;/a&gt;)&lt;/span&gt; work, I think Geoff Cumming walked this out nicely in Chapter 11 of his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; text. Here we’ll consider two versions of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; from Feingold’s paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Two effect sizes can be calculated from an IGPP design, one using the standard deviation of the change scores in the denominator and the other using the standard deviation of the raw scores &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-morrisEstimatingEffectSizes2008&#34; role=&#34;doc-biblioref&#34;&gt;Morris, 2008&lt;/a&gt;; &lt;a href=&#34;#ref-morrisCombiningEffectSizeEstimates2002&#34; role=&#34;doc-biblioref&#34;&gt;Morris &amp;amp; DeShon, 2002&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
d_\text{IGPP-change} &amp;amp; = (M_\text{change-T} / SD_\text{change-T}) - (M_\text{change-C} / SD_\text{change-C}) \\
&amp;amp; = (6/0.82) - (3/0.82) = 3.67,
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{change-T}\)&lt;/span&gt; is the [standard deviation for the change scores]&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; for the treatment group and &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{change-T}\)&lt;/span&gt; is the [standard deviation for the change scores] for the control group. (If homogeneity of variance across conditions is assumed, each of these terms can be replaced by &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{change}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
d_\text{IGPP-raw} &amp;amp; = (M_\text{change-T} / SD_\text{raw(pre-T)}) - (M_\text{change-C} / SD_\text{raw(pre-C)}) \\
&amp;amp; = (6/1.15) - (3/1.15) = 2.60,
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(M_\text{change-T}\)&lt;/span&gt; is the mean of the change scores for the treatment group, &lt;span class=&#34;math inline&#34;&gt;\(M_\text{change-C}\)&lt;/span&gt; is the mean of change scores for the control group, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-T)}\)&lt;/span&gt; is the pretest &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; for the treatment group, and &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt; is the pretest &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; for the control group. (If homogeneity of variance is assumed, each of the last two terms can be replaced by &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw}\)&lt;/span&gt;.) (p. 47)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We should practice computing these values by hand. First, we compute the group-level summary statistics and save each value separately for further use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group-level change score means
m_change_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t4-t1`)) %&amp;gt;% pull()  # 6
m_change_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t4-t1`)) %&amp;gt;% pull()  # 3

# group-level change score sds
sd_change_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(s = sd(`t4-t1`)) %&amp;gt;% pull()  # 0.8164966
sd_change_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(s = sd(`t4-t1`)) %&amp;gt;% pull()  # 0.8164966

# group-level baseline sds
sd_raw_pre_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(s = sd(t1)) %&amp;gt;% pull()  # 1.154701
sd_raw_pre_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(s = sd(t1)) %&amp;gt;% pull()  # 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With all those values saved, here’s how we might use the first equation, above, to compute Feingold’s &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(m_change_t / sd_change_t) - (m_change_c / sd_change_c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.674235&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a similar way, here’s how we might the second equation to compute &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(m_change_t / sd_raw_pre_t) - (m_change_c / sd_raw_pre_c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.598076&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In most areas of psychology, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;’s of this size would seem large. Whether researchers prefer the &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; approach or the &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt; approach, both return effect sizes in the form of a &lt;em&gt;standardized difference of differences&lt;/em&gt;. The primary question is what values to standardize the differences by (i.e., which &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; estimates might we place in the denominators). As discussed by both &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;Cumming&lt;/a&gt; (&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;, researchers are at liberty to make rational decisions on how to standardize their variables, and thus how to compute &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Whatever you decide for your research, just make sure you clarify your choice and your formulas for your audience.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extensions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extensions.&lt;/h3&gt;
&lt;p&gt;This is about as far as we’re going to go with the IGPP version of Feingold’s synthetic data. But if you do end up with data of this kind or similar, there are other things to consider. Real brief, here are two:&lt;/p&gt;
&lt;div id=&#34;its-good-to-express-uncertainty.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;It’s good to express uncertainty.&lt;/h4&gt;
&lt;p&gt;Just like any other statistical estimate, we should express the uncertainty in our effect sizes, somehow. In the seventh edition of the APA &lt;em&gt;Publication Manual&lt;/em&gt;, we read: “whenever possible, provide a confidence interval for each effect size reported to indicate the precision of estimation of the effect size” &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-apaPublicationManual2020&#34; role=&#34;doc-biblioref&#34;&gt;American Psychological Association, 2020, p. 89&lt;/a&gt;)&lt;/span&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Since the ultimate purpose of this mini blog series is to show how to compute effect sizes for multilevel growth models, I am not going to dive into this issue, here. We’re just warming up for the main event in the next post. But if you ever need to compute 95% CIs for &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt; based on IGPP data, check out Chapter 11 in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;Cumming&lt;/a&gt; (&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;there-are-many-more-ds.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;There are many more &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;’s.&lt;/h4&gt;
&lt;p&gt;We’ve already mentioned there are several kinds of Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; effect sizes. With Feingold’s data, we practiced two: &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt;. Feingold built his paper on the foundation of &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-morrisCombiningEffectSizeEstimates2002&#34; role=&#34;doc-biblioref&#34;&gt;Morris &amp;amp; DeShon&lt;/a&gt; (&lt;a href=&#34;#ref-morrisCombiningEffectSizeEstimates2002&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt;, which covered a larger variety of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;’s suited for cross-sectional and pre/post designs with one or two experimental conditions. Morris and DeShon’s writing style was accessible and I liked their statistical notation. You might check out their paper and expand your &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; repertoire.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-we-learned-and-whats-soon-to-come&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What we learned and what’s soon to come&lt;/h2&gt;
&lt;p&gt;In this first post, we learned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Effect sizes for experimental trials analyzed with multilevel growth models aren’t straightforward.&lt;/li&gt;
&lt;li&gt;Much of the effect size literature is based on simple cross-sectional or two-time-point designs with one or two groups.&lt;/li&gt;
&lt;li&gt;Effect sizes can be standardized or unstandardized.&lt;/li&gt;
&lt;li&gt;“Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;” can refer to either a general class of standardized mean differences, or to a specific standardized mean differences.&lt;/li&gt;
&lt;li&gt;As discussed in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;, the two effect sizes recommended for IGPP designs are &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt;, both of which can be computed with simple summary statistics.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stay tuned for the second post in this series, where we’ll extend these skills to two-group experimental data with more than two time points. The multilevel growth model will make its grand appearance and it’ll just be great!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] patchwork_1.1.1 forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5    
##  [5] purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0   
##  [9] ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.1.0  xfun_0.22         haven_2.3.1       colorspace_2.0-0 
##  [5] vctrs_0.3.6       generics_0.1.0    viridisLite_0.3.0 htmltools_0.5.1.1
##  [9] yaml_2.2.1        utf8_1.1.4        rlang_0.4.10      pillar_1.5.1     
## [13] withr_2.4.1       glue_1.4.2        DBI_1.1.0         dbplyr_2.0.0     
## [17] modelr_0.1.8      readxl_1.3.1      lifecycle_1.0.0   munsell_0.5.0    
## [21] blogdown_1.3      gtable_0.3.0      cellranger_1.1.0  rvest_0.3.6      
## [25] evaluate_0.14     labeling_0.4.2    knitr_1.31        fansi_0.4.2      
## [29] highr_0.8         broom_0.7.5       Rcpp_1.0.6        scales_1.1.1     
## [33] backports_1.2.1   jsonlite_1.7.2    farver_2.0.3      fs_1.5.0         
## [37] hms_0.5.3         digest_0.6.27     stringi_1.5.3     bookdown_0.21    
## [41] grid_4.0.4        cli_2.3.1         tools_4.0.4       magrittr_2.0.1   
## [45] crayon_1.4.1      pkgconfig_2.0.3   ellipsis_0.3.1    xml2_1.3.2       
## [49] reprex_0.3.0      lubridate_1.7.9.2 assertthat_0.2.1  rmarkdown_2.7    
## [53] httr_1.4.2        rstudioapi_0.13   R6_2.5.0          compiler_4.0.4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-apaPublicationManual2020&#34; class=&#34;csl-entry&#34;&gt;
American Psychological Association. (2020). &lt;em&gt;Publication manual of the &lt;span&gt;American Psychological Association&lt;/span&gt;&lt;/em&gt; (Seventh Edition). &lt;span&gt;American Psychological Association&lt;/span&gt;. &lt;a href=&#34;https://apastyle.apa.org/products/publication-manual-7th-edition&#34;&gt;https://apastyle.apa.org/products/publication-manual-7th-edition&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cohenStatisticalPowerAnalysis1988a&#34; class=&#34;csl-entry&#34;&gt;
Cohen, J. (1988). &lt;em&gt;Statistical power analysis for the behavioral sciences&lt;/em&gt;. &lt;span&gt;L. Erlbaum Associates&lt;/span&gt;. &lt;a href=&#34;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&#34;&gt;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummingUnderstandingTheNewStatistics2012&#34; class=&#34;csl-entry&#34;&gt;
Cumming, G. (2012). &lt;em&gt;Understanding the new statistics: &lt;span&gt;Effect&lt;/span&gt; sizes, confidence intervals, and meta-analysis&lt;/em&gt;. &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&#34;&gt;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-feingoldEffectSizeForGMA2009&#34; class=&#34;csl-entry&#34;&gt;
Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(1), 43. &lt;a href=&#34;https://doi.org/10.1037/a0014699&#34;&gt;https://doi.org/10.1037/a0014699&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-friedThe52SymptomsOfMajorDepression2017&#34; class=&#34;csl-entry&#34;&gt;
Fried, E. I. (2017). The 52 symptoms of major depression: &lt;span&gt;Lack&lt;/span&gt; of content overlap among seven common depression scales. &lt;em&gt;Journal of Affective Disorders&lt;/em&gt;, &lt;em&gt;208&lt;/em&gt;, 191–197. &lt;a href=&#34;https://doi.org/10.1016/j.jad.2016.10.019&#34;&gt;https://doi.org/10.1016/j.jad.2016.10.019&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-grolemundDataScience2017&#34; class=&#34;csl-entry&#34;&gt;
Grolemund, G., &amp;amp; Wickham, H. (2017). &lt;em&gt;R for data science&lt;/em&gt;. &lt;span&gt;O’Reilly&lt;/span&gt;. &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;https://r4ds.had.co.nz&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hoffmanLongitudinalAnalysisModeling2015&#34; class=&#34;csl-entry&#34;&gt;
Hoffman, L. (2015). &lt;em&gt;Longitudinal analysis: &lt;span&gt;Modeling&lt;/span&gt; within-person fluctuation and change&lt;/em&gt; (1 edition). &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&#34;&gt;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kelley2012effect&#34; class=&#34;csl-entry&#34;&gt;
Kelley, K., &amp;amp; Preacher, K. J. (2012). On effect size. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;17&lt;/em&gt;(2), 137. &lt;a href=&#34;https://doi.org/10.1037/a0028086&#34;&gt;https://doi.org/10.1037/a0028086&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-morrisEstimatingEffectSizes2008&#34; class=&#34;csl-entry&#34;&gt;
Morris, S. B. (2008). Estimating effect sizes from pretest-posttest-control group designs. &lt;em&gt;Organizational Research Methods&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(2), 364–386. &lt;a href=&#34;https://doi.org/10.1177/1094428106291059&#34;&gt;https://doi.org/10.1177/1094428106291059&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-morrisCombiningEffectSizeEstimates2002&#34; class=&#34;csl-entry&#34;&gt;
Morris, S. B., &amp;amp; DeShon, R. P. (2002). Combining effect size estimates in meta-analysis with repeated measures and independent-groups designs. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;7&lt;/em&gt;(1), 105. &lt;a href=&#34;https://doi.org/10.1037/1082-989X.7.1.105&#34;&gt;https://doi.org/10.1037/1082-989X.7.1.105&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pekReportingEffectSizes2018&#34; class=&#34;csl-entry&#34;&gt;
Pek, J., &amp;amp; Flora, D. B. (2018). Reporting effect sizes in original psychological research: &lt;span&gt;A&lt;/span&gt; discussion and tutorial. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;23&lt;/em&gt;(2), 208. https://doi.org/&lt;a href=&#34;https://doi.apa.org/fulltext/2017-10871-001.html&#34;&gt;https://doi.apa.org/fulltext/2017-10871-001.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-raudenbushHLM2002&#34; class=&#34;csl-entry&#34;&gt;
Raudenbush, S. W., &amp;amp; Bryk, A. S. (2002). &lt;em&gt;Hierarchical linear models: &lt;span&gt;Applications&lt;/span&gt; and data analysis methods&lt;/em&gt; (Second Edition). &lt;span&gt;SAGE Publications, Inc&lt;/span&gt;. &lt;a href=&#34;https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230&#34;&gt;https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-singerAppliedLongitudinalData2003&#34; class=&#34;csl-entry&#34;&gt;
Singer, J. D., &amp;amp; Willett, J. B. (2003). &lt;em&gt;Applied longitudinal data analysis: &lt;span&gt;Modeling&lt;/span&gt; change and event occurrence&lt;/em&gt;. &lt;span&gt;Oxford University Press, USA&lt;/span&gt;. &lt;a href=&#34;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you are unfamiliar with causal inference and confused over why causal inference might lead us to limit our focus in this way, check out Chapters 18 through 21 in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;Gelman et al.&lt;/a&gt; (&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I’m not in love with introducing this new acronym. But if we want to follow along with Feingold, we may as well get used to his term.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;In the original paper, Feingold used the term “mean change score” here as well as a bit later in the sentence. After reading this through several times and working through his examples, I’m confident these were typos. With technical material of this kind, it’s hard to avoid a typo or two.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;We Bayesians, of course, can forgive the frequentist bias in the wording of the APA’s otherwise sound recommendation.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Could you compute Bayesian credible intervals for IGPP effect sizes by adjusting some of the strategies from my earlier blog post, &lt;a href=&#34;Regression%20models%20for%202-timepoint%20non-experimental%20data&#34;&gt;&lt;em&gt;Regression models for 2-timepoint non-experimental data&lt;/em&gt;&lt;/a&gt;? Yes, you could.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

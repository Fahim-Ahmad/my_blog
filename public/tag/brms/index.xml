<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>brms | Fahim Ahmad</title>
    <link>/tag/brms/</link>
      <atom:link href="/tag/brms/index.xml" rel="self" type="application/rss+xml" />
    <description>brms</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Fahim Ahmad (2020)</copyright><lastBuildDate>Wed, 17 Nov 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>brms</title>
      <link>/tag/brms/</link>
    </image>
    
    <item>
      <title>Conditional logistic models with brms: Rough draft.</title>
      <link>/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/</link>
      <pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/</guid>
      <description>
&lt;script src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble&lt;/h2&gt;
&lt;p&gt;After tremendous help from &lt;a href=&#34;http://singmann.org/&#34;&gt;Henrik Singmann&lt;/a&gt; and &lt;a href=&#34;https://sites.google.com/view/mattansb&#34;&gt;Mattan Ben-Shachar&lt;/a&gt;, I finally have two (!) workflows for conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. These workflows are on track to make it into the next update of my ebook translation of Kruschke’s text (see &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;here&lt;/a&gt;). But these models are new to me and I’m not entirely confident I’ve walked them out properly.&lt;/p&gt;
&lt;p&gt;The goal of this blog post is to present a draft of my workflow, which will eventually make it’s way into &lt;a href=&#34;https://bookdown.org/content/3686/nominal-predicted-variable.html&#34;&gt;Chapter 22&lt;/a&gt; of the ebook. If you have any constrictive criticisms, please pass them along on&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/22&#34;&gt;GitHub&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433&#34;&gt;this thread&lt;/a&gt; in the Stan forums,&lt;/li&gt;
&lt;li&gt;or on &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1461016859420024842&#34;&gt;twitter&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To streamline this post a little, I have removed the content on the softmax model. For that material, just go to the ebook proper.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# in the ebook, this code will have already been executed
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)

theme_set(
  theme_gray() +
    theme(panel.grid = element_blank())
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nominal-predicted-variable&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nominal Predicted Variable&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;This chapter considers data structures that have a nominal predicted variable. When the nominal predicted variable has only two possible values, this reduces to the case of the dichotomous predicted variable considered in the previous chapter. In the present chapter, we generalize to cases in which the predicted variable has three or more categorical values…&lt;/p&gt;
&lt;p&gt;The traditional treatment of this sort of data structure is called multinomial logistic regression or conditional logistic regression. We will consider Bayesian approaches to these methods. As usual, in Bayesian software it is easy to generalize the traditional models so they are robust to outliers, allow different variances within levels of a nominal predictor, and have hierarchical structure to share information across levels or factors as appropriate. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke, 2015, p. 649&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;softmax-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Softmax regression&lt;/h2&gt;
&lt;div id=&#34;softmax-reduces-to-logistic-for-two-outcomes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Softmax reduces to logistic for two outcomes.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;independence-from-irrelevant-attributes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Independence from irrelevant attributes.&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional logistic regression&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Softmax regression conceives of each outcome as an independent change in log odds from the reference outcome, and a special case of that is dichotomous logistic regression. But we can generalize logistic regression another way, which may better capture some patterns of data. The idea of this generalization is that we divide the set of outcomes into a hierarchy of two-set divisions, and use a logistic to describe the probability of each branch of the two-set divisions. (p. 655)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The model follows the generic equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\phi_{S^* | S} = \operatorname{logistic}(\lambda_{S^* | S}) \\
\lambda_{S^* | S} = \beta_{0, S^* | S} + \beta_{1, {S^* | S}} x,
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the conditional response probability (i.e., the goal of the analysis) is &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(S^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; denote the subset of outcomes and larger set of outcomes, respectively, and &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{S^* | S}\)&lt;/span&gt; is the propensity based on some linear model. The overall point is these “regression coefficients refer to the conditional probability of outcomes for the designated subsets, not necessarily to a single outcome among the full set of outcomes” (p. 655).&lt;/p&gt;
&lt;p&gt;In Figure 22.2 (p. 656), Kruschke depicted the two hierarchies of binary divisions of the models he fit to the data in his &lt;code&gt;CondLogistRegData1.csv&lt;/code&gt; and &lt;code&gt;CondLogistRegData2.csv&lt;/code&gt; files. Here we load those data, save them as &lt;code&gt;d3&lt;/code&gt; and &lt;code&gt;d4&lt;/code&gt;, and take a look at their structures.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d3 &amp;lt;- read_csv(&amp;quot;data.R/CondLogistRegData1.csv&amp;quot;)
d4 &amp;lt;- read_csv(&amp;quot;data.R/CondLogistRegData2.csv&amp;quot;)

glimpse(d3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 475
## Columns: 3
## $ X1 &amp;lt;dbl&amp;gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.5…
## $ X2 &amp;lt;dbl&amp;gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.774…
## $ Y  &amp;lt;dbl&amp;gt; 2, 1, 3, 1, 3, 3, 2, 3, 2, 4, 1, 2, 2, 3, 4, 2, 2, 4, 2, 3, 4, 2, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(d4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 475
## Columns: 3
## $ X1 &amp;lt;dbl&amp;gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.5…
## $ X2 &amp;lt;dbl&amp;gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.774…
## $ Y  &amp;lt;dbl&amp;gt; 4, 4, 3, 4, 2, 3, 4, 3, 4, 4, 2, 4, 4, 3, 3, 4, 4, 4, 4, 3, 4, 4, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In both data sets, the nominal criterion is &lt;code&gt;Y&lt;/code&gt; and the two predictors are &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt;. Though the data seem simple, the conditional logistic models are complex enough that it seems like we’ll be better served by focusing on them one at a time, which means I’m going to break up Figure 22.2. Here’s how to make the diagram in the left panel.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the big numbers
numbers &amp;lt;- tibble(
  x = c(3, 5, 2, 4, 1, 3, 2),
  y = c(0, 0, 1, 1, 2, 2, 3),
  label = c(&amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3,4&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2,3,4&amp;quot;, &amp;quot;1,2,3,4&amp;quot;)
)

# the smaller Greek numbers
greek &amp;lt;- tibble(
  x = c(3.4, 4.6, 2.4, 3.6, 1.4, 2.6),
  y = c(0.5, 0.5, 1.5, 1.5, 2.5, 2.5),
  hjust = c(1, 0, 1, 0, 1, 0),
  label = c(&amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;)
)

# arrows
tibble(
  x = c(4, 4, 3, 3, 2, 2),
  y = c(0.85, 0.85, 1.85, 1.85, 2.85, 2.85),
  xend = c(3, 5, 2, 4, 1, 3),
  yend = c(0.15, 0.15, 1.15, 1.15, 2.15, 2.15)
) %&amp;gt;%  
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_segment(aes(xend = xend, yend = yend),
               size = 1/4,
               arrow = arrow(length = unit(0.08, &amp;quot;in&amp;quot;), type = &amp;quot;closed&amp;quot;)) +
  geom_text(data = numbers,
            aes(label = label),
            size = 5, family = &amp;quot;Times&amp;quot;)+
  geom_text(data = greek,
            aes(label = label, hjust = hjust),
            size = 4.25, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-1, 7) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The large numbers are the four levels in the criterion &lt;code&gt;Y&lt;/code&gt; and the smaller numbers in the curly braces are various sets of those numbers. The diagram shows three levels of outcome-set divisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 versus 2, 3, or 4;&lt;/li&gt;
&lt;li&gt;2 versus 3 or 4; and&lt;/li&gt;
&lt;li&gt;3 versus 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The divisions in each of these levels can be expressed as linear models which we’ll denote &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Given our data with two predictors &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt;, we can express the three linear models as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\lambda_{\{ 1 \} | \{ 1,2,3,4 \}} &amp;amp; = \beta_{0,\{ 1 \} | \{ 1,2,3,4 \}} + \beta_{1,\{ 1 \} | \{ 1,2,3,4 \}} \text{X1} + \beta_{2,\{ 1 \} | \{ 1,2,3,4 \}} \text{X2} \\
\lambda_{\{ 2 \} | \{ 2,3,4 \}}   &amp;amp; = \beta_{0,\{ 2 \} | \{ 2,3,4 \}} + \beta_{1,\{ 2 \} | \{ 2,3,4 \}} \text{X1} + \beta_{2,\{ 2 \} | \{ 2,3,4 \}} \text{X2} \\
\lambda_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \beta_{0,\{ 3 \} | \{ 3,4 \}} + \beta_{1,\{ 3 \} | \{ 3,4 \}} \text{X1} + \beta_{2,\{ 3 \} | \{ 3,4 \}} \text{X2},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, for convenience, we’re omitting the typical &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; subscripts. As these linear models are all defined within the context of the logit link, we can express the conditional probabilities of the outcome sets as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_{\{ 1 \} | \{ 1,2,3,4 \}} &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_{\{ 2 \} | \{ 2,3,4 \}}   &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 2 \} | \{ 2,3,4 \}} \right) \\
\phi_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 3 \} | \{ 3,4 \}} \right),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 1 \} | \{ 1,2,3,4 \}}\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 3 \} | \{ 3,4 \}}\)&lt;/span&gt; are the conditional probabilities for the outcome sets. If, however, we want the conditional probabilities for the actual levels of the criterion &lt;code&gt;Y&lt;/code&gt;, we define those with a series of (in this case) four equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \phi_{\{ 2 \} | \{ 2,3,4 \}} \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the sum of the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\phi_1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(\phi_4\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. To get a sense of what this all means in practice, let’s visualize the data and the data-generating equations for our version of Figure 22.3. As with the previous figure, I’m going to break this figure up to focus on one model at a time. Thus, here’s the left panel of Figure 22.3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the various population parameters

# lambda 1
b01 &amp;lt;- -4
b11 &amp;lt;- -5
b21 &amp;lt;- 0.01  # rounding up to avoid dividing by zero
# lambda 2
b02 &amp;lt;- -2
b12 &amp;lt;- 1
b22 &amp;lt;- -5
# lambda 3
b03 &amp;lt;- -1
b13 &amp;lt;- 3
b23 &amp;lt;- 3

# use the parameters to define the lines 
lines &amp;lt;- tibble(
  intercept = c(-b01 / b21, -b02 / b22, -b03 / b23),
  slope = c(-b11 / b21, -b12 / b22, -b13 / b23),
  label = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;)
)

# wrangle
d3 %&amp;gt;% 
  mutate(Y = factor(Y)) %&amp;gt;% 
  
  # plot!
  ggplot() +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_text(aes(x = X1, y = X2, label = Y, color = Y),
            size = 3, show.legend = F) +
  geom_abline(data = lines,
              aes(intercept = intercept,
                  slope = slope,
                  linetype = label)) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]==-4+-5*x[1]+0*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]==-2+1*x[1]+-5*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]==-1+3*x[1]+3*x[2]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;240&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Recall back on page 629, Kruschke showed the equation for the 50% threshold of a logistic regression model given two continuous predictors was&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_2 = (-\beta_0 / \beta_2) + (-\beta_1 / \beta_2) x_1.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It was that equation that gave us the values for the &lt;code&gt;intercept&lt;/code&gt; and &lt;code&gt;slope&lt;/code&gt; arguments (&lt;span class=&#34;math inline&#34;&gt;\(-\beta_0 / \beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-\beta_1 / \beta_2\)&lt;/span&gt;, respectively) for the &lt;code&gt;geom_abline()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;It still might not be clear how the various &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt; values connect to the data. Though not in the text, here’s an alternative way of expressing the relations in Figure 22.3. This time the plot is faceted by the three levels of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt; and the background fill is based on those conditional probabilities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define a grid of X1 and X2 values
crossing(X1 = seq(from = -2, to = 2, length.out = 50),
         X2 = seq(from = -2, to = 2, length.out = 50)) %&amp;gt;% 
  # compute the lambda&amp;#39;s
  mutate(`lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]` = b01 + b11 * X1 + b21 * X2,
         `lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]`   = b02 + b12 * X1 + b22 * X2,
         `lambda[&amp;#39;{3}|{3,4}&amp;#39;]`     = b03 + b13 * X1 + b23 * X2) %&amp;gt;% 
  # compute the phi&amp;#39;s
  mutate(`phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]` = plogis(`lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]`),
         `phi[&amp;#39;{2}|{2,3,4}&amp;#39;]`   = plogis(`lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]`),
         `phi[&amp;#39;{3}|{3,4}&amp;#39;]`     = plogis(`lambda[&amp;#39;{3}|{3,4}&amp;#39;]`)) %&amp;gt;% 
  # wrangle
  pivot_longer(contains(&amp;quot;phi&amp;quot;), values_to = &amp;quot;phi&amp;quot;) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = X1, y = X2)) +
  geom_raster(aes(fill = phi),
              interpolate = T) +
  # note how we&amp;#39;re subsetting the d3 data by facet
  geom_text(data = bind_rows(
    d3 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;),
    d3 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;gt; 1),
    d3 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;gt; 2)),
            aes(label = Y),
            size = 2.5, color = &amp;quot;grey20&amp;quot;) +
  scale_fill_viridis_c(expression(phi[italic(S)*&amp;quot;*|&amp;quot;*italic(S)]),
                        option = &amp;quot;F&amp;quot;, breaks = 0:2 / 2, limits = c(0, 1)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_equal() +
  theme(legend.position = c(0.8, 0.2)) +
  facet_wrap(~ name, labeller = label_parsed, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how because each of the levels of &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is defined by a different subset of the data, each of the facets contains a different subset of the &lt;code&gt;d3&lt;/code&gt; data, too. For example, since &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 1 \} | \{ 1,2,3,4 \}}\)&lt;/span&gt; is defined by the full subset of the possible values of &lt;code&gt;Y&lt;/code&gt;, you see all the &lt;code&gt;Y&lt;/code&gt; data displayed by &lt;code&gt;geom_text()&lt;/code&gt; for that facet. In contrast, since &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 3 \} | \{ 3,4 \}}\)&lt;/span&gt; is defined by a subset of the data for which &lt;code&gt;Y&lt;/code&gt; is only &lt;code&gt;3&lt;/code&gt; or &lt;code&gt;4&lt;/code&gt;, those are the only values you see displayed within that facet of the plot.&lt;/p&gt;
&lt;p&gt;Now we’ll consider an alternative way to set up the binary-choices hierarchy, as seen in the right panel of Figure 22.2. First, here’s that half of the figure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the big numbers
numbers &amp;lt;- tibble(
  x = c(0, 2, 6, 8, 1, 7, 4),
  y = c(0, 0, 0, 0, 1, 1, 2),
  label = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;1,2&amp;quot;, &amp;quot;3,4&amp;quot;, &amp;quot;1,2,3,4&amp;quot;)
)

# the smaller Greek numbers
greek &amp;lt;- tibble(
  x = c(0.4, 1.6, 6.4, 7.6, 2.1, 5.8),
  y = c(0.5, 0.5, 0.5, 0.5, 1.5, 1.5),
  hjust = c(1, 0, 1, 0, 1, 0),
  label = c(&amp;quot;phi[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;)
)

# arrows
tibble(
  x = c(1, 1, 7, 7, 4, 4),
  y = c(0.85, 0.85, 0.85, 0.85, 1.85, 1.85),
  xend = c(0, 2, 6, 8, 1, 7),
  yend = c(0.15, 0.15, 0.15, 0.15, 1.15, 1.15)
) %&amp;gt;%  
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_segment(aes(xend = xend, yend = yend),
               size = 1/4,
               arrow = arrow(length = unit(0.08, &amp;quot;in&amp;quot;), type = &amp;quot;closed&amp;quot;)) +
  geom_text(data = numbers,
            aes(label = label),
            size = 5, family = &amp;quot;Times&amp;quot;)+
  geom_text(data = greek,
            aes(label = label, hjust = hjust),
            size = 4.25, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-1, 10) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This diagram shows three levels of outcome-set divisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 or 2 versus 3 or 4;&lt;/li&gt;
&lt;li&gt;1 versus 2; and&lt;/li&gt;
&lt;li&gt;3 versus 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given our data with two predictors &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt;, we can express the three linear models as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\lambda_{\{ 1,2 \} | \{ 1,2,3,4 \}} &amp;amp; = \beta_{0,\{ 1,2 \} | \{ 1,2,3,4 \}} + \beta_{1,\{ 1,2 \} | \{ 1,2,3,4 \}} \text{X1} + \beta_{2,\{ 1,2 \} | \{ 1,2,3,4 \}} \text{X2} \\
\lambda_{\{ 1 \} | \{ 1,2 \}}   &amp;amp; = \beta_{0,\{ 1 \} | \{ 1,2 \}} + \beta_{1,\{ 1 \} | \{ 1,2 \}} \text{X1} + \beta_{2,\{ 1 \} | \{ 1,2 \}} \text{X2} \\
\lambda_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \beta_{0,\{ 3 \} | \{ 3,4 \}} + \beta_{1,\{ 3 \} | \{ 3,4 \}} \text{X1} + \beta_{2,\{ 3 \} | \{ 3,4 \}} \text{X2}.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can then express the conditional probabilities of the outcome sets as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_{\{ 1 \} | \{ 1,2 \}}   &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 1 \} | \{ 1,2 \}} \right) \\
\phi_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 3 \} | \{ 3,4 \}} \right).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the conditional probabilities of the actual levels of the criterion &lt;code&gt;Y&lt;/code&gt;, we define those with a series of (in this case) four equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2 \}} \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \left ( 1 - \phi_{\{ 1 \} | \{ 1,2 \}} \right) \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the sum of the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\phi_1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(\phi_4\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. To get a sense of what this all means, let’s visualize the data and the data-generating equations in our version of the right panel of Figure 22.3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d4 %&amp;gt;% 
  mutate(Y = factor(Y)) %&amp;gt;% 
  
  ggplot() +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_text(aes(x = X1, y = X2, label = Y, color = Y),
            size = 3, show.legend = F) +
  geom_abline(data = lines,
              aes(intercept = intercept,
                  slope = slope,
                  linetype = label)) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]==-4+-5*x[1]+0*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{1}|{1,2}&amp;#39;]==-2+1*x[1]+-5*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]==-1+3*x[1]+3*x[2]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;240&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s an alternative way of expression the relations in the right panel of Figure 22.3. This time the plot is faceted by the three levels of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt; and the background fill is based on those conditional probabilities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define a grid of X1 and X2 values
crossing(X1 = seq(from = -2, to = 2, length.out = 50),
         X2 = seq(from = -2, to = 2, length.out = 50)) %&amp;gt;% 
  # compute the lambda&amp;#39;s
  mutate(`lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]` = b01 + b11 * X1 + b21 * X2,
         `lambda[&amp;#39;{1}|{1,2}&amp;#39;]`       = b02 + b12 * X1 + b22 * X2,
         `lambda[&amp;#39;{3}|{3,4}&amp;#39;]`       = b03 + b13 * X1 + b23 * X2) %&amp;gt;% 
  # compute the phi&amp;#39;s
  mutate(`phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]` = plogis(`lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]`),
         `phi[&amp;#39;{1}|{1,2}&amp;#39;]`       = plogis(`lambda[&amp;#39;{1}|{1,2}&amp;#39;]`),
         `phi[&amp;#39;{3}|{3,4}&amp;#39;]`       = plogis(`lambda[&amp;#39;{3}|{3,4}&amp;#39;]`)) %&amp;gt;% 
  # wrangle
  pivot_longer(contains(&amp;quot;phi&amp;quot;), values_to = &amp;quot;phi&amp;quot;) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = X1, y = X2)) +
  geom_raster(aes(fill = phi),
              interpolate = T) +
  # note how we&amp;#39;re subsetting the d3 data by facet
  geom_text(data = bind_rows(
    d4 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;),
    d4 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;lt; 3),
    d4 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;gt; 2)),
            aes(label = Y),
            size = 2.5, color = &amp;quot;grey20&amp;quot;) +
  scale_fill_viridis_c(expression(phi[italic(S)*&amp;quot;*|&amp;quot;*italic(S)]),
                       option = &amp;quot;F&amp;quot;, breaks = 0:2 / 2, limits = c(0, 1)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_equal() +
  theme(legend.position = c(0.8, 0.2)) +
  facet_wrap(~ name, labeller = label_parsed, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It could be easy to miss due to the way we broke up our workflow, but if you look closely at the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; equations at the top of both panels of Figure 22.3, you’ll see the right-hand side of the equations are the same. But because of the differences in the two data hierarchies, those &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; equations had different consequences for how the &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; values generated the &lt;code&gt;Y&lt;/code&gt; data. Also,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In general, conditional logistic regression requires that there is a linear division between two subsets of the outcomes, and then within each of those subsets there is a linear division of smaller subsets, and so on. This sort of linear division is not required of the softmax regression model… Real data can be extremely noisy, and there can be multiple predictors, so it can be challenging or impossible to visually ascertain which sort of model is most appropriate. The choice of model is driven primarily by theoretical meaningfulness. (p. 659)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-in-jags-brms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implementation in &lt;del&gt;JAGS&lt;/del&gt; brms&lt;/h2&gt;
&lt;div id=&#34;softmax-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Softmax model.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conditional logistic model.&lt;/h3&gt;
&lt;p&gt;The conditional logistic regression models are not natively supported in &lt;strong&gt;brms&lt;/strong&gt; at this time. Based on &lt;a href=&#34;https://github.com/paul-buerkner/brms/issues/560&#34;&gt;issue #560&lt;/a&gt; in the &lt;strong&gt;brms&lt;/strong&gt; GitHub, there are ways to fit them using the nonlinear syntax. If you compare the syntax Bürkner used in that thread on January 30&lt;sup&gt;th&lt;/sup&gt; to the JAGS syntax Kruschke showed on pages 661 and 662, you’ll see they appear to follow contrasting parameterizations.&lt;/p&gt;
&lt;p&gt;However, there are at least two other ways to fit conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. Based on insights from &lt;a href=&#34;http://singmann.org/&#34;&gt;Henrik Singmann&lt;/a&gt;, we can define conditional logistic models using the custom family approach. In contrast, &lt;a href=&#34;https://sites.google.com/view/mattansb&#34;&gt;Mattan Ben-Shachar&lt;/a&gt; has shown we can also fit conditional logistic models using a tricky application of sequential ordinal regression. Rather than present them in the abstract, here, we will showcase both of these approaches in the sections below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results-interpreting-the-regression-coefficients.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Results: Interpreting the regression coefficients.&lt;/h3&gt;
&lt;div id=&#34;softmax-model.-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Softmax model.&lt;/h4&gt;
&lt;div id=&#34;bonus-consider-the-interceps-only-softmax-model.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Bonus: Consider the interceps-only softmax model.&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-model.-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Conditional logistic model.&lt;/h4&gt;
&lt;p&gt;Since we will be fitting the conditional logistic model with two different strategies, I’m going to deviate from how Kruschke organized this part of the text and break this section up into two subsections:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First we’ll walk through the custom family approach.&lt;/li&gt;
&lt;li&gt;Second we’ll explore the sequential ordinal approach.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;conditional-logistic-models-with-custom-likelihoods.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Conditional logistic models with custom likelihoods.&lt;/h5&gt;
&lt;p&gt;As we briefly learned in [Section 8.6.1][Defining new likelihood functions.], &lt;strong&gt;brms&lt;/strong&gt; users can define their own custom likelihood functions, which Bürkner outlined in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Define&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; vignette, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html&#34;&gt;&lt;em&gt;Define custom response distributions with brms&lt;/em&gt;&lt;/a&gt;. As part of the &lt;a href=&#34;https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433&#34;&gt;&lt;em&gt;Nominal data and Kruschke’s “conditional logistic” approach&lt;/em&gt;&lt;/a&gt; thread on the Stan forums, Henrik Singmann showed how you can use this functionality to fit conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. We will practice how to do this for the models of both the &lt;code&gt;d3&lt;/code&gt; and &lt;code&gt;d4&lt;/code&gt; data sets, which were showcased in the left and right panels of Figure 22.3 in &lt;a href=&#34;#conditional-logistic-regression&#34;&gt;Section 22.2&lt;/a&gt;. Going in order, we’ll focus first on how to model the data in &lt;code&gt;d3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the first step, we use the &lt;code&gt;custom_family()&lt;/code&gt; function to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;name the new family with the &lt;code&gt;name&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;name the family’s parameters with the &lt;code&gt;dpars&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;name the link function(s) with the &lt;code&gt;links&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;define whether the distribution is discrete or continuous with the &lt;code&gt;type&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;provide the names of any variables that are part of the internal workings of the family but are not among the distributional parameters with the &lt;code&gt;vars&lt;/code&gt; argument, and&lt;/li&gt;
&lt;li&gt;provide supporting information with the &lt;code&gt;specials&lt;/code&gt; argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cond_log_1 &amp;lt;- custom_family(
  name     = &amp;quot;cond_log_1&amp;quot;, 
  dpars    = c(&amp;quot;mu&amp;quot;, &amp;quot;mub&amp;quot;, &amp;quot;muc&amp;quot;), 
  links    = &amp;quot;identity&amp;quot;, 
  type     = &amp;quot;int&amp;quot;,
  vars     = c(&amp;quot;n_cat&amp;quot;),
  specials = &amp;quot;categorical&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the second step, we use the &lt;code&gt;stanvar()&lt;/code&gt; function to define our custom probability mass function and the corresponding function that will allow us to return predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_lpmf_1 &amp;lt;- stanvar(block = &amp;quot;functions&amp;quot;, 
                       scode = &amp;quot;
real cond_log_1_lpmf(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mu;
  prob[2] = p_mub * (1 - p_mu);
  prob[3] = p_muc * (1 - p_mub) * (1 - p_mu);
  prob[4] = (1 - p_mu) * (1 - p_mub) * (1 - p_muc);
  return(categorical_lpmf(y | prob));
}

vector cond_log_1_pred(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mu;
  prob[2] = p_mub * (1 - p_mu);
  prob[3] = p_muc * (1 - p_mub) * (1 - p_mu);
  prob[4] = (1 - p_mu) * (1 - p_mub) * (1 - p_muc);
  return(prob);
}
&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how we have defined the four &lt;code&gt;prob[i]&lt;/code&gt; values based on the four equations from above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \phi_{\{ 2 \} | \{ 2,3,4 \}} \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Third, we save another &lt;code&gt;stanvar()&lt;/code&gt; object with additional information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stanvars &amp;lt;- stanvar(x = 4, name = &amp;quot;n_cat&amp;quot;, scode = &amp;quot;  int n_cat;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to fit the model with &lt;code&gt;brm()&lt;/code&gt;. Notice how our use of the &lt;code&gt;family&lt;/code&gt; and &lt;code&gt;stanvars&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.3 &amp;lt;-
  brm(data = d3, 
      family = cond_log_1,
      Y ~ 1 + X1 + X2,
      prior = c(prior(normal(0, 20), class = Intercept, dpar = mu2),
                prior(normal(0, 20), class = Intercept, dpar = mu3),
                prior(normal(0, 20), class = Intercept, dpar = mu4),
                prior(normal(0, 20), class = b, dpar = mu2),
                prior(normal(0, 20), class = b, dpar = mu3),
                prior(normal(0, 20), class = b, dpar = mu4)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      stanvars = stan_lpmf_1 + stanvars,
      file = &amp;quot;fits/fit22.03&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cond_log_1 
##   Links: mu2 = identity; mu3 = identity; mu4 = identity 
## Formula: Y ~ 1 + X1 + X2 
##    Data: d3 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## mu2_Intercept    -4.02      0.47    -5.01    -3.17 1.00     3108     2656
## mu3_Intercept    -2.13      0.36    -2.88    -1.50 1.00     2490     2211
## mu4_Intercept    -0.96      0.32    -1.62    -0.38 1.00     3014     2674
## mu2_X1           -4.92      0.54    -6.03    -3.94 1.00     3029     2832
## mu2_X2            0.01      0.20    -0.37     0.40 1.00     4950     2436
## mu3_X1            0.74      0.30     0.16     1.35 1.00     3343     2736
## mu3_X2           -5.21      0.63    -6.54    -4.06 1.00     2748     2794
## mu4_X1            3.00      0.49     2.10     4.05 1.00     3002     2280
## mu4_X2            3.10      0.53     2.16     4.25 1.00     2646     2412
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As they aren’t the most intuitive, here’s how to understand our three prefixes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mu2_&lt;/code&gt; has to do with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\{ 1 \} | \{ 1,2,3,4 \}}\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu3_&lt;/code&gt; has to do with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\{ 2 \} | \{ 2,3,4 \}}\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu4_&lt;/code&gt; has to do with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\{ 3 \} | \{ 3,4 \}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you compare those posterior means of each of those parameters from the data-generating equations at the top of Figure 22.3, you’ll see they are spot on (within simulation variance). Here’s how we might visualize those posteriors in our version of the histograms in the top right panel(s) of Figure 22.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws
post &amp;lt;- posterior_samples(fit22.3)

# wrangle
p1 &amp;lt;-
  post %&amp;gt;% 
  pivot_longer(-lp__) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(number = str_extract(name, &amp;quot;[2-4]+&amp;quot;)) %&amp;gt;% 
  mutate(lambda    = case_when(number == &amp;quot;2&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;,
                               number == &amp;quot;3&amp;quot; ~ &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;,
                               number == &amp;quot;4&amp;quot; ~ &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;),
         parameter = case_when(str_detect(name, &amp;quot;Intercept&amp;quot;) ~ &amp;quot;beta[0]&amp;quot;,
                               str_detect(name, &amp;quot;X1&amp;quot;)        ~ &amp;quot;beta[1]&amp;quot;,
                               str_detect(name, &amp;quot;X2&amp;quot;)        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95, size = 1,
                    normalize = &amp;quot;panels&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we use the threshold formula from above,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_2 = (-\beta_0 / \beta_2) + (-\beta_1 / \beta_2)x_1,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to the posterior draws, we can make our version of the upper left panel of Figure 22.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22)

p2 &amp;lt;-
  post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_mu&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;lambda&amp;quot;, &amp;quot;parameter&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  mutate(intercept = -Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now combine the two ggplots, add a little formatting, and show the full upper half of Figure 22.6, based on the &lt;code&gt;custom_family()&lt;/code&gt; approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p2 + p1) &amp;amp; 
  plot_layout(widths = c(1, 2)) &amp;amp;
  plot_annotation(title = &amp;quot;Figure 22.6, upper half&amp;quot;,
                  subtitle = &amp;quot;Results from the conditional logistic model fit to the d3 data via the custom-family approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though it isn’t necessary to reproduce any of the plots in this section of Kruschke’s text, we’ll want to use the &lt;code&gt;expose_functions()&lt;/code&gt; function if we wanted to use any of the &lt;strong&gt;brms&lt;/strong&gt; post-processing functions for our model fit with the custom likelihood.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expose_functions(fit22.3, vectorize = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what we’d need to do before computing information criteria estimates, such as with the WAIC.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_lik_cond_log_1 &amp;lt;- function(i, prep) {
  mu  &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mub &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  muc &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  cond_log_1_lpmf(y, mu, mub, muc, n_cat)
}

fit22.3 &amp;lt;- add_criterion(fit22.3, criterion = &amp;quot;waic&amp;quot;)

waic(fit22.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 4000 by 475 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -230.8 16.8
## p_waic         9.3  1.1
## waic         461.6 33.6
## 
## 2 (0.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to use one of the functions that relies on conditional expectations, such as &lt;code&gt;conditional_effects()&lt;/code&gt;, we’d execute something like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_epred_cond_log_1 &amp;lt;- function(prep) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y
  prob &amp;lt;- cond_log_1_pred(y = y, mu = mu, mu_b = mu_b, mu_c = mu_c, n_cat = n_cat)
  dim(prob) &amp;lt;- c(dim(prob)[1], dim(mu))
  prob &amp;lt;- aperm(prob, c(2,3,1))
  dimnames(prob) &amp;lt;- list(
    as.character(seq_len(dim(prob)[1])), 
    NULL, 
    as.character(seq_len(dim(prob)[3]))
  )
  prob
}

ce &amp;lt;- conditional_effects(
  fit22.3, 
  categorical = T,
  effects = &amp;quot;X1&amp;quot;)

plot(ce, plot = FALSE)[[1]] + 
  scale_fill_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we wanted to do a posterior predictive check with the &lt;code&gt;pp_check()&lt;/code&gt; function, we’d need to do something like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_predict_cond_log_1 &amp;lt;- function(i, prep, ...) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  prob &amp;lt;- cond_log_1_pred(y, mu, mu_b, mu_c, n_cat)
  # make sure you have the extraDistr package
  extraDistr::rcat(length(mu), t(prob))
}

pp_check(fit22.3, 
         type = &amp;quot;bars&amp;quot;, 
         ndraws = 100, 
         size = 1/2, 
         fatten = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So far all of this has been with the conditional logistic model based on the first hierarchy of two-set divisions, which Kruschke used to simulate the &lt;code&gt;d3&lt;/code&gt; data. Now we’ll switch to consider the second hierarchy of two-set divisions, with which Kruschke simulated the &lt;code&gt;d4&lt;/code&gt; data. That second hierarchy, recall, resulted in the following definition for the conditional probabilities for the four levels of &lt;code&gt;Y&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2 \}} \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \left ( 1 - \phi_{\{ 1 \} | \{ 1,2 \}} \right) \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This will require us to define a new custom family, which we’ll call &lt;code&gt;cond_log_2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cond_log_2 &amp;lt;- custom_family(
  name     = &amp;quot;cond_log_2&amp;quot;, 
  dpars    = c(&amp;quot;mu&amp;quot;, &amp;quot;mub&amp;quot;, &amp;quot;muc&amp;quot;), 
  links    = &amp;quot;identity&amp;quot;, 
  type     = &amp;quot;int&amp;quot;,
  vars     = c(&amp;quot;n_cat&amp;quot;),
  specials = &amp;quot;categorical&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we use the &lt;code&gt;stanvar()&lt;/code&gt; function to define our custom probability mass function and the corresponding function that will allow us to return predictions, which we’ll just save as &lt;code&gt;stan_lpmf_2&lt;/code&gt;. Other than the names, notice that the major change is how we have defined the &lt;code&gt;prob[i]&lt;/code&gt; parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_lpmf_2 &amp;lt;- stanvar(block = &amp;quot;functions&amp;quot;, 
                       scode = &amp;quot;
real cond_log_2_lpmf(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mub * p_mu;
  prob[2] = (1 - p_mub) * p_mu;
  prob[3] = p_muc * (1 - p_mu);
  prob[4] = (1 - p_muc) * (1 - p_mu);
  return(categorical_lpmf(y | prob));
}

vector cond_log_2_pred(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mub * p_mu;
  prob[2] = (1 - p_mub) * p_mu;
  prob[3] = p_muc * (1 - p_mu);
  prob[4] = (1 - p_muc) * (1 - p_mu);
  return(prob);
}
&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to fit the model with &lt;code&gt;brm()&lt;/code&gt;. Again, notice how our use of the &lt;code&gt;family&lt;/code&gt; and &lt;code&gt;stanvars&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.4 &amp;lt;-
  brm(data = d4, 
      family = cond_log_2,
      Y ~ 1 + X1 + X2,
      prior = c(prior(normal(0, 20), class = Intercept, dpar = mu2),
                prior(normal(0, 20), class = Intercept, dpar = mu3),
                prior(normal(0, 20), class = Intercept, dpar = mu4),
                prior(normal(0, 20), class = b, dpar = mu2),
                prior(normal(0, 20), class = b, dpar = mu3),
                prior(normal(0, 20), class = b, dpar = mu4)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      stanvars = stan_lpmf_2 + stanvars,
      file = &amp;quot;fits/fit22.04&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cond_log_2 
##   Links: mu2 = identity; mu3 = identity; mu4 = identity 
## Formula: Y ~ 1 + X1 + X2 
##    Data: d4 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## mu2_Intercept    -4.05      0.46    -5.02    -3.23 1.00     2757     2184
## mu3_Intercept    -1.39      1.19    -3.80     0.87 1.00     2632     2185
## mu4_Intercept    -1.02      0.23    -1.50    -0.60 1.00     2824     2668
## mu2_X1           -4.79      0.52    -5.90    -3.84 1.00     2769     2286
## mu2_X2            0.35      0.20    -0.04     0.74 1.00     4245     2441
## mu3_X1            1.54      0.88    -0.13     3.28 1.00     2668     2139
## mu3_X2           -5.36      1.16    -7.86    -3.39 1.00     3306     2241
## mu4_X1            3.03      0.38     2.34     3.81 1.00     1872     2428
## mu4_X2            3.13      0.36     2.49     3.86 1.00     2219     2511
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the same basic workflow as before to make our version of the upper half of Figure 22.7.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws
post &amp;lt;- posterior_samples(fit22.4)

# 2D thresholds on the left
set.seed(22)

p1 &amp;lt;-
  post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_mu&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;lambda&amp;quot;, &amp;quot;parameter&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  mutate(intercept = -Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d4,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)

# marginal posteriors on the right
p2 &amp;lt;-
  post %&amp;gt;% 
  pivot_longer(-lp__) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(number = str_extract(name, &amp;quot;[2-4]+&amp;quot;)) %&amp;gt;% 
  mutate(lambda    = case_when(number == &amp;quot;2&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;,
                               number == &amp;quot;3&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;,
                               number == &amp;quot;4&amp;quot; ~ &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;),
         parameter = case_when(str_detect(name, &amp;quot;Intercept&amp;quot;) ~ &amp;quot;beta[0]&amp;quot;,
                               str_detect(name, &amp;quot;X1&amp;quot;)        ~ &amp;quot;beta[1]&amp;quot;,
                               str_detect(name, &amp;quot;X2&amp;quot;)        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95, size = 1,
                    normalize = &amp;quot;panels&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)

# combine, entitle, and display the results
(p1 + p2) &amp;amp; 
  plot_layout(widths = c(1, 2)) &amp;amp;
  plot_annotation(title = &amp;quot;Figure 22.7, upper half&amp;quot;,
                  subtitle = &amp;quot;Results from the conditional logistic model fit to the d4 data via the custom-family approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As Kruschke pointed out in the text,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;notice that the estimates for &lt;span class=&#34;math inline&#34;&gt;\(\lambda_2\)&lt;/span&gt; are more uncertain, with wider HDI’s, than the other coefficients. This uncertainty is also shown in the threshold lines on the data: The lines separating the 1’s from the 2’s have a much wider spread than the other boundaries. Inspection of the scatter plot explains why: There is only a small zone of data that informs the separation of 1’s from 2’s, and therefore the estimate must be relatively ambiguous. (p. 665)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’m not going to go through a full demonstration like before, but if you want to use more &lt;strong&gt;brms&lt;/strong&gt; post processing functions for &lt;code&gt;fit22.4&lt;/code&gt; or any other model fit with our custom &lt;code&gt;cond_log_2&lt;/code&gt; function, you’d need to execute this block of code first. Then post process to your heart’s desire.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expose_functions(fit22.4, vectorize = TRUE)

# for information criteria
log_lik_cond_log_2 &amp;lt;- function(i, prep) {
  mu  &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mub &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  muc &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  cond_log_2_lpmf(y, mu, mub, muc, n_cat)
}

# for conditional expectations
posterior_epred_cond_log_2 &amp;lt;- function(prep) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y
  prob &amp;lt;- cond_log_2_pred(y = y, mu = mu, mu_b = mu_b, mu_c = mu_c, n_cat = n_cat)
  dim(prob) &amp;lt;- c(dim(prob)[1], dim(mu))
  prob &amp;lt;- aperm(prob, c(2,3,1))
  dimnames(prob) &amp;lt;- list(
    as.character(seq_len(dim(prob)[1])), 
    NULL, 
    as.character(seq_len(dim(prob)[3]))
  )
  prob
}

# for posterior predictions
posterior_predict_cond_log_2 &amp;lt;- function(i, prep, ...) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  prob &amp;lt;- cond_log_2_pred(y, mu, mu_b, mu_c, n_cat)
  # make sure you have the extraDistr package
  extraDistr::rcat(length(mu), t(prob))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this section of the text, Kruschke also showed the results of when he analyzed the two data sets with the non-data-generating likelihoods. In the lower half of Figure 22.6, he showed the results of his second version of the conditional logistic model applied to the &lt;code&gt;d3&lt;/code&gt; data. In the lower half of Figure 22.7, he showed the results of his first version of the conditional logistic model applied to the &lt;code&gt;d4&lt;/code&gt; data. Since this section is already complicated enough, we’re not going to do that. But if you’d like to see what happens, consider it a personal homework assignment.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In principle, the different conditional logistic models could be put into an overarching hierarchical model comparison. If you have only a few specific candidate models to compare, this could be a feasible approach. But it is not an easily pursued approach to selecting a partition of outcomes from all possible partitions of outcomes when there are many outcomes… Therefore, it is typical to consider a single model, or small set of models, that are motivated by being meaningful in the context of the application, and interpreting the parameter estimates in that meaningful context. (p. 667)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kruschke finished this section with:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Finally, when you run the models in JAGS, you may find that there is high autocorrelation in the MCMC chains (even with standardized data), which requires a very long chain for adequate ESS. This suggests that Stan might be a more efficient approach.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since we fit our models with Stan via &lt;strong&gt;brms&lt;/strong&gt;, high autocorrelations and low effective sample sizes weren’t a problem. For example, here are the bulk and tail effective sample sizes for both of our two models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(posterior)

bind_rows(
  posterior_samples(fit22.3) %&amp;gt;% summarise_draws(),
  posterior_samples(fit22.4) %&amp;gt;% summarise_draws()
  ) %&amp;gt;% 
  mutate(fit = rep(c(&amp;quot;fit22.3&amp;quot;, &amp;quot;fit22.4&amp;quot;), each = n() / 2)) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;ess&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 250) +
  xlim(0, NA) +
  facet_grid(fit ~ name)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The values look pretty good. We may as well look at the autocorrelations. To keep things simple, this time we’ll restrict our analysis to &lt;code&gt;fit22.4&lt;/code&gt;. [The results are largely the same for &lt;code&gt;fit22.3&lt;/code&gt;.]&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

ac &amp;lt;-
  posterior_samples(fit22.4, add_chain = T) %&amp;gt;% 
  select(b_mu2_Intercept:b_mu4_X2, chain) %&amp;gt;% 
  mcmc_acf(lags = 5)

ac$data %&amp;gt;% 
  filter(Lag &amp;gt; 0) %&amp;gt;% 
  
  ggplot(aes(x = AC)) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_histogram(binwidth = 0.05) +
  scale_x_continuous(&amp;quot;autocorrelation&amp;quot;, limits = c(-1, 1),
                     labels = c(&amp;quot;-1&amp;quot;, &amp;quot;-.5&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;.5&amp;quot;, &amp;quot;1&amp;quot;)) +
  facet_grid(Lag ~ Chain, labeller = label_both)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the whole, the autocorrelations are reasonably low across all parameters, chains, and lags.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-models-by-sequential-ordinal-regression.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Conditional logistic models by sequential ordinal regression.&lt;/h5&gt;
&lt;p&gt;In their &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerOrdinalRegressionModels2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; paper, &lt;a href=&#34;https://psyarxiv.com/x8swp/&#34;&gt;&lt;em&gt;Ordinal regression models in psychology: A tutorial&lt;/em&gt;&lt;/a&gt;, Bürkner and Vourre outlined a framework for fitting a variety of orginal models with &lt;strong&gt;brms&lt;/strong&gt;. We’ll learn more about ordinal models in [Chapter 23][Ordinal Predicted Variable]. In this section, we’ll use Mattan Ben-Shachar’s strategy and purpose one of the ordinal models to fit a conditional logistic model to our nominal data.&lt;/p&gt;
&lt;p&gt;As outlined in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-burknerOrdinalRegressionModels2019&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner &amp;amp; Vuorre&lt;/a&gt; (&lt;a href=&#34;#ref-burknerOrdinalRegressionModels2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, and as we will learn in greater detain in the next chapter, many ordinal regression models presume an underlying continuous process. However, you can use a sequential model in cases where one level of the criterion is only possible after the lower levels of the criterion have been achieved. Although this is not technically correct for the nominal variable &lt;code&gt;Y&lt;/code&gt; in the &lt;code&gt;d3&lt;/code&gt; data set, the simple hierarchical sequence Kruschke used to model those data does follow that same pattern. Ben-Shachar’s insight was that if we treat our nominal variable &lt;code&gt;Y&lt;/code&gt; as ordinal, the sequential model will mimic the sequential-ness of Kruschke’s binary-choices hierarchy. To get this to work, we first have to save an ordinal version of &lt;code&gt;Y&lt;/code&gt;, which we’ll call &lt;code&gt;Y_ord&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d3 &amp;lt;-
  d3 %&amp;gt;% 
  mutate(Y_ord = ordered(Y))

# what are the new attributes?
attributes(d3$Y_ord)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $levels
## [1] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot;
## 
## $class
## [1] &amp;quot;ordered&amp;quot; &amp;quot;factor&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within &lt;code&gt;brm()&lt;/code&gt; we fit sequential models using &lt;code&gt;family = sratio&lt;/code&gt;, which defaults to the logit link. If you want to use predictors in a model of this kind and you would like those coefficients to vary across the different levels of the criterion, you need to insert the predictor terms within the &lt;code&gt;cs()&lt;/code&gt; function. Here’s how to fit the model with &lt;code&gt;brm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.5 &amp;lt;-
  brm(data = d3, 
      family = sratio,
      Y_ord ~ 1 + cs(X1) + cs(X2),
      prior = c(prior(normal(0, 20), class = Intercept),
                prior(normal(0, 20), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      file = &amp;quot;fits/fit22.05&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: sratio 
##   Links: mu = logit; disc = identity 
## Formula: Y_ord ~ 1 + cs(X1) + cs(X2) 
##    Data: d3 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -4.01      0.47    -4.97    -3.14 1.00     2850     2689
## Intercept[2]    -2.12      0.35    -2.84    -1.48 1.00     2864     2875
## Intercept[3]    -0.97      0.32    -1.61    -0.37 1.00     3162     3039
## X1[1]            4.92      0.54     3.93     6.04 1.00     2820     2570
## X1[2]           -0.74      0.29    -1.30    -0.19 1.00     3511     3244
## X1[3]           -3.00      0.50    -4.02    -2.10 1.00     3307     2966
## X2[1]           -0.01      0.20    -0.38     0.38 1.00     4316     2519
## X2[2]            5.22      0.63     4.07     6.52 1.00     2837     2616
## X2[3]           -3.11      0.53    -4.23    -2.14 1.00     2902     2640
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing that might not be apparent at first glance is that although this model is essentially equivalent to the &lt;code&gt;family = cond_log_1&lt;/code&gt; version of the model we fit with &lt;code&gt;fit22.3&lt;/code&gt;, above, the parameters are a little different. The intercepts are largely the same. However, the coefficients for the &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; predictors have switched signs. This will be easier to see with a coefficient plot comparing &lt;code&gt;fit22.3&lt;/code&gt; and &lt;code&gt;fit22.5&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws for fit22.5
post &amp;lt;- posterior_samples(fit22.5)

# wrangle and combine the draws from the two models
bind_rows(
  # family = &amp;quot;cond_log_1&amp;quot;
  posterior_samples(fit22.3) %&amp;gt;% 
    pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
    mutate(name = str_remove(name, &amp;quot;b_mu&amp;quot;)) %&amp;gt;% 
    separate(name, into = c(&amp;quot;lambda&amp;quot;, &amp;quot;parameter&amp;quot;)) %&amp;gt;% 
    mutate(lambda = as.double(lambda) - 1,
           family = &amp;quot;cond_log_1&amp;quot;) %&amp;gt;% 
    select(family, lambda, parameter, value),
  # family = &amp;quot;sratio&amp;quot;
  post %&amp;gt;% 
    pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
    mutate(name = str_remove(name, &amp;quot;b_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
    separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;%
    mutate(lambda = as.double(lambda),
           family = &amp;quot;sratio&amp;quot;) %&amp;gt;% 
    select(family, lambda, parameter, value)
) %&amp;gt;% 
  mutate(lambda    = str_c(&amp;quot;lambda==&amp;quot;, lambda),
         parameter = case_when(str_detect(parameter, &amp;quot;Intercept&amp;quot;) ~ &amp;quot;beta[0]&amp;quot;,
                               str_detect(parameter, &amp;quot;X1&amp;quot;)        ~ &amp;quot;beta[1]&amp;quot;,
                               str_detect(parameter, &amp;quot;X2&amp;quot;)        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = value, y = family)) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  stat_pointinterval(.width = .95, point_size = 1.5, size = 1) +
  labs(x = &amp;quot;marginal posterior&amp;quot;,
       y = NULL) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even though the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; parameters switched signs, their magnitudes are about the same. Thus, if we want to use our &lt;code&gt;fit22.5&lt;/code&gt; to plot the thresholds as in Figure 22.6, we’ll have to update our threshold formula to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_2 = (\color{red}{+} \color{black}{\beta_0 / \beta_2) + (\beta_1 / \beta_2)x_1.}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With that adjustment in line, here’s our updated version of the left panel of Figure 22.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22)

post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  # this line is different
  mutate(intercept = Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though we used a different likelihood and a different formula for the thresholds, we got same basic model results. They’re just parameterized in a slightly different way. The nice thing with the &lt;code&gt;family = sratio&lt;/code&gt; approach is all of the typical &lt;strong&gt;brms&lt;/strong&gt; post processing functions will work out of the box. For example, here’s the posterior predictive check via &lt;code&gt;pp_check()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp_check(fit22.5, 
         type = &amp;quot;bars&amp;quot;, 
         ndraws = 100, 
         size = 1/2, 
         fatten = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Also note how the information criteria estimates for the two approaches are essentially the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.5 &amp;lt;- add_criterion(fit22.5, criterion = &amp;quot;waic&amp;quot;)

loo_compare(fit22.3, fit22.5, criterion = &amp;quot;waic&amp;quot;) %&amp;gt;% 
  print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  
## fit22.3    0.0       0.0  -230.8      16.8          9.3    1.1     461.6
## fit22.5   -0.1       0.1  -230.9      16.8          9.4    1.1     461.8
##         se_waic
## fit22.3   33.6 
## fit22.5   33.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A limitation of the &lt;code&gt;family = sratio&lt;/code&gt; method for conditional logistic models is it requires a simple binary-divisions hierarchy that resembles the one we just used, the one in the left panel of Figure 22.2. It is not well suited for the more complicated hierarchy displayed in the right panel of Figure 22.2, nor will it help you make sense of data generated by that kind of mechanism. For example, consider what happens when we try to use &lt;code&gt;family = sratio&lt;/code&gt; with the &lt;code&gt;d4&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make an ordinal version of Y
d4 &amp;lt;-
  d4 %&amp;gt;% 
  mutate(Y_ord = ordered(Y))

# fit the model
fit22.6 &amp;lt;-
  brm(data = d4, 
      family = sratio,
      Y_ord ~ 1 + cs(X1) + cs(X2),
      prior = c(prior(normal(0, 20), class = Intercept),
                prior(normal(0, 20), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      file = &amp;quot;fits/fit22.06&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: sratio 
##   Links: mu = logit; disc = identity 
## Formula: Y_ord ~ 1 + cs(X1) + cs(X2) 
##    Data: d4 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -6.03      0.76    -7.66    -4.67 1.00     2040     2169
## Intercept[2]    -5.40      0.68    -6.85    -4.18 1.00     1757     2104
## Intercept[3]    -1.02      0.24    -1.50    -0.59 1.00     2666     2522
## X1[1]            2.71      0.47     1.85     3.70 1.00     2251     2266
## X1[2]            5.56      0.70     4.30     7.04 1.00     1900     2424
## X1[3]           -3.03      0.39    -3.84    -2.32 1.00     2565     2447
## X2[1]            2.38      0.43     1.60     3.26 1.00     2525     2449
## X2[2]           -1.16      0.29    -1.75    -0.63 1.00     3524     2736
## X2[3]           -3.13      0.37    -3.89    -2.46 1.00     2718     2738
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look at the parameter summary, nothing obviously bad happened. The computer didn’t crash or anything. To get a better sense of the damage, we plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws for fit22.6
post &amp;lt;- posterior_samples(fit22.6)

# 2D thresholds on the left
set.seed(22)

p1 &amp;lt;- 
  post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  # still using the adjusted formula for the thresholds
  mutate(intercept = Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)

# marginal posteriors on the right
p2 &amp;lt;-
post %&amp;gt;% 
  pivot_longer(cols = c(-disc, -lp__)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;)%&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;% 
  mutate(lambda    = case_when(lambda == &amp;quot;1&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;,
                               lambda == &amp;quot;2&amp;quot; ~ &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;,
                               lambda == &amp;quot;3&amp;quot; ~ &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;),
         parameter = case_when(parameter == &amp;quot;Intercept&amp;quot; ~ &amp;quot;beta[0]&amp;quot;,
                               parameter == &amp;quot;X1&amp;quot;        ~ &amp;quot;beta[1]&amp;quot;,
                               parameter == &amp;quot;X2&amp;quot;        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95, size = 1,
                    normalize = &amp;quot;panels&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)

# combine, entitle, and display the results
(p1 + p2) &amp;amp; 
  plot_layout(widths = c(1, 2)) &amp;amp;
  plot_annotation(title = &amp;quot;Figure 22.7, lower half&amp;quot;,
                  subtitle = &amp;quot;Results from the conditional logistic model fit to the d4 data via the sequential-ordinal approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We ended up with our version of the lower half of Figure 22.7. As with the previous model, the sequential-ordinal approach reverses the signs for the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; parameters, which isn’t a big deal as long as you keep that in mind. The larger issue is that the thresholds displayed in the left panel do a poor job differentiating among the various &lt;code&gt;Y&lt;/code&gt; categories. The model underlying those thresholds is a bad match for the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-wrap-up.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Conditional logistic wrap-up.&lt;/h5&gt;
&lt;p&gt;To wrap this section up, we walked through approaches for fitting conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. First we considered Singmann’s method for using the &lt;strong&gt;brms&lt;/strong&gt; custom family functionality to define bespoke likelihood functions. Though it requires a lot of custom coding and an above-average knowledge of the inner workings of &lt;strong&gt;brms&lt;/strong&gt; and Stan, the custom family approach is very general and will possibly work for all your conditional-logistic needs. Then we considered Ben-Shachar sequential-ordinal approach. Ben-Shachar’s insight was that if we are willing to augment the nominal data with the &lt;code&gt;ordered()&lt;/code&gt; function, modeling them with a sequential-ordinal model via &lt;code&gt;family = sratio&lt;/code&gt; will return near equivalent results to the conditional logistic method. Though this method is attractive in that it uses a built-in likelihood and thus avoids a lot of custom coding, it is limited in that it will only handle nominal data which are well described by the simple binary-divisions hierarchy displayed in the left panel of Figure 22.2.&lt;/p&gt;
&lt;p&gt;In closing, I would like to thank Singmann and Ben-Shachar for their time and insights. 🍻 I could not have finished this section without them. If you would like more examples of both of their methods applied to different data sets, check out the Stan forum thread called &lt;a href=&#34;https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433&#34;&gt;Nominal data and Kruschke’s “conditional logistic” approach&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.1 (2021-08-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bayesplot_1.8.1      posterior_1.1.0.9000 patchwork_1.1.1     
##  [4] tidybayes_3.0.1      brms_2.16.2          Rcpp_1.0.7          
##  [7] forcats_0.5.1        stringr_1.4.0        dplyr_1.0.7         
## [10] purrr_0.3.4          readr_2.0.1          tidyr_1.1.3         
## [13] tibble_3.1.6         ggplot2_3.3.5        tidyverse_1.3.1     
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.3.0      RcppEigen_0.3.3.9.1 
##   [4] plyr_1.8.6           igraph_1.2.6         svUnit_1.0.6        
##   [7] splines_4.1.1        crosstalk_1.1.1      TH.data_1.0-10      
##  [10] rstantools_2.1.1     inline_0.3.19        digest_0.6.28       
##  [13] htmltools_0.5.2      rsconnect_0.8.24     fansi_0.5.0         
##  [16] BH_1.75.0-0          magrittr_2.0.1       checkmate_2.0.0     
##  [19] tzdb_0.1.2           modelr_0.1.8         RcppParallel_5.1.4  
##  [22] matrixStats_0.61.0   vroom_1.5.4          xts_0.12.1          
##  [25] sandwich_3.0-1       prettyunits_1.1.1    colorspace_2.0-2    
##  [28] rvest_1.0.1          ggdist_3.0.0         haven_2.4.3         
##  [31] xfun_0.25            callr_3.7.0          crayon_1.4.2        
##  [34] jsonlite_1.7.2       lme4_1.1-27.1        survival_3.2-11     
##  [37] zoo_1.8-9            glue_1.5.0           gtable_0.3.0        
##  [40] emmeans_1.6.3        V8_3.4.2             distributional_0.2.2
##  [43] pkgbuild_1.2.0       rstan_2.26.3         abind_1.4-5         
##  [46] scales_1.1.1         mvtnorm_1.1-2        emo_0.0.0.9000      
##  [49] DBI_1.1.1            miniUI_0.1.1.1       viridisLite_0.4.0   
##  [52] xtable_1.8-4         HDInterval_0.2.2     diffobj_0.3.4       
##  [55] bit_4.0.4            stats4_4.1.1         StanHeaders_2.26.3  
##  [58] DT_0.19              htmlwidgets_1.5.3    httr_1.4.2          
##  [61] threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.2      
##  [64] pkgconfig_2.0.3      loo_2.4.1            farver_2.1.0        
##  [67] sass_0.4.0           dbplyr_2.1.1         utf8_1.2.2          
##  [70] labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.12        
##  [73] reshape2_1.4.4       later_1.3.0          munsell_0.5.0       
##  [76] cellranger_1.1.0     tools_4.1.1          cli_3.1.0           
##  [79] generics_0.1.1       broom_0.7.9          ggridges_0.5.3      
##  [82] evaluate_0.14        fastmap_1.1.0        yaml_2.2.1          
##  [85] bit64_4.0.5          processx_3.5.2       knitr_1.33          
##  [88] fs_1.5.0             nlme_3.1-152         mime_0.11           
##  [91] projpred_2.0.2       xml2_1.3.2           compiler_4.1.1      
##  [94] shinythemes_1.2.0    rstudioapi_0.13      gamm4_0.2-6         
##  [97] curl_4.3.2           reprex_2.0.1         bslib_0.3.0         
## [100] stringi_1.7.4        highr_0.9            ps_1.6.0            
## [103] blogdown_1.5         Brobdingnag_1.2-6    lattice_0.20-44     
## [106] Matrix_1.3-4         nloptr_1.2.2.2       markdown_1.1        
## [109] shinyjs_2.0.0        tensorA_0.36.2       vctrs_0.3.8         
## [112] pillar_1.6.4         lifecycle_1.0.1      jquerylib_0.1.4     
## [115] bridgesampling_1.1-2 estimability_1.3     httpuv_1.6.2        
## [118] extraDistr_1.9.1     R6_2.5.1             bookdown_0.23       
## [121] promises_1.2.0.1     gridExtra_2.3        codetools_0.2-18    
## [124] boot_1.3-28          colourpicker_1.1.0   MASS_7.3-54         
## [127] gtools_3.9.2         assertthat_0.2.1     withr_2.4.2         
## [130] shinystan_2.5.0      multcomp_1.4-17      mgcv_1.8-36         
## [133] parallel_4.1.1       hms_1.1.0            grid_4.1.1          
## [136] coda_0.19-4          minqa_1.2.4          rmarkdown_2.10      
## [139] shiny_1.6.0          lubridate_1.7.10     base64enc_0.1-3     
## [142] dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-Bürkner2021Define&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Define custom response distributions with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerOrdinalRegressionModels2019&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C., &amp;amp; Vuorre, M. (2019). Ordinal regression models in psychology: &lt;span&gt;A&lt;/span&gt; tutorial. &lt;em&gt;Advances in Methods and Practices in Psychological Science&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;(1), 77–101. &lt;a href=&#34;https://doi.org/10.1177/2515245918823199&#34;&gt;https://doi.org/10.1177/2515245918823199&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>One-step Bayesian imputation when you have dropout in your RCT</title>
      <link>/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/</link>
      <pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/</guid>
      <description>
&lt;script src=&#34;/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble&lt;/h2&gt;
&lt;p&gt;Suppose you’ve got data from a randomized controlled trial (RCT) where participants received either treatment or control. Further suppose you only collected data at two time points, pre- and post-treatment. Even in the best of scenarios, you’ll probably have some dropout in those post-treatment data. To get the full benefit of your data, you can use one-step Bayesian imputation when you compute your effect sizes. In this post, I’ll show you how.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;For this post, I’m presuming you have a passing familiarity with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should be familiar with effect sizes, particularly with standardized mean differences. If you need to brush up, consider Cohen’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988&lt;/a&gt;)&lt;/span&gt; authoritative text, or Cummings newer &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; text. For nice conceptual overview, I also recommend Kelley and Preacher’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kelley2012effect&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; paper, &lt;a href=&#34;https://www3.nd.edu/~kkelley/publications/articles/Kelley_and_Preacher_Psychological_Methods_2012.pdf&#34;&gt;&lt;em&gt;On effect size&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You should be familiar with Bayesian regression. For thorough introductions, I recommend either edition of McElreath’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text; or Gelman, Hill, and Vehtari’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; text. If you go with McElreath, he has a fine series of freely-available lectures &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Though we won’t be diving deep into it, here, you’ll want to have some familiarity with contemporary missing data theory. You can find brief overviews in the texts by McElreath and Gelman et al, above. For a deeper dive, I recommend &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-enders2010applied&#34; role=&#34;doc-biblioref&#34;&gt;Enders&lt;/a&gt; (&lt;a href=&#34;#ref-enders2010applied&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-little2019statistical&#34; role=&#34;doc-biblioref&#34;&gt;Little &amp;amp; Rubin&lt;/a&gt; (&lt;a href=&#34;#ref-little2019statistical&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. Also, heads up: &lt;a href=&#34;https://twitter.com/AmandaKMontoya/status/1341936335301406722&#34;&gt;word on the street&lt;/a&gt; is Enders is working on a second edition of his book.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;. Data wrangling and plotting were done with help from the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt; and &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;. The data were simulated with help from the &lt;a href=&#34;https://github.com/debruine/faux&#34;&gt;&lt;strong&gt;faux&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-faux&#34; role=&#34;doc-biblioref&#34;&gt;DeBruine, 2021&lt;/a&gt;)&lt;/span&gt; and the Bayesian models were fit using &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we load our primary &lt;strong&gt;R&lt;/strong&gt; packages and adjust the global plotting theme defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(faux)
library(tidybayes)
library(brms)

# adjust the global plotting theme
theme_set(
  theme_tidybayes() +
    theme(axis.line.x = element_blank(),
          axis.line.y = element_blank(),
          panel.border = element_rect(color = &amp;quot;grey85&amp;quot;, size = 1, fill = NA))
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;For this post, we’ll be simulating our data with help from the handy &lt;code&gt;faux::rnorm_multi()&lt;/code&gt; function. To start out, we’ll make two data sets, one for treatment (&lt;code&gt;d_treatment&lt;/code&gt;) and one for control (&lt;code&gt;d_control&lt;/code&gt;). Each will contain outcomes at &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; treatment, with the population parameters for both conditions at &lt;code&gt;pre&lt;/code&gt; being &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(5, 1)\)&lt;/span&gt;. Whereas those parameters stay the same at &lt;code&gt;post&lt;/code&gt; for those in the control condition, the population parameters for those in the treatment condition will raise at &lt;code&gt;post&lt;/code&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(5.7, 1)\)&lt;/span&gt;. Notice that not only did their mean value increase, but their standard deviation increased a bit, too, which is not uncommon in treatment data. Importantly, the correlation between &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; is &lt;span class=&#34;math inline&#34;&gt;\(.75\)&lt;/span&gt; for both conditions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many per group?
n &amp;lt;- 100

set.seed(1)

d_treatment &amp;lt;- rnorm_multi(
  n = n,
  mu = c(5, 5.7),
  sd = c(1, 1.1), 
  r = .75, 
  varnames = list(&amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;)
)

d_control &amp;lt;- rnorm_multi(
  n = n,
  mu = c(5, 5),
  sd = c(1, 1), 
  r = .75, 
  varnames = list(&amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we combine the two data sets and make an explicit &lt;code&gt;tx&lt;/code&gt; variable to distinguish the conditions. Then we simulate missingness in the &lt;code&gt;post&lt;/code&gt; variable in two steps: We use the &lt;code&gt;rbinom()&lt;/code&gt; function to simulate whether a case will be missing and then use a little &lt;code&gt;ifelse()&lt;/code&gt; to make a &lt;code&gt;post_observed&lt;/code&gt; variable that is the same as &lt;code&gt;post&lt;/code&gt; except that the vales are missing in every row for which &lt;code&gt;missing == 1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

d &amp;lt;- bind_rows(
  d_control,
  d_treatment
) %&amp;gt;% 
  mutate(tx = rep(c(&amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;), each = n)) %&amp;gt;% 
  mutate(tx = factor(tx, levels = c(&amp;quot;treatment&amp;quot;, &amp;quot;control&amp;quot;))) %&amp;gt;% 
  mutate(missing = rbinom(n(), size = 1, prob = 0.3)) %&amp;gt;%
  mutate(post_observed = ifelse(missing == 1, NA, post))

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        pre     post      tx missing post_observed
## 1 5.066999 5.698922 control       0      5.698922
## 2 6.950072 6.209520 control       0      6.209520
## 3 5.787145 7.181091 control       0      7.181091
## 4 4.826099 4.554830 control       1            NA
## 5 2.277529 3.447187 control       0      3.447187
## 6 6.801701 7.870996 control       1            NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get a sense for the data, here’s a scatter plot of &lt;code&gt;pre&lt;/code&gt; versus &lt;code&gt;post&lt;/code&gt;, by &lt;code&gt;tx&lt;/code&gt; and &lt;code&gt;missing&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  mutate(missing = factor(missing,
                          levels = 1:0,
                          labels = c(&amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = pre, y = post, color = missing, shape = missing)) +
  geom_point() +
  scale_color_viridis_d(option = &amp;quot;D&amp;quot;, begin = .35, end = .75, direction = -1) +
  scale_shape_manual(values = 17:16) +
  coord_equal(xlim = c(1.5, 8.5),
              ylim = c(1.5, 8.5)) +
  facet_wrap(~ tx)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/index_files/figure-html/geom_point-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that high correlation between &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; in the shapes of the data clouds. To look at the data in another way, here are a few summary statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;p&amp;quot;), names_to = &amp;quot;time&amp;quot;) %&amp;gt;% 
  mutate(time = factor(time, levels = c(&amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;, &amp;quot;post_observed&amp;quot;))) %&amp;gt;% 
  group_by(tx, time) %&amp;gt;% 
  summarise(mean = mean(value, na.rm = T),
            sd = sd(value, na.rm = T),
            min = min(value, na.rm = T),
            max = max(value, na.rm = T)) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
## # Groups:   tx [2]
##   tx        time           mean    sd   min   max
##   &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 treatment pre            5.11  0.91  3.22  7.24
## 2 treatment post           5.8   0.99  3.16  8.35
## 3 treatment post_observed  5.74  1.05  3.16  8.35
## 4 control   pre            5.01  0.99  2.28  7.5 
## 5 control   post           5.05  1.06  1.74  7.87
## 6 control   post_observed  5.1   1.04  1.74  7.45&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistical models&lt;/h2&gt;
&lt;p&gt;We’ll be analyzing the RCT data in two ways. First, we’ll fit a model on the version of the data with no missingness in &lt;code&gt;post&lt;/code&gt;. That will be our benchmark. Then we’ll practice fitting the model with one-step Bayesian imputation and the &lt;code&gt;post_observed&lt;/code&gt; variable. Once we’ve fit and evaluated our models, we’ll then walk out how to compute the effect sizes.&lt;/p&gt;
&lt;div id=&#34;fit-the-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the models.&lt;/h3&gt;
&lt;p&gt;There are a lot of ways to analyze pre/post RCT data. To get a sense of the various strategies, see &lt;a href=&#34;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/models-for-longitudinal-experiments-pre-post-designs.html&#34;&gt;this chapter&lt;/a&gt; in Jeffrey Walker’s free &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-walkerElementsOfStatisticalModeling2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; text and my &lt;a href=&#34;https://solomonkurz.netlify.app/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/&#34;&gt;complimentary blog post&lt;/a&gt; on pre/post non-experimental data. In this post, we’ll be taking the multivariate approach where we simultaneously model &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; as bivariate normal, such that both the mean and standard deviation parameters for both vary depending on the experimental condition (&lt;code&gt;tx&lt;/code&gt;). Importantly, the correlation between &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; is captured in the correlation between the two residual standard deviation parameters.&lt;/p&gt;
&lt;p&gt;Here’s how to fit the model to the full data with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- brm(
  data = d,
  family = gaussian,
  bf(pre ~ 0 + tx, sigma ~ 0 + tx) +
    bf(post ~ 0 + tx, sigma ~ 0 + tx) +
    set_rescor(rescor = TRUE),
  prior = c(prior(normal(5, 1), class = b, resp = pre),
            prior(normal(5, 1), class = b, resp = post),
            prior(normal(log(1), 1), class = b, dpar = sigma, resp = pre),
            prior(normal(log(1), 1), class = b, dpar = sigma, resp = post),
            prior(lkj(2), class = rescor)),
  cores = 4,
  seed = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The priors in this post follow the weakly-regularizing approach McElreath advocated for in the second edition of this text. Also note that because we are allowing the residual standard deviation parameter to vary by &lt;code&gt;tx&lt;/code&gt;, the &lt;strong&gt;brms&lt;/strong&gt; default is to use the log link, which will become important for interpretation and post processing. Here’s the parameter summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = log
##          mu = identity; sigma = log 
## Formula: pre ~ 0 + tx 
##          sigma ~ 0 + tx
##          post ~ 0 + tx 
##          sigma ~ 0 + tx
##    Data: d (Number of observations: 200) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## pre_txtreatment            5.00      0.10     4.81     5.19 1.00     2802     3091
## pre_txcontrol              5.11      0.09     4.93     5.29 1.00     3300     2885
## sigma_pre_txtreatment     -0.03      0.06    -0.15     0.09 1.00     3700     3273
## sigma_pre_txcontrol       -0.08      0.07    -0.21     0.06 1.00     3536     3105
## post_txtreatment           5.74      0.11     5.52     5.96 1.00     3027     2940
## post_txcontrol             5.08      0.09     4.90     5.26 1.00     3305     2758
## sigma_post_txtreatment     0.14      0.06     0.02     0.27 1.00     3555     3230
## sigma_post_txcontrol      -0.08      0.07    -0.21     0.05 1.00     3607     2764
## 
## Residual Correlations: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(pre,post)     0.73      0.03     0.67     0.79 1.00     3475     3305
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After exponentiating the standard deviations, all the parameter summaries look close to the data generating values from our &lt;code&gt;faux::rnorm_multi()&lt;/code&gt; code. This, however, is all just a warm-up. Our goal was to use one-step Bayesian imputation for when we have missing data at post-intervention time point. From a syntax perspective, that involves a few minor changes to our &lt;code&gt;fit1&lt;/code&gt; code. First, we replace the &lt;code&gt;post&lt;/code&gt; variable with &lt;code&gt;post_observed&lt;/code&gt;, which had about &lt;span class=&#34;math inline&#34;&gt;\(30\%\)&lt;/span&gt; of the values missing. In a similar way, we have to adjust a few of the &lt;code&gt;resp&lt;/code&gt; arguments within the &lt;code&gt;prior()&lt;/code&gt; statements. Finally and most crucially, we have to include the &lt;code&gt;| mi()&lt;/code&gt; syntax when defining the linear model for &lt;code&gt;post_observed&lt;/code&gt;. Otherwise, &lt;strong&gt;brms&lt;/strong&gt; will simply drop all the cases with missingness on &lt;code&gt;post_observed&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here’s the code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- brm(
  data = d,
  family = gaussian,
  bf(pre ~ 0 + tx,
     sigma ~ 0 + tx) +
    # notice the changes in this line
    bf(post_observed | mi() ~ 0 + tx,
       sigma ~ 0 + tx) +
    set_rescor(rescor = TRUE),
  prior = c(prior(normal(5, 1), class = b, resp = pre),
            # notice the changes in the `resp` argument
            prior(normal(5, 1), class = b, resp = postobserved),
            prior(normal(log(1), 1), class = b, dpar = sigma, resp = pre),
            # notice the changes in the `resp` argument
            prior(normal(log(1), 1), class = b, dpar = sigma, resp = postobserved),
            prior(lkj(2), class = rescor)),
  cores = 4,
  seed = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of looking at the &lt;code&gt;print()&lt;/code&gt; output, it might be more informative if we compare the results of our two models in a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the parameter names
parameters &amp;lt;- c(
  &amp;quot;mu[treatment]^pre&amp;quot;, &amp;quot;mu[control]^pre&amp;quot;, &amp;quot;sigma[treatment]^pre&amp;quot;, &amp;quot;sigma[control]^pre&amp;quot;,
  &amp;quot;mu[treatment]^post&amp;quot;, &amp;quot;mu[control]^post&amp;quot;, &amp;quot;sigma[treatment]^post&amp;quot;, &amp;quot;sigma[control]^post&amp;quot;, 
  &amp;quot;rho&amp;quot;
  )

# define the deisred order for the parameter names
levels &amp;lt;- c(
  &amp;quot;mu[treatment]^pre&amp;quot;, &amp;quot;mu[control]^pre&amp;quot;, &amp;quot;mu[treatment]^post&amp;quot;, &amp;quot;mu[control]^post&amp;quot;, 
  &amp;quot;sigma[treatment]^pre&amp;quot;, &amp;quot;sigma[control]^pre&amp;quot;, &amp;quot;sigma[treatment]^post&amp;quot;, &amp;quot;sigma[control]^post&amp;quot;, 
  &amp;quot;rho&amp;quot;
  )

# combine the posterior summaries for the two models
rbind(
  posterior_summary(fit1)[1:9, -2],
  posterior_summary(fit2)[1:9, -2]
  ) %&amp;gt;% 
  # wrangle
  data.frame() %&amp;gt;% 
  mutate(data = rep(c(&amp;quot;complete data&amp;quot;, &amp;quot;30% missing&amp;quot;), each = n() / 2),
         par  = rep(parameters, times = 2)) %&amp;gt;% 
  mutate(par      = factor(par, levels = levels),
         Estimate = ifelse(str_detect(par, &amp;quot;sigma&amp;quot;), exp(Estimate), Estimate),
         Q2.5     = ifelse(str_detect(par, &amp;quot;sigma&amp;quot;), exp(Q2.5), Q2.5),
         Q97.5    = ifelse(str_detect(par, &amp;quot;sigma&amp;quot;), exp(Q97.5), Q97.5)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = data)) +
  geom_pointrange(fatten = 1.1) +
  labs(x = &amp;quot;marginal posterior&amp;quot;,
       y = NULL) +
  xlim(0, NA) +
  facet_wrap(~ par, labeller = label_parsed, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/index_files/figure-html/coefficient_plot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since the &lt;code&gt;post_observed&lt;/code&gt; data were missing completely at random (MCAR&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;), it should be no surprise the coefficients are nearly the same between the two models. This, however, will not always (ever?) be the case with your real-world RCT data. Friends, don’t let your friends drop cases or carry the last value forward. Use the &lt;strong&gt;brms&lt;/strong&gt; &lt;code&gt;mi()&lt;/code&gt; syntax, instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;effect-sizes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Effect sizes.&lt;/h3&gt;
&lt;p&gt;At this point, you may be wondering why I didn’t use the familiar dummy-variable approach in either of the models and you might be further wondering why I bothered to allow the standard deviation parameters to vary. One of the sub-goals of this post is to show how to compute the model output into standardized effect sizes. My go-to standardized effect size is good old Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, of which there are many variations. In the case of our pre/post RCT with two conditions, we actually have three &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;’s of interest:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the standardized mean difference for the treatment condition (which we hope is large),&lt;/li&gt;
&lt;li&gt;the standardized mean difference for the control condition (which we hope is near zero), and&lt;/li&gt;
&lt;li&gt;the difference in those first two standardized mean differences (which we also hope is large).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As with all standardized mean differences, it’s a big deal to choose a good value to standardize with. With data like ours, a good default choice is the pooled standard deviation between the two conditions at baseline, which we might define as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_p^\text{pre} = \sqrt{\frac{ \left (\sigma_\text{treatment}^\text{pre} \right )^2 + \left (\sigma_\text{control}^\text{pre} \right)^2}{2}},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the notation is admittedly a little verbose. My hope, however, is this notation will make it easier to map the terms onto the model parameters from above. Anyway, with our definition of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_p^\text{pre}\)&lt;/span&gt; in hand, we’re in good shape to define our three effect sizes of interest as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
d_\text{treatment} &amp;amp; = \frac{\mu_\text{treatment}^\text{post} - \mu_\text{treatment}^\text{pre}}{\sigma_p^\text{pre}}, \\
d_\text{control}   &amp;amp; = \frac{\mu_\text{control}^\text{post} - \mu_\text{control}^\text{pre}}{\sigma_p^\text{pre}}, \; \text{and} \\
d_\text{treatment - control} &amp;amp; = \frac{\left ( \mu_\text{treatment}^\text{post} - \mu_\text{treatment}^\text{pre} \right ) - \left ( \mu_\text{control}^\text{post} - \mu_\text{control}^\text{pre} \right )}{\sigma_p^\text{pre}} \\
&amp;amp; = \left (\frac{\mu_\text{treatment}^\text{post} - \mu_\text{treatment}^\text{pre}}{\sigma_p^\text{pre}} \right ) - \left ( \frac{\mu_\text{control}^\text{post} - \mu_\text{control}^\text{pre}}{\sigma_p^\text{pre}} \right ) \\
&amp;amp; = \left ( d_\text{treatment} \right ) - \left ( d_\text{control} \right ).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The reason we analyzed the RCT data with a bivariate model with varying means and standard deviations was because the parameter values from that model correspond directly with the various &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; terms in the equations for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_p^\text{pre}\)&lt;/span&gt; and our three &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;’s. This insight comes from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke&lt;/a&gt; (&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;, particularly Section 16.3. For a walk-through of that section with a &lt;strong&gt;brms&lt;/strong&gt; + &lt;strong&gt;tidyverse&lt;/strong&gt; workflow, see &lt;a href=&#34;https://bookdown.org/content/3686/metric-predicted-variable-on-one-or-two-groups.html#two-groups&#34;&gt;this section&lt;/a&gt; of my ebook translation of his text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2021&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;)&lt;/span&gt;. The approach we’re taking, here, is a direct bivariate generalization of the material in Kruschke’s text.&lt;/p&gt;
&lt;p&gt;Okay, here’s how to work with the posterior samples from our missing-data model, &lt;code&gt;fit2&lt;/code&gt;, to compute those effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(fit2) %&amp;gt;% 
  # exponentiate the log sd parameters
  mutate(`sigma[treatment]^pre` = exp(b_sigma_pre_txtreatment),
         `sigma[control]^pre`   = exp(b_sigma_pre_txcontrol)) %&amp;gt;% 
  # pooled standard deviation (at pre)
  mutate(`sigma[italic(p)]^pre` = sqrt((`sigma[treatment]^pre`^2 + `sigma[control]^pre`^2) / 2)) %&amp;gt;% 
  # within-condition pre/post effect sizes
  mutate(`italic(d)[treatment]` = (b_postobserved_txtreatment - b_pre_txtreatment) / `sigma[italic(p)]^pre`,
         `italic(d)[control]`   = (b_postobserved_txcontrol   - b_pre_txcontrol)   / `sigma[italic(p)]^pre`) %&amp;gt;%
  # between-condition effect size (i.e., difference in differences)
  mutate(`italic(d)[&amp;#39;treatment - control&amp;#39;]` = `italic(d)[treatment]` - `italic(d)[control]`) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now inspect the posteriors for our three &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;’s and the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_p^\text{pre}\)&lt;/span&gt; in a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels &amp;lt;- c(
  &amp;quot;sigma[italic(p)]^pre&amp;quot;, &amp;quot;italic(d)[&amp;#39;treatment - control&amp;#39;]&amp;quot;, 
  &amp;quot;italic(d)[control]&amp;quot;, &amp;quot;italic(d)[treatment]&amp;quot;
  )

post %&amp;gt;% 
  # wrangle
  pivot_longer(`sigma[italic(p)]^pre`:`italic(d)[&amp;#39;treatment - control&amp;#39;]`) %&amp;gt;% 
  mutate(parameter = factor(name, levels = levels)) %&amp;gt;% 
  
  # plot
ggplot(aes(x = value, y = parameter)) +
  stat_pointinterval(.width = .95, size = 1/2) +
  scale_y_discrete(labels = ggplot2:::parse_safe) +
  labs(x = &amp;quot;marginal posterior&amp;quot;,
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/index_files/figure-html/d_plot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you look back at the data-generating values from above, our effect sizes are about where we’d hope them to be.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-what-about-that-one-step-imputation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;But what about that one-step imputation?&lt;/h3&gt;
&lt;p&gt;From a practical standpoint, one-step Bayesian imputation is a lot like full-information maximum likelihood or multiple imputation–it’s a way to use all of your data that allows you to avoid the biases that come with older methods such as mean imputation or last observation carried forward. In short, one-step Bayesian imputation fits a joint model that expresses both the uncertainty in the model parameters and the uncertainty in the missing data. When we use MCMC methods, the uncertainty in our model parameters is expressed in the samples from the posterior. We worked with those with our &lt;code&gt;posterior_samples()&lt;/code&gt; code, above. In the same way, one-step Bayesian imputation with MCMC also gives us posterior samples for the missing data, too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit2) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of space, I’m not going to show the results of the code block, above. If you were to execute it yourself, you’d see there were a bunch of &lt;code&gt;Ymi_postobserved[i]&lt;/code&gt; columns. Those columns contain the posterior samples for the missing values. The &lt;code&gt;i&lt;/code&gt; part of their names indexes the row number from the original data which had the missing &lt;code&gt;post_observed&lt;/code&gt; value. Just like with the posterior samples of our parameters, we can examine the posterior samples for our missing data with plots, summaries, and so on. Here instead of using the &lt;code&gt;posterior_samples()&lt;/code&gt; output, we’ll use the &lt;code&gt;posterior_summary()&lt;/code&gt; function, instead. This will summarize each parameter and imputed value by its mean, standard deviation, and percentile-based 95% interval. After a little wrangling, we’ll display the results in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_summary(fit2) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  rownames_to_column(&amp;quot;par&amp;quot;) %&amp;gt;% 
  # isolate the imputed values
  filter(str_detect(par, &amp;quot;Ymi&amp;quot;)) %&amp;gt;% 
  mutate(row = str_extract(par, &amp;quot;\\d+&amp;quot;) %&amp;gt;% as.integer()) %&amp;gt;% 
  # join the original data
  left_join(
    d %&amp;gt;% mutate(row = 1:n()),
    by = &amp;quot;row&amp;quot;
  ) %&amp;gt;% 

  # plot!
  ggplot(aes(x = pre, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = tx)) +
  geom_pointrange(fatten = 1, size = 1/4) +
  scale_color_viridis_d(NULL, option = &amp;quot;F&amp;quot;, begin = .2, end = .6, direction = -1) +
  ylab(&amp;quot;post (imputed)&amp;quot;) +
  coord_equal(xlim = c(1.5, 8.5),
              ylim = c(1.5, 8.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We ended up with something of a mash-up of a scatter plot and a coefficient plot. The &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis shows the summaries for the imputed values, summarized by their posterior means (dots) and 95% intervals (vertical lines). In the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis, we’ve connected them with their original &lt;code&gt;pre&lt;/code&gt; values. Notice the strong correlation between the two axes. That’s the consequence of fitting a bivariate model where &lt;code&gt;pre&lt;/code&gt; has a residual correlation with &lt;code&gt;post_observed&lt;/code&gt;. That original data-generating value, recall, was &lt;span class=&#34;math inline&#34;&gt;\(.75\)&lt;/span&gt;. Here’s the summary of the residual correlation from &lt;code&gt;fit2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_summary(fit2)[&amp;quot;rescor__pre__postobserved&amp;quot;, ] %&amp;gt;% 
  round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
##      0.71      0.04      0.63      0.78&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using language perhaps more familiar to those from a structural equation modeling background, the &lt;code&gt;pre&lt;/code&gt; values acted like a missing data covariate for the missing &lt;code&gt;post_observed&lt;/code&gt; values. Had that residual correlation been lower, the relation in the two axes of our plot would have been less impressive, too. Anyway, the point is that one-step Bayesian imputation gives users a nice way to explore the missing data assumptions they’ve imposed in their models, which I think is pretty cool.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;would-you-like-more&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Would you like more?&lt;/h2&gt;
&lt;p&gt;To my knowledge, the introductory material on applied missing data analysis seems awash with full-information maximum likelihood and multiple imputation. One-step Bayesian imputation methods haven’t made it into the mainstream, yet. McElreath covered the one-step approach in both editions of his text and since the way he covered the material was quite different in the two editions, I really recommend you check out both &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;. My ebook translations of McElreath’s texts covered that material from a &lt;strong&gt;brms&lt;/strong&gt; + &lt;strong&gt;tidyverse&lt;/strong&gt; perspective &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2021&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2021&lt;/a&gt;, &lt;a href=&#34;#ref-kurzStatisticalRethinkingBrms2020&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt;. Otherwise, you should check out Bürkner’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021HandleMissingValues&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; vignette, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html&#34;&gt;&lt;em&gt;Handle missing values with brms&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are aware of any other applied text books covering one-step Bayesian imputation, please drop a comment on this tweet.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New blog up!&lt;a href=&#34;https://t.co/Drxd9jrdp1&#34;&gt;https://t.co/Drxd9jrdp1&lt;/a&gt;&lt;br&gt;&lt;br&gt;This time we explore how to handle missing data in a 2-timepoint RCT with the one-step Bayesian imputation approach. It’s slick and simple and another reason to love &lt;a href=&#34;https://twitter.com/hashtag/brms?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#brms&lt;/a&gt;.&lt;/p&gt;&amp;mdash; Solomon Kurz (@SolomonKurz) &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1420434272142434304?ref_src=twsrc%5Etfw&#34;&gt;July 28, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] brms_2.15.0     Rcpp_1.0.6      tidybayes_3.0.0 faux_1.0.0      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.6    
##  [8] purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.2    ggplot2_3.3.5   tidyverse_1.3.1
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6         svUnit_1.0.3        
##   [6] splines_4.0.4        crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17       
##  [11] digest_0.6.27        htmltools_0.5.1.1    rsconnect_0.8.16     fansi_0.4.2          checkmate_2.0.0     
##  [16] magrittr_2.0.1       modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1          
##  [21] sandwich_3.0-0       prettyunits_1.1.1    colorspace_2.0-0     rvest_1.0.1          ggdist_3.0.0        
##  [26] haven_2.3.1          xfun_0.23            callr_3.7.0          crayon_1.4.1         jsonlite_1.7.2      
##  [31] lme4_1.1-25          survival_3.2-10      zoo_1.8-8            glue_1.4.2           gtable_0.3.0        
##  [36] emmeans_1.5.2-1      V8_3.4.0             distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2        
##  [41] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [46] viridisLite_0.4.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [51] htmlwidgets_1.5.3    httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0   posterior_1.0.1     
##  [56] ellipsis_0.3.2       farver_2.1.0         pkgconfig_2.0.3      loo_2.4.1            sass_0.3.1          
##  [61] dbplyr_2.1.1         utf8_1.2.1           labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.11        
##  [66] reshape2_1.4.4       later_1.2.0          munsell_0.5.0        cellranger_1.1.0     tools_4.0.4         
##  [71] cli_3.0.1            generics_0.1.0       broom_0.7.6          ggridges_0.5.3       evaluate_0.14       
##  [76] fastmap_1.1.0        yaml_2.2.1           fs_1.5.0             processx_3.5.2       knitr_1.33          
##  [81] nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2           compiler_4.0.4      
##  [86] bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13      curl_4.3             gamm4_0.2-6         
##  [91] reprex_2.0.0         statmod_1.4.35       bslib_0.2.4          stringi_1.6.2        highr_0.9           
##  [96] ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2        
## [101] nloptr_1.2.2.2       markdown_1.1         tensorA_0.36.2       shinyjs_2.0.0        vctrs_0.3.8         
## [106] pillar_1.6.1         lifecycle_1.0.0      jquerylib_0.1.4      bridgesampling_1.0-0 estimability_1.3    
## [111] httpuv_1.6.0         R6_2.5.0             bookdown_0.22        promises_1.2.0.1     gridExtra_2.3       
## [116] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [121] assertthat_0.2.1     withr_2.4.2          shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33         
## [126] parallel_4.0.4       hms_1.1.0            grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [131] rmarkdown_2.8        shiny_1.6.0          lubridate_1.7.10     base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-Bürkner2021HandleMissingValues&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Handle missing values with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cohenStatisticalPowerAnalysis1988a&#34; class=&#34;csl-entry&#34;&gt;
Cohen, J. (1988). &lt;em&gt;Statistical power analysis for the behavioral sciences&lt;/em&gt;. &lt;span&gt;L. Erlbaum Associates&lt;/span&gt;. &lt;a href=&#34;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&#34;&gt;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummingUnderstandingTheNewStatistics2012&#34; class=&#34;csl-entry&#34;&gt;
Cumming, G. (2012). &lt;em&gt;Understanding the new statistics: &lt;span&gt;Effect&lt;/span&gt; sizes, confidence intervals, and meta-analysis&lt;/em&gt;. &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&#34;&gt;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-faux&#34; class=&#34;csl-entry&#34;&gt;
DeBruine, L. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;faux&lt;/span&gt;: &lt;span&gt;Simulation&lt;/span&gt; for factorial designs&lt;/em&gt; [Manual]. &lt;a href=&#34;https://github.com/debruine/faux&#34;&gt;https://github.com/debruine/faux&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-enders2010applied&#34; class=&#34;csl-entry&#34;&gt;
Enders, C. K. (2010). &lt;em&gt;Applied missing data analysis&lt;/em&gt;. &lt;span&gt;Guilford press&lt;/span&gt;. &lt;a href=&#34;http://www.appliedmissingdata.com/&#34;&gt;http://www.appliedmissingdata.com/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kelley2012effect&#34; class=&#34;csl-entry&#34;&gt;
Kelley, K., &amp;amp; Preacher, K. J. (2012). On effect size. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;17&lt;/em&gt;(2), 137. &lt;a href=&#34;https://doi.org/10.1037/a0028086&#34;&gt;https://doi.org/10.1037/a0028086&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2021&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2021). &lt;em&gt;Statistical rethinking with brms, &lt;span class=&#34;nocase&#34;&gt;ggplot2&lt;/span&gt;, and the tidyverse: &lt;span&gt;Second Edition&lt;/span&gt;&lt;/em&gt; (version 0.2.0). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzDoingBayesianData2021&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis in brms and the tidyverse&lt;/em&gt; (version 0.4.0). &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingBrms2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Statistical rethinking with brms, &lt;span class=&#34;nocase&#34;&gt;ggplot2&lt;/span&gt;, and the tidyverse&lt;/em&gt; (version 1.2.0). &lt;a href=&#34;https://doi.org/10.5281/zenodo.3693202&#34;&gt;https://doi.org/10.5281/zenodo.3693202&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34; class=&#34;csl-entry&#34;&gt;
Little, R. J., &amp;amp; Rubin, D. B. (2019). &lt;em&gt;Statistical analysis with missing data&lt;/em&gt; (third, Vol. 793). &lt;span&gt;John Wiley &amp;amp; Sons&lt;/span&gt;. &lt;a href=&#34;https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798&#34;&gt;https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-walkerElementsOfStatisticalModeling2018&#34; class=&#34;csl-entry&#34;&gt;
Walker, J. A. (2018). &lt;em&gt;Elements of statistical modeling for experimental biology&lt;/em&gt; (&#34;2020–11th–22&#34; ed.). &lt;a href=&#34;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/&#34;&gt;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;As has been noted by others &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020&lt;/a&gt;)&lt;/span&gt; missing-data jargon is generally awful. I’m so sorry you have to contend with acronyms like MCAR, MAR (missing at random) and MNAR (missing not at random), but that’s just the way it is. If you’re not sure about the difference between the three, do consider spending some time with one of the missing data texts I recommended, above.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Got overdispersion? Try observation-level random effects with the Poisson-lognormal mixture</title>
      <link>/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/</link>
      <pubDate>Mon, 12 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/</guid>
      <description>
&lt;script src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;what&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What?&lt;/h2&gt;
&lt;p&gt;One of &lt;a href=&#34;https://twitter.com/tjmahr&#34;&gt;Tristan Mahr&lt;/a&gt;’s recent Twitter threads almost broke my brain.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;wait when people talk about treating overdispersion by using random effects, they sometimes put a random intercept on each row?? &lt;a href=&#34;https://t.co/7NjG4uw3nz&#34;&gt;https://t.co/7NjG4uw3nz&lt;/a&gt; &lt;a href=&#34;https://t.co/fo8Ylcejqv&#34;&gt;pic.twitter.com/fo8Ylcejqv&lt;/a&gt;&lt;/p&gt;&amp;mdash; tj mahr (originally Doki Doki Panic in Japan) 🍍🍕 (@tjmahr) &lt;a href=&#34;https://twitter.com/tjmahr/status/1413186646783242242?ref_src=twsrc%5Etfw&#34;&gt;July 8, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;It turns out that you can use random effects on cross-sectional count data. Yes, that’s right. Each count gets its own random effect. Some people call this observation-level random effects and it can be a tricky way to handle overdispersion. The purpose of this post is to show how to do this and to try to make sense of what it even means.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;First, I should clarify a bit. Mahr’s initial post and much of the thread to follow primarily focused on counts within the context of binomial data. If you’ve ever read a book on the generalized linear model (GLM), you know that the two broad frameworks for modeling counts are as binomial or Poisson. The basic difference is if your counts are out of a known number of trials (e.g., I got 3 out of 5 questions correct in my pop quiz, last week&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;), the binomial is generally the way to go. However, if your counts aren’t out of a well-defined total (e.g., I drank 1497 cups of coffee&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, last year), the Poisson distribution offers a great way to think about your data. In this post, we’ll be focusing on Poisson-like counts.&lt;/p&gt;
&lt;p&gt;The Poisson distribution is named after the French mathematician &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e8/E._Marcellot_Siméon-Denis_Poisson_1804.jpg&#34;&gt;Siméon Denis Poisson&lt;/a&gt;, who lived and died about 200 years ago. Poisson’s distribution is valid for non-negative integers, which is basically what counts are. The distribution has just one parameter, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, which controls both its mean and variance and imposes the assumption that the mean of your counts is the same as the variance. On the one hand, this is great because it keeps things simple–parsimony and all. On the other hand, holding the mean and variance the same is a really restrictive assumption and it just doesn’t match up well with a lot of real-world data.&lt;/p&gt;
&lt;p&gt;This Poisson assumption that the mean equals the variance is sometimes called &lt;em&gt;equidispersion&lt;/em&gt;. Count data violate the equidispersion assumption when their variance is smaller than their mean (&lt;em&gt;underdispersion&lt;/em&gt;) or when their variance is larger than their mean (&lt;em&gt;overdispersion&lt;/em&gt;). In practice, overdispersion tends to crop up most often. Real-world count data are overdispersed so often that statisticians have had to come up with a mess of strategies to handle the problem. In the applied statistics that I’m familiar with, the two most common ways to handle overdispersed count data are with the negative-binomial model, or with random effects. We’ll briefly cover both.&lt;/p&gt;
&lt;div id=&#34;negative-binomial-counts.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Negative-binomial counts.&lt;/h3&gt;
&lt;p&gt;As its name implies, the negative-binomial model has a deep relationship with the binomial model. I’m not going to go into those details, but Hilbe covered them in his well-named &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hilbeNegativeBinomialRegression2011&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt; textbook, if you’re curious. Basically, the negative-binomial model adds a dispersion parameter to the Poisson. Different authors refer to it with different names. Hilbe, for example, called it both &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-Bürkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner&lt;/a&gt; (&lt;a href=&#34;#ref-Bürkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;2021b&lt;/a&gt;)&lt;/span&gt; and the &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-standevelopmentteamStanFunctionsReference2021&#34; role=&#34;doc-biblioref&#34;&gt;Stan Development Team&lt;/a&gt; (&lt;a href=&#34;#ref-standevelopmentteamStanFunctionsReference2021&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; both call it &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. By which ever name, the negative-binomial overdispersion parameter helps disentangle the mean from the variance in a set of counts. The way it does it is by re-expressing the count data as coming from a mixture where each count is from its own Poisson distribution with its own &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter. Importantly, the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;’s in this mixture of Poissons follow a gamma distribution, which is why the negative binomial is also sometimes referred to as a gamma-Poisson model. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;, for example, generally prefers to speak in terms of the gamma-Poisson.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poission-counts-with-random-intercepts.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poission counts with random intercepts.&lt;/h3&gt;
&lt;p&gt;Another way to handle overdispersion is to ask whether the data are grouped. In my field, this naturally occurs when you collect longitudinal data. My counts, over time, will differ form your counts, over time, and we accommodate that by adding a multilevel structure to the model. This, then, takes us to the generalized linear &lt;em&gt;mixed&lt;/em&gt; model (GLMM), which is covered in text books like &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-cameron2013regression&#34; role=&#34;doc-biblioref&#34;&gt;Cameron &amp;amp; Trivedi&lt;/a&gt; (&lt;a href=&#34;#ref-cameron2013regression&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gelmanDataAnalysisUsing2006&#34; role=&#34;doc-biblioref&#34;&gt;Gelman &amp;amp; Hill&lt;/a&gt; (&lt;a href=&#34;#ref-gelmanDataAnalysisUsing2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;. Say your data have &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; groups. With a simple random-intercept Poisson model, each group of counts gets its own &lt;span class=&#34;math inline&#34;&gt;\(\lambda_j\)&lt;/span&gt; parameter and the population of those &lt;span class=&#34;math inline&#34;&gt;\(\lambda_j\)&lt;/span&gt;’s is described in terms of a grand mean (an overall &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; intercept) and variation around that grand mean (typically a standard deviation or variance parameter). Thus, if your &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; data are counts from &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; cases clustered within &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; groups, the random-intercept Poisson model can be expressed as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ij} &amp;amp; \sim \operatorname{Poisson}(\lambda_{ij}) \\
\log(\lambda_{ij}) &amp;amp; = \beta_0 + \zeta_{0j} \\
\zeta_{0j} &amp;amp; \sim \operatorname{Normal}(0, \sigma_0)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the grand mean is &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, the group-specific deviations around the grand mean are the &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0j}\)&lt;/span&gt;’s, and the variation across those &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0j}\)&lt;/span&gt;’s is expressed by a standard-deviation parameter &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt;. Thus following the typical GLMM convention, we model the group-level deviations with the normal distribution. Also notice that whether we’re talking about single-level GLMs or multilevel GLMMs, we typically model &lt;span class=&#34;math inline&#34;&gt;\(\log \lambda\)&lt;/span&gt;, instead of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. This prevents the model from predicting negative counts. Keep this in mind.&lt;/p&gt;
&lt;p&gt;Anyway, the random-intercept Poisson model can go a long way for handling overdispersion when your data are grouped. It’s also possible to combine this approach with the last one and fit a negative-binomial model with a random intercept, too. Though I haven’t seen this used much in practice, you can even take a distributional model approach &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Distributional&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2021a&lt;/a&gt;)&lt;/span&gt; and set the negative-binomial dispersion parameter to random, too. That, for example, could look like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ij} &amp;amp; \sim \operatorname{Gamma-Poisson}(\lambda_{ij}, \phi_{ij}) \\
\log(\lambda_{ij}) &amp;amp; = \beta_0 + \zeta_{0j} \\
\log(\phi_{ij}) &amp;amp; = \gamma_0 + \zeta_{1j} \\
\zeta_{0j} &amp;amp; \sim \operatorname{Normal}(0, \sigma_0) \\
\zeta_{1j} &amp;amp; \sim \operatorname{Normal}(0, \sigma_1).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theres-a-third-option-the-poisson-lognormal.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;There’s a third option: The Poisson-lognormal.&lt;/h3&gt;
&lt;p&gt;Now a typical condition for a random-intercept model (whether using the Poison, the negative-binomial, or any other likelihood function) is that at least some of the &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; groups, if not most or all, contain two or more cases. For example, in a randomized controlled trial you might measure the outcome variable 3 or 5 or 10 times over the course of the trial. In a typical non-experimental experience-sampling study, you might get 10 or 50 or a few hundred measurements from each participant over the course of a few days, weeks, or months. Either way, we tend to have multiple &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;’s within each level of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt;. As it turns out, you don’t have to restrict yourself that way. With the observation-level random effects (OLRE) approach, each case (each level of &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;) gets its own random effect &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-harrison2014using&#34; role=&#34;doc-biblioref&#34;&gt;Harrison, 2014&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But why would you do that?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Think back to the conventional regression model where some variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is predicting some continuous variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. We can express the model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 x_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the residual variance not accounted for by &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is captured in &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Thus &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; can be seen as a residual-variance term. The conventional Poisson model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) &amp;amp; = \beta_0 + \beta_1 x_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;doesn’t have a residual-variance term. Rather, the variance in the data is deterministically controlled by the linear model on &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt;, which works great in the case of equidispersion, but fails when the data are overdispersed. Hence the negative-binomial and the random-intercept models. But what if we &lt;em&gt;could&lt;/em&gt; tack on a residual variance term? It might take on a form like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) &amp;amp; = \beta_0 + \beta_1 x_i + \epsilon_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; is the residual variation in &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; not captured by the deterministic part of the linear model for &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt;. Following the conventional regression model, we might make our lives simple and further presume &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i \sim \operatorname{Normal}(0, \sigma_\epsilon)\)&lt;/span&gt;. Though he didn’t use this style of notation, that’s basically the insight from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-bulmer1974OnFitting&#34; role=&#34;doc-biblioref&#34;&gt;Bulmer&lt;/a&gt; (&lt;a href=&#34;#ref-bulmer1974OnFitting&#34; role=&#34;doc-biblioref&#34;&gt;1974&lt;/a&gt;)&lt;/span&gt;. But rather than speak in terms of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; and residual variance, Bulmer proposed an alternative to the gamma-Poisson mixture and asked his audience to imagine each count in the data was from its own Poisson distribution with its own &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter, but that those &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameters were distributed according to the lognormal distribution. Now Bulmer had a substantive motivation for proposing the lognormal based on the species-abundance data and I’m not going to get into any of that. But the basic point was, if we can have a gamma-distributed mixture of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;’s, why not a lognormal mixture, instead?&lt;/p&gt;
&lt;p&gt;The trouble with Bulmer’s lognormal-mixture approach is it’s not readily available in most software packages. However, notice what happens when you specify an OLRE model with the Poisson likelihood:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) &amp;amp; = \beta_0 + \zeta_{0i} \\
\zeta_{0i} &amp;amp; \sim \operatorname{Normal}(0, \sigma_0).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0i}\)&lt;/span&gt; now looks a lot like the &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; term in a standard intercepts-only regression model. Further, since the linear model is defined for &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt;, that means the &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0i}\)&lt;/span&gt; terms will be log-normally distributed in the exponentiated &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)\)&lt;/span&gt; space. In essence, the OLRE-Poisson model is a way to hack your multilevel regression software to fit a Poisson-lognormal model for overdispersed counts.&lt;/p&gt;
&lt;p&gt;Now we have a sense of the theory, it’s time to fit some models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-example-salamander-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Empirical example: Salamander data&lt;/h2&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;As per usual, we’ll be working within &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;. We’ll be fitting our models with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and most of our data wrangling and plotting work will be done with aid from the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt; and friends–&lt;strong&gt;patchwork&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-patchwork&#34; role=&#34;doc-biblioref&#34;&gt;Pedersen, 2019&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;tidybayes&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;. We’ll take our data set from McElreath’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-rethinking&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt; &lt;strong&gt;rethinking&lt;/strong&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)
library(tidyverse)
library(tidybayes)
library(patchwork)

data(salamanders, package = &amp;quot;rethinking&amp;quot;)

glimpse(salamanders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 47
## Columns: 4
## $ SITE      &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…
## $ SALAMAN   &amp;lt;int&amp;gt; 13, 11, 11, 9, 8, 7, 6, 6, 5, 5, 4, 3, 3, 3, 3, 3, 2, 2, 2, …
## $ PCTCOVER  &amp;lt;int&amp;gt; 85, 86, 90, 88, 89, 83, 83, 91, 88, 90, 87, 83, 87, 89, 92, …
## $ FORESTAGE &amp;lt;int&amp;gt; 316, 88, 548, 64, 43, 368, 200, 71, 42, 551, 675, 217, 212, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are in the &lt;code&gt;salamanders&lt;/code&gt; data frame, which contains counts of salamanders from 47 locations in northern California &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-welsh1995habitat&#34; role=&#34;doc-biblioref&#34;&gt;Welsh Jr &amp;amp; Lind, 1995&lt;/a&gt;)&lt;/span&gt;. Our count variable is &lt;code&gt;SALAMAN&lt;/code&gt;. The location for each count is indexed by the &lt;code&gt;SITE&lt;/code&gt; column. You could use the other two variables as covariates, but we won’t be focusing on those in this post. Here’s what &lt;code&gt;SALAMAN&lt;/code&gt; looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# adjust the global plotting theme
theme_set(theme_classic())

salamanders %&amp;gt;% 
  ggplot(aes(x = SALAMAN)) +
  geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Those data look overdispersed. We can get a quick sense of the overdispersion with sample statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;salamanders %&amp;gt;% 
  summarise(mean = mean(SALAMAN),
            variance = var(SALAMAN)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       mean variance
## 1 2.468085 11.38483&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For small-&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; data, we shouldn’t expect the mean to be exactly the same as the variance in Poisson data. This big of a difference, though, suggests&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; overdispersion even with a modest &lt;span class=&#34;math inline&#34;&gt;\(N = 47\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the models.&lt;/h3&gt;
&lt;p&gt;We’ll fit three intercepts-only models. The first will be a conventional Poisson model and the second will be the negative binomial (a.k.a. the gamma-Poisson mixture). We’ll finish off with our Poisson-lognormal mixture via the OLRE technique. Since we’re working with Bayesian software, we’ll need priors. Though I’m not going to explain them in any detail, we’ll be using the weakly-regularizing approach advocated for in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here’s how to fit the models with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# conventional Poisson
fit1 &amp;lt;-
  brm(data = salamanders, 
      family = poisson,
      SALAMAN ~ 1,
      prior(normal(log(3), 0.5), class = Intercept),
      cores = 4, seed = 1)

# gamma-Poisson mixture
fit2 &amp;lt;-
  brm(data = salamanders, 
      family = negbinomial,
      SALAMAN ~ 1,
      prior = c(prior(normal(log(3), 0.5), class = Intercept),
                prior(gamma(0.01, 0.01), class = shape)),
      cores = 4, seed = 1)

# Poisson-lognormal mixture
fit3 &amp;lt;-
  brm(data = salamanders, 
      family = poisson,
      SALAMAN ~ 1 + (1 | SITE),
      prior = c(prior(normal(log(3), 0.5), class = Intercept),
                prior(exponential(1), class = sd)),
      cores = 4, seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-the-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluate the models.&lt;/h3&gt;
&lt;p&gt;Here’s a quick parameter summary for each of the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: SALAMAN ~ 1 
##    Data: salamanders (Number of observations: 47) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.91      0.09     0.73     1.08 1.00     1483     1967
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: negbinomial 
##   Links: mu = log; shape = identity 
## Formula: SALAMAN ~ 1 
##    Data: salamanders (Number of observations: 47) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.95      0.21     0.54     1.36 1.00     2907     2195
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## shape     0.58      0.18     0.30     1.01 1.00     3608     2640
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: SALAMAN ~ 1 + (1 | SITE) 
##    Data: salamanders (Number of observations: 47) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~SITE (Number of levels: 47) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.28      0.23     0.89     1.80 1.00     1117     1435
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.34      0.22    -0.12     0.75 1.00     1687     2533
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might use the &lt;code&gt;pp_check()&lt;/code&gt; function to get a graphic sense of how well each model fit the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;-
  pp_check(fit1, type = &amp;quot;bars&amp;quot;, nsample = 150, fatten = 1, size = 1/2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 15),
                  ylim = c(0, 26)) +
  labs(title = &amp;quot;fit1&amp;quot;,
       subtitle = &amp;quot;Conventional Poisson&amp;quot;)

p2 &amp;lt;-
  pp_check(fit2, type = &amp;quot;bars&amp;quot;, nsample = 150, fatten = 1, size = 1/2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 15),
                  ylim = c(0, 26)) +
  labs(title = &amp;quot;fit2&amp;quot;,
       subtitle = &amp;quot;Gamma-Poisson mixture&amp;quot;)

p3 &amp;lt;-
  pp_check(fit3, type = &amp;quot;bars&amp;quot;, nsample = 150, fatten = 1, size = 1/2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 15),
                  ylim = c(0, 26)) +
  labs(title = &amp;quot;fit3&amp;quot;,
       subtitle = &amp;quot;Poisson-lognormal mixture&amp;quot;)

p1 + p2 + p3 + plot_layout(guides = &amp;quot;collect&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The conventional Poisson model seems like a disaster. Both the gamma-Poisson and the Poisson-lognormal models seemed to capture the data much better. We also might want to compare the models with information criteria. Here we’ll use the LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- add_criterion(fit1, criterion = &amp;quot;loo&amp;quot;)
fit2 &amp;lt;- add_criterion(fit2, criterion = &amp;quot;loo&amp;quot;)
fit3 &amp;lt;- add_criterion(fit3, criterion = &amp;quot;loo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I first executed that code, I got the following warning message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Found 29 observations with a pareto_k &amp;gt; 0.7 in model ‘fit3.’ It is recommended to set ‘moment_match = TRUE’ in order to perform moment matching for problematic observations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To use the &lt;code&gt;moment_match = TRUE&lt;/code&gt; option within the &lt;code&gt;add_criterion()&lt;/code&gt; function, you have to specify &lt;code&gt;save_pars = save_pars(all = TRUE)&lt;/code&gt; within &lt;code&gt;brm()&lt;/code&gt; when fitting the model. Here’s how to do that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit the Poisson-lognormal mixture, again
fit3 &amp;lt;-
  brm(data = salamanders, 
      family = poisson,
      SALAMAN ~ 1 + (1 | SITE),
      prior = c(prior(normal(log(3), 0.5), class = Intercept),
                prior(exponential(1), class = sd)),
      cores = 4, seed = 1,
      # here&amp;#39;s the new part
      save_pars = save_pars(all = TRUE))

# add the LOO
fit3 &amp;lt;- add_criterion(
  fit3, criterion = &amp;quot;loo&amp;quot;, 
  # this part is new, too
  moment_match = TRUE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to compare the models with the LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(fit1, fit2, fit3, criterion = &amp;quot;loo&amp;quot;) %&amp;gt;% 
  print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic
## fit3    0.0       0.0   -89.8      6.6        20.2    1.4    179.6   13.2  
## fit2   -8.2       2.3   -98.0      8.3         1.6    0.2    196.0   16.6  
## fit1  -50.5      13.9  -140.4     17.4         4.3    1.2    280.7   34.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even after accounting for model complexity, the Poisson-lognormal model appears to be the best fit for the data. Next we consider how, exactly, does one interprets the parameters of the Poisson-lognormal model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-does-one-interpret-the-poisson-lognormal-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How does one interpret the Poisson-lognormal model?&lt;/h3&gt;
&lt;p&gt;A nice quality of both the conventional Poisson model and the gamma-Poisson model is the intercept for each corresponds directly with the mean of the original data, after exponentiation. The mean of the &lt;code&gt;SALAMAN&lt;/code&gt; variable, recall, was 2.5. Here are the summaries for their exponentiated intercepts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# conventional Poisson
fixef(fit1)[, -2] %&amp;gt;% exp() %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
##     2.48     2.06     2.94&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# gamma-Poisson
fixef(fit2)[, -2] %&amp;gt;% exp() %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
##     2.58     1.72     3.89&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both are really close to the sample mean. Here’s the exponentiated intercept for the Poisson-lognormal model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit3)[, -2] %&amp;gt;% exp() %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
##     1.40     0.89     2.12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow, that’s not even close! What gives? Well, keep in mind that with the OLRE Poisson-lognormal model, the intercept is the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; parameter for the lognormal distribution of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameters. In a similar way, the level-2 standard deviation (execute &lt;code&gt;posterior_summary(fit3)[&#34;sd_SITE__Intercept&#34;, ]&lt;/code&gt;) is the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameter for that lognormal distribution. Keeping things simple, for the moment, here’s what that lognormal distribution looks like if we take the posterior means for those parameters and insert them into the parameter arguments of the &lt;code&gt;dlnorm()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;-
  tibble(lambda = seq(from = 0, to = 13, length.out = 500)) %&amp;gt;% 
  mutate(d = dlnorm(lambda, 
                    meanlog = posterior_summary(fit3)[1, 1], 
                    sdlog = posterior_summary(fit3)[2, 1])) %&amp;gt;% 
  
  ggplot(aes(x = lambda, y = d)) +
  geom_area(fill = &amp;quot;grey50&amp;quot;) +
  scale_x_continuous(expression(lambda), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(&amp;quot;density&amp;quot;, breaks = NULL, 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 12),
                  ylim = c(0, 0.8)) +
  labs(title = &amp;quot;Population lognormal distribution&amp;quot;,
       subtitle = &amp;quot;The parameters are summarized by their posterior means.&amp;quot;)

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using just the posterior means for the parameters ignores the uncertainty in the distribution. To bring that into the plot, we’ll want to work with the posterior samples, themselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many posterior ddraws would you like?
n_draw &amp;lt;- 100

set.seed(1)

p2 &amp;lt;-
  posterior_samples(fit3) %&amp;gt;% 
  slice_sample(n = n_draw) %&amp;gt;% 
  transmute(iter  = 1:n(),
            mu    = b_Intercept,
            sigma = sd_SITE__Intercept) %&amp;gt;% 
  expand(nesting(iter, mu, sigma),
         lambda = seq(from = 0, to = 13, length.out = 500)) %&amp;gt;% 
  mutate(d = dlnorm(lambda, meanlog = mu, sdlog = sigma)) %&amp;gt;% 
  
  ggplot(aes(x = lambda, y = d, group = iter)) +
  geom_line(size = 1/6, alpha = 1/2) +
  scale_x_continuous(expression(lambda), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(&amp;quot;density&amp;quot;, breaks = NULL, 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 12),
                  ylim = c(0, 0.8)) +
  labs(title = &amp;quot;Population lognormal distribution&amp;quot;,
       subtitle = &amp;quot;The parameters are summarized by 100 posterior draws.&amp;quot;)

p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These, recall, are 100 credible lognormal distributions for the case-level &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameters, not for the data themselves. We’ll get to the data in a moment. Since we’re working with a multilevel model, we have posteriors for each of the case-level &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameters, too. Here they are in a dot plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;-
  coef(fit3)$SITE[, &amp;quot;Estimate&amp;quot;, &amp;quot;Intercept&amp;quot;] %&amp;gt;% 
  exp() %&amp;gt;% 
  data.frame() %&amp;gt;% 
  set_names(&amp;quot;lambda_i&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = lambda_i)) +
  geom_dots(fill = &amp;quot;grey50&amp;quot;, color = &amp;quot;grey50&amp;quot;) +
  scale_x_continuous(expression(lambda[italic(i)]), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(&amp;quot;normalized density&amp;quot;, breaks = NULL, 
                     expand = expansion(mult = c(0, 0.05))) +
  coord_cartesian(xlim = c(0, 12)) +
  labs(title = expression(&amp;quot;Dotplot of individual &amp;quot;*lambda[italic(i)]*&amp;quot; parameters&amp;quot;),
       subtitle = &amp;quot;The parameters are summarized by their posterior means.&amp;quot;)

p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To reduce visual complexity, we just plotted the &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameters by their posterior means. But that might be frustrating the way it ignores uncertainty. A different way to look at them might be a rank-ordered coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p4 &amp;lt;-
  coef(fit3)$SITE[, -2, &amp;quot;Intercept&amp;quot;] %&amp;gt;% 
  exp() %&amp;gt;% 
  data.frame() %&amp;gt;% 
  arrange(Estimate) %&amp;gt;% 
  mutate(rank = 1:n()) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +
  geom_pointrange(fatten = 1, size = 1/2) +
  scale_x_continuous(expression(lambda[italic(i)]), breaks = 0:6 * 2, 
                     expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(breaks = NULL, expand = c(0.02, 0.02)) +
  coord_cartesian(xlim = c(0, 12)) +
  labs(title = expression(&amp;quot;Ranked coefficient plot of individual &amp;quot;*lambda[italic(i)]*&amp;quot; parameters&amp;quot;),
       subtitle = &amp;quot;The parameters are summarized by their posterior means and 95% CIs.&amp;quot;)

p4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since each &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; parameter is based in the data from a single case, it’s no surprise that their 95% intervals are all on the wide side. Just for kicks, here are the last four subplots all shown together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 + p2 + p3 + p4 &amp;amp; 
  theme_classic(base_size = 8.25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this point, though, you may be wondering how this model, with all its lognormal &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; glory, can inform us about actual counts. You know, the kind of counts that allowed us to fit such a wacky model. We’ll want to work with the posterior draws for that, too. First we extract all of the posterior draws for the population parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;-
  posterior_samples(fit3) %&amp;gt;% 
  transmute(mu    = b_Intercept,
            sigma = sd_SITE__Intercept)

# what is this?
glimpse(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 2
## $ mu    &amp;lt;dbl&amp;gt; 0.16424013, 0.07509823, 0.26418683, 0.22422613, 0.36427113, -0.1…
## $ sigma &amp;lt;dbl&amp;gt; 1.4128424, 1.4236003, 1.2986505, 1.4523181, 1.2731463, 1.5761309…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next code block is a little chunky, so I’ll try to explain what we’re doing before we dive in. Our goal is to use the posterior draws to make a posterior predictive check, by hand. My reasoning is doing this kind of check by hand, rather than relying on &lt;code&gt;pp_check()&lt;/code&gt;, requires you to understand the guts of the model. In our check, we are going to compare the histogram of the original &lt;code&gt;SALAMAN&lt;/code&gt; counts with the histograms of a few data sets simulated from the model. So first, we need to decide how many simulations we want. Since I want a faceted plot of 12 histograms, that means we’ll need 11 simulations. We set that number with the opening &lt;code&gt;n_facet &amp;lt;- 12&lt;/code&gt; line. Next, we set our seed for reproducibility and took 11 random draws from the &lt;code&gt;post&lt;/code&gt; data frame. In the first &lt;code&gt;mutate()&lt;/code&gt; line, we added an iteration index. Then with the &lt;code&gt;purrr::map2()&lt;/code&gt; function, we drew 47 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values (47 was the original &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; in the &lt;code&gt;salamanders&lt;/code&gt; data) based on the lognormal distribution defined by the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; values from each iteration. After &lt;code&gt;unnest()&lt;/code&gt;-ing those results, we used &lt;code&gt;rpois()&lt;/code&gt; within the next &lt;code&gt;mutate()&lt;/code&gt; line to use those simulated &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values to simulate actual counts. The remaining lines clean up the data format a bit and tack on the original &lt;code&gt;salamanders&lt;/code&gt; data. Then we plot.&lt;/p&gt;
&lt;p&gt;Okay, here it is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many facets would you like?
n_facet &amp;lt;- 12

set.seed(1)

post %&amp;gt;% 
  # take 11 samples from the posterior iterations
  slice_sample(n = n_facet - 1) %&amp;gt;% 
  # take 47 random draws from each iteration
  mutate(iter   = 1:n(),
         lambda = map2(mu, sigma, ~ rlnorm(n = 47, meanlog = mu, sdlog = sigma))) %&amp;gt;% 
  unnest(lambda) %&amp;gt;% 
  # use the lambdas to generate the counts
  mutate(count = rpois(n(), lambda = lambda)) %&amp;gt;% 
  transmute(sample = str_c(&amp;quot;sample #&amp;quot;, iter),
            SALAMAN = count) %&amp;gt;% 
  # combine the original data
  bind_rows(
    salamanders %&amp;gt;% 
      select(SALAMAN) %&amp;gt;% 
      mutate(sample = &amp;quot;original data&amp;quot;)
  ) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = SALAMAN, fill = sample == &amp;quot;original data&amp;quot;)) +
  geom_bar() +
  scale_fill_viridis_d(option = &amp;quot;A&amp;quot;, begin = .15, end = .55, breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 30)) +
  labs(title = &amp;quot;Handmade posterior predictive check&amp;quot;) +
  facet_wrap(~sample) +
  theme(strip.background = element_rect(size = 0, fill = &amp;quot;grey92&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This, friends, is how you can use our intercepts-only Poisson-lognormal mixture model to simulate count data resembling the original count data. Data simulation is cool, but you might wonder how to compute the mean of the model-implied lognormal distribution. Recall that we can’t just exponentiate the model’s intercept. As it turns out, &lt;span class=&#34;math inline&#34;&gt;\(\exp \mu\)&lt;/span&gt; returns the &lt;strong&gt;median&lt;/strong&gt; for the lognormal distribution. The formula for the mean of the lognormal distribution is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{mean} = \exp \left ( \mu + \frac{\sigma^2}{2}\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So here’s how to work with the posterior draws to compute that value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  mutate(mean = exp(mu + sigma^2 / 2)) %&amp;gt;% 
  
  ggplot(aes(x = mean, y = 0)) +
  stat_halfeye(.width = c(.5, .95)) +
  geom_vline(xintercept = mean(salamanders$SALAMAN), 
             color = &amp;quot;purple4&amp;quot;, linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 10)) +
  xlab(expression(&amp;quot;mean of the lognormal &amp;quot;*lambda*&amp;quot; distribution&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For reference, we superimposed the mean of the &lt;code&gt;SALAMAN&lt;/code&gt; data with a dashed line.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrap-up&lt;/h2&gt;
&lt;p&gt;Okay, this is about as far as I’d like to go with this one. To be honest, the Poisson-lognormal mixture is a weird model and I’m not sure if it’s a good fit for the kind of data I tend to work with. But exposure to new options seems valuable and I’m content to low-key chew on this one for a while.&lt;/p&gt;
&lt;p&gt;If you’d like to learn more, do check out Bulmer’s original &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bulmer1974OnFitting&#34; role=&#34;doc-biblioref&#34;&gt;1974&lt;/a&gt;)&lt;/span&gt; paper and the more recent OLRE paper by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-harrison2014using&#34; role=&#34;doc-biblioref&#34;&gt;Harrison&lt;/a&gt; (&lt;a href=&#34;#ref-harrison2014using&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;. The great &lt;a href=&#34;https://twitter.com/bolkerb&#34;&gt;Ben Bolker&lt;/a&gt; wrote up a vignette (&lt;a href=&#34;https://glmm.wdfiles.com/local--files/examples/overdispersion.pdf&#34;&gt;here&lt;/a&gt;) on how to fit the OLRE Poisson-lognormal with &lt;strong&gt;lme4&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-lme4&#34; role=&#34;doc-biblioref&#34;&gt;Bates et al., 2015&lt;/a&gt;)&lt;/span&gt; and Michael Clark wrote up a very quick example of the model with &lt;strong&gt;brms&lt;/strong&gt; &lt;a href=&#34;https://m-clark.github.io/easy-bayes/posterior-predictive-checks.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy modeling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] patchwork_1.1.1 tidybayes_2.3.1 forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.6     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
##  [9] tibble_3.1.2    ggplot2_3.3.3   tidyverse_1.3.0 brms_2.15.0    
## [13] Rcpp_1.0.6     
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.3         splines_4.0.4       
##   [7] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0  
##  [19] xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [22] colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.23            callr_3.7.0         
##  [28] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2          
##  [34] gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2        
##  [40] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [43] DBI_1.1.0            miniUI_0.1.1.1       viridisLite_0.4.0   
##  [46] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.3    httr_1.4.2          
##  [52] threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.2      
##  [55] farver_2.1.0         pkgconfig_2.0.3      loo_2.4.1           
##  [58] sass_0.3.1           dbplyr_2.0.0         utf8_1.2.1          
##  [61] labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.11        
##  [64] reshape2_1.4.4       later_1.2.0          munsell_0.5.0       
##  [67] cellranger_1.1.0     tools_4.0.4          cli_2.5.0           
##  [70] generics_0.1.0       broom_0.7.6          ggridges_0.5.3      
##  [73] evaluate_0.14        fastmap_1.1.0        yaml_2.2.1          
##  [76] processx_3.5.2       knitr_1.33           fs_1.5.0            
##  [79] nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [82] xml2_1.3.2           rstudioapi_0.13      compiler_4.0.4      
##  [85] bayesplot_1.8.0      shinythemes_1.1.2    curl_4.3            
##  [88] gamm4_0.2-6          reprex_0.3.0         statmod_1.4.35      
##  [91] bslib_0.2.4          stringi_1.6.2        highr_0.9           
##  [94] ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6   
##  [97] lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
## [100] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.8         
## [103] pillar_1.6.1         lifecycle_1.0.0      jquerylib_0.1.4     
## [106] bridgesampling_1.0-0 estimability_1.3     httpuv_1.6.0        
## [109] R6_2.5.0             bookdown_0.22        promises_1.2.0.1    
## [112] gridExtra_2.3        codetools_0.2-18     boot_1.3-26         
## [115] colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [118] assertthat_0.2.1     withr_2.4.2          shinystan_2.5.0     
## [121] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [124] hms_0.5.3            grid_4.0.4           coda_0.19-4         
## [127] minqa_1.2.4          rmarkdown_2.8        shiny_1.6.0         
## [130] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-R-lme4&#34; class=&#34;csl-entry&#34;&gt;
Bates, D., Mächler, M., Bolker, B., &amp;amp; Walker, S. (2015). Fitting linear mixed-effects models using &lt;span class=&#34;nocase&#34;&gt;lme4&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;67&lt;/em&gt;(1), 1–48. &lt;a href=&#34;https://doi.org/10.18637/jss.v067.i01&#34;&gt;https://doi.org/10.18637/jss.v067.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bulmer1974OnFitting&#34; class=&#34;csl-entry&#34;&gt;
Bulmer, M. (1974). On fitting the &lt;span&gt;Poisson&lt;/span&gt; lognormal distribution to species-abundance data. &lt;em&gt;Biometrics&lt;/em&gt;, &lt;em&gt;30&lt;/em&gt;(1), 101–110. &lt;a href=&#34;https://doi.org/10.2307/2529621&#34;&gt;https://doi.org/10.2307/2529621&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Bürkner2021Distributional&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021a). &lt;em&gt;Estimating distributional models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Bürkner2021Parameterization&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021b). &lt;em&gt;Parameterization of response distributions in brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cameron2013regression&#34; class=&#34;csl-entry&#34;&gt;
Cameron, A. C., &amp;amp; Trivedi, P. K. (2013). &lt;em&gt;Regression analysis of count data&lt;/em&gt; (Second Edition). &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9781139013567&#34;&gt;https://doi.org/10.1017/CBO9781139013567&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanDataAnalysisUsing2006&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., &amp;amp; Hill, J. (2006). &lt;em&gt;Data analysis using regression and multilevel/hierarchical models&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9780511790942&#34;&gt;https://doi.org/10.1017/CBO9780511790942&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-harrison2014using&#34; class=&#34;csl-entry&#34;&gt;
Harrison, X. A. (2014). Using observation-level random effects to model overdispersion in count data in ecology and evolution. &lt;em&gt;PeerJ&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;, e616. &lt;a href=&#34;https://doi.org/10.7717/peerj.616&#34;&gt;https://doi.org/10.7717/peerj.616&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hilbeNegativeBinomialRegression2011&#34; class=&#34;csl-entry&#34;&gt;
Hilbe, J. M. (2011). &lt;em&gt;Negative binomial regression&lt;/em&gt; (Second Edition). &lt;a href=&#34;https://doi.org/10.1017/CBO9780511973420&#34;&gt;https://doi.org/10.1017/CBO9780511973420&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020a). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rethinking&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;rethinking&lt;/span&gt; &lt;span&gt;R&lt;/span&gt; package&lt;/em&gt;. &lt;a href=&#34;https://xcelab.net/rm/software/&#34;&gt;https://xcelab.net/rm/software/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-patchwork&#34; class=&#34;csl-entry&#34;&gt;
Pedersen, T. L. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;patchwork&lt;/span&gt;: &lt;span&gt;The&lt;/span&gt; composer of plots&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;https://CRAN.R-project.org/package=patchwork&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-standevelopmentteamStanFunctionsReference2021&#34; class=&#34;csl-entry&#34;&gt;
Stan Development Team. (2021). &lt;em&gt;Stan functions reference&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/docs/2_26/functions-reference/index.html&#34;&gt;https://mc-stan.org/docs/2_26/functions-reference/index.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-welsh1995habitat&#34; class=&#34;csl-entry&#34;&gt;
Welsh Jr, H. H., &amp;amp; Lind, A. J. (1995). Habitat correlates of the &lt;span&gt;Del Norte&lt;/span&gt; salamander, &lt;span&gt;Plethodon&lt;/span&gt; elongatus (&lt;span&gt;Caudata&lt;/span&gt;: &lt;span&gt;Plethodontidae&lt;/span&gt;), in northwestern &lt;span&gt;California&lt;/span&gt;. &lt;em&gt;Journal of Herpetology&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(2), 198–210. &lt;a href=&#34;https://doi.org/10.2307/1564557&#34;&gt;https://doi.org/10.2307/1564557&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;That’s a lie. There was no pop quiz, last week.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I’m making this number up, too, but it’s probably not far off. ☕ ☕ ☕&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;One could also, of course, express that model as &lt;span class=&#34;math inline&#34;&gt;\(y_i = \beta_0 + \beta_1 x_i + \epsilon_i\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i \sim \operatorname{Normal}(0, \sigma)\)&lt;/span&gt;. But come on. That’s weak sauce. For more on why, see page 84 in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;I say “suggests” because a simple Poisson model can be good enough IF you have a set of high-quality predictors which can “explain” all that extra-looking variability. We, however, will be fitting intercept-only models.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Make ICC plots for your brms IRT models</title>
      <link>/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/</guid>
      <description>
&lt;script src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;context&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Context&lt;/h2&gt;
&lt;p&gt;Someone recently posted a &lt;a href=&#34;https://discourse.mc-stan.org/t/item-characteristic-curves-and-item-information-curves-from-item-response-models/22964&#34;&gt;thread on the Stan forums&lt;/a&gt; asking how one might make item-characteristic curve (ICC) and item-information curve (IIC) plots for an item-response theory (IRT) model fit with &lt;strong&gt;brms&lt;/strong&gt;. People were slow to provide answers and I came up disappointingly empty handed after a quick web search. The purpose of this blog post is to show how one might make ICC and IIC plots for &lt;strong&gt;brms&lt;/strong&gt; IRT models using general-purpose data wrangling steps.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;This tutorial is for those with a passing familiarity with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You’ll want to be familiar with the &lt;strong&gt;brms&lt;/strong&gt; package &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt;. In addition to the references I just cited, you can find several helpful vignettes at &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;https://github.com/paul-buerkner/brms&lt;/a&gt;. I’ve also written a few ebooks highlighting &lt;strong&gt;brms&lt;/strong&gt;, which you can find at &lt;a href=&#34;https://solomonkurz.netlify.app/bookdown/&#34;&gt;https://solomonkurz.netlify.app/bookdown/&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll want to be familiar with Bayesian multilevel regression. In addition to the resources, above, I recommend either edition of McElreath’s introductory text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; or Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; introductory text.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll want to be familiar with IRT. The framework in this blog comes most directly from Bürkner’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; preprint. Though I’m not in a position to vouch for them myself, I’ve had people recommend the texts by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-crockerIntroductionToClassical2006&#34; role=&#34;doc-biblioref&#34;&gt;Crocker &amp;amp; Algina&lt;/a&gt; (&lt;a href=&#34;#ref-crockerIntroductionToClassical2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-deayalaTheoryAndPractice2008&#34; role=&#34;doc-biblioref&#34;&gt;De Ayala&lt;/a&gt; (&lt;a href=&#34;#ref-deayalaTheoryAndPractice2008&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-reckaseMultidimensionalIRT2009&#34; role=&#34;doc-biblioref&#34;&gt;Reckase&lt;/a&gt; (&lt;a href=&#34;#ref-reckaseMultidimensionalIRT2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-bonifayMultidimensionalIRT2019&#34; role=&#34;doc-biblioref&#34;&gt;Bonifay&lt;/a&gt; (&lt;a href=&#34;#ref-bonifayMultidimensionalIRT2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-albanoIntroductionToEducational2020&#34; role=&#34;doc-biblioref&#34;&gt;Albano&lt;/a&gt; (&lt;a href=&#34;#ref-albanoIntroductionToEducational2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. Probably the best place to learn about the &lt;strong&gt;tidyverse&lt;/strong&gt;-style of coding, as well as an introduction to &lt;strong&gt;R&lt;/strong&gt;, is Grolemund and Wickham’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-grolemundDataScience2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; freely-available online text, &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R for data science&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Load the primary &lt;strong&gt;R&lt;/strong&gt; packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for this post come from the preprint by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;Loram et al.&lt;/a&gt; (&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, who generously shared their data and code on &lt;a href=&#34;https://github.com/Lingtax/2018_measures_study&#34;&gt;GitHub&lt;/a&gt; and the &lt;a href=&#34;https://osf.io/t9w2x/&#34;&gt;Open Science Framework&lt;/a&gt;. In their paper, they used IRT to make a self-report measure of climate change denial. After pruning their initial item set, Loram and colleagues settled on eight binary items for their measure. Here we load the data for those items&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;data/ccdrefined02.rda&amp;quot;)

ccdrefined02 %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 206
## Columns: 8
## $ ccd05 &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0…
## $ ccd18 &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0…
## $ ccd11 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0…
## $ ccd13 &amp;lt;dbl&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0…
## $ ccd08 &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0…
## $ ccd06 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0…
## $ ccd09 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0…
## $ ccd16 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you walk through the code in Loram and colleagues’ &lt;a href=&#34;https://github.com/Lingtax/2018_measures_study/blob/master/Rcode/2018_Loram_CC_IRT.R&#34;&gt;&lt;code&gt;2018_Loram_CC_IRT.R&lt;/code&gt;&lt;/a&gt; file, you’ll see where this version of the data comes from. For our purposes, we’ll want to make an explicit participant number column and then convert the data to the long format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_long &amp;lt;- ccdrefined02 %&amp;gt;% 
  mutate(id = 1:n()) %&amp;gt;% 
  pivot_longer(-id, names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;y&amp;quot;) %&amp;gt;% 
  mutate(item = str_remove(item, &amp;quot;ccd&amp;quot;))

# what did we do?
head(dat_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##      id item      y
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 05        0
## 2     1 18        0
## 3     1 11        1
## 4     1 13        1
## 5     1 08        0
## 6     1 06        1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now responses (&lt;code&gt;y&lt;/code&gt;) are nested within participants (&lt;code&gt;id&lt;/code&gt;) and items (&lt;code&gt;item&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;irt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IRT&lt;/h2&gt;
&lt;p&gt;In his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; preprint, Bürkner outlined the framework for the multilevel Bayesian approach to IRT, as implemented in &lt;strong&gt;brms&lt;/strong&gt;. In short, IRT allows one to decompose the information from assessment measures into person parameters &lt;span class=&#34;math inline&#34;&gt;\((\theta)\)&lt;/span&gt; and item parameters &lt;span class=&#34;math inline&#34;&gt;\((\xi)\)&lt;/span&gt;. The IRT framework offers a large variety of model types. In this post, we’ll focus on the widely-used 1PL and 2PL models. First, we’ll briefly introduce them within the context of Bürkner’s multilevel Bayesian approach. Then we’ll fit those models to the &lt;code&gt;dat_long&lt;/code&gt; data. Finally, we’ll show how to explore those models using ICC and IIC plots.&lt;/p&gt;
&lt;div id=&#34;what-is-the-1pl&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is the 1PL?&lt;/h3&gt;
&lt;p&gt;With a set of binary data &lt;span class=&#34;math inline&#34;&gt;\(y_{pi}\)&lt;/span&gt;, which vary across &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; persons and &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; items, we can express the simple one-parameter logistic (1PL) model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \theta_p + \xi_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(p_{pi}\)&lt;/span&gt; parameter from the Bernoulli distribution indicates the probability of &lt;code&gt;1&lt;/code&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(p\text{th}\)&lt;/span&gt; person on the &lt;span class=&#34;math inline&#34;&gt;\(i\text{th}\)&lt;/span&gt; item. To constrain the model predictions to within the &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt; probability space, we use the logit link. Note that with this parameterization, the linear model itself is just the additive sum of the person parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and item parameter &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Within our multilevel Bayesian framework, we will expand this a bit to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \beta_0 + \theta_p + \xi_i \\
\theta_p &amp;amp; \sim \operatorname{Normal}(0, \sigma_\theta) \\
\xi_i    &amp;amp; \sim \operatorname{Normal}(0, \sigma_\xi),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the new parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the grand mean. Now our &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; parameters are expressed as deviations around the grand mean &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;. As is typical within the multilevel framework, we model these deviations as normally distributed with means set to zero and standard deviations (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\xi\)&lt;/span&gt;) estimated from the data&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To finish off our multilevel Bayesian version the 1PL, we just need to add in our priors. In this blog post, we’ll follow the weakly-regularizing approach and set&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\beta_0 &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\sigma_\theta &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\sigma_\xi    &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; superscripts indicate the Student-&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; priors for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters are restricted to non-negative values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-about-the-2pl&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How about the 2PL?&lt;/h3&gt;
&lt;p&gt;We can express the two-parameter logistic (2PL) model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \alpha_i \theta_p + \alpha_i \xi_i \\
                             &amp;amp; = \alpha_i(\theta_p + \xi_i),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; parameters are now both multiplied by the discrimination parameter &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; subscript indicates the discrimination parameter varies across the items, but not across persons. We should note that because we are now multiplying parameters, this makes the 2PL a non-liner model. Within our multilevel Bayesian framework, we might express the 2PL as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \alpha (\beta_0 + \theta_p + \xi_i) \\
\alpha &amp;amp; = \beta_1 + \alpha_i \\
\theta_p &amp;amp; \sim \operatorname{Normal}(0, \sigma_\theta) \\
\begin{bmatrix} \alpha_i \\ \xi_i \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal}(\mathbf 0, \mathbf \Sigma) \\
\Sigma    &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_\alpha &amp;amp; 0 \\ 0 &amp;amp; \sigma_\xi \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix} ,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; term is multiplied by &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, in addition to the &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; parameters. But note that &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is itself a composite of its own grand mean &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and the item-level deviations around it, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt;. Since both &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; vary across items, they are modeled as multivariate normal, with a mean vector of zeros and variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt;. As is typical with &lt;strong&gt;brms&lt;/strong&gt;, we will decompose &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; into a diagonal matrix of standard deviations &lt;span class=&#34;math inline&#34;&gt;\((\mathbf S)\)&lt;/span&gt; and a correlation matrix &lt;span class=&#34;math inline&#34;&gt;\((\mathbf R)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As Bürkner &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; discussed in Section 5, this particular model might have identification problems without strong priors. The issue is “a switch in the sign of &lt;span class=&#34;math inline&#34;&gt;\([\alpha]\)&lt;/span&gt; can be corrected for by a switch in the sign of &lt;span class=&#34;math inline&#34;&gt;\([(\beta_0 + \theta_p + \xi_i)]\)&lt;/span&gt; without a change in the overall likelihood.” One solution, then, would be to constrain &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; to be positive. We can do that with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \color{#8b0000}{ \exp(\log \alpha) } \color{#000000}{\times (\beta_0 + \theta_p + \xi_i)} \\
\color{#8b0000}{\log \alpha} &amp;amp; = \beta_1 + \alpha_i \\
\theta_p &amp;amp; \sim \operatorname{Normal}(0, \sigma_\theta) \\
\begin{bmatrix} \alpha_i \\ \xi_i \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal}(\mathbf 0, \mathbf \Sigma) \\
\Sigma    &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_\alpha &amp;amp; 0 \\ 0 &amp;amp; \sigma_\xi \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;wherein we are now modeling &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; on the log scale and then exponentiating &lt;span class=&#34;math inline&#34;&gt;\(\log \alpha\)&lt;/span&gt; within the linear formula for &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit}(p_{pi})\)&lt;/span&gt;. Continuing on with our weakly-regularizing approach, we will express our priors for this model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\beta_0 &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\beta_1 &amp;amp; \sim \operatorname{Normal}(0, 1) \\
\sigma_\theta &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\sigma_\alpha &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\sigma_\xi    &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\mathbf R &amp;amp; \sim \operatorname{LKJ}(2),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where LKJ is the Lewandowski, Kurowicka, and Joe prior for correlation matrices &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lewandowski2009generating&#34; role=&#34;doc-biblioref&#34;&gt;Lewandowski et al., 2009&lt;/a&gt;)&lt;/span&gt;. With &lt;span class=&#34;math inline&#34;&gt;\(\eta = 2\)&lt;/span&gt;, the LKJ weakly regularizes the correlations away from extreme values&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fire-up-brms.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fire up &lt;strong&gt;brms&lt;/strong&gt;.&lt;/h3&gt;
&lt;p&gt;With &lt;code&gt;brms::brm()&lt;/code&gt;, we can fit our 1PL model with conventional multilevel syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;irt1 &amp;lt;- brm(
  data = dat_long,
  family = brmsfamily(&amp;quot;bernoulli&amp;quot;, &amp;quot;logit&amp;quot;),
  y ~ 1 + (1 | item) + (1 | id),
  prior = c(prior(normal(0, 1.5), class = Intercept),
            prior(student_t(10, 0, 1), class = sd)),
  cores = 4, seed = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our non-linear 2PL model, however, will require the &lt;strong&gt;brms&lt;/strong&gt; non-linear syntax &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Non_linear&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2021&lt;/a&gt;)&lt;/span&gt;. Here we’ll follow the same basic configuration Bürkner used in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; IRT preprint.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;irt2 &amp;lt;- brm(
  data = dat_long,
  family = brmsfamily(&amp;quot;bernoulli&amp;quot;, &amp;quot;logit&amp;quot;),
  bf(
    y ~ exp(logalpha) * eta,
    eta ~ 1 + (1 |i| item) + (1 | id),
    logalpha ~ 1 + (1 |i| item),
    nl = TRUE
  ),
  prior = c(prior(normal(0, 1.5), class = b, nlpar = eta),
            prior(normal(0, 1), class = b, nlpar = logalpha),
            prior(student_t(10, 0, 1), class = sd, nlpar = eta),
            prior(student_t(10, 0, 1), class = sd, nlpar = logalpha),
            prior(lkj(2), class = cor)),
  cores = 4, seed = 1,
  control = list(adapt_delta = .99)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that for &lt;code&gt;irt2&lt;/code&gt;, we had to adjust the &lt;code&gt;adapt_delta&lt;/code&gt; settings to stave off a few divergent transitions. Anyway, here are the parameter summaries for the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(irt1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: bernoulli 
##   Links: mu = logit 
## Formula: y ~ 1 + (1 | item) + (1 | id) 
##    Data: dat_long (Number of observations: 1648) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 206) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     3.81      0.38     3.12     4.61 1.01     1129     2107
## 
## ~item (Number of levels: 8) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.96      0.29     0.55     1.67 1.00     1815     2835
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -2.88      0.47    -3.82    -1.94 1.00     1308     2040
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(irt2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: bernoulli 
##   Links: mu = logit 
## Formula: y ~ exp(logalpha) * eta 
##          eta ~ 1 + (1 | i | item) + (1 | id)
##          logalpha ~ 1 + (1 | i | item)
##    Data: dat_long (Number of observations: 1648) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 206) 
##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(eta_Intercept)     1.78      0.69     0.68     3.39 1.00     1933     2321
## 
## ~item (Number of levels: 8) 
##                                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(eta_Intercept)                         0.50      0.25     0.16     1.16 1.00     1521     2181
## sd(logalpha_Intercept)                    0.36      0.16     0.13     0.74 1.00     1544     2075
## cor(eta_Intercept,logalpha_Intercept)     0.44      0.31    -0.26     0.90 1.00     2847     2922
## 
## Population-Level Effects: 
##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## eta_Intercept         -1.41      0.57    -2.71    -0.51 1.00     1700     2140
## logalpha_Intercept     0.91      0.42     0.16     1.82 1.00     1944     2282
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m not going to bother interpreting these results because, well, this isn’t a full-blown IRT tutorial. For our purposes, we’ll just note that the &lt;span class=&#34;math inline&#34;&gt;\(\widehat R\)&lt;/span&gt; and effective sample size values all look good and nothing seems off with the parameter summaries. They’re not shown here, but the trace plots look good, too. We’re on good footing to explore the models with our ICC and IIC plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iccs.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ICCs.&lt;/h3&gt;
&lt;p&gt;For IRT models of binary items, item-characteristic curves (ICCs) show the expected relation between one’s underlying “ability” and the probability of scoring 1 on a given item. In our models, above, each participant in the data had a their underlying ability estimated by way of the &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters. However, what we want, here, is is to specify the relevant part of the parameter space for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; without reference to any given participant. Since the the 1PL and 2PL models are fit with the logit link, this will mean entertaining &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values ranging within an interval like &lt;span class=&#34;math inline&#34;&gt;\([-4, 4]\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\([-6, 6]\)&lt;/span&gt;. This range will define our &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; axis. Since our &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis has to do with probabilities, it will range from 0 to 1. The trick is knowing how to work with the posterior draws to compute the relevant probability values for their corresponding &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values.&lt;/p&gt;
&lt;p&gt;We’ll start with our 1PL model, &lt;code&gt;irt1&lt;/code&gt;. First, we extract the posterior draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(irt1)

# what is this?
glimpse(post)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m not showing the output for &lt;code&gt;glimpse(post)&lt;/code&gt; because &lt;code&gt;post&lt;/code&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(4{,}000 \times 218\)&lt;/span&gt; data frame and all that output is just too much for a blog post. Here’s a more focused look at the primary columns of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 9
## $ b_Intercept            &amp;lt;dbl&amp;gt; -2.347027, -2.710615, -2.613853, -3.166867, -2.676470, -2.794820, -3.079290, …
## $ `r_item[05,Intercept]` &amp;lt;dbl&amp;gt; -1.6323003, -1.8058115, -1.8621189, -1.6090339, -2.5323344, -1.5353327, -1.43…
## $ `r_item[06,Intercept]` &amp;lt;dbl&amp;gt; 0.1559534455, -0.2575624215, 0.0149043141, 0.4396739597, -0.2644399951, 0.115…
## $ `r_item[08,Intercept]` &amp;lt;dbl&amp;gt; -0.4202201485, -0.3789631590, -0.5076833653, -0.1256742164, -0.9035688171, -0…
## $ `r_item[09,Intercept]` &amp;lt;dbl&amp;gt; 0.16016701, 0.04171247, 0.19869473, 0.92882887, 0.07287032, 0.41171578, 0.935…
## $ `r_item[11,Intercept]` &amp;lt;dbl&amp;gt; -0.80410817, -1.31470954, -0.98800140, -0.58681735, -1.75515227, -1.20933771,…
## $ `r_item[13,Intercept]` &amp;lt;dbl&amp;gt; -0.78569824, -0.68159922, -0.69046836, -0.33288882, -0.96657408, -0.01837113,…
## $ `r_item[16,Intercept]` &amp;lt;dbl&amp;gt; 0.2638089, 0.4226998, 0.1968664, 0.9465201, 0.1394158, 0.3445774, 1.0695805, …
## $ `r_item[18,Intercept]` &amp;lt;dbl&amp;gt; -1.1746881, -1.2400993, -1.4093647, -1.1124993, -1.8502372, -0.9874308, -1.10…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each of our 8 questionnaire items, we compute their conditional probability with the equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y = 1) = \operatorname{logit}^{-1}(\beta_0 + \xi_i + \theta),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit}^{-1}\)&lt;/span&gt; is the inverse logit function&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\exp(x)}{1 + \exp(x)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;brms&lt;/strong&gt;, we have access to the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit}^{-1}\)&lt;/span&gt; function by way of the convenience function called &lt;code&gt;inv_logit_scaled()&lt;/code&gt;. Before we put the &lt;code&gt;inv_logit_scaled()&lt;/code&gt; function to use, we’ll want to rearrange our &lt;code&gt;post&lt;/code&gt; samples into the long format so that all the &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; draws for each of the eight items are nested within a single column, which we’ll call &lt;code&gt;xi&lt;/code&gt;. We’ll index which draw corresponds to which of the eight items with a nominal &lt;code&gt;item&lt;/code&gt; column. And to make this all work within the context of 4,000 posterior draws, we’ll also need to make an iteration index, which we’ll call &lt;code&gt;iter&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;), names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;xi&amp;quot;) %&amp;gt;% 
  mutate(item = str_extract(item, &amp;quot;\\d+&amp;quot;)) 

# what is this?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   b_Intercept  iter item      xi
##         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
## 1       -2.35     1 05    -1.63 
## 2       -2.35     1 06     0.156
## 3       -2.35     1 08    -0.420
## 4       -2.35     1 09     0.160
## 5       -2.35     1 11    -0.804
## 6       -2.35     1 13    -0.786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to compute our probabilities, conditional in different ability &lt;span class=&#34;math inline&#34;&gt;\((\theta)\)&lt;/span&gt; levels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  expand(nesting(iter, b_Intercept, item, xi),
         theta = seq(from = -6, to = 6, length.out = 100)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(b_Intercept + xi + theta)) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(p = mean(p))

# what have we done?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
## # Groups:   theta [1]
##   theta item          p
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1    -6 05    0.0000364
## 2    -6 06    0.000241 
## 3    -6 08    0.000162 
## 4    -6 09    0.000300 
## 5    -6 11    0.0000797
## 6    -6 13    0.000121&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With those summaries in hand, it’s trivial to make the ICC plot with good old &lt;strong&gt;ggplot2&lt;/strong&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  ggplot(aes(x = theta, y = p, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;ICCs for the 1PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior mean.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = expression(italic(p)(y==1))) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since each item had a relatively low response probability, you have to go pretty far into the right-hand side of the &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; range before the curves start to approach the top of the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis.&lt;/p&gt;
&lt;p&gt;To make the ICCs for the 2PL model, the data wrangling will require a couple more steps. First, we extract the posterior draws and take a quick look at the columns of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(irt2) 

# what do we care about?
post %&amp;gt;% 
  select(b_eta_Intercept, b_logalpha_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 18
## $ b_eta_Intercept                  &amp;lt;dbl&amp;gt; -1.3855075, -1.4968294, -1.6918246, -1.6252046, -1.7095222, -1.4090…
## $ b_logalpha_Intercept             &amp;lt;dbl&amp;gt; 0.7814705, 0.6435611, 0.3971659, 0.5084503, 0.7547493, 1.2154113, 1…
## $ `r_item__eta[05,Intercept]`      &amp;lt;dbl&amp;gt; -0.7922848, -0.7423704, -1.1893399, -1.0718978, -0.2636685, -0.3965…
## $ `r_item__eta[06,Intercept]`      &amp;lt;dbl&amp;gt; 0.26895443, 0.21153136, 0.24488884, 0.20332673, 0.52650711, 0.51766…
## $ `r_item__eta[08,Intercept]`      &amp;lt;dbl&amp;gt; 0.1730248531, -0.0251183662, 0.0966877581, -0.0626794892, 0.3235805…
## $ `r_item__eta[09,Intercept]`      &amp;lt;dbl&amp;gt; 0.50089389, 0.39236895, 0.35678623, 0.41680088, 0.66285740, 0.53730…
## $ `r_item__eta[11,Intercept]`      &amp;lt;dbl&amp;gt; -0.17619531, -1.00068058, -0.41426442, -0.71686852, -0.15409689, -0…
## $ `r_item__eta[13,Intercept]`      &amp;lt;dbl&amp;gt; 0.02890276, -0.19689487, 0.12965889, -0.13639774, 0.35999998, 0.250…
## $ `r_item__eta[16,Intercept]`      &amp;lt;dbl&amp;gt; 0.5650797, 0.3484697, 0.5692639, 0.4691862, 0.5286982, 0.7387770, 0…
## $ `r_item__eta[18,Intercept]`      &amp;lt;dbl&amp;gt; -0.03100911, -0.94785287, -0.40602161, -1.06680784, -0.29232278, 0.…
## $ `r_item__logalpha[05,Intercept]` &amp;lt;dbl&amp;gt; -0.445733584, -0.168897075, -0.175487957, -0.023652177, -0.38447241…
## $ `r_item__logalpha[06,Intercept]` &amp;lt;dbl&amp;gt; 0.106865933, 0.183709540, 0.352641061, 0.372592174, 0.606879092, 0.…
## $ `r_item__logalpha[08,Intercept]` &amp;lt;dbl&amp;gt; 0.279715442, 0.224570579, 0.613387942, 0.344415036, 0.249641920, -0…
## $ `r_item__logalpha[09,Intercept]` &amp;lt;dbl&amp;gt; 0.46984590, 0.59974910, 0.61973239, 0.57372330, 0.67782554, 0.38758…
## $ `r_item__logalpha[11,Intercept]` &amp;lt;dbl&amp;gt; -0.23306444, -0.85055396, -0.31303715, -0.60098567, -0.40559846, -0…
## $ `r_item__logalpha[13,Intercept]` &amp;lt;dbl&amp;gt; -0.008225223, 0.116253481, 0.122579334, 0.291166675, 0.689798894, 0…
## $ `r_item__logalpha[16,Intercept]` &amp;lt;dbl&amp;gt; 0.092113896, 0.104240677, 0.092359783, 0.097844755, 0.494488271, 0.…
## $ `r_item__logalpha[18,Intercept]` &amp;lt;dbl&amp;gt; 0.09498269, -0.15278053, 0.01792382, -0.01208098, -0.31325790, -0.0…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now there are 16 &lt;code&gt;r_item__&lt;/code&gt; columns, half of which correspond to the &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; deviations and the other half of which correspond to the &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; deviations. In addition, we also have the &lt;code&gt;b_logalpha_Intercept&lt;/code&gt; columns to content with. So this time, we’ll follow up our &lt;code&gt;pivot_longer()&lt;/code&gt; code with subsequent &lt;code&gt;mutate()&lt;/code&gt; and &lt;code&gt;select()&lt;/code&gt; steps, and complete the task with &lt;code&gt;pivot_wider()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  select(b_eta_Intercept, b_logalpha_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(item      = str_extract(name, &amp;quot;\\d+&amp;quot;),
         parameter = ifelse(str_detect(name, &amp;quot;eta&amp;quot;), &amp;quot;xi&amp;quot;, &amp;quot;logalpha&amp;quot;)) %&amp;gt;% 
  select(-name) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value)

# what does this look like, now?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   b_eta_Intercept b_logalpha_Intercept  iter item       xi logalpha
##             &amp;lt;dbl&amp;gt;                &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1           -1.39                0.781     1 05    -0.792  -0.446  
## 2           -1.39                0.781     1 06     0.269   0.107  
## 3           -1.39                0.781     1 08     0.173   0.280  
## 4           -1.39                0.781     1 09     0.501   0.470  
## 5           -1.39                0.781     1 11    -0.176  -0.233  
## 6           -1.39                0.781     1 13     0.0289 -0.00823&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this configuration, it’s only a little more complicated to compute the probability summaries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  expand(nesting(iter, b_eta_Intercept, b_logalpha_Intercept, item, xi, logalpha),
         theta = seq(from = -6, to = 6, length.out = 100)) %&amp;gt;% 
  # note the difference in the equation
  mutate(p = inv_logit_scaled(exp(b_logalpha_Intercept + logalpha) * (b_eta_Intercept + theta + xi))) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(p = mean(p))

# what have we done?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
## # Groups:   theta [1]
##   theta item           p
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;
## 1    -6 05    0.0000152 
## 2    -6 06    0.00000543
## 3    -6 08    0.00000967
## 4    -6 09    0.00000105
## 5    -6 11    0.000127  
## 6    -6 13    0.00000111&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  ggplot(aes(x = theta, y = p, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;ICCs for the 2PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior mean.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = expression(italic(p)(y==1))) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like those &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; parameters made a big difference for the ICCs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iics.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;IICs.&lt;/h3&gt;
&lt;p&gt;From a computational standpoint, item information curves (IICs) are a transformation of the ICCs. Recall that the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis for the ICC is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, the probability &lt;span class=&#34;math inline&#34;&gt;\(y = 1\)&lt;/span&gt; for a given item. For the IIC plots, the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis shows information, which is a simple transformation of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{information} = p(1 - p).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So here’s how to use that equation and make the IIC plot for our 1PL model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these wrangling steps are all the same as before
posterior_samples(irt1) %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;), names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;xi&amp;quot;) %&amp;gt;% 
  mutate(item = str_extract(item, &amp;quot;\\d+&amp;quot;)) %&amp;gt;% 
  expand(nesting(iter, b_Intercept, item, xi),
         theta = seq(from = -6, to = 6, length.out = 200)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(b_Intercept + xi + theta)) %&amp;gt;% 
  
  # this part, right here, is what&amp;#39;s new
  mutate(i = p * (1 - p)) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(i = median(i)) %&amp;gt;%
  
  # now plot!
  ggplot(aes(x = theta, y = i, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;IICs for the 1PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior median.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = &amp;quot;information&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For kicks and giggles, we used the posterior medians, rather than the means. It’s similarly easy to compute the item-level information for the 2PL.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these wrangling steps are all the same as before
posterior_samples(irt2) %&amp;gt;% 
  select(b_eta_Intercept, b_logalpha_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(item      = str_extract(name, &amp;quot;\\d+&amp;quot;),
         parameter = ifelse(str_detect(name, &amp;quot;eta&amp;quot;), &amp;quot;xi&amp;quot;, &amp;quot;logalpha&amp;quot;)) %&amp;gt;% 
  select(-name) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  expand(nesting(iter, b_eta_Intercept, b_logalpha_Intercept, item, xi, logalpha),
         theta = seq(from = -6, to = 6, length.out = 200)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(exp(b_logalpha_Intercept + logalpha) * (b_eta_Intercept + theta + xi))) %&amp;gt;% 

  # again, here&amp;#39;s the new part
  mutate(i = p * (1 - p)) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(i = median(i)) %&amp;gt;%
  
  # now plot!
  ggplot(aes(x = theta, y = i, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;IICs for the 2PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior median.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = &amp;quot;information&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;tic.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;TIC.&lt;/h4&gt;
&lt;p&gt;Sometimes researchers want to get a overall sense of the information in a group of items. For simplicity, here, we’ll just call groups of items a &lt;em&gt;test&lt;/em&gt;. The test information curve (TIC) is a special case of the IIC, but applied to the whole test. In short, you compute the TIC by summing up the information for the individual items at each level of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Using the 1PL as an example, here’s how we might do that by hand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(irt1) %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;), names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;xi&amp;quot;) %&amp;gt;% 
  mutate(item = str_extract(item, &amp;quot;\\d+&amp;quot;)) %&amp;gt;% 
  expand(nesting(iter, b_Intercept, item, xi),
         theta = seq(from = -6, to = 6, length.out = 200)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(b_Intercept + xi + theta)) %&amp;gt;% 
  mutate(i = p * (1 - p)) %&amp;gt;% 
  
  # this is where the TIC magic happens
  group_by(theta, iter) %&amp;gt;% 
  summarise(sum_i = sum(i)) %&amp;gt;% 
  group_by(theta) %&amp;gt;% 
  summarise(i = median(sum_i)) %&amp;gt;%
  
  # we plot
  ggplot(aes(x = theta, y = i)) +
  geom_line() +
  labs(title = &amp;quot;The test information curve for the 1PL&amp;quot;,
       subtitle = &amp;quot;The curve is based on the posterior median.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = &amp;quot;information&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Taken as a whole, the combination of the eight items &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;Loram et al.&lt;/a&gt; (&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; settled on does a reasonable job differentiating among those with high &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; values. But this combination of items isn’t going to be the best at differentiating among those on the lower end of the &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; scale. You might say these eight items make for a difficult test.&lt;/p&gt;
&lt;p&gt;Our method of extending the 1PL IIC to the TIC should work the same for the 2PL. I’ll leave it as an exercise for the interested reader.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;We might outlines the steps in this post as:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fit your &lt;strong&gt;brms&lt;/strong&gt; IRT model.&lt;/li&gt;
&lt;li&gt;Inspect the model with all your standard quality checks (e.g., &lt;span class=&#34;math inline&#34;&gt;\(\widehat R\)&lt;/span&gt; values, trace plots).&lt;/li&gt;
&lt;li&gt;Extract your posterior draws with the &lt;code&gt;posterior_samples()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;Isolate the item-related columns. Within the multilevel IRT context, this will typically involve an overall intercept (e.g., &lt;code&gt;b_Intercept&lt;/code&gt; for our 1PL &lt;code&gt;irt1&lt;/code&gt;) and item-specific deviations (e.g., the columns starting with &lt;code&gt;r_item&lt;/code&gt; in our 1PL &lt;code&gt;irt1&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Arrange the data into a format that makes it easy to add the overall intercept in question to each of the item-level deviations in question. For me, this seemed easiest with the long format via the &lt;code&gt;pivot_longer()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;Expand the data over a range of ability &lt;span class=&#34;math inline&#34;&gt;\((\theta)\)&lt;/span&gt; values. For me, this worked well with the &lt;code&gt;expand()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;Use the model-implied formula to compute the &lt;span class=&#34;math inline&#34;&gt;\(p(y = 1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Group the results by item and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and summarize the &lt;span class=&#34;math inline&#34;&gt;\(p(y = 1)\)&lt;/span&gt; distributions with something like the mean or median.&lt;/li&gt;
&lt;li&gt;Plot the results with &lt;code&gt;ggplot2::geom_line()&lt;/code&gt; and friends.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;You should be able to generalize this workflow to IRT models for data with more than two categories. You’ll just have to be careful about juggling your thresholds. You might find some inspiration along these lines &lt;a href=&#34;https://bookdown.org/content/4857/monsters-and-mixtures.html#ordered-categorical-outcomes&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://bookdown.org/content/3686/ordinal-predicted-variable.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You could totally switch up this workflow to use some of the data wrangling helpers from the &lt;a href=&#34;https://CRAN.R-project.org/package=tidybayes&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;. That could be a nifty little blog post in and of itself.&lt;/p&gt;
&lt;p&gt;One thing that’s super lame about conventional ICC/IIC plots is there’s no expression of uncertainty. To overcome that, you could compute the 95% intervals (or 50% or whatever) in the same &lt;code&gt;summarise()&lt;/code&gt; line where you computed the mean and then express those interval bounds with something like &lt;code&gt;geom_ribbon()&lt;/code&gt; in your plot. The difficulty I foresee is it will result in overplotting for any models with more than like five items. Perhaps faceting would be the solution, there.&lt;/p&gt;
&lt;p&gt;I’m no IRT jock and may have goofed some of the steps or equations. To report mistakes or provide any other constructive criticism, just chime in on this Twitter thread:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New blog up!&lt;a href=&#34;https://t.co/ZOy9Cxrqat&#34;&gt;https://t.co/ZOy9Cxrqat&lt;/a&gt;&lt;br&gt;&lt;br&gt;This time we practice making ICC plots for &lt;a href=&#34;https://twitter.com/hashtag/brms?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#brms&lt;/a&gt;-based multilevel IRT models.&lt;/p&gt;&amp;mdash; Solomon Kurz (@SolomonKurz) &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1409951540228628482?ref_src=twsrc%5Etfw&#34;&gt;June 29, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.6     purrr_0.3.4    
##  [7] readr_1.4.0     tidyr_1.1.3     tibble_3.1.2    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6         splines_4.0.4       
##   [6] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17        digest_0.6.27       
##  [11] htmltools_0.5.1.1    rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [16] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [21] colorspace_2.0-0     rvest_0.3.6          haven_2.3.1          xfun_0.23            callr_3.7.0         
##  [26] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [31] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0             pkgbuild_1.2.0      
##  [36] rstan_2.21.2         abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0           
##  [41] miniUI_0.1.1.1       viridisLite_0.4.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [46] DT_0.16              htmlwidgets_1.5.3    httr_1.4.2           threejs_0.3.3        ellipsis_0.3.2      
##  [51] farver_2.1.0         pkgconfig_2.0.3      loo_2.4.1            sass_0.3.1           dbplyr_2.0.0        
##  [56] utf8_1.2.1           labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.11         reshape2_1.4.4      
##  [61] later_1.2.0          munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.5.0           
##  [66] generics_0.1.0       broom_0.7.6          ggridges_0.5.3       evaluate_0.14        fastmap_1.1.0       
##  [71] yaml_2.2.1           processx_3.5.2       knitr_1.33           fs_1.5.0             nlme_3.1-152        
##  [76] mime_0.10            projpred_2.0.2       xml2_1.3.2           rstudioapi_0.13      compiler_4.0.4      
##  [81] bayesplot_1.8.0      shinythemes_1.1.2    curl_4.3             gamm4_0.2-6          reprex_0.3.0        
##  [86] statmod_1.4.35       bslib_0.2.4          stringi_1.6.2        highr_0.9            ps_1.6.0            
##  [91] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [96] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.8          pillar_1.6.1         lifecycle_1.0.0     
## [101] jquerylib_0.1.4      bridgesampling_1.0-0 estimability_1.3     httpuv_1.6.0         R6_2.5.0            
## [106] bookdown_0.22        promises_1.2.0.1     gridExtra_2.3        codetools_0.2-18     boot_1.3-26         
## [111] colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1     withr_2.4.2         
## [116] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.8        shiny_1.6.0         
## [126] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-albanoIntroductionToEducational2020&#34; class=&#34;csl-entry&#34;&gt;
Albano, T. (2020). &lt;em&gt;Introduction to educational and psychological measurement using &lt;span&gt;R&lt;/span&gt;&lt;/em&gt;. &lt;a href=&#34;https://www.thetaminusb.com/intro-measurement-r/&#34;&gt;https://www.thetaminusb.com/intro-measurement-r/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bonifayMultidimensionalIRT2019&#34; class=&#34;csl-entry&#34;&gt;
Bonifay, W. (2019). &lt;em&gt;Multidimensional item response theory&lt;/em&gt;. &lt;span&gt;SAGE Publications&lt;/span&gt;. &lt;a href=&#34;https://us.sagepub.com/en-us/nam/multidimensional-item-response-theory/book257740&#34;&gt;https://us.sagepub.com/en-us/nam/multidimensional-item-response-theory/book257740&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBayesianItemResponse2020&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020a). Bayesian item response modeling in &lt;span&gt;R&lt;/span&gt; with brms and &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;arXiv:1905.09501 [Stat]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/1905.09501&#34;&gt;http://arxiv.org/abs/1905.09501&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Bürkner2021Non_linear&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Estimating non-linear models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-crockerIntroductionToClassical2006&#34; class=&#34;csl-entry&#34;&gt;
Crocker, L., &amp;amp; Algina, J. (2006). &lt;em&gt;Introduction to classical and modern test theory&lt;/em&gt;. &lt;span&gt;Cengage Learning&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-deayalaTheoryAndPractice2008&#34; class=&#34;csl-entry&#34;&gt;
De Ayala, R. J. (2008). &lt;em&gt;The theory and practice of item response theory&lt;/em&gt;. &lt;span&gt;Guilford Publications&lt;/span&gt;. &lt;a href=&#34;https://www.guilford.com/books/The-Theory-and-Practice-of-Item-Response-Theory/R-de-Ayala/9781593858698&#34;&gt;https://www.guilford.com/books/The-Theory-and-Practice-of-Item-Response-Theory/R-de-Ayala/9781593858698&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-grolemundDataScience2017&#34; class=&#34;csl-entry&#34;&gt;
Grolemund, G., &amp;amp; Wickham, H. (2017). &lt;em&gt;R for data science&lt;/em&gt;. &lt;span&gt;O’Reilly&lt;/span&gt;. &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;https://r4ds.had.co.nz&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lewandowski2009generating&#34; class=&#34;csl-entry&#34;&gt;
Lewandowski, D., Kurowicka, D., &amp;amp; Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt;, &lt;em&gt;100&lt;/em&gt;(9), 1989–2001. &lt;a href=&#34;https://doi.org/10.1016/j.jmva.2009.04.008&#34;&gt;https://doi.org/10.1016/j.jmva.2009.04.008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-loramValidationOfANovel2019&#34; class=&#34;csl-entry&#34;&gt;
Loram, G., Ling, M., Head, A., &amp;amp; Clarke, E. J. R. (2019). &lt;em&gt;Validation of a novel climate change denial measure using item response theory&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/57nbk&#34;&gt;https://doi.org/10.31234/osf.io/57nbk&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-reckaseMultidimensionalIRT2009&#34; class=&#34;csl-entry&#34;&gt;
Reckase, M. D. (2009). &lt;em&gt;Multidimensional item response theory models&lt;/em&gt;. &lt;span&gt;Springer&lt;/span&gt;. &lt;a href=&#34;https://www.springer.com/gp/book/9780387899756&#34;&gt;https://www.springer.com/gp/book/9780387899756&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I should disclose that although I have not read through Bonifay’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bonifayMultidimensionalIRT2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; text, he offered to send me a copy around the time I uploaded this post.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;You can find a copy of these data on my GitHub &lt;a href=&#34;https://github.com/ASKurz/blogdown/tree/main/content/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/data&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Adopting the three-term multilevel structure–&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \theta_p + \xi_i\)&lt;/span&gt;, where the latter two terms are &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, \sigma_x)\)&lt;/span&gt;–places this form of the 1PL model squarely within the generalized linear multilevel model (GLMM). McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;, Chapter 12)&lt;/span&gt; referred to this particular model type as a cross-classified model. Coming from another perspective, Kruschke &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;, Chapters 19 and 20)&lt;/span&gt; described this as a kind of multilevel analysis of variance (ANOVA).&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;For a nice blog post on the LKJ, check out Stephen Martin’s &lt;a href=&#34;http://srmart.in/is-the-lkj1-prior-uniform-yes/&#34;&gt;&lt;em&gt;Is the LKJ(1) prior uniform? “Yes”&lt;/em&gt;&lt;/a&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Don&#39;t forget your inits</title>
      <link>/post/2021-06-05-don-t-forget-your-inits/</link>
      <pubDate>Sat, 05 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-06-05-don-t-forget-your-inits/</guid>
      <description>
&lt;script src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;When your MCMC chains look a mess, you might have to manually set your initial values. If you’re a fancy pants, you can use a custom function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;context&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Context&lt;/h2&gt;
&lt;p&gt;A collaborator asked me to help model some reaction-time data. One of the first steps was to decide on a reasonable likelihood function. You can see a productive Twitter thread on that process &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1398000353875005444&#34;&gt;here&lt;/a&gt;. Although I’ve settled on the shifted-lognormal function, I also considered the exponentially modified Gaussian function (a.k.a. exGaussian). As it turns out, the exGaussian can be fussy to work with! After several frustrating attempts, I solved the problem by fiddling with my initial values. The purpose of this post is to highlight the issue and give you some options.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This post is for Bayesians. For thorough introductions to contemporary Bayesian regression, I recommend either edition of McElreath’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text; or Gelman, Hill, and Vehtari’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; text.&lt;/li&gt;
&lt;li&gt;Though not necessary, it will help if you’re familiar with multilevel regression. The texts by McElreath and Kruschke, from above, can both help with that.&lt;/li&gt;
&lt;li&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with an emphasis on the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. We will also make good use of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;, the &lt;strong&gt;patchwork&lt;/strong&gt; package &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-patchwork&#34; role=&#34;doc-biblioref&#34;&gt;Pedersen, 2019&lt;/a&gt;)&lt;/span&gt;, and &lt;strong&gt;ggmcmc&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-marinGgmcmcAnalysisMCMC2016&#34; role=&#34;doc-biblioref&#34;&gt;Fernández i Marín, 2016&lt;/a&gt;, &lt;a href=&#34;#ref-R-ggmcmc&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. We will also use the &lt;strong&gt;lisa&lt;/strong&gt; package &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-lisa&#34; role=&#34;doc-biblioref&#34;&gt;Littlefield, 2020&lt;/a&gt;)&lt;/span&gt; to select the color palette for our figures.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Load the primary &lt;strong&gt;R&lt;/strong&gt; packages and adjust the global plotting theme defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load
library(tidyverse)
library(brms)
library(patchwork)
library(ggmcmc)
library(lisa)

# define the color palette
fk &amp;lt;- lisa_palette(&amp;quot;FridaKahlo&amp;quot;, n = 31, type = &amp;quot;continuous&amp;quot;)

# adjust the global plotting theme
theme_set(
  theme_gray(base_size = 13) +
    theme(
      text = element_text(family = &amp;quot;Times&amp;quot;, color = fk[1]),
      axis.text = element_text(family = &amp;quot;Times&amp;quot;, color = fk[1]),
      axis.ticks = element_line(color = fk[1]),
      legend.background = element_blank(),
      legend.box.background = element_blank(),
      legend.key = element_blank(),
      panel.background = element_rect(fill = alpha(fk[16], 1/4), color = &amp;quot;transparent&amp;quot;),
      panel.grid = element_blank(),
      plot.background = element_rect(fill = alpha(fk[16], 1/4), color = &amp;quot;transparent&amp;quot;)
    )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The color palette in this post is inspired by &lt;a href=&#34;https://en.wikipedia.org/wiki/Frida_Kahlo&#34;&gt;Frida Kahlo&lt;/a&gt;’s &lt;a href=&#34;https://en.wikipedia.org/wiki/Self-Portrait_with_Thorn_Necklace_and_Hummingbird&#34;&gt;&lt;em&gt;Self-Portrait with Thorn Necklace and Hummingbird&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need data&lt;/h2&gt;
&lt;p&gt;I’m not at liberty to share the original data. However, I have simulated a new data set that has the essential features of the original and I have saved the file on GitHub. You can load it like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(url(&amp;quot;https://github.com/ASKurz/blogdown/raw/main/content/post/2021-06-05-don-t-forget-your-inits/data/dat.rda?raw=true&amp;quot;))

# what is this?
glimpse(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 29,281
## Columns: 2
## $ id &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a…
## $ rt &amp;lt;dbl&amp;gt; 689.0489, 552.8998, 901.0891, 992.2104, 1218.2256, 1356.5888, 679.0385, 663.7340, 771.3938, 996.2…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our primary variable of interest is &lt;code&gt;rt&lt;/code&gt;, which is simulated reaction times in milliseconds. The reaction times are nested within 26 participants, who are denoted by the &lt;code&gt;id&lt;/code&gt; column. The data are not balanced.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  count(id, name = &amp;quot;trials&amp;quot;) %&amp;gt;% 
  count(trials)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 2
##   trials     n
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
## 1    320     2
## 2    640     4
## 3    960     3
## 4   1121     1
## 5   1280    14
## 6   1600     1
## 7   2560     1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whereas most participants have 1,280 trials, their numbers range from 320 to 2,560, which means we’ll want a multilevel model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-can-describe-the-data-with-the-exgaussian-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We can describe the data with the exGaussian function&lt;/h2&gt;
&lt;p&gt;To start getting a sense of the &lt;code&gt;rt&lt;/code&gt; data, we’ll make a density plot of the overall distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  ggplot(aes(x = rt)) +
  geom_density(fill = fk[3], color = fk[3])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As is typical of reaction times, the data are continuous, non-negative, and strongly skewed to the right. There are any number of likelihood functions one can use to model data of this kind. One popular choice is the exGaussian. The exGaussian distribution has three parameters: &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters govern the mean and standard deviation for the central Gaussian portion of the distribution. The &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter governs the rate of the exponential distribution, which is tacked on to the right-hand side of the distribution. Within &lt;strong&gt;R&lt;/strong&gt;, you can compute the density of various exGaussian distributions using the &lt;code&gt;brms::dexgaussian()&lt;/code&gt; function. If you fool around with the parameter settings, a bit, you can make an exGaussian curve that fits pretty closely to the shape of our &lt;code&gt;rt&lt;/code&gt; data. For example, here’s what it looks like when we set &lt;code&gt;mu = 1300&lt;/code&gt;, &lt;code&gt;sigma = 150&lt;/code&gt;, and &lt;code&gt;beta = 520&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(rt = seq(from = 0, to = 5500, length.out = 300),
       d = dexgaussian(rt, mu = 1300, sigma = 150, beta = 520)) %&amp;gt;% 
  
  ggplot(aes(x = rt)) +
  geom_density(data = dat,
               fill = fk[3], color = fk[3]) +
  geom_line(aes(y = d), 
            color = fk[31], size = 5/4) +
  # zoom in on the bulk of the values
  coord_cartesian(xlim = c(0, 5000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The fit isn’t perfect, but it gives a sense of where things are headed. It’s time to talk about modeling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models&lt;/h2&gt;
&lt;p&gt;In this post, we will explore three options for modeling the reaction-time data. The first will use default options. The second option will employ manually-set starting points. For the third option, we will use pseudorandom number generators to define the starting points, all within a custom function.&lt;/p&gt;
&lt;div id=&#34;model-1-use-the-exgaussian-with-default-settings.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model 1: Use the exGaussian with default settings.&lt;/h3&gt;
&lt;p&gt;When using &lt;strong&gt;brms&lt;/strong&gt;, you can fit an exGaussian model by setting &lt;code&gt;family = exgaussian()&lt;/code&gt;. Here we’ll allow the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; parameters to vary by participant, but keep the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters fixed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- brm(
  data = dat,
  family = exgaussian(),
  formula = rt ~ 1 + (1 | id),
  cores = 4, seed = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m not going to show them all, here, for the sake of space, but this model returned warnings about 604 transitions, 1 chain for which the estimated Bayesian Fraction of Missing Information was low, a large R-hat value of 2.85, and low bulk and tail effective sample sizes. In other words, this was a disaster. To help bring these all into focus, we’ll want to take a look at the chains in a trace plot. Since we’ll be doing this a few times, let’s go ahead and make a custom trace plot geom to suit our purposes. We’ll call it &lt;code&gt;geom_trace()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;geom_trace &amp;lt;- function(subtitle = NULL, 
                       xlab = &amp;quot;iteration&amp;quot;, 
                       xbreaks = 0:4 * 500) {
  
  list(
    annotate(geom = &amp;quot;rect&amp;quot;, 
             xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,
             fill = fk[16], alpha = 1/2, size = 0),
    geom_line(size = 1/3),
    scale_color_manual(values = fk[c(3, 8, 27, 31)]),
    scale_x_continuous(xlab, breaks = xbreaks, expand = c(0, 0)),
    labs(subtitle = subtitle),
    theme(panel.grid = element_blank())
  )
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For your real-world models, it’s good to look at the tract plots for all major model parameters. Here we’ll just focus on the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; intercept.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggs(fit1, burnin = TRUE) %&amp;gt;%
  filter(Parameter == &amp;quot;b_Intercept&amp;quot;) %&amp;gt;% 
  mutate(chain = factor(Chain),
         intercept = value) %&amp;gt;% 
  
  ggplot(aes(x = Iteration, y = intercept, color = chain)) +
  geom_trace(subtitle = &amp;quot;fit1 (default settings)&amp;quot;) +
  scale_y_continuous(breaks = c(0, 650, 1300), limits = c(NA, 1430))

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since we pulled the chains using the &lt;code&gt;ggmcmc::ggs()&lt;/code&gt; function, we were able to plot the warmup iterations (darker beige background on the left) along with the post-warmup iterations (lighter beige background on the right). Although one of our chains eventually made its way to the posterior, three out of the four stagnated near their starting values. This brings us to a major point in this post: &lt;em&gt;Starting points can be a big deal&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-points-can-be-a-big-deal.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Starting points can be a big deal.&lt;/h3&gt;
&lt;p&gt;I’m not going to go into the theory underlying Markov chain Monte Carlo (MCMC) methods in any detail. For that, check out some of the latter chapters in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gillBayesianMethods2015&#34; role=&#34;doc-biblioref&#34;&gt;Gill&lt;/a&gt; (&lt;a href=&#34;#ref-gillBayesianMethods2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gelman2013bayesian&#34; role=&#34;doc-biblioref&#34;&gt;Gelman et al.&lt;/a&gt; (&lt;a href=&#34;#ref-gelman2013bayesian&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;. In brief, if you run a Markov chain for an infinite number of iterations, it will converge on the correct posterior distribution. The problem is we can’t run our chains for that long, which means we have to be careful about whether our finite-length chains have converged properly. Starting points are one of the factors that can influence this process.&lt;/p&gt;
&lt;p&gt;One of the ways to help make sure your MCMC chains are sampling well is to run multiple chains for a while and check to see whether they have all converged around the same parameter space. Ideally, each chain will start from a different initial value. In practice, the first several iterations following the starting values are typically discarded. With older methods, like the Gibbs sampler, this was called the “burn-in” period. With Hamiltonian Monte Carlo (HMC), which is what &lt;strong&gt;brms&lt;/strong&gt; uses, we have a similar period called “warmup.” When everything goes well, the MCMC chains will all have traversed from their starting values to sampling probabilistically from the posterior distribution once they have emerged from the warmup phase. However, this isn’t always the case. Sometimes the chains get stuck around their stating values and continue to linger there, even after you have terminated the warmup period. When this happens, you’ll end up with samples that are still tainted by their starting values and are not yet representative of the posterior distribution.&lt;/p&gt;
&lt;p&gt;In our example, above, we used the &lt;strong&gt;brms&lt;/strong&gt; default settings of four chains, each of which ran for 1,000 warmup iterations and then 1,000 post-warmup iterations. We also used the &lt;strong&gt;brms&lt;/strong&gt; default for the starting values. These defaults are based on the Stan defaults, which is to randomly select the starting points from a uniform distribution ranging from -2 to 2. For details, see the &lt;a href=&#34;https://mc-stan.org/docs/2_25/reference-manual/initialization.html#random-initial-values&#34;&gt;&lt;em&gt;Random initial values&lt;/em&gt;&lt;/a&gt; section of the &lt;em&gt;Stan Reference Manual&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-standevelopmentteamStanReferenceManual2021&#34; role=&#34;doc-biblioref&#34;&gt;Stan Development Team, 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In my experience, the &lt;strong&gt;brms&lt;/strong&gt; defaults are usually pretty good. My models often quickly traverse from their starting values to concentrate in the posterior, just like our second chain did, above. When things go wrong, sometimes adding stronger priors can work. Other times it makes sense to rescale or reparameterize the model, somehow. In this case, I have reasons to want to (a) use default priors and to (b) stick to the default parameterization applied to the transformed data. Happily, we have another trick at out disposal: We can adjust the starting points.&lt;/p&gt;
&lt;p&gt;Within &lt;code&gt;brms::brm()&lt;/code&gt;, we can control the starting values with the &lt;code&gt;inits&lt;/code&gt; argument. The default is &lt;code&gt;inits = &#34;random&#34;&lt;/code&gt;, which follows the Stan convention of sampling from &lt;span class=&#34;math inline&#34;&gt;\((-2, 2)\)&lt;/span&gt;, as discussed above. Another option is to fix all starting values to zero by setting &lt;code&gt;inits = &#34;0&#34;&lt;/code&gt;. This often works surprisingly well, but it wasn’t the solution in this case. If you look at the trace plot, above, you’ll see that all the starting values are a long ways from the target range, which is somewhere around 1,300. So why not just put the starting values near there?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-2-fit-the-model-with-initial-values-set-by-hand.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model 2: Fit the model with initial values set by hand.&lt;/h3&gt;
&lt;p&gt;When you specify start values for the parameters in your Stan models, you need to do so with a list of lists. Each MCMC chain will need its own list. In our case, that means we’ll need four separate lists, each of which will be nested within a single higher-order list. For example, here we’ll define a single list called &lt;code&gt;inits&lt;/code&gt;, which will have starting values defined for our primary three population-level parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inits &amp;lt;- list(
  Intercept = 1300,
  sigma     = 150,
  beta      = 520
  )

# what is this?
inits&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Intercept
## [1] 1300
## 
## $sigma
## [1] 150
## 
## $beta
## [1] 520&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we didn’t bother setting a starting value for the standard-deviation parameter for the random intercepts. That parameter, then, will just get the &lt;strong&gt;brms&lt;/strong&gt; default. The others will the the start values, as assigned. Now, since we have four chains to assign start values to, a quick and dirty method is to just use the same ones for all four chains. Here’s how to do that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_of_inits &amp;lt;- list(inits, inits, inits, inits)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our &lt;code&gt;list_of_inits&lt;/code&gt; object is a list into which we have saved four copies of our &lt;code&gt;inits&lt;/code&gt; list. Here’s how to use those values within &lt;code&gt;brms::brm()&lt;/code&gt;. Just plug them into the &lt;code&gt;inits&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- brm(
  data = dat,
  family = exgaussian(),
  formula = rt ~ 1 + (1 | id),
  cores = 4, seed = 1,
  inits = list_of_inits
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The effective sample sizes are still a little low, but the major pathologies are now gone. Compare the updated traceplot for the intercept to the first one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# adjust fit1
p1 &amp;lt;- p1 +
  geom_trace(subtitle = &amp;quot;fit1 (default settings)&amp;quot;,
             xlab = NULL, xbreaks = NULL)

# fit2
p2 &amp;lt;- ggs(fit2) %&amp;gt;%
  filter(Parameter == &amp;quot;b_Intercept&amp;quot;) %&amp;gt;% 
  mutate(chain = factor(Chain),
         intercept = value) %&amp;gt;% 
  
  ggplot(aes(x = Iteration, y = intercept, color = chain)) +
  geom_trace(subtitle = &amp;quot;fit2 (manual copy/paste inits settings)&amp;quot;) +
  coord_cartesian(ylim = c(1200, 1400))

# combine
p1 / p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Man that looks better! See how all four of our chains started out at 1,300? That’s because of how we copy/pasted &lt;code&gt;inits&lt;/code&gt; four times within our &lt;code&gt;list_of_inits&lt;/code&gt; object. This is kinda okay, but we can do better.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-3-set-the-initial-values-with-a-custom-function.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model 3: Set the initial values with a custom function.&lt;/h3&gt;
&lt;p&gt;Returning back to MCMC theory, a bit, it’s generally a better idea to assign each chain its own starting value. Then, if all chains converge into the same part in the parameter space, that provides more convincing evidence they’re all properly exploring the posterior. To be clear, this isn’t rigorous evidence. It’s just better evidence than if we started them all in the same spot.&lt;/p&gt;
&lt;p&gt;One way to give each chain its own starting value would be to manually set them. Here’s what that would look like if we were only working with two chains.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set the values for the first chain
inits1 &amp;lt;- list(
  Intercept = 1250,
  sigma     = 140,
  beta      = 500
  )

# set new values for the second chain
inits2 &amp;lt;- list(
  Intercept = 1350,
  sigma     = 160,
  beta      = 540
  )

# combine the two lists into a single list
list_of_inits &amp;lt;- list(inits1, inits2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach will work fine, but it’s tedious, especially if you’d like to apply it to a large number of parameters. A more programmatic approach would be to use a pseudorandom number-generating function to randomly set the starting values. Since the intercept is an unbounded parameter, the posterior for which will often look Gaussian, the &lt;code&gt;rnorm()&lt;/code&gt; function can be a great choice for selecting its starting values. Since both &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters need to be non-negative, a better choice might be the &lt;code&gt;runif()&lt;/code&gt; or &lt;code&gt;rgamma()&lt;/code&gt; functions. Here we’ll just use &lt;code&gt;runif()&lt;/code&gt; for each.&lt;/p&gt;
&lt;p&gt;Since we’re talking about using the pseudorandom number generators to pick our values, it would be nice if the results were reproducible. We can do that by working in the &lt;code&gt;set.seed()&lt;/code&gt; function. Finally, it would be really sweet if we had a way to wrap &lt;code&gt;set.seed()&lt;/code&gt; and the various number-generating functions into a single higher-order function. Here’s one way to make such a function, which I’m calling &lt;code&gt;set_inits()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set_inits &amp;lt;- function(seed = 1) {
  
  set.seed(seed)
  list(
    Intercept = rnorm(n = 1, mean = 1300, sd = 100),
    sigma     = runif(n = 1, min = 100, max = 200),
    beta      = runif(n = 1, min = 450, max = 550)
  )
  
}

# try it out
set_inits(seed = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Intercept
## [1] 1426.295
## 
## $sigma
## [1] 137.2124
## 
## $beta
## [1] 507.2853&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how we set the parameters within the &lt;code&gt;rnorm()&lt;/code&gt; and &lt;code&gt;runif()&lt;/code&gt; functions to values that seemed reasonable given our model. These values aren’t magic and you could adjust them to your own needs. Now, here’s how to use our handy &lt;code&gt;set_inits()&lt;/code&gt; function to choose similar, but distinct, starting values for each of our four chains. We save the results in a higher-order list called &lt;code&gt;my_second_list_of_inits&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_second_list_of_inits &amp;lt;- list(
  # different seed values will return different results
  set_inits(seed = 1),
  set_inits(seed = 2),
  set_inits(seed = 3),
  set_inits(seed = 4)
)

# what have we done?
str(my_second_list_of_inits)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 4
##  $ :List of 3
##   ..$ Intercept: num 1237
##   ..$ sigma    : num 157
##   ..$ beta     : num 541
##  $ :List of 3
##   ..$ Intercept: num 1210
##   ..$ sigma    : num 157
##   ..$ beta     : num 467
##  $ :List of 3
##   ..$ Intercept: num 1204
##   ..$ sigma    : num 138
##   ..$ beta     : num 483
##  $ :List of 3
##   ..$ Intercept: num 1322
##   ..$ sigma    : num 129
##   ..$ beta     : num 478&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now just plug &lt;code&gt;my_second_list_of_inits&lt;/code&gt; into the &lt;code&gt;inits&lt;/code&gt; argument and fit the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;- brm(
  data = dat,
  family = exgaussian(),
  formula = rt ~ 1 + (1 | id),
  cores = 4, seed = 1,
  inits = my_second_list_of_inits
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with &lt;code&gt;fit2&lt;/code&gt;, our &lt;code&gt;fit3&lt;/code&gt; came out okay. Let’s inspect the intercept parameter with a final trace plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# adjust fit2
p2 &amp;lt;- p2 +
  geom_trace(subtitle = &amp;quot;fit2 (manual copy/paste inits settings)&amp;quot;,
             xlab = NULL, xbreaks = NULL)

# fit3
p3 &amp;lt;- ggs(fit3) %&amp;gt;%
  filter(Parameter == &amp;quot;b_Intercept&amp;quot;) %&amp;gt;% 
  mutate(chain = factor(Chain),
         intercept = value) %&amp;gt;% 
  
  ggplot(aes(x = Iteration, y = intercept, color = chain)) +
  geom_trace(subtitle = &amp;quot;fit3 (inits by a custom function)&amp;quot;) +
  coord_cartesian(ylim = c(1200, 1400))

# combine
p1 / p2 / p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we have visual evidence that even though all four chains started at different places in the parameter space, they all converged into the same area. This still isn’t fully rigorous evidence our chains are performing properly, but it’s a major improvement from &lt;code&gt;fit1&lt;/code&gt; and a minor improvement from &lt;code&gt;fit2&lt;/code&gt;. They aren’t shown here, but the same point holds for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;p&gt;Okay, just for kicks and giggles, let’s see how well our last model did by way of a posterior predictive check.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bayesplot::color_scheme_set(fk[c(31, 31, 31, 3, 3, 3)])

pp_check(fit3, nsamples = 100) + 
  # we don&amp;#39;t need to see the whole right tail
  coord_cartesian(xlim = c(0, 5000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-05-don-t-forget-your-inits/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model could be better, but it’s moving in the right direction and there don’t appear to be any major pathologies, like what we saw with &lt;code&gt;fit1&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recap&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If your try to fit a model with MCMC, you may sometimes end up with pathologies, such as divergent transitions, large numbers of transitions, high R-hat values, and/or very low effective sample size estimates.&lt;/li&gt;
&lt;li&gt;Sometimes these pathologies arise when the starting values for your chains are far away from the centers of their posterior densities.&lt;/li&gt;
&lt;li&gt;When using &lt;strong&gt;brms&lt;/strong&gt;, you can solve this problem by setting the starting values with the &lt;code&gt;inits&lt;/code&gt; argument.&lt;/li&gt;
&lt;li&gt;One approach is to manually set the starting values, saving them in a list of lists.&lt;/li&gt;
&lt;li&gt;Another approach is to use the pseudorandom number generators, such as &lt;code&gt;rnorm()&lt;/code&gt; and &lt;code&gt;runif()&lt;/code&gt;, to assign starting values within user-defined ranges.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] lisa_0.1.2      ggmcmc_1.5.1.1  patchwork_1.1.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1  
##  [7] stringr_1.4.0   dplyr_1.0.6     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.2   
## [13] ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6         splines_4.0.4       
##   [6] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17        digest_0.6.27       
##  [11] htmltools_0.5.1.1    rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [16] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [21] colorspace_2.0-0     rvest_0.3.6          haven_2.3.1          xfun_0.23            callr_3.7.0         
##  [26] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [31] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0             pkgbuild_1.2.0      
##  [36] rstan_2.21.2         abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1        GGally_2.1.1        
##  [41] DBI_1.1.0            miniUI_0.1.1.1       xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [46] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3        RColorBrewer_1.1-2  
##  [51] ellipsis_0.3.2       farver_2.1.0         reshape_0.8.8        pkgconfig_2.0.3      loo_2.4.1           
##  [56] sass_0.3.1           dbplyr_2.0.0         utf8_1.2.1           labeling_0.4.2       tidyselect_1.1.1    
##  [61] rlang_0.4.11         reshape2_1.4.4       later_1.2.0          munsell_0.5.0        cellranger_1.1.0    
##  [66] tools_4.0.4          cli_2.5.0            generics_0.1.0       broom_0.7.6          ggridges_0.5.3      
##  [71] evaluate_0.14        fastmap_1.1.0        yaml_2.2.1           processx_3.5.2       knitr_1.33          
##  [76] fs_1.5.0             nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [81] rstudioapi_0.13      compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2    curl_4.3            
##  [86] gamm4_0.2-6          reprex_0.3.0         statmod_1.4.35       bslib_0.2.4          stringi_1.6.2       
##  [91] highr_0.9            ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41     
##  [96] Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0        vctrs_0.3.8         
## [101] pillar_1.6.1         lifecycle_1.0.0      jquerylib_0.1.4      bridgesampling_1.0-0 estimability_1.3    
## [106] httpuv_1.6.0         R6_2.5.0             bookdown_0.22        promises_1.2.0.1     gridExtra_2.3       
## [111] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [116] assertthat_0.2.1     withr_2.4.2          shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33         
## [121] parallel_4.0.4       hms_0.5.3            grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [126] rmarkdown_2.8        shiny_1.6.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-Bürkner2021Parameterization&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Parameterization of response distributions in brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-marinGgmcmcAnalysisMCMC2016&#34; class=&#34;csl-entry&#34;&gt;
Fernández i Marín, X. (2016). &lt;span class=&#34;nocase&#34;&gt;ggmcmc&lt;/span&gt;: &lt;span&gt;Analysis&lt;/span&gt; of &lt;span&gt;MCMC&lt;/span&gt; samples and &lt;span&gt;Bayesian&lt;/span&gt; inference. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;70&lt;/em&gt;(9), 1–20. &lt;a href=&#34;https://doi.org/10.18637/jss.v070.i09&#34;&gt;https://doi.org/10.18637/jss.v070.i09&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-ggmcmc&#34; class=&#34;csl-entry&#34;&gt;
Fernández i Marín, X. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;ggmcmc&lt;/span&gt;: &lt;span&gt;Tools&lt;/span&gt; for analyzing &lt;span&gt;MCMC&lt;/span&gt; simulations from &lt;span&gt;Bayesian&lt;/span&gt; inference&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=ggmcmc&#34;&gt;https://CRAN.R-project.org/package=ggmcmc&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp;amp; Rubin, D. B. (2013). &lt;em&gt;Bayesian data analysis&lt;/em&gt; (Third Edition). &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://stat.columbia.edu/~gelman/book/&#34;&gt;https://stat.columbia.edu/~gelman/book/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gillBayesianMethods2015&#34; class=&#34;csl-entry&#34;&gt;
Gill, J. (2015). &lt;em&gt;Bayesian methods: &lt;span&gt;A&lt;/span&gt; social and behavioral sciences approach&lt;/em&gt; (Third Edition). &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Bayesian-Methods-A-Social-and-Behavioral-Sciences-Approach-Third-Edition/Gill/p/book/9781439862483&#34;&gt;https://www.routledge.com/Bayesian-Methods-A-Social-and-Behavioral-Sciences-Approach-Third-Edition/Gill/p/book/9781439862483&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-lisa&#34; class=&#34;csl-entry&#34;&gt;
Littlefield, T. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;lisa&lt;/span&gt;: &lt;span&gt;Color&lt;/span&gt; palettes from color lisa&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=lisa&#34;&gt;https://CRAN.R-project.org/package=lisa&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-patchwork&#34; class=&#34;csl-entry&#34;&gt;
Pedersen, T. L. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;patchwork&lt;/span&gt;: &lt;span&gt;The&lt;/span&gt; composer of plots&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;https://CRAN.R-project.org/package=patchwork&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-standevelopmentteamStanReferenceManual2021&#34; class=&#34;csl-entry&#34;&gt;
Stan Development Team. (2021). &lt;em&gt;Stan reference manual, &lt;span&gt;Version&lt;/span&gt; 2.27&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/docs/2_27/reference-manual/&#34;&gt;https://mc-stan.org/docs/2_27/reference-manual/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;There are different ways to parameterize the exGaussian distribution and these differences may involve different ways to express what we’re calling &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Since our parameterization is based on Paul Bürkner’s work, you might check out the &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/vignettes/brms_families.html#response-time-models&#34;&gt;&lt;em&gt;Response time models&lt;/em&gt;&lt;/a&gt; section in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; document, &lt;em&gt;Parameterization of response distributions in brms&lt;/em&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Effect sizes for experimental trials analyzed with multilevel growth models: Two of two</title>
      <link>/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/</link>
      <pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/</guid>
      <description>
&lt;script src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;orientation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Orientation&lt;/h2&gt;
&lt;p&gt;This post is the second and final installment of a two-part series. In the &lt;a href=&#34;https://solomonkurz.netlify.app/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/&#34;&gt;first post&lt;/a&gt;, we explored how one might compute an effect size for two-group experimental data with only &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; time points. In this second post, we fulfill our goal to show how to generalize this framework to experimental data collected over &lt;span class=&#34;math inline&#34;&gt;\(3+\)&lt;/span&gt; time points. The data and overall framework come from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;i-still-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I still make assumptions.&lt;/h3&gt;
&lt;p&gt;As with the &lt;a href=&#34;https://solomonkurz.netlify.app/post/2021-01-26-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-one-of-two/#i-make-assumptions.&#34;&gt;first post&lt;/a&gt;, I make a handful of assumptions about your background knowledge. Though I won’t spell them out again, here, I should stress that you’ll want to be familiar with multilevel models to get the most out of this post. To brush up, I recommend &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;Raudenbush &amp;amp; Bryk&lt;/a&gt; (&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett&lt;/a&gt; (&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As before, all code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;. Here we load our primary &lt;strong&gt;R&lt;/strong&gt; packages–&lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;, and the &lt;a href=&#34;http://style.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;–and adjust the global plotting theme defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)
library(tidybayes)
library(tidyverse)

# adjust the global plotting theme
theme_set(
  theme_linedraw() +
    theme(text = element_text(family = &amp;quot;Times&amp;quot;),
          panel.grid = element_blank(),
          strip.text = element_text(margin = margin(b = 3, t = 3)))
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;Once again, we use the &lt;a href=&#34;https://tibble.tidyverse.org/reference/tribble.html&#34;&gt;tribble&lt;/a&gt; approach to enter the synthetic data Feingold displayed in his Table 1 (p. 46).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  tribble(
    ~id, ~tx, ~t1, ~t2, ~t3, ~t4,
    101, -0.5, 3, 5, 5,  7,
    102, -0.5, 4, 4, 6,  6,
    103, -0.5, 4, 5, 7,  8,
    104, -0.5, 5, 6, 6,  8,
    105, -0.5, 5, 6, 7,  8,
    106, -0.5, 5, 7, 7,  7,
    107, -0.5, 5, 6, 8,  8,
    108, -0.5, 6, 6, 7,  9,
    109, -0.5, 6, 8, 9,  10,
    110, -0.5, 7, 7, 8,  9,
    111,  0.5, 3, 5, 7,  9,
    112,  0.5, 4, 7, 9,  11,
    113,  0.5, 4, 6, 8,  11,
    114,  0.5, 5, 7, 9,  10,
    115,  0.5, 5, 6, 9,  11,
    116,  0.5, 5, 7, 10, 10,
    117,  0.5, 5, 8, 8,  11,
    118,  0.5, 6, 7, 9,  12,
    119,  0.5, 6, 9, 11, 13,
    120,  0.5, 7, 8, 10, 12
  ) %&amp;gt;% 
  mutate(`t4-t1`   = t4 - t1,
         condition = ifelse(tx == -0.5, &amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;))

# inspect the first six rows
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##      id    tx    t1    t2    t3    t4 `t4-t1` condition
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    
## 1   101  -0.5     3     5     5     7       4 control  
## 2   102  -0.5     4     4     6     6       2 control  
## 3   103  -0.5     4     5     7     8       4 control  
## 4   104  -0.5     5     6     6     8       3 control  
## 5   105  -0.5     5     6     7     8       3 control  
## 6   106  -0.5     5     7     7     7       2 control&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To reacquaint ourselves with the data, we might make a plot. Last time we plotted a subset of the individual trajectories next to the averages, by treatment group. Here we’ll superimpose all the individual-level trajectories atop the group averages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  pivot_longer(t1:t4) %&amp;gt;% 
  mutate(time      = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double(),
         condition = ifelse(tx &amp;lt; 0, &amp;quot;tx = -0.5 (control)&amp;quot;, &amp;quot;tx = 0.5 (treatment)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = time, y = value)) +
  stat_smooth(aes(color = condition),
              method = &amp;quot;lm&amp;quot;, formula = &amp;#39;y ~ x&amp;#39;,
              se = F, size = 4) +
  geom_line(aes(group = id),
            size = 1/4) +
  scale_color_viridis_d(end = .75, direction = -1, breaks = NULL) +
  facet_wrap(~ condition)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig1-1.png&#34; width=&#34;624&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The thick lines are the group averages and the thinner lines are for the individual participants. Though participants tend to increase in both groups, those in the treatment condition appear to have increased at a more rapid pace. We want a standardized effect size that can capture those differences in a familiar metric. We’ll begin to explain what that will be, next.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;div id=&#34;we-need-a-framework.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need a framework.&lt;/h3&gt;
&lt;p&gt;Traditional analytic strategies, such as ordinary least squares (OLS) regression and the analysis of variance (ANOVA) framework, can work okay with data collected on one or two time points. In his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;, &lt;a href=&#34;#ref-feingoldARegressionFramework2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; work, which is the inspiration for this blog series, Feingold recommended what he called growth-modeling analysis (GMA) for data collected on &lt;span class=&#34;math inline&#34;&gt;\(3+\)&lt;/span&gt; time points. If you’re not familiar with the term GMA, it’s a longitudinal version of what others have called hierarchical linear models, mixed-effects models, random-effects models, or multilevel models. For longitudinal data, I’m fond of the term &lt;em&gt;multilevel growth model&lt;/em&gt;, but you can use whatever term you like. If you’re interested, Raudenbush and Bryk touched on the historic origins of several of these terms in the first chapter of their &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-raudenbushHLM2002&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt; text.&lt;/p&gt;
&lt;p&gt;Though multilevel growth models, GMAs, have become commonplace in many applied areas, it’s not immediately obvious how to compute standardized effect sizes when one uses them. In his Discussion section, Feingold &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009, p. 49&lt;/a&gt;)&lt;/span&gt; pointed out this topic is missing from many text books and software user’s guides. For example, though I took five statistics courses in graduate school, one of which even focused on the longitudinal growth model, none of my courses covered how to compute an effect size in a longitudinal growth model and none of my text books covered the topic, either. It’s hard to expect researchers to use strategies we don’t bother to teach, which is the reason for this blog series.&lt;/p&gt;
&lt;p&gt;We might walk out the framework with statistical notation. If we say our outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; varies across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; partitcipants and &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; time points, we might use Feingold’s Raudenbusch-&amp;amp;-Bryk-type notation to express our upcoming statistical model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{align*}
y_{ti} &amp;amp; = \beta_{00} + \beta_{01} (\text{treatment})_i + \beta_{10} (\text{time})_{ti} + \color{darkred}{\beta_{11}} (\text{treatment})_i (\text{time})_{ti} \\
&amp;amp; \;\;\; + [r_{0i} + r_{1i} (\text{time})_{ti} + e_{ti}],
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where variance in &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; is decomposed into the last three terms, &lt;span class=&#34;math inline&#34;&gt;\(r_{0i}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(r_{1i}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(e_{ti}\)&lt;/span&gt;. Here we follow the usual assumption that within-participant variance is normally distributed, &lt;span class=&#34;math inline&#34;&gt;\(e_{ti} \sim \operatorname N(0, \sigma_\epsilon^2)\)&lt;/span&gt;, and the &lt;span class=&#34;math inline&#34;&gt;\(r_{\text{x}i}\)&lt;/span&gt; values follow a bivariate normal distribution,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{bmatrix} r_{0i} \\ r_{1i} \end{bmatrix} \sim \operatorname N \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \tau_{00} &amp;amp; \tau_{01} \\ \tau_{01} &amp;amp; \tau_{11} \end{bmatrix} \right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; terms on the diagonal are the variances and the off-diagonal &lt;span class=&#34;math inline&#34;&gt;\(\tau_{01}\)&lt;/span&gt; is their covariance. We’ll be fitting this model with Bayesian software, which means all parameters will be given prior distributions. But since our goal is to emphasize the effect size and the multilevel framework, I’m just going to use the &lt;strong&gt;brms&lt;/strong&gt; default settings for the priors and will avoid expressing them in formal statistical notation&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this model, the four &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters are often called the “fixed effects,” or the population parameters. Our focal parameter will be &lt;span class=&#34;math inline&#34;&gt;\(\color{darkred}{\beta_{11}}\)&lt;/span&gt;, which is why we marked it off in red. This parameter is the interaction between time and treatment condition. Put another way, &lt;span class=&#34;math inline&#34;&gt;\(\color{darkred}{\beta_{11}}\)&lt;/span&gt; is the difference in the average rate of change, by treatment. Once we fit our multilevel growth model, we will explore how one might transform the &lt;span class=&#34;math inline&#34;&gt;\(\color{darkred}{\beta_{11}}\)&lt;/span&gt; parameter into our desired effect size.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model.&lt;/h3&gt;
&lt;p&gt;As you’ll learn in any good multilevel text book, multilevel models typically require the data to be in the long format. Here we’ll transform our data into that format and call the results &lt;code&gt;d_long&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wrangle
d_long &amp;lt;-
  d %&amp;gt;% 
  pivot_longer(t1:t4, values_to = &amp;quot;y&amp;quot;) %&amp;gt;% 
  mutate(time = str_extract(name, &amp;quot;\\d&amp;quot;) %&amp;gt;% as.double()) %&amp;gt;% 
  mutate(time_f = (time * 2) - 5,
         time_c = time - mean(time),
         time0  = time - 1,
         time01 = (time - 1) / 3)

# what have we done?
head(d_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 11
##      id    tx `t4-t1` condition name      y  time time_f time_c time0 time01
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1   101  -0.5       4 control   t1        3     1     -3   -1.5     0  0    
## 2   101  -0.5       4 control   t2        5     2     -1   -0.5     1  0.333
## 3   101  -0.5       4 control   t3        5     3      1    0.5     2  0.667
## 4   101  -0.5       4 control   t4        7     4      3    1.5     3  1    
## 5   102  -0.5       2 control   t1        4     1     -3   -1.5     0  0    
## 6   102  -0.5       2 control   t2        4     2     -1   -0.5     1  0.333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-feingoldEffectSizeForGMA2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; paper, Feingold mentioned he coded time as a factor which&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;was mean centered by using linear weights (-3, -1, 1, and 3 for T1 through T4, respectively) for a four-level design obtained from a table of orthogonal polynomials (Snedecor &amp;amp; Cochran, 1967) for the within-subjects (Level 1 in HLM terminology) facet of the analysis. (p. 47)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can find this version of the time variable in the &lt;code&gt;time_f&lt;/code&gt; column. However, I have no interest in modeling with time coded according to a scheme of orthogonal polynomials. But I do think it makes sense to center time or scale it so the lowest value is zero. You can find those versions of time in the &lt;code&gt;time_c&lt;/code&gt; and &lt;code&gt;time0&lt;/code&gt; columns. The model, below, uses &lt;code&gt;time0&lt;/code&gt;. Although this will change the scale of our model parameters relative to those in Feingold’s paper, it will have little influence on how we compute the effect size of interest.&lt;/p&gt;
&lt;p&gt;Here’s how we might fit the multilevel growth model for the two treatment conditions with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d_long,
      family = gaussian,
      y ~ 1 + time0 + tx + time0:tx + (1 + time0 | id),
      cores = 4,
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Review the parameter summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time0 + tx + time0:tx + (1 + time0 | id) 
##    Data: d_long (Number of observations: 80) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 20) 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)            1.09      0.22     0.74     1.60 1.00     1372     2034
## sd(time0)                0.10      0.07     0.01     0.27 1.00     1063     1908
## cor(Intercept,time0)    -0.08      0.51    -0.93     0.91 1.00     4260     2290
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     5.00      0.27     4.47     5.55 1.00     1262     1502
## time0         1.50      0.06     1.38     1.62 1.00     5705     2527
## tx           -0.01      0.53    -1.02     1.05 1.01     1167     1781
## time0:tx      1.00      0.12     0.75     1.25 1.00     3793     2208
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.57      0.06     0.47     0.70 1.00     3132     2986
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything looks fine. If you check them, the trace plots of the chains look good, too&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. If you execute the code below, you’ll see our primary results cohere nicely with the maximum likelihood results from the frequentist &lt;strong&gt;lme4&lt;/strong&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lme4::lmer(data = d_long,
           y ~ 1 + time0 + tx + time0:tx + (1 + time0 | id)) %&amp;gt;% 
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Regardless on whether you focus on the output from &lt;strong&gt;brms&lt;/strong&gt; or &lt;strong&gt;lme4&lt;/strong&gt;, our coefficients will differ a bit from those Feingold reported because of our different scaling of the time variable. But from a high-level perspective, it’s the same model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unstandardized-effect-size.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unstandardized effect size.&lt;/h3&gt;
&lt;p&gt;Our interest lies in the &lt;code&gt;time0:tx&lt;/code&gt; interaction, which is the unstandardized effect size for the “difference between the means of the slopes of the treatment and the control group” (p. 47). You might also describe this as a difference in differences. Here’s a focused summary of that coefficient, which Feingold called &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;time0:tx&amp;quot;, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
## 1.0000705 0.1246595 0.7534974 1.2493457&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since there are three units of time between baseline (&lt;code&gt;time0 == 0&lt;/code&gt;) and the final assessment point (&lt;code&gt;time0 == 3&lt;/code&gt;), we can get the difference in pre/post differences by multiplying that &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt; coefficient by &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;time0:tx&amp;quot;, -2] * 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
## 3.000212 2.260492 3.748037&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thinking back to the original wide-formatted &lt;code&gt;d&lt;/code&gt; data, this value is the multilevel growth model version of the difference in change scores (&lt;code&gt;t4-t1&lt;/code&gt;) in the treatment conditions, &lt;span class=&#34;math inline&#34;&gt;\(M_\text{change-T} - M_\text{change-C}\)&lt;/span&gt;. Here compute that value by hand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group-level change score means
m_change_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t4-t1`)) %&amp;gt;% pull()  # 6
m_change_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t4-t1`)) %&amp;gt;% pull()  # 3

# difference in change score means
m_change_t - m_change_c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the reasons we went through the trouble of fitting a multilevel model is so we could accompany that difference in change scores with high-quality 95% intervals. Here they are in a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(fixef(fit1)[, -2] * 3) %&amp;gt;% 
  rownames_to_column(&amp;quot;coefficient&amp;quot;) %&amp;gt;% 
  filter(coefficient == &amp;quot;time0:tx&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = 0)) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_pointrange(fatten = 1) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(&amp;quot;unstandardized difference in change scores&amp;quot;~(beta[1][1])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig2-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The population average could be anywhere from 2.25 to 3.75, but the best guess is it’s about 3. However, since the metric on this outcome variable is arbitrary (these data were simulated, remember), it’s hard to interpret how “large” this is. A standardized effect size can help.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-to-define-the-standardized-mean-difference-for-the-multilevel-growth-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need to define the standardized mean difference for the multilevel growth model.&lt;/h3&gt;
&lt;p&gt;Based on &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-raudenbushEffectsOfStudyDuration2001&#34; role=&#34;doc-biblioref&#34;&gt;Raudenbush &amp;amp; Liu&lt;/a&gt; (&lt;a href=&#34;#ref-raudenbushEffectsOfStudyDuration2001&#34; role=&#34;doc-biblioref&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt;, Feingold presented two effect-size formulas for our multilevel growth model. The first, which he called &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-change}\)&lt;/span&gt;, is on a completely different scale from any of the effect sizes mentioned in the first post (e.g., &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-change}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d_\text{IGPP-raw}\)&lt;/span&gt;). Importantly, it turns out Raudenbush and Liu recommended their &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-change}\)&lt;/span&gt; formula should be used for power calculations, but not necessarily to convey the magnitude of an effect. Thus we will not consider it further&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Feingold reported the formula for their other effect size was&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_\text{GMA-raw} = \beta_{11}(\text{time}) / SD_\text{raw}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt; in Feingold’s equation is the multilevel interaction term between time and experimental condition–what we just visualized in a coefficient plot. The &lt;span class=&#34;math inline&#34;&gt;\((\text{time})\)&lt;/span&gt; part in the equation is a stand-in for the quantity of time units from the beginning of the study to the end point. Since our multilevel model used the &lt;code&gt;time0&lt;/code&gt; variable, which was &lt;code&gt;0&lt;/code&gt; at baseline and &lt;code&gt;3&lt;/code&gt; at the final time point, we would enter a 3 into the equation (i.e., &lt;span class=&#34;math inline&#34;&gt;\(3 - 0 = 3\)&lt;/span&gt;). The part of Feingold’s equation that’s left somewhat vague is what he meant by the denominator, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw}\)&lt;/span&gt;. On page 47, he used the value of 1.15 in his example. Without any reference to experimental condition in the subscript, one might assume that value is the standard deviation for the criterion across all time points or, perhaps, just at baseline. It turns out that’s not the case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# standard deviation for the criterion across all time points
d_long %&amp;gt;% 
  summarise(sd = sd(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##      sd
##   &amp;lt;dbl&amp;gt;
## 1  2.22&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# standard deviation for the criterion at baseline
d_long %&amp;gt;% 
  filter(time == 1) %&amp;gt;% 
  summarise(sd = sd(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##      sd
##   &amp;lt;dbl&amp;gt;
## 1  1.12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this particular data set, the value Feingold used is the same as the standard deviation for either of the experimental conditions at baseline.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_raw_pre_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(s = sd(t1)) %&amp;gt;% pull()  # treatment baseline SD
sd_raw_pre_c &amp;lt;- filter(d, tx == &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(s = sd(t1)) %&amp;gt;% pull()  # control baseline SD

sd_raw_pre_c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_raw_pre_t&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But since he didn’t use a subscript, I suspect Feingold meant to convey a pooled standard deviation, following the equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SD_\text{pooled} = \sqrt{\frac{SD_\text{raw(pre-T)}^2 + SD_\text{raw(pre-C)}^2}{2}},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is a sample version of Cohen’s original equation 2.3.2 &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;1988, p. 44&lt;/a&gt;)&lt;/span&gt;. Here’s how to compute the pooled standard deviation by hand, which we’ll save as &lt;code&gt;sd_raw_pre_p&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_raw_pre_p &amp;lt;- sqrt((sd_raw_pre_c^2 + sd_raw_pre_t^2) / 2)
sd_raw_pre_p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since Feingold’s synthetic data are a special case where &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-T)} = SD_\text{raw(pre-C)} = SD_\text{pooled}\)&lt;/span&gt;, these distinctions might all seem dull and pedantic. Yet if your real-world data look anything like mine, this won’t be the case and you’ll need to understand how distinguish between and choose from among these options.&lt;/p&gt;
&lt;p&gt;Another thing to consider is that whereas Feingold’s synthetic data have the desirable quality where the sample sizes are the same across the experimental conditions (&lt;span class=&#34;math inline&#34;&gt;\(n_\text{T} = n_\text{C} = 10\)&lt;/span&gt;), this won’t always be the case. If you end up with unbalanced experimental data, you might consider the sample-size weighted pooled standard deviation, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}^*\)&lt;/span&gt;, which I believe has its origins in Hedges’ work &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hedgesDistributionTheoryforGlass1981&#34; role=&#34;doc-biblioref&#34;&gt;1981, p. 110&lt;/a&gt;)&lt;/span&gt;. It follows the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SD_\text{pooled}^* = \sqrt{\frac{(n_\text{T} - 1)SD_\text{raw(pre-T)}^2 + (n_\text{C} - 1)SD_\text{raw(pre-C)}^2}{n_\text{T} + n_\text{C} - 2}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here it is for Feingold’s data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the sample sizes
n_t &amp;lt;- 10
n_c &amp;lt;- 10

# compute the sample size robust pooled SD
sqrt(((n_t - 1) * sd_raw_pre_c^2 + (n_c - 1) * sd_raw_pre_t^2) / (n_t + n_c - 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.154701&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, in the special case of these synthetic data, &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}^*\)&lt;/span&gt; happens to be the same value as &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}\)&lt;/span&gt;, which is also the same value as &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-T)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt;. This will not always the case with your real-world data. Choose your &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; with care and make sure to report which ever formula you use. Don’t be coy with your effect-size calculations.&lt;/p&gt;
&lt;p&gt;You may be wondering, though, whether you can use the standard deviations for one of the treatment conditions rather than a variant of the pooled standard deviation. &lt;em&gt;Yes&lt;/em&gt;, you can. I think Cumming &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cummingUnderstandingTheNewStatistics2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;, Chapter 11)&lt;/span&gt; did a nice job walking through this issue. For example, if we thought of our control condition as a true benchmark for what we’d expect at baseline, we could just use &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt; as our standardizer. This is sometimes referred to as a Glass’ &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; or Glass’ &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt;. Whatever you choose and whatever you call it, just make sure to clearly define your standardizing formula for your audience.&lt;/p&gt;
&lt;p&gt;Therefore, if we use &lt;span class=&#34;math inline&#34;&gt;\(SD_\text{raw(pre-C)}\)&lt;/span&gt; (&lt;code&gt;sd_raw_pre_c&lt;/code&gt;) as our working value, we can compute &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-raw}\)&lt;/span&gt; as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;time0:tx&amp;quot;, 1] * 3 / sd_raw_pre_c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.598259&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within the Bayesian framework, we can get a full posterior distribution for the standardized version of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-raw}\)&lt;/span&gt;, by working directly with all the posterior draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit1) %&amp;gt;% 
  mutate(d = `b_time0:tx` * 3 / sd_raw_pre_p) %&amp;gt;% 
  
  ggplot(aes(x = d, y = 0)) +
  geom_vline(xintercept = 0, linetype = 2) +
  stat_halfeye(.width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(italic(d)[GMA-raw]~(&amp;quot;standardized difference in change scores&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig3-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The population average could be anywhere from 2 to 3.25, but the best guess is it’s about 2.5. In my field (clinical psychology), this would be considered a very large effect size. Anyway, here are the numeric values for the posterior median and percentile-based 95% interval.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit1) %&amp;gt;% 
  mutate(d = `b_time0:tx` * 3 / sd_raw_pre_p) %&amp;gt;% 
  median_qi(d) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     d .lower .upper .width .point .interval
## 1 2.6   1.96   3.25   0.95 median        qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to compute this is to work with the model formula and the posterior samples from the fixed effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit1) %&amp;gt;% 
  # simplify the output
  select(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  # compute the treatment-level means for pre and post
  mutate(m_pre_t  = b_Intercept + b_time0 * 0 + b_tx *  0.5 + `b_time0:tx`* 0 *  0.5,
         m_pre_c  = b_Intercept + b_time0 * 0 + b_tx * -0.5 + `b_time0:tx`* 0 * -0.5,
         m_post_t = b_Intercept + b_time0 * 3 + b_tx *  0.5 + `b_time0:tx`* 3 *  0.5,
         m_post_c = b_Intercept + b_time0 * 3 + b_tx * -0.5 + `b_time0:tx`* 3 * -0.5) %&amp;gt;% 
  # compute the treatment-level change scores
  mutate(m_change_t = m_post_t - m_pre_t,
         m_change_c = m_post_c - m_pre_c) %&amp;gt;% 
  # compute the difference of differences
  mutate(beta_11 = m_change_t - m_change_c) %&amp;gt;% 
  # compute the multilevel effect size
  mutate(d_GAM_raw = beta_11 / sd_raw_pre_c) %&amp;gt;% 
  # wrangle and summarize
  pivot_longer(m_pre_t:d_GAM_raw) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  mean_qi(value) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 7
##   name       value .lower .upper .width .point .interval
##   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
## 1 beta_11     3      2.26   3.75   0.95 mean   qi       
## 2 d_GAM_raw   2.6    1.96   3.25   0.95 mean   qi       
## 3 m_change_c  3      2.48   3.53   0.95 mean   qi       
## 4 m_change_t  6      5.49   6.53   0.95 mean   qi       
## 5 m_post_c    8.01   7.26   8.8    0.95 mean   qi       
## 6 m_post_t   11     10.2   11.8    0.95 mean   qi       
## 7 m_pre_c     5.01   4.26   5.77   0.95 mean   qi       
## 8 m_pre_t     5      4.27   5.75   0.95 mean   qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how the summary values in the rows for &lt;code&gt;beta_11&lt;/code&gt; and &lt;code&gt;d_GAM_raw&lt;/code&gt; match up with those we computed, above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;you-may-want-options.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;You may want options.&lt;/h3&gt;
&lt;p&gt;Turns out there’s an other way to compute the standardized mean difference for experimental longitudinal data. You can just fit the model to the standardized data. As with our approach, above, the trick is to make sure you standardized the data with a defensible standardizer. I recommend you default to the pooled standard deviation at baseline (&lt;span class=&#34;math inline&#34;&gt;\(SD_\text{pooled}\)&lt;/span&gt;). To do so, we first compute the weighted mean at baseline.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group-level baseline means
m_raw_pre_t &amp;lt;- filter(d, tx ==  &amp;quot;0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t1`)) %&amp;gt;% pull()
m_raw_pre_c &amp;lt;- filter(d, tx ==  &amp;quot;-0.5&amp;quot;) %&amp;gt;% summarise(m = mean(`t1`)) %&amp;gt;% pull()

# weighted (pooled) baseline mean
m_raw_pre_p &amp;lt;- (m_raw_pre_t * n_t + m_raw_pre_c * n_c) / (n_t + n_c)

m_raw_pre_p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next use the weighted baseline mean and the pooled baseline standard deviation to standardize the data, saving the results as &lt;code&gt;z&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_long &amp;lt;-
  d_long %&amp;gt;% 
  mutate(z = (y - m_raw_pre_p) / sd_raw_pre_p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now just fit a multilevel growth model with our new standardized variable &lt;code&gt;z&lt;/code&gt; as the criterion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;-
  brm(data = d_long,
      family = gaussian,
      z ~ 1 + time0 + tx + time0:tx + (1 + time0 | id),
      cores = 4,
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the parameter summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: z ~ 1 + time0 + tx + time0:tx + (1 + time0 | id) 
##    Data: d_long (Number of observations: 80) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 20) 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)            0.95      0.20     0.63     1.42 1.00     1293     1768
## sd(time0)                0.08      0.06     0.00     0.23 1.00      937     1827
## cor(Intercept,time0)    -0.07      0.51    -0.90     0.91 1.00     4454     2198
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.01      0.24    -0.48     0.45 1.00     1000     1533
## time0         1.30      0.05     1.19     1.41 1.00     5287     2785
## tx           -0.01      0.46    -0.93     0.93 1.00     1069     1714
## time0:tx      0.87      0.11     0.66     1.08 1.00     4651     2963
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.49      0.05     0.41     0.59 1.00     2769     2610
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, our focal parameter is &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit2)[&amp;quot;time0:tx&amp;quot;, -2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate      Q2.5     Q97.5 
## 0.8688574 0.6575292 1.0765760&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But since our data are coded such that baseline is &lt;code&gt;time0 == 0&lt;/code&gt; and the final time point is &lt;code&gt;time0 == 3&lt;/code&gt;, we’ll need to multiply that coefficient by 3 to get the effect size in the pre/post metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit2)[&amp;quot;time0:tx&amp;quot;, -2] * 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
## 2.606572 1.972588 3.229728&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is it, within simulation variance of the effect size from the last section. Let’s compare them with a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(fixef(fit1)[&amp;quot;time0:tx&amp;quot;, -2] * 3 / sd_raw_pre_p,
      fixef(fit2)[&amp;quot;time0:tx&amp;quot;, -2] * 3) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  mutate(data = c(&amp;quot;unstandardized data&amp;quot;, &amp;quot;standardized data&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = data)) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_pointrange(fatten = 1) +
  labs(x = expression(italic(d)[GMA-raw]~(&amp;quot;standardized difference in change scores&amp;quot;)),
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-04-22-effect-sizes-for-experimental-trials-analyzed-with-multilevel-growth-models-two-of-two/index_files/figure-html/fig4-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yep, they’re pretty much the same.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sum-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sum up&lt;/h2&gt;
&lt;p&gt;Yes, one can compute a standardized mean difference effect size for experimental data analyzed with a multilevel growth model. The focal parameter is the treatment-time interaction, what we called &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt;. The trick is to divide that parameter by the pooled standard deviation at baseline. This will put the effect size, what Feingold called &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-raw}\)&lt;/span&gt;, into a conventional Cohen’s-&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-type metric. But be mindful that this method may require you to multiply the effect by a number that corrects for how you have scaled the time variable. In the example we worked through, we multiplied by 3.&lt;/p&gt;
&lt;p&gt;As an alternative workflow, you can also fit the model on data that were standardized using the pooled standard deviation at baseline. This will automatically put the &lt;span class=&#34;math inline&#34;&gt;\(\beta_{11}\)&lt;/span&gt; in the effect-size metric. But as with the other method, you still might have to correct for how you scaled the time variable.&lt;/p&gt;
&lt;p&gt;Though we’re not covering it, here, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-feingoldARegressionFramework2013&#34; role=&#34;doc-biblioref&#34;&gt;Feingold&lt;/a&gt; (&lt;a href=&#34;#ref-feingoldARegressionFramework2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; extended this framework to other contexts. For example, he discussed how to apply it to data with nonlinear trends and to models with other covariates. Just know the foundation is right here:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_\text{GMA-raw} = \beta_{11}(\text{time}) / SD_\text{raw}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0   
##  [8] ggplot2_3.3.3   tidyverse_1.3.0 tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6     
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6         splines_4.0.4       
##   [6] svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17       
##  [11] digest_0.6.27        htmltools_0.5.1.1    rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000    haven_2.3.1         
##  [26] xfun_0.22            callr_3.5.1          crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1     
##  [36] V8_3.4.0             distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1       viridisLite_0.3.0   
##  [46] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16              htmlwidgets_1.5.2   
##  [51] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.1       pkgconfig_2.0.3     
##  [56] loo_2.4.1            farver_2.0.3         dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2      
##  [61] tidyselect_1.1.0     rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [66] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0       broom_0.7.5         
##  [71] ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1        yaml_2.2.1           fs_1.5.0            
##  [76] processx_3.4.5       knitr_1.31           nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [81] xml2_1.3.2           rstudioapi_0.13      compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [86] curl_4.3             gamm4_0.2-6          reprex_0.3.0         statmod_1.4.35       stringi_1.5.3       
##  [91] highr_0.8            ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41     
##  [96] Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
## [101] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [106] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3        codetools_0.2-18    
## [111] boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1    
## [116] withr_2.4.1          shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [126] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brms2021RM&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt; reference manual, &lt;span&gt;Version&lt;/span&gt; 2.15.0&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/brms.pdf&#34;&gt;https://CRAN.R-project.org/package=brms/brms.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cohenStatisticalPowerAnalysis1988a&#34; class=&#34;csl-entry&#34;&gt;
Cohen, J. (1988). &lt;em&gt;Statistical power analysis for the behavioral sciences&lt;/em&gt;. &lt;span&gt;L. Erlbaum Associates&lt;/span&gt;. &lt;a href=&#34;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&#34;&gt;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummingUnderstandingTheNewStatistics2012&#34; class=&#34;csl-entry&#34;&gt;
Cumming, G. (2012). &lt;em&gt;Understanding the new statistics: &lt;span&gt;Effect&lt;/span&gt; sizes, confidence intervals, and meta-analysis&lt;/em&gt;. &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&#34;&gt;https://www.routledge.com/Understanding-The-New-Statistics-Effect-Sizes-Confidence-Intervals-and/Cumming/p/book/9780415879682&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-feingoldEffectSizeForGMA2009&#34; class=&#34;csl-entry&#34;&gt;
Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(1), 43. &lt;a href=&#34;https://doi.org/10.1037/a0014699&#34;&gt;https://doi.org/10.1037/a0014699&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-feingoldARegressionFramework2013&#34; class=&#34;csl-entry&#34;&gt;
Feingold, A. (2013). A regression framework for effect size assessments in longitudinal modeling of group differences. &lt;em&gt;Review of General Psychology&lt;/em&gt;, &lt;em&gt;17&lt;/em&gt;(1), 111–121. &lt;a href=&#34;https://doi.org/10.1037/a0030048&#34;&gt;https://doi.org/10.1037/a0030048&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hedgesDistributionTheoryforGlass1981&#34; class=&#34;csl-entry&#34;&gt;
Hedges, L. V. (1981). Distribution theory for &lt;span&gt;Glass&lt;/span&gt;’s estimator of effect size and related estimators. &lt;em&gt;Journal of Educational Statistics&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(2), 107–128. &lt;a href=&#34;https://doi.org/10.3102/10769986006002107&#34;&gt;https://doi.org/10.3102/10769986006002107&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hoffmanLongitudinalAnalysisModeling2015&#34; class=&#34;csl-entry&#34;&gt;
Hoffman, L. (2015). &lt;em&gt;Longitudinal analysis: &lt;span&gt;Modeling&lt;/span&gt; within-person fluctuation and change&lt;/em&gt; (1 edition). &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&#34;&gt;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-raudenbushHLM2002&#34; class=&#34;csl-entry&#34;&gt;
Raudenbush, S. W., &amp;amp; Bryk, A. S. (2002). &lt;em&gt;Hierarchical linear models: &lt;span&gt;Applications&lt;/span&gt; and data analysis methods&lt;/em&gt; (Second Edition). &lt;span&gt;SAGE Publications, Inc&lt;/span&gt;. &lt;a href=&#34;https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230&#34;&gt;https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-raudenbushEffectsOfStudyDuration2001&#34; class=&#34;csl-entry&#34;&gt;
Raudenbush, S. W., &amp;amp; Liu, X.-F. (2001). Effects of study duration, frequency of observation, and sample size on power in studies of group differences in polynomial change. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(4), 387. &lt;a href=&#34;https://doi.org/10.1037/1082-989X.6.4.387&#34;&gt;https://doi.org/10.1037/1082-989X.6.4.387&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-singerAppliedLongitudinalData2003&#34; class=&#34;csl-entry&#34;&gt;
Singer, J. D., &amp;amp; Willett, J. B. (2003). &lt;em&gt;Applied longitudinal data analysis: &lt;span&gt;Modeling&lt;/span&gt; change and event occurrence&lt;/em&gt;. &lt;span&gt;Oxford University Press, USA&lt;/span&gt;. &lt;a href=&#34;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you’re curious about our priors, fit the models on your computer and then execute &lt;code&gt;fit1$prior&lt;/code&gt;. To learn more about &lt;strong&gt;brms&lt;/strong&gt; default priors, spend some time with the &lt;a href=&#34;https://CRAN.R-project.org/package=brms/brms.pdf&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; reference manual&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brms2021RM&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2021&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;If you’re not into the whole Bayesian framework I’m using, you can just ignore the part about trace plots and chains. If you’re into it, execute &lt;code&gt;plot(fit1)&lt;/code&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Really. If you are interested in communicating your research results to others, do not mess with the &lt;span class=&#34;math inline&#34;&gt;\(d_\text{GMA-change}\)&lt;/span&gt;. It’s on a totally different metric from the conventional Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and you’ll just end up confusing people.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Regression models for 2-timepoint non-experimental data</title>
      <link>/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/</link>
      <pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/</guid>
      <description>
&lt;script src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;purpose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Purpose&lt;/h2&gt;
&lt;p&gt;In the contemporary longitudinal data analysis literature, 2-timepoint data (a.k.a. pre/post data) get a bad wrap. Singer and Willett &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003, p. 10&lt;/a&gt;)&lt;/span&gt; described 2-timepoint data as only “marginally better” than cross-sectional data and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-rogosaGrowthCurveApproach1982&#34; role=&#34;doc-biblioref&#34;&gt;Rogosa et al.&lt;/a&gt; (&lt;a href=&#34;#ref-rogosaGrowthCurveApproach1982&#34; role=&#34;doc-biblioref&#34;&gt;1982&lt;/a&gt;)&lt;/span&gt; give a technical overview on the limitations of 2-timepoint data. Limitations aside, sometimes two timepoints are all you have. In those cases, researchers should have a good sense of which data analysis options they have at their disposal. I recently came across &lt;a href=&#34;https://twitter.com/jwalkrunski&#34;&gt;Jeffrey Walker&lt;/a&gt;’s free &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-walkerElementsOfStatisticalModeling2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; text, &lt;a href=&#34;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/&#34;&gt;&lt;em&gt;Elements of statistical modeling for experimental biology&lt;/em&gt;&lt;/a&gt;, which contains a &lt;a href=&#34;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/models-for-longitudinal-experiments-pre-post-designs.html&#34;&gt;nice chapter&lt;/a&gt; on 2-timepoint experimental designs. Inspired by his work, this post aims to explore how one might analyze &lt;em&gt;non-experimental&lt;/em&gt; 2-timepoint data within a regression model paradigm.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;In this post, I’m presuming you are familiar with longitudinal data analysis with conventional and multilevel regression. Though I don’t emphasize it much, it will also help if you’re familiar with Bayesian statistics. All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. The statistical models will be fit with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and we’ll also make some use of the &lt;strong&gt;tidybayes&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;patchwork&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-patchwork&#34; role=&#34;doc-biblioref&#34;&gt;Pedersen, 2019&lt;/a&gt;)&lt;/span&gt; packages. If you need to shore up, I list some educational resources at the &lt;a href=&#34;#next-steps&#34;&gt;end of the post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Load the primary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;warm-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Warm-up&lt;/h2&gt;
&lt;p&gt;Before we jump into 2-timepoint data, we’ll first explore how one might analyze a fuller data set of 6 timepoints. We will then reduce the data set to two different 2-timepoint versions for use in the remainder of the post.&lt;/p&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;We will simulate the data based on a conventional multilevel growth model of the kind you can learn about in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett&lt;/a&gt; (&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;, or Kurz &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, Chapter 14)&lt;/span&gt;. We’ll have one criterion variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; which will vary across participants &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and over time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. For simplicity, the systemic change over time will be linear. We might express it in statistical notation&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} + u_{0i} + u_{1i} \text{time}_{ti} \\
\sigma &amp;amp; = \sigma_\epsilon \\
\begin{bmatrix} u_{0i} \\ u_{1i} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf \Sigma \right) \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_1 \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the population-level intercept (initial status) and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the population-level slope (change over time). The &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}\)&lt;/span&gt; terms are the participant-level deviations around the population-level intercept and slope. Those &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; deviations follow a bivariate normal distribution centered on zero (they are deviations, after all) and including a covariance matrix, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt;. As is typical within the &lt;strong&gt;brms&lt;/strong&gt; framework &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; is decomposed into a matrix of standard deviations (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf S\)&lt;/span&gt;) and a correlation matrix (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf R\)&lt;/span&gt;). Also notice we renamed our original &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameter as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt; to help distinguish it from the multilevel standard deviations in the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S\)&lt;/span&gt; matrix (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt;). In this way, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt; capture differences &lt;em&gt;between&lt;/em&gt; participants in their intercepts and slopes, whereas &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt; captures the differences &lt;em&gt;within&lt;/em&gt; participants over time that occur apart from their linear trajectories.&lt;/p&gt;
&lt;p&gt;To simulate data of this kind, we’ll first set the true values for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0, \beta_1, \sigma_0, \sigma_1, \rho,\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b0     &amp;lt;- 0      # starting point (average intercept)
b1     &amp;lt;- 1      # growth over time (average slope)
sigma0 &amp;lt;- 1      # std dev in intercepts
sigma1 &amp;lt;- 1      # std dev in slopes
rho    &amp;lt;- -.5    # correlation between intercepts and slopes
sigma_e &amp;lt;- 0.75  # std dev within participants&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now combine several of those values to define the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S, \mathbf R,\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; matrices. Then simulate &lt;span class=&#34;math inline&#34;&gt;\(N = 100\)&lt;/span&gt; participant-level intercepts and slopes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu     &amp;lt;- c(b0, b1)          # combine the means in a vector
sigmas &amp;lt;- c(sigma0, sigma1)  # combine the std devs in a vector

s &amp;lt;- diag(sigmas)      # standard deviation matrix
r &amp;lt;- matrix(c(1, rho,  # correlation matrix
             rho, 1), nrow = 2)

# now matrix multiply s and r to get a covariance matrix
sigma &amp;lt;- s %*% r %*% s

# how many participants would you like?
n_id &amp;lt;- 100

# make the simulation reproducible
set.seed(1)

vary_effects &amp;lt;- 
  MASS::mvrnorm(n_id, mu = mu, Sigma = sigma) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  set_names(&amp;quot;intercepts&amp;quot;, &amp;quot;slopes&amp;quot;) %&amp;gt;% 
  mutate(id = 1:n_id) %&amp;gt;% 
  select(id, everything())

head(vary_effects)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id  intercepts     slopes
## 1  1  0.85270825  0.7676584
## 2  2 -0.18009772  1.1379818
## 3  3  1.17913643  0.7317852
## 4  4 -1.46056809  2.3025393
## 5  5  0.04193022  1.6126544
## 6  6 -0.17309717 -0.5941901&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have our random intercepts and slopes, we’re almost ready to simulate our &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; values. We just need to decide on how many values we’d like to collect over time and how we’d like to structure those assessment periods. To keep things simple, I’m going to specify six evenly-spaced timepoints. The first timepoint will be set to 0, the last timepoint will be set to 1, and the four timepoints in the middle will be the corresponding fractions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many timepoints?
time_points &amp;lt;- 6

d &amp;lt;-
  vary_effects %&amp;gt;% 
  # add in time
  expand(nesting(id, intercepts, slopes),
         time = seq(from = 0, to = 1, length.out = time_points)) %&amp;gt;% 
  # now use the model formula to compute y
  mutate(y = rnorm(n(), mean = intercepts + slopes * time, sd = sigma_e))

head(d, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 5
##       id intercepts slopes  time      y
##    &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1     1      0.853  0.768   0    1.16 
##  2     1      0.853  0.768   0.2  2.27 
##  3     1      0.853  0.768   0.4  2.35 
##  4     1      0.853  0.768   0.6  1.07 
##  5     1      0.853  0.768   0.8 -0.247
##  6     1      0.853  0.768   1    3.49 
##  7     2     -0.180  1.14    0    0.320
##  8     2     -0.180  1.14    0.2  0.453
##  9     2     -0.180  1.14    0.4  0.265
## 10     2     -0.180  1.14    0.6  0.885&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we move on, I should acknowledge that this simulation workflow is heavily influenced by McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, Chapter 14)&lt;/span&gt;. You can find a similar workflow in the &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-debruineUnderstandingMixedEffects2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; preprint by DeBruine and Barr, &lt;a href=&#34;https://psyarxiv.com/xp5cy/&#34;&gt;&lt;em&gt;Understanding mixed effects models through data simulation&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explore-the-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explore the data.&lt;/h3&gt;
&lt;p&gt;Before fitting the model, it might help if we look at what we’ve done. Here’s a scatter plot of the random intercepts and slopes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set the global plotting theme
theme_set(theme_linedraw() +
            theme(text = element_text(family = &amp;quot;Times&amp;quot;),
                  panel.grid = element_blank()))

p1 &amp;lt;-
  vary_effects %&amp;gt;% 
  ggplot(aes(x = intercepts, y = slopes)) +
  geom_point() +
  stat_ellipse(color = &amp;quot;grey50&amp;quot;)

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The 95%-interval ellipse helps point out the negative correlation between the intercepts and slopes. Here’s the Pearson’s correlation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vary_effects %&amp;gt;% 
  summarise(rho = cor(intercepts, slopes))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          rho
## 1 -0.4502206&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s no coincidence that value is very close to our data-generating &lt;code&gt;rho&lt;/code&gt; value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rho&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now check the sample means and standard deviations of our &lt;code&gt;intercepts&lt;/code&gt; and &lt;code&gt;slopes&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vary_effects %&amp;gt;% 
  summarise(b0 = mean(intercepts),
            b1 = mean(slopes),
            sigma0 = sd(intercepts),
            sigma1 = sd(slopes)) %&amp;gt;% 
  mutate_all(round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      b0   b1 sigma0 sigma1
## 1 -0.08 1.11   0.91   0.91&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those aren’t quite the true data-generating values for &lt;code&gt;b0&lt;/code&gt; through &lt;code&gt;sigma1&lt;/code&gt;, from above. But they’re pretty decent sample approximations. With only &lt;span class=&#34;math inline&#34;&gt;\(N = 100\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T = 6\)&lt;/span&gt;, this is about as close as we should expect.&lt;/p&gt;
&lt;p&gt;To get a sense of the &lt;code&gt;time&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values, we’ll plot them in two ways. First we’ll plot a random subset from nine of our simulated participants. Then we’ll plot the linear trajectories from all 100 participants, along with the grand mean trajectory.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

p2 &amp;lt;-
  d %&amp;gt;% 
  nest(data = c(intercepts, slopes, time, y)) %&amp;gt;% 
  slice_sample(n = 9) %&amp;gt;% 
  unnest(data) %&amp;gt;% 
  
  ggplot(aes(x = time, y = y)) +
  geom_point() +
  geom_abline(aes(intercept = intercepts, slope = slopes),
              color = &amp;quot;blue&amp;quot;) +
  labs(subtitle = &amp;quot;random subset of 9 participants&amp;quot;) +
  theme(strip.background = element_blank(),
        strip.text = element_blank()) +
  facet_wrap(~slopes)

p3 &amp;lt;-
  d %&amp;gt;% 
  ggplot(aes(x = time, y = y)) +
  geom_point(color = &amp;quot;transparent&amp;quot;) +
  geom_abline(aes(intercept = intercepts, slope = slopes, group = id),
              color = &amp;quot;blue&amp;quot;, size = 1/10, alpha = 1/2) +
  geom_abline(intercept = b0, slope = b1,
              color = &amp;quot;blue&amp;quot;, size = 2) +
  labs(subtitle = &amp;quot;All participant-level trajectories, along\nwith the grand mean&amp;quot;)

# combine
(p2 + p3) &amp;amp;
  scale_x_continuous(breaks = 0:5 / 5, labels = c(0, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, 1)) &amp;amp;
  coord_cartesian(ylim = c(-2.5, 3.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;See how the points in the plots on the left deviate quite a bit from their linear trajectories? That’s the result of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-multilevel-growth-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the multilevel growth model.&lt;/h3&gt;
&lt;p&gt;Now we’ll use &lt;strong&gt;brms&lt;/strong&gt; to fit the multilevel growth model. If all goes well, we should largely reproduce our data-generating values in the posterior. Before fitting the model, we should consider a few things about the &lt;code&gt;brm()&lt;/code&gt; syntax.&lt;/p&gt;
&lt;p&gt;In this model and in most of the models to follow, we’re relying on the default &lt;code&gt;brm()&lt;/code&gt; priors. When fitting real-world models, you are much better off going beyond the defaults. However, I will generally deemphasize priors, in this post, to help keep the focus on the conceptual models.&lt;/p&gt;
&lt;p&gt;Note how we set the &lt;code&gt;seed&lt;/code&gt; argument. Though you don’t need to do this, setting the &lt;code&gt;seed&lt;/code&gt; makes the results more reproducible.&lt;/p&gt;
&lt;p&gt;Also, note the custom settings for &lt;code&gt;iter&lt;/code&gt; and &lt;code&gt;warmup&lt;/code&gt;. Often times, the default settings are fine. But since we’ll be comparing a lot of models, I want to make sure we have enough posterior draws from each to ensure stable estimates.&lt;/p&gt;
&lt;p&gt;Okay, fit the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m0 &amp;lt;-
  brm(data = d,
      y ~ 1 + time + (1 + time | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time + (1 + time | id) 
##    Data: d (Number of observations: 600) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)           0.89      0.09     0.72     1.07 1.00     3663     5989
## sd(time)                0.84      0.15     0.53     1.14 1.00     1782     2936
## cor(Intercept,time)    -0.40      0.15    -0.65    -0.05 1.00     3758     4228
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.13      0.11    -0.33     0.08 1.00     4153     6181
## time          1.16      0.13     0.91     1.42 1.00     6636     6477
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.81      0.03     0.76     0.87 1.00     4706     5720
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A series of plots might help show how well our model captured the data-generating values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# name the parameters with the Greek terms
names &amp;lt;- c(&amp;quot;beta[0]&amp;quot;, &amp;quot;beta[1]&amp;quot;, &amp;quot;sigma[0]&amp;quot;, &amp;quot;sigma[1]&amp;quot;, &amp;quot;rho&amp;quot;, &amp;quot;sigma[epsilon]&amp;quot;)

# for the vertical lines marking off the true values
vline &amp;lt;-
  tibble(name = names,
         true_value = c(b0, b1, sigma0, sigma1, rho, sigma_e))

# wrangle
posterior_samples(m0) %&amp;gt;% 
  select(b_Intercept:sigma) %&amp;gt;% 
  set_names(names) %&amp;gt;% 
  pivot_longer(everything()) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(.width = .95, normalize = &amp;quot;panels&amp;quot;, size = 1/2) +
  geom_vline(data = vline,
             aes(xintercept = true_value),
             size = 1/4, linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_wrap(~name, scales = &amp;quot;free&amp;quot;, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The marginal posterior distribution for all the major summary parameters is summarized by the median (dot) and percentile-based 95% interval (horizontal line). The true values are shown in the dashed vertical lines. Overall, we did okay.&lt;/p&gt;
&lt;p&gt;As fun as this has all been, we’ve just been warming up.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;make-the-2-timepoint-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Make the 2-timepoint data.&lt;/h3&gt;
&lt;p&gt;Before fitting the 2-timepoint longitudinal models, we’ll need to adjust the data, which currently contains values over six timepoints. Since it’s easy to think of 2-timepoint data in terms of pre and post, we’ll keep the data points for which &lt;code&gt;time == 0&lt;/code&gt; and &lt;code&gt;time == 1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_long &amp;lt;-
  d %&amp;gt;% 
  filter(time == 0 | time == 1) %&amp;gt;% 
  select(-intercepts, -slopes) %&amp;gt;% 
  mutate(`pre/post` = factor(if_else(time == 0, &amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;),
                             levels = c(&amp;quot;pre&amp;quot;, &amp;quot;post&amp;quot;))) 

head(small_data_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##      id  time     y `pre/post`
##   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;     
## 1     1     0 1.16  pre       
## 2     1     1 3.49  post      
## 3     2     0 0.320 pre       
## 4     2     1 1.27  post      
## 5     3     0 0.879 pre       
## 6     3     1 0.971 post&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the name implies, the &lt;code&gt;small_data_long&lt;/code&gt; data are still in the long format. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; described this as the &lt;em&gt;stacked format&lt;/em&gt; and Singer and Willett &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt; called this a &lt;em&gt;person-period data set&lt;/em&gt;. Each level of &lt;code&gt;id&lt;/code&gt; has two rows, one for each level of &lt;code&gt;time&lt;/code&gt;, which is an explicit variable. In this formulation, &lt;code&gt;time == 0&lt;/code&gt; is the same as the “pre” timepoint and &lt;code&gt;time == 1&lt;/code&gt; is the same as “post.” To help clarify that, we added a &lt;code&gt;pre/post&lt;/code&gt; column.&lt;/p&gt;
&lt;p&gt;We’ll need a second variant of this data set, this time in the wide format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide &amp;lt;-
  small_data_long %&amp;gt;% 
  select(-time) %&amp;gt;% 
  pivot_wider(names_from = `pre/post`, values_from = y) %&amp;gt;% 
  mutate(change = post - pre)

head(small_data_wide)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##      id    pre   post change
##   &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1     1  1.16   3.49  2.33  
## 2     2  0.320  1.27  0.953 
## 3     3  0.879  0.971 0.0920
## 4     4 -0.979  0.586 1.57  
## 5     5 -0.825  1.85  2.68  
## 6     6 -0.912 -0.841 0.0715&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With our &lt;code&gt;small_data_wide&lt;/code&gt; data, each level of &lt;code&gt;id&lt;/code&gt; only has one row. The time-structured &lt;code&gt;y&lt;/code&gt; column was broken up into a &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; column, and we no longer have a variable explicitly defining &lt;em&gt;time&lt;/em&gt;. We have a new column, &lt;code&gt;change&lt;/code&gt;, which is the result of subtracting &lt;code&gt;pre&lt;/code&gt; from &lt;code&gt;post&lt;/code&gt;. In her text, Hoffman referred to this type of data structure as the &lt;em&gt;multivariate format&lt;/em&gt; and Singer and Willett called it a &lt;em&gt;person-level data set&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The information is essentially the same in these two data sets, &lt;code&gt;small_data_long&lt;/code&gt; and &lt;code&gt;small_data_wide&lt;/code&gt;. Yet, the models supported by them will provide different insights.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;timepoint-longitudinal-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2-timepoint longitudinal models&lt;/h2&gt;
&lt;p&gt;Before we start fitting and interpreting models, we should prepare ourselves with an overview.&lt;/p&gt;
&lt;div id=&#34;overview.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview.&lt;/h3&gt;
&lt;p&gt;We will consider 20 ways to fit models based on 2-timepoint data. It seems like there multiple ways to categorize these. Here we’ll break them up into four groupings.&lt;/p&gt;
&lt;p&gt;The first four model types will take &lt;code&gt;post&lt;/code&gt; as the criterion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_1 \colon\)&lt;/span&gt; The unconditional post model (&lt;code&gt;post ~ 1&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_2 \colon\)&lt;/span&gt; The simple autoregressive model (&lt;code&gt;post ~ 1 + pre&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_3 \colon\)&lt;/span&gt; The bivariate autoregressive model (&lt;code&gt;bf(post ~ 1 + pre) + bf(pre ~ 1) + set_rescor(rescor = FALSE)&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_4 \colon\)&lt;/span&gt; The bivariate correlational pre/post model (&lt;code&gt;bf(post ~ 1) + bf(pre ~ 1) + set_rescor(rescor = TRUE)&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next four model types will take &lt;code&gt;change&lt;/code&gt; as the criterion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_5 \colon\)&lt;/span&gt; The unconditional change-score model (&lt;code&gt;change ~ 1&lt;/code&gt;) and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_6 \colon\)&lt;/span&gt; The conditional change-score model (&lt;code&gt;change ~ 1 + pre&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_7 \colon\)&lt;/span&gt; The bivariate conditional change-score model (&lt;code&gt;bf(change ~ 1 + pre) + bf(pre ~ 1) + set_rescor(rescor = FALSE)&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_8 \colon\)&lt;/span&gt; The bivariate correlational pre/change model (&lt;code&gt;bf(change ~ 1) + bf(pre ~ 1) + set_rescor(rescor = TRUE)&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next eight model types will take &lt;code&gt;y&lt;/code&gt; as the criterion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_9 \colon\)&lt;/span&gt; The grand-mean model (&lt;code&gt;y ~ 1&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{10} \colon\)&lt;/span&gt; The random-intercept model (&lt;code&gt;y ~ 1 + (1 | id)&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{11} \colon\)&lt;/span&gt; The cross-classified model (&lt;code&gt;y ~ 1 + (1 | id) + (1 | time)&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{12} \colon\)&lt;/span&gt; The simple liner model (&lt;code&gt;y ~ 1 + time&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{13} \colon\)&lt;/span&gt; The liner model with a random intercept (&lt;code&gt;y ~ 1 + time + (1 | id)&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{14} \colon\)&lt;/span&gt; The liner model with a random slope (&lt;code&gt;y ~ 1 + time + (0 + time | id)&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{15} \colon\)&lt;/span&gt; The multilevel growth model with regularizing priors (&lt;code&gt;y ~ 1 + time + (1 + time | id)&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{16} \colon\)&lt;/span&gt; The fixed effects with correlated error model (&lt;code&gt;y ~ 1 + time + ar(time = time, p = 1, gr = id&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final four model types will expand previous ones with robust variance parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{17} \colon\)&lt;/span&gt; The cross-classified model with robust variances for discrete time (&lt;code&gt;bf(y ~ 1 + (1 | id) + (1 | time), sigma ~ 0 + factor(time))&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{18} \colon\)&lt;/span&gt; The simple liner model with robust variance for linear time (&lt;code&gt;bf(y ~ 1 + time, sigma ~ 1 + time)&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{19} \colon\)&lt;/span&gt; The liner model with correlated random intercepts for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; (&lt;code&gt;bf(y ~ 1 + time + (1 |x| id), sigma ~ 1 + (1 |x| id))&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{20} \colon\)&lt;/span&gt; The liner model with a random slope for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and uncorrelated random intercept for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; (&lt;code&gt;bf(y ~ 1 + time + (0 + time | id), sigma ~ 1 + (1 | id))&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As far as these model names go, I’m making no claim they are canonical. Call them what you want. My goal, here, is to use names that are minimally descriptive and similar to the terms you might find used by other authors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;models-focusing-on-the-second-timepoint-post.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models focusing on the second timepoint, &lt;code&gt;post&lt;/code&gt;.&lt;/h3&gt;
&lt;div id=&#34;mathcal-m_1-colon-the-unconditional-post-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_1 \colon\)&lt;/span&gt; The unconditional post model.&lt;/h4&gt;
&lt;p&gt;We can write the unconditional post model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{post}_i &amp;amp; \sim \operatorname{Normal}(\mu, \sigma) \\
\mu &amp;amp; = \beta_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is both the model intercept and the estimate for the mean value of &lt;code&gt;post&lt;/code&gt;. The focus this model places on &lt;code&gt;post&lt;/code&gt; comes at the cost of any contextual information on what earlier values we might compare &lt;code&gt;post&lt;/code&gt; to. Also, since the only variable in the model is &lt;code&gt;post&lt;/code&gt;, this technically is &lt;em&gt;not&lt;/em&gt; a 2-timepoint model. But given its connection to the models to follow, it’s worth working through.&lt;/p&gt;
&lt;p&gt;Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;-
  brm(data = small_data_wide,
      post ~ 1,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: post ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.02      0.12     0.79     1.25 1.00     6972     5683
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.17      0.08     1.02     1.36 1.00     8412     6599
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might compare those parameters with their sample values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(mean = mean(post),
            sd = sd(post))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##    mean    sd
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1  1.02  1.16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you only care about computing the population estimates for &lt;span class=&#34;math inline&#34;&gt;\(\mu_\text{post}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{post}\)&lt;/span&gt;, this model does a great job. With no other variables in the model, this approach does a poor job telling us about growth processes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_2-colon-the-simple-autoregressive-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_2 \colon\)&lt;/span&gt; The simple autoregressive model.&lt;/h4&gt;
&lt;p&gt;The simple model with the &lt;code&gt;pre&lt;/code&gt; scores predicting &lt;code&gt;post&lt;/code&gt; is a substantial improvement from the previous one. It follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{post}_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 \text{pre}_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected value of &lt;code&gt;post&lt;/code&gt; when &lt;code&gt;pre&lt;/code&gt; is at zero. As with many other regression contexts, centering the predictor &lt;code&gt;pre&lt;/code&gt; at the mean or some other meaningful value can help make &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; more interpretable. Of greater interest is the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficient, which is the expected deviation from &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; for a one-unit increase in &lt;code&gt;pre&lt;/code&gt;. But since &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; are really the same variable &lt;code&gt;y&lt;/code&gt; measured at two timepoints, it might be helpful if we express this model in another way. In perhaps more technical form, the simple model with &lt;code&gt;pre&lt;/code&gt; predicting &lt;code&gt;post&lt;/code&gt; is really an autoregressive model following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \phi y_{t - 1,i},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the criterion &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; varies across persons &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and timepoints &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Here we only have two timepoints, &lt;span class=&#34;math inline&#34;&gt;\(\text{post} = t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{pre} = t - 1\)&lt;/span&gt;. The strength of association between &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{t - 1}\)&lt;/span&gt; captured by the autoregressive parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, which is often expressed in a correlation metric.&lt;/p&gt;
&lt;p&gt;Here’s how to fit the model with &lt;code&gt;brm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m2 &amp;lt;-
  brm(data = small_data_wide,
      post ~ 1 + pre,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: post ~ 1 + pre 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.05      0.12     0.83     1.28 1.00     8995     6957
## pre           0.24      0.10     0.04     0.43 1.00     9548     6404
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.15      0.08     1.00     1.32 1.00     8407     7109
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s compare the &lt;code&gt;pre&lt;/code&gt; coefficient with the Pearson’s correlation between &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(correlation = cor(pre, post))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   correlation
##         &amp;lt;dbl&amp;gt;
## 1       0.232&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well look at that. Recall that the &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; parameter is the expected value in &lt;code&gt;post&lt;/code&gt; when the predictor &lt;code&gt;pre&lt;/code&gt; is at zero. Though the sample mean for &lt;code&gt;pre&lt;/code&gt; is very close to zero, it’s not exactly so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(pre_mean = mean(pre))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   pre_mean
##      &amp;lt;dbl&amp;gt;
## 1   -0.154&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how to use that information to predict the mean value for &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m2)[1, 1] + fixef(m2)[2, 1] * mean(small_data_wide$pre)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.015851&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get a full posterior summary with aid from &lt;code&gt;fitted()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nd &amp;lt;- tibble(pre = mean(small_data_wide$pre))

fitted(m2, newdata = nd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Estimate Est.Error      Q2.5    Q97.5
## [1,] 1.015851 0.1142147 0.7939083 1.241721&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_3-colon-the-bivariate-autoregressive-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_3 \colon\)&lt;/span&gt; The bivariate autoregressive model.&lt;/h4&gt;
&lt;p&gt;Though the simple autoregressive model gives us a sense of the strength of association between &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt;–and thus a sense of the stability in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; over time–, it still lacks an explicit parameter for mean value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(t - 1\)&lt;/span&gt;. Enter the bivariate autoregressive model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{post}_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\text{pre}_i &amp;amp; \sim \operatorname{Normal}(\nu, \tau) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 \text{pre}_i \\
\nu   &amp;amp; = \gamma_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{post}_i\)&lt;/span&gt; is still modeled as a simple linear function of &lt;span class=&#34;math inline&#34;&gt;\(\text{pre}_i\)&lt;/span&gt;, but now we also include an unconditional model for &lt;span class=&#34;math inline&#34;&gt;\(\text{pre}_i\)&lt;/span&gt;. This will give us an explicit comparison for where we started at the outset (&lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;) and where we ended up (&lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;). We can fit this model using the &lt;strong&gt;brms&lt;/strong&gt; multivariate syntax where the two submodels are encased in &lt;code&gt;bf()&lt;/code&gt; statements &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Multivariate&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2021b&lt;/a&gt;)&lt;/span&gt;. Also, be careful to use &lt;code&gt;set_rescor(rescor = FALSE)&lt;/code&gt; to omit a residual correlation between the two. Their association is already handled with the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m3 &amp;lt;-
  brm(data = small_data_wide,
      bf(post ~ 1 + pre) +
        bf(pre ~ 1) +
        set_rescor(rescor = FALSE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: post ~ 1 + pre 
##          pre ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## post_Intercept     1.05      0.12     0.82     1.28 1.00    10717     7470
## pre_Intercept     -0.15      0.11    -0.38     0.07 1.00    12822     7706
## post_pre           0.24      0.10     0.04     0.43 1.00    11030     7641
## 
## Family Specific Parameters: 
##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_post     1.15      0.08     1.00     1.33 1.00    10955     7694
## sigma_pre      1.16      0.08     1.01     1.33 1.00    13273     7703
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’ve broken out the multivariate syntax, we might consider a second bivariate model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_4-colon-the-bivariate-correlational-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_4 \colon\)&lt;/span&gt; The bivariate correlational model.&lt;/h4&gt;
&lt;p&gt;The bivariate correlational model follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\begin{bmatrix} \text{post}_i \\ \text{pre}_i \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} \mu \\ \nu \end{bmatrix}, \mathbf \Sigma \right) \\
\mu &amp;amp; = \beta_0 \\
\nu &amp;amp; = \gamma_0 \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma &amp;amp; 0 \\ 0 &amp;amp; \tau \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where means of both &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; are modeled in intercept-only models. However, the association between the two timepoints is captured in the residual correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. Yet because we have no predictors in for either variable, the “residual” correlation is really just a correlation. We might also gain some insights if we re-express this model in terms of &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{t - 1,i}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\begin{bmatrix} y_{ti} \\ y_{t - 1,i} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} \mu_t \\ \mu_{t - 1} \end{bmatrix}, \mathbf \Sigma \right) \\
\mu_t &amp;amp; = \beta_t \\
\mu_{t - 1} &amp;amp; = \beta_{t - 1} \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_t &amp;amp; 0 \\ 0 &amp;amp; \sigma_{t - 1} \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where what we formerly called an autoregressive coefficient in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_2\)&lt;/span&gt;, we’re now calling a correlation. Note also that this model freely estimates &lt;span class=&#34;math inline&#34;&gt;\(\sigma_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{t - 1}\)&lt;/span&gt;. In some contexts, these are presumed to be equal. Though we won’t be imposing that constraint, here, I believe it is possible with the &lt;strong&gt;brms&lt;/strong&gt; &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&#34;&gt;non-linear syntax&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Non_linear&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2021c&lt;/a&gt;)&lt;/span&gt;. Anyway, here’s how to fit the model with the &lt;code&gt;brm()&lt;/code&gt; function .&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m4 &amp;lt;-
  brm(data = small_data_wide,
      bf(post ~ 1) +
        bf(pre ~ 1) +
        set_rescor(rescor = TRUE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note out use of the &lt;code&gt;set_rescor(rescor = TRUE)&lt;/code&gt; syntax in the model &lt;code&gt;formula&lt;/code&gt;. This explicitly told &lt;code&gt;brm()&lt;/code&gt; to include the residual correlation. Here’s the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: post ~ 1 
##          pre ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## post_Intercept     1.02      0.12     0.79     1.24 1.00    10992     8033
## pre_Intercept     -0.15      0.12    -0.39     0.08 1.00    10679     8012
## 
## Family Specific Parameters: 
##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_post     1.18      0.08     1.03     1.36 1.00    11342     7819
## sigma_pre      1.16      0.08     1.02     1.34 1.00    11150     7777
## 
## Residual Correlations: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(post,pre)     0.23      0.09     0.03     0.40 1.00    10074     7292
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the intercept and sigma parameters do a good job capturing the sample statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  pivot_longer(pre:post) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  summarise(mean = mean(value),
            sd = sd(value)) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   name   mean    sd
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 post   1.02  1.16
## 2 pre   -0.15  1.14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new ‘rescor’ line at the bottom of the &lt;code&gt;print()&lt;/code&gt; summary approximates the Pearson’s correlation of the two variables, much like the autoregressive parameter did two models up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(correlation = cor(pre, post))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   correlation
##         &amp;lt;dbl&amp;gt;
## 1       0.232&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another nice quality of this model is if you subtract &lt;span class=&#34;math inline&#34;&gt;\(\gamma_0\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\beta_t - \beta_{t - 1}\)&lt;/span&gt;), you’d end up with the posterior mean of the change score.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m4) %&amp;gt;% 
  mutate(change = b_post_Intercept - b_pre_Intercept) %&amp;gt;% 
  summarise(mu_change = mean(change))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mu_change
## 1  1.168898&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keeping that in mind, let’s switch gears to the first of the change-score models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;change-score-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Change-score models.&lt;/h3&gt;
&lt;p&gt;Instead of modeling &lt;code&gt;post&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt;, we might instead want to focus on the change from &lt;code&gt;pre&lt;/code&gt; to &lt;code&gt;post&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y_{ti} - y_{t - 1,i}\)&lt;/span&gt;. When you subtract &lt;code&gt;pre&lt;/code&gt; from &lt;code&gt;post&lt;/code&gt; in your data set–like we did to make the &lt;code&gt;change&lt;/code&gt; variable–, the product is often referred to as a change score or difference score, &lt;span class=&#34;math inline&#34;&gt;\(y_\Delta\)&lt;/span&gt;. Though they’re conceptually intuitive and simple to compute, change scores have a long history of criticisms in the methodological literature, particularly around issues of reliability &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-lordStatisticalTheoriesMental1968&#34; role=&#34;doc-biblioref&#34;&gt;Lord &amp;amp; Novick, 1968&lt;/a&gt;; &lt;a href=&#34;#ref-rogosaGrowthCurveApproach1982&#34; role=&#34;doc-biblioref&#34;&gt;Rogosa et al., 1982&lt;/a&gt;; cf. &lt;a href=&#34;#ref-kisbu2013monte&#34; role=&#34;doc-biblioref&#34;&gt;Kisbu-Sakarya et al., 2013&lt;/a&gt;)&lt;/span&gt;. Here we consider four change-score models simply as options.&lt;/p&gt;
&lt;div id=&#34;mathcal-m_5-colon-the-unconditional-change-score-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_5 \colon\)&lt;/span&gt; The unconditional change-score model.&lt;/h4&gt;
&lt;p&gt;We’ve already saved that in our &lt;code&gt;small_data_wide&lt;/code&gt; data as &lt;code&gt;change&lt;/code&gt;. Here’s what the unconditional change-score model&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; looks like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{change}_i &amp;amp; \sim \operatorname{Normal}(\mu, \sigma) \\
\mu &amp;amp; = \beta_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected value for &lt;span class=&#34;math inline&#34;&gt;\(\text{change}_i\)&lt;/span&gt;. In the terms of the last model, &lt;span class=&#34;math inline&#34;&gt;\(\text{change}_i = \text{post}_i - \text{pre}_i\)&lt;/span&gt; or, in the terms of the simple autoregressive model, &lt;span class=&#34;math inline&#34;&gt;\(y_{\Delta i} = y_{ti} - y_{t - 1,i}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m5 &amp;lt;-
  brm(data = small_data_wide,
      change ~ 1,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: change ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.17      0.14     0.89     1.45 1.00     8075     6240
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.44      0.10     1.26     1.66 1.00     8673     6829
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, this model suggests the average change from &lt;code&gt;pre&lt;/code&gt; to &lt;code&gt;post&lt;/code&gt; was about 1.2 units. We can compute the sample statistics for that in two ways.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(change = mean(change),
            `post - pre` = mean(post - pre))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   change `post - pre`
##    &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1   1.17         1.17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The major deficit in the unconditional change model is that change is disconnected from any reference points. We have no explicit way of knowing what number we changed from or what number we changed to. The next model offers a solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_6-colon-the-conditional-change-score-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_6 \colon\)&lt;/span&gt; The conditional change-score model.&lt;/h4&gt;
&lt;p&gt;Instead of fitting an unconditional model of &lt;code&gt;change&lt;/code&gt;, why not condition on the initial &lt;code&gt;pre&lt;/code&gt; value? We might express this as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{change}_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 \text{pre}_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is now the expected value for &lt;code&gt;change&lt;/code&gt; when &lt;code&gt;pre&lt;/code&gt; is at zero. As with the simple autoregressive model, centering the predictor &lt;code&gt;pre&lt;/code&gt; at the mean or some other meaningful value can help make &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; more interpretable. Perhaps of greater interest, the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficient allows us to predict different levels of &lt;code&gt;change&lt;/code&gt;, conditional in the initial values at &lt;code&gt;pre&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m6 &amp;lt;-
  brm(data = small_data_wide,
      change ~ 1 + pre,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: change ~ 1 + pre 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.05      0.12     0.82     1.28 1.00     9315     7108
## pre          -0.76      0.10    -0.96    -0.57 1.00    10559     7804
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.15      0.08     1.00     1.33 1.00     9745     7268
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the intercept in this model is the expected &lt;code&gt;change&lt;/code&gt; value based on when &lt;code&gt;pre == 0&lt;/code&gt;, it might be easiest to interpret that value when using the mean of &lt;code&gt;pre&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m6)[1, 1] + fixef(m6)[2, 1] * mean(small_data_wide$pre)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.167458&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is the point prediction for the mean of &lt;code&gt;change&lt;/code&gt;. Let’s compare that to the sample value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  summarise(mean = mean(change))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##    mean
##   &amp;lt;dbl&amp;gt;
## 1  1.17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, we can get a fuller summary using the &lt;code&gt;fitted()&lt;/code&gt; method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nd &amp;lt;- tibble(pre = mean(small_data_wide$pre))

fitted(m6, newdata = nd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Estimate Est.Error      Q2.5   Q97.5
## [1,] 1.167458 0.1151598 0.9428394 1.39075&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how the coefficient for &lt;code&gt;pre&lt;/code&gt; is about -0.76. This is a rough analogue of the negative correlation among the intercepts and slopes in the original data-generating model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rho&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It tells us something very similar; participants with higher values at &lt;code&gt;pre&lt;/code&gt; tended to have lower &lt;code&gt;change&lt;/code&gt; values. Much like with the simple autoregressive model, a deficit of this model is there is no explicit parameter for the expected value of &lt;code&gt;pre&lt;/code&gt;, which we can amend by fitting a bivariate model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_7-colon-the-bivariate-conditional-change-score-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_7 \colon\)&lt;/span&gt; The bivariate conditional change-score model.&lt;/h4&gt;
&lt;p&gt;The bivariate conditional change-score model follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{change}_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\text{pre}_i &amp;amp; \sim \operatorname{Normal}(\nu, \tau) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 \text{pre}_i \\
\nu   &amp;amp; = \gamma_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the simple linear model of &lt;span class=&#34;math inline&#34;&gt;\(\text{change}_i\)&lt;/span&gt; conditional on &lt;span class=&#34;math inline&#34;&gt;\(\text{pre}_i\)&lt;/span&gt; is coupled with an unconditional intercept-only model for &lt;span class=&#34;math inline&#34;&gt;\(\text{pre}_i\)&lt;/span&gt;. We can fit this model with &lt;strong&gt;brms&lt;/strong&gt; by way of the multivariate syntax, where the two submodels are encased in &lt;code&gt;bf()&lt;/code&gt; statements and we set &lt;code&gt;set_rescor(rescor = FALSE)&lt;/code&gt; to omit a residual correlation between the two.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m7 &amp;lt;-
  brm(data = small_data_wide,
      bf(change ~ 1 + pre) +
        bf(pre ~ 1) +
        set_rescor(rescor = FALSE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: change ~ 1 + pre 
##          pre ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## change_Intercept     1.05      0.12     0.82     1.28 1.00    11908     7575
## pre_Intercept       -0.16      0.12    -0.39     0.07 1.00    13113     8030
## change_pre          -0.76      0.10    -0.96    -0.57 1.00    12023     7190
## 
## Family Specific Parameters: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_change     1.15      0.08     1.00     1.33 1.00    13064     7696
## sigma_pre        1.16      0.08     1.01     1.33 1.00    11419     7142
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have an intercept for &lt;code&gt;pre&lt;/code&gt;, we can use the model parameters to compute the expected values for &lt;code&gt;pre&lt;/code&gt;, &lt;code&gt;change&lt;/code&gt;, and &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m7) %&amp;gt;% 
  mutate(pre    = b_pre_Intercept,
         change = b_change_Intercept + b_change_pre * b_pre_Intercept) %&amp;gt;% 
  mutate(post = pre + change) %&amp;gt;% 
  pivot_longer(pre:post) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  mean_qi(value) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 7
##   name   value .lower .upper .width .point .interval
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
## 1 change  1.17   0.88   1.45   0.95 mean   qi       
## 2 post    1.01   0.78   1.25   0.95 mean   qi       
## 3 pre    -0.16  -0.39   0.07   0.95 mean   qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The posterior means are in the &lt;code&gt;value&lt;/code&gt; column and the lower- and upper-levels of the percentile-based 95% intervals are in the &lt;code&gt;.lower&lt;/code&gt; and &lt;code&gt;.upper&lt;/code&gt; columns. Now compare those mean estimates with the sample means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_wide %&amp;gt;% 
  pivot_longer(-id) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  summarise(mean = mean(value)) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   name    mean
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 change  1.17
## 2 post    1.02
## 3 pre    -0.15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As handy as this model is, the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficient might not be in the most intuitive metric. Let’s reparameterize.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_8-colon-the-bivariate-correlational-prechange-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_8 \colon\)&lt;/span&gt; The bivariate correlational pre/change model.&lt;/h4&gt;
&lt;p&gt;The bivariate correlational pre/change model follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\begin{bmatrix} \text{change}_i \\ \text{pre}_i \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} \mu \\ \nu \end{bmatrix}, \mathbf \Sigma \right) \\
\mu &amp;amp; = \beta_0 \\
\nu &amp;amp; = \gamma_0 \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma &amp;amp; 0 \\ 0 &amp;amp; \tau \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where means of both &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;change&lt;/code&gt; are modeled in intercept-only models and the association between the two is captured by the correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. And again, because we have no predictors in for either variable, the “residual” correlation is really just a correlation. Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m8 &amp;lt;-
  brm(data = small_data_wide,
      bf(change ~ 1) +
        bf(pre ~ 1) +
        set_rescor(rescor = TRUE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: change ~ 1 
##          pre ~ 1 
##    Data: small_data_wide (Number of observations: 100) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## change_Intercept     1.17      0.15     0.88     1.45 1.00     8200     7551
## pre_Intercept       -0.15      0.12    -0.38     0.08 1.00     8160     7194
## 
## Family Specific Parameters: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_change     1.44      0.10     1.26     1.67 1.00     8811     7147
## sigma_pre        1.16      0.08     1.01     1.33 1.00     8996     7487
## 
## Residual Correlations: 
##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(change,pre)    -0.60      0.06    -0.71    -0.46 1.00     8299     7233
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the parameter in the ‘Residual Correlations’ section of the summary output is a close analogue to our original data-generating &lt;code&gt;rho&lt;/code&gt; parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rho&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those with higher &lt;code&gt;pre&lt;/code&gt; values tended to have lower &lt;code&gt;change&lt;/code&gt; values. We can look at that with a plot of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2 &amp;lt;-
  small_data_wide %&amp;gt;% 
  ggplot(aes(x = pre, y = change)) +
  geom_point() +
  stat_ellipse(color = &amp;quot;grey50&amp;quot;)

(p1 + p2) &amp;amp;
  coord_cartesian(xlim = range(small_data_wide$pre),
                  ylim = range(small_data_wide$change))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we’ve placed the scatter plot of the data-generating &lt;code&gt;id&lt;/code&gt;-level &lt;code&gt;intercepts&lt;/code&gt; and &lt;code&gt;slopes&lt;/code&gt; next to the scatter plot of the &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;change&lt;/code&gt; scores. They are not exactly the same, but the latter are a partial consequence of the former. This is why the correlation parameter in our &lt;code&gt;m8&lt;/code&gt; model closely, but not exactly, resembled the data-generating &lt;code&gt;rho&lt;/code&gt; parameter.&lt;/p&gt;
&lt;p&gt;Okay, let’s switch gears again.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;models-modeling-the-criterion-y_ti-directly.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models modeling the criterion &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; directly.&lt;/h3&gt;
&lt;p&gt;All of the models focusing on &lt;code&gt;post&lt;/code&gt; or &lt;code&gt;change&lt;/code&gt; used the wide version of the data, &lt;code&gt;small_data_wide&lt;/code&gt;. The remaining models will all take advantage of the long data set, &lt;code&gt;small_data_long&lt;/code&gt;, and take &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; as the criterion. Consequently, most of these models will use some version of the multilevel model.&lt;/p&gt;
&lt;div id=&#34;mathcal-m_9-colon-the-grand-mean-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_9 \colon\)&lt;/span&gt; The grand-mean model.&lt;/h4&gt;
&lt;p&gt;The simplest model we might fit using the long version of the 2-timepoint data, &lt;code&gt;small_data_long&lt;/code&gt;, is what we might call the grand-mean model, or what &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; called the &lt;em&gt;between-person empty model&lt;/em&gt;. It follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu, \sigma) \\
\mu &amp;amp; = \beta_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected value value for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; across all &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; participants and &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; timepoints. We fit this with &lt;code&gt;brm()&lt;/code&gt; much like we fit the unconditional post model and the unconditional change-score model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m9 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 ,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.43      0.09     0.25     0.61 1.00     8486     6236
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.30      0.07     1.18     1.43 1.00     9455     6868
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might check these with the sample statistics for &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data_long %&amp;gt;% 
  summarise(mean = mean(y),
            sd = sd(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##    mean    sd
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 0.431  1.29&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though this model did to a good job describing the population values for the mean and standard deviation for &lt;code&gt;y&lt;/code&gt;, it did a terrible job telling us about change in &lt;code&gt;y&lt;/code&gt;, about individual differences in that change, or about anything else of interest we might have as longitudinal researchers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_10-colon-the-random-intercept-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{10} \colon\)&lt;/span&gt; The random-intercept model.&lt;/h4&gt;
&lt;p&gt;Now that we have a grand mean, we might want to ask what kinds of variables would help explain the variation around the grand mean. From a multilevel perspective, the first source of variation of interest will be across participants, which we can express by allowing the mean to vary by participant. This is what Hoffman called the &lt;em&gt;within-person empty model&lt;/em&gt; and the &lt;em&gt;empty means, random intercept model&lt;/em&gt;. It follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  &amp;amp; = \beta_0 + u_{\text{id},i} \\
u_\text{id} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{id}) \\
\sigma &amp;amp; = \sigma_\epsilon,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is now the grand mean among the participant-level means in &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt;. The participant-level deviations from the grand mean are expressed as &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{id},i}\)&lt;/span&gt;, which is normally distributed with a mean at zero (these are &lt;em&gt;deviations&lt;/em&gt;, after all) and a standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{id}\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt; parameter is a mixture of the variation within participants and over time. With &lt;strong&gt;brms&lt;/strong&gt;, we can fit the random-intercept model model like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m10 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + (1 | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .99))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + (1 | id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.23      0.15     0.01     0.56 1.00     2079     4188
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.43      0.09     0.24     0.61 1.00    13669     6976
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.27      0.07     1.14     1.41 1.00     6211     5523
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; intercept returned a very similar estimate for the grand mean, but we now interpret it as the grand mean for the within-person means for &lt;code&gt;y&lt;/code&gt;. The variation in the &lt;code&gt;id&lt;/code&gt;-level deviations around the grand mean, which we called &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{id},i}\)&lt;/span&gt;, is summarized by &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{id}\)&lt;/span&gt; in the ‘Group-Level Effects’ section of the summary output.&lt;/p&gt;
&lt;p&gt;Authors of many longitudinal text books &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman, 2015&lt;/a&gt;; &lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett, 2003&lt;/a&gt;)&lt;/span&gt; typically present this model as a way to directly compare the between- and within-person variation in the data by way of the intraclass correlation coefficient (ICC),&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{ICC} = \frac{\text{between-person variance}}{\text{total variance}} = \frac{\sigma_\text{id}^2}{\sigma_\text{id}^2 + \sigma_\epsilon^2}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When using frequentist methods, the ICC is typically expressed with a point estimate. When working with all our posterior draws, we can get full posterior distributions within the Bayesian framework.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m10) %&amp;gt;% 
  mutate(icc = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&amp;gt;% 
  ggplot(aes(x = icc, y = 0)) +
  stat_halfeye(.width = .95) +
  scale_x_continuous(&amp;quot;Intraclass correlation coefficient (ICC)&amp;quot;, 
                     breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The ICC is a proportion, which limits it to the range of zero to one. Here it suggests that 0–20% of the variation in our data is due to differences &lt;em&gt;between&lt;/em&gt; participants; the remaining variation occurs within them. Given that our data were collected across time, it might make sense to fit a model that explicitly accounts for time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_11-colon-the-cross-classified-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{11} \colon\)&lt;/span&gt; The cross-classified model.&lt;/h4&gt;
&lt;p&gt;A direct extension of the random-intercept model is the cross-classified multilevel model &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2015&lt;/a&gt;, Chapter 12)&lt;/span&gt;, which we might express as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti}   &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} &amp;amp; = \beta_0 + u_{\text{id},i} + u_{\text{time},i} \\
u_\text{id} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{id}) \\
u_\text{time} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{time}) \\
\sigma &amp;amp; = \sigma_\epsilon,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{id}\)&lt;/span&gt; captures systemic differences between participants, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{time}\)&lt;/span&gt; captures systemic variation across the two timepoints, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt; captures the variation within participants over time. Another way to think of this model is as a Bayesian multilevel version of the repeated-measures ANOVA&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, where variance is partitioned into a between level (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{id} \approx \text{SS}_\text{between}\)&lt;/span&gt;), a model level (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{time} \approx \text{SS}_\text{model}\)&lt;/span&gt;), and error (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon \approx \text{SS}_\text{error}\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m11 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + (1 | time) + (1 | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .9999,
                     max_treedepth = 11))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m11)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + (1 | time) + (1 | id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.51      0.16     0.14     0.78 1.00     1519     1545
## 
## ~time (Number of levels: 2) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.67      1.30     0.41     5.00 1.00     5240     7037
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.46      1.07    -1.78     2.81 1.00     4541     4935
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.04      0.08     0.90     1.20 1.00     2313     3750
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The intercept is the grand mean across all measures of &lt;code&gt;y&lt;/code&gt;. The first row in the ‘Group-Level Effects’ section is our summary for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{id}\)&lt;/span&gt;, which gives us a sense of the variation between participants in their overall tendencies in the criterion &lt;code&gt;y&lt;/code&gt;. If we use the &lt;code&gt;posterior_samples()&lt;/code&gt;, we can even look at the posteriors for the &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{id},i}\)&lt;/span&gt; parameters, themselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m11) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_id&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = value, y = reorder(name, value))) +
  stat_pointinterval(point_interval = mean_qi, .width = .95, size = 1/6) +
  scale_y_discrete(expression(italic(i)), breaks = NULL) +
  labs(subtitle = expression(sigma[id]~is~the~summary~of~the~variation~across~these),
       x = expression(italic(u)[id][&amp;#39;,&amp;#39;*italic(i)]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;336&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given that each of the &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{id},i}\)&lt;/span&gt; parameters is based primarily on two data points (the two data points per participant), it should be no surprise they are fairly wide. Even a few more measurement occasions within participants will narrow them substantially. If you fit the same model using the original 6-timepoint data, you’ll see the 95% intervals are almost half as wide.&lt;/p&gt;
&lt;p&gt;Perhaps of greater interest are the &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{time},i}\)&lt;/span&gt; parameters. If you combine them with the intercept, you’ll get the model-based expected values at both timepoints.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m11) %&amp;gt;% 
  transmute(pre  = b_Intercept + `r_time[0,Intercept]`,
            post = b_Intercept + `r_time[1,Intercept]`) %&amp;gt;% 
  pivot_longer(everything()) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  mean_qi(value) %&amp;gt;% 
  mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   name  value .lower .upper .width .point .interval
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
## 1 post   1.01   0.78   1.24   0.95 mean   qi       
## 2 pre   -0.15  -0.38   0.09   0.95 mean   qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final variance parameter, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{time}\)&lt;/span&gt;, captures the within-participant variation over time. With a model like this, it seems natural to directly compare the magnitudes of the three variance parameters, which answers the question: &lt;em&gt;Where’s the variance at&lt;/em&gt;? Here we’ll do so with a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m11) %&amp;gt;% 
  select(sd_id__Intercept:sigma) %&amp;gt;% 
  set_names(&amp;quot;sigma[id]&amp;quot;, &amp;quot;sigma[time]&amp;quot;, &amp;quot;sigma[epsilon]&amp;quot;) %&amp;gt;% 
  pivot_longer(everything()) %&amp;gt;% 
  mutate(name = factor(name, levels = c(&amp;quot;sigma[epsilon]&amp;quot;, &amp;quot;sigma[time]&amp;quot;, &amp;quot;sigma[id]&amp;quot;))) %&amp;gt;%
  
  ggplot(aes(x = value, y = name)) +
  tidybayes::stat_halfeye(.width = .95, size = 1, normalize = &amp;quot;xy&amp;quot;) +
  scale_x_continuous(&amp;quot;marginal posterior&amp;quot;, expand = expansion(mult = c(0, 0.05)), breaks = c(0, 1, 2, 5)) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(xlim = c(0, 5.25),
                  ylim = c(1.5, 3.5)) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-48-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At first glance, it might be surprising how wide the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{time}\)&lt;/span&gt; posterior is compared to the other two. Yet recall this parameter is summarizing the standard deviation of only two levels. If you have experience with multilevel models, you’ll know that it can be difficult to estimate a variance parameter with few levels–two levels is the extreme lower limit. This is why we had to fiddle with the &lt;code&gt;adapt_delta&lt;/code&gt; and &lt;code&gt;max_treedepth&lt;/code&gt; parameters within the &lt;code&gt;brm()&lt;/code&gt; function to get the model to sample properly. Though we pulled this off using default priors, don’t be surprised if you have to use tighter priors when fitting a model like this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_12-colon-the-simple-liner-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{12} \colon\)&lt;/span&gt; The simple liner model.&lt;/h4&gt;
&lt;p&gt;The last three models focused on the grand mean and sources of variance around that grand mean. A more familiar looking approach might be to fit a simple linear model with &lt;code&gt;y&lt;/code&gt; conditional on &lt;code&gt;time&lt;/code&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected value at the first timepoint and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; captures the change in &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; for the final timepoint.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m12 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + time,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.15      0.12    -0.38     0.08 1.00     9459     7344
## time          1.17      0.16     0.85     1.49 1.00     9582     7294
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.16      0.06     1.05     1.28 1.00     9982     7362
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now our &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; parameters are the direct pre/post single-level analogues to the population-level &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; parameters from our original multilevel model based on the full 6-timepoint data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m0) %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Estimate Est.Error  Q2.5 Q97.5
## Intercept    -0.13      0.11 -0.33  0.08
## time          1.16      0.13  0.91  1.42&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The major deficit in this model, which is the reason you’ll see it criticized in the methodological literature, is it ignores how the &lt;code&gt;y&lt;/code&gt; values are nested within levels of &lt;code&gt;id&lt;/code&gt;. The only variance parameter, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, was estimated under the typical assumption that the residuals are all independent of one another. Sure, the model formula accounted for the overall trend in &lt;code&gt;time&lt;/code&gt;, but it ignored the insights revealed from many of the other models the capture between-participant correlations in intercepts and slopes. This means that if you know something about the value of one’s residual for when &lt;code&gt;time == 0&lt;/code&gt;, you’ll also know something about where to expect their residual for when &lt;code&gt;time == 1&lt;/code&gt;. The two are not independent. As long as we’re working with the data in the long format, we’ll want to account for this, somehow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_13-colon-the-liner-model-with-a-random-intercept.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{13} \colon\)&lt;/span&gt; The liner model with a random intercept.&lt;/h4&gt;
&lt;p&gt;A natural first step to accounting for how the &lt;code&gt;y&lt;/code&gt; values are nested within levels of &lt;code&gt;id&lt;/code&gt; is to fit a random-intercept model, or what &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; called the &lt;em&gt;fixed linear time, random intercept model&lt;/em&gt;. It follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} &amp;amp; = \beta_{0i} + \beta_1 \text{time}_{ti} \\
\beta_{0i} &amp;amp; = \gamma_0 + u_{0i} \\
u_{0i}  &amp;amp; \sim \operatorname{Normal}(0, \sigma_0) \\
\sigma &amp;amp; = \sigma_\epsilon,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0i}\)&lt;/span&gt; is the intercept and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the time slope. Although this parameterization holds the variation in slopes constant across participants, between-participant variation is at least captured in &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0i}\)&lt;/span&gt;, which is decomposed into a grand mean, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_0\)&lt;/span&gt;, and participant-level deviations around that grand mean, &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt;. Those participant-level deviations are summarized by the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt; parameter. In this model, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt; is a mixture of within-participant variation and between-participant variation in slopes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m13 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + time + (1 | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m13)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time + (1 | id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.52      0.15     0.14     0.78 1.00     1645     1960
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.15      0.12    -0.38     0.08 1.00     9441     7715
## time          1.17      0.15     0.88     1.46 1.00    14821     6993
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.03      0.08     0.90     1.19 1.00     2403     4360
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters are very similar to those from the simple linear model, above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m12) %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Estimate Est.Error  Q2.5 Q97.5
## Intercept    -0.15      0.12 -0.38  0.08
## time          1.17      0.16  0.85  1.49&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But now look at the size of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt;, which suggests substantial differences in staring points. To get a sense of what this means, we’ll plot all 100 participant-level trajectories with a little help from &lt;code&gt;fitted()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nd &amp;lt;- distinct(small_data_long, id, time)

fitted(m13, 
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m13)[1, 1],
              slope = fixef(m13)[2, 1],
              size = 3, color = &amp;quot;blue&amp;quot;) +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = &amp;quot;Random intercepts, fixed slope&amp;quot;,
       y = &amp;quot;y&amp;quot;) +
  coord_cartesian(ylim = c(-1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-53-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The bold blue line in the middle is based on the population-level intercept and slope, whereas the thinner black lines are the participant-level trajectories. To keep from &lt;a href=&#34;https://www.data-to-viz.com/caveat/overplotting.html&#34;&gt;overplotting&lt;/a&gt;, we’re only showing the posterior means, here. Because we only allowed the intercept to vary across participants, all the slopes are identical. And indeed, look at all the variation we see in the intercepts–an insight lacking in the simple linear model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_14-colon-the-liner-model-with-a-random-slope.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{14} \colon\)&lt;/span&gt; The liner model with a random slope.&lt;/h4&gt;
&lt;p&gt;The counterpoint to the last model is to allow the time slopes, but not the intercepts, vary across participants:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_{1i} \text{time}_{ti} \\
\beta_{1i} &amp;amp; = \gamma_1 + u_{1i} \\
u_{1i} &amp;amp; \sim \operatorname{Normal}(0, \sigma_1) \\
\sigma   &amp;amp; = \sigma_\epsilon,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the intercept for all participants. Now &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1i}\)&lt;/span&gt; is the population mean for the distribution of slopes, which vary across participants, the standard deviation for which is measured by &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m14 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + time + (0 + time | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .95))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m14)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time + (0 + time | id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(time)     0.33      0.21     0.02     0.75 1.00     2240     4641
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.15      0.11    -0.38     0.08 1.00    20513     7195
## time          1.17      0.16     0.85     1.49 1.00    18079     7133
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.13      0.06     1.01     1.26 1.00     6825     6149
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s random-slopes alternative to the random-intercepts plot from the last section.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(m14, 
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m14)[1, 1],
              slope = fixef(m14)[2, 1],
              size = 3, color = &amp;quot;blue&amp;quot;) +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = &amp;quot;Fixed intercept, random slopes&amp;quot;,
       y = &amp;quot;y&amp;quot;) +
  coord_cartesian(ylim = c(-1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-55-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But why choose between random intercepts or random slopes?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_15-colon-the-multilevel-growth-model-with-regularizing-priors.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{15} \colon\)&lt;/span&gt; The multilevel growth model with regularizing priors.&lt;/h4&gt;
&lt;p&gt;If you were modeling 2-timepoint data with conventional frequentist estimators (e.g., maximum likelihood), you can have random intercepts or random slopes, but you can’t have both; that would require data from three timepoints or more. But because Bayesian models bring in extra information by way of the priors, you can actually fit a full multilevel growth model with both random intercepts and slopes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti}   &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma_\epsilon ) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} + u_{0i} + u_{1i} \text{time}_{ti} \\
\begin{bmatrix} u_{0i} \\ u_{1i} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf \Sigma \right) \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_1 \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix}.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The trick is you have to go beyond the diffuse &lt;strong&gt;brms&lt;/strong&gt; default settings for the priors for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt;. If you have high-quality information from theory or previous studies, you can base the priors on those. Another approach is to use regularizing priors. Given standardized data, members of the Stan team like either &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}^+(0, 1)\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Student-t}^+(3, 0, 1)\)&lt;/span&gt; for variance parameters (see the &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#generic-prior-for-anything&#34;&gt;Generic prior for anything&lt;/a&gt; section from the &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;&lt;em&gt;Prior choice recommendations&lt;/em&gt; wiki&lt;/a&gt;). In the second edition of his text, McElreath generally favored the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Exponential}(1)\)&lt;/span&gt; prior &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020&lt;/a&gt;)&lt;/span&gt;, which is the approach we’ll experiment with, here. It’ll also help if we use a regularizing prior on the correlation among the intercepts and slopes, &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m15 &amp;lt;-
  brm(data = small_data_long,
      y ~ 1 + time + (1 + time | id),
      seed = 1,
      prior = prior(exponential(1), class = sd) +
        prior(lkj(4), class = cor),
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .9995))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even with our tighter priors, we still had to adjust the &lt;code&gt;adapt_delta&lt;/code&gt; parameter to improve the quality of the MCMC sampling. Take a look at the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + time + (1 + time | id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)           0.52      0.18     0.12     0.84 1.01     1012     1473
## sd(time)                0.31      0.23     0.01     0.88 1.00     1021      876
## cor(Intercept,time)    -0.05      0.33    -0.63     0.59 1.00     4039     6272
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.15      0.11    -0.38     0.07 1.00    10022     8348
## time          1.17      0.15     0.88     1.46 1.00    13870     7175
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.01      0.10     0.80     1.18 1.00     1060      946
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though the posteriors, particularly for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; parameters, are not as precise as with the 6-timepoint data, we now have a model with a summary mirroring the structure of the data-generating model. Yet compared to the data-generating values, the estimates for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; are particularly biased toward zero. Here’s a look at the trajectories.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(m15,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m15)[1, 1],
              slope = fixef(m15)[2, 1],
              size = 3, color = &amp;quot;blue&amp;quot;) +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = &amp;quot;Random intercepts AND random slopes\n(2-timepoint data)&amp;quot;,
       y = &amp;quot;y&amp;quot;) +
  coord_cartesian(ylim = c(-1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-57-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For comparision, here’s the plot for the original 6-timepoint model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(m0,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m0)[1, 1],
              slope = fixef(m0)[2, 1],
              size = 3, color = &amp;quot;blue&amp;quot;) +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = &amp;quot;Random intercepts AND random slopes\n(6-timepoint data)&amp;quot;,
       y = &amp;quot;y&amp;quot;) +
  coord_cartesian(ylim = c(-1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-58-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There wasn’t enough information in the 2-timepoint data set to capture the complexity in the full 6-timepoint data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_16-colon-the-fixed-effects-with-correlated-error-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{16} \colon\)&lt;/span&gt; The fixed effects with correlated error model.&lt;/h4&gt;
&lt;p&gt;Though our Bayesian 2-timepoint version of the full multilevel growth model was exciting, it’s not generally used in the wild. Even with our tighter regularizing priors, there just wasn’t enough information in the data to do the model justice. A very different and humbler approach is to combine the simple linear model with the autoregressive model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \mathbf \Sigma) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma &amp;amp; 0 \\ 0 &amp;amp; \sigma \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; captures the correlation between the responses in the two timepoints, &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t - 1\)&lt;/span&gt;, which is an alternative to the way the mixed model from above handles the dependencies (a.k.a &lt;a href=&#34;https://en.wikipedia.org/wiki/Heteroscedasticity&#34;&gt;heteroskedasticity&lt;/a&gt;) inherent in longitudinal data. Note how &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; in this model is defined very differently from the full multilevel growth model from above. To fit this model with &lt;strong&gt;brms&lt;/strong&gt;, we use the &lt;code&gt;ar()&lt;/code&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m16 &amp;lt;-
  brm(data = small_data_long,
      y ~ time + ar(time = time, p = 1, gr = id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m16)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ time + ar(time = time, p = 1, gr = id) 
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Correlation Structures:
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## ar[1]     0.23      0.10     0.04     0.43 1.00     9766     7458
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.15      0.12    -0.38     0.07 1.00    10593     7136
## time          1.17      0.15     0.89     1.46 1.00     9607     6546
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.15      0.06     1.04     1.27 1.00    10604     7665
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This summary suggests that, after you account for the linear trend, the correlation between &lt;span class=&#34;math inline&#34;&gt;\(y_{ti}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{t - 1,i}\)&lt;/span&gt; is about 0.23. Though we don’t get &lt;code&gt;id&lt;/code&gt;-specific variance parameters, this model does account for the nonindependence of the data over time. If you scroll back up, notice how similar this is to the correlation from the bivariate correlational model, &lt;code&gt;m4&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-models-with-robust-variance-parameters.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The models with robust variance parameters.&lt;/h3&gt;
&lt;p&gt;In the social sciences, many of our theories and statistical models are comparisons of or changes in group means. Every model in this blog post uses the normal likelihood, which parameterizes the criterion in terms of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Every time we added some kind of linear model, we focused that model around the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. But contemporary Bayesian software allows us to model the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameter, too. Within the &lt;strong&gt;brms&lt;/strong&gt; framework, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-Bürkner2021Distributional&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner&lt;/a&gt; (&lt;a href=&#34;#ref-Bürkner2021Distributional&#34; role=&#34;doc-biblioref&#34;&gt;2021a&lt;/a&gt;)&lt;/span&gt; calls these &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&#34;&gt;distributional models&lt;/a&gt;. The final four models under consideration all use some form of the distributional modeling syntax to relax unnecessarily restrictive assumptions on the variance parameters. Though this section is not exhaustive, it should give a sense of how flexible this approach can be.&lt;/p&gt;
&lt;div id=&#34;mathcal-m_17-colon-the-cross-classified-model-with-robust-variances-for-discrete-time.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{17} \colon\)&lt;/span&gt; The cross-classified model with robust variances for discrete time.&lt;/h4&gt;
&lt;p&gt;One of the criticisms of the conventional repeated measures ANOVA approach is how it presumes the variances in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are constant over time. However, we can relax that constraint with a model like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} &amp;amp; = \beta_0 + u_{\text{id},i} + u_{\text{time},t} \\
\log (\sigma_{ti}) &amp;amp; = \eta_{\text{time},t} \\
u_\text{id} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{id}) \\
u_\text{time} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{time}),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we are now modeling both parameters in the likelihood, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; AND &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. The second line shows a typical-looking model for &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ti}\)&lt;/span&gt;. All the excitement lies in the third line, which contains the linear model for &lt;span class=&#34;math inline&#34;&gt;\(\log (\sigma_{ti})\)&lt;/span&gt;. The reason we are modeling &lt;span class=&#34;math inline&#34;&gt;\(\log (\sigma_{ti})\)&lt;/span&gt; rather than directly modeling &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is to avoid solutions that predict negative values for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. For this model, it’s unlikely we’d run into that problem. But since the &lt;strong&gt;brms&lt;/strong&gt; default is to use the log link anytime we model &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; within the distributional modeling syntax, we’ll just get used to the log link right from the start. If you are unfamiliar with link functions, they’re widely used within the generalized linear modeling framework. Logistic regression with the logit link and Poisson regression with the log link are two widely-used examples. For more on link functions and the generalized linear model, check out the texts by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-agrestiFoundationsLinearGeneralized2015&#34; role=&#34;doc-biblioref&#34;&gt;Agresti&lt;/a&gt; (&lt;a href=&#34;#ref-agrestiFoundationsLinearGeneralized2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Gelman, Hill, and Vehtari &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;; and McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Anyway, the linear model for &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ti}\)&lt;/span&gt; is exactly the same as with the original cross-classified model, &lt;code&gt;m11&lt;/code&gt;; it includes a grand mean (&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;) and two kinds of deviations around that grand mean (&lt;span class=&#34;math inline&#34;&gt;\(u_{\text{id},i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{time},t}\)&lt;/span&gt;). The model for &lt;span class=&#34;math inline&#34;&gt;\(\log (\sigma_{ti})\)&lt;/span&gt; contains an intercept, which varies across the two levels of time, &lt;span class=&#34;math inline&#34;&gt;\(\eta_{\text{time},t}\)&lt;/span&gt;. Here’s how to fit the model with &lt;code&gt;brms::brm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m17 &amp;lt;-
  brm(data = small_data_long,
      bf(y ~ 1 + (1 | time) + (1 | id),
         sigma ~ 0 + factor(time)),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .999,
                     max_treedepth = 12))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m17)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: y ~ 1 + (1 | time) + (1 | id) 
##          sigma ~ 0 + factor(time)
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.53      0.15     0.15     0.79 1.00     1376     1194
## 
## ~time (Number of levels: 2) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.66      1.26     0.41     5.00 1.00     5133     6854
## 
## Population-Level Effects: 
##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept             0.45      1.07    -1.78     2.66 1.00     4653     5056
## sigma_factortime0     0.01      0.10    -0.18     0.20 1.00     3068     5577
## sigma_factortime1     0.03      0.10    -0.17     0.22 1.00     2678     4488
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even though we didn’t explicitly ask to use the log link in our &lt;code&gt;brm()&lt;/code&gt; syntax, you can look at the second line in the &lt;code&gt;print()&lt;/code&gt; output to see that it was automatically used. Though I won’t explore how to do so, here, one can fit this model without the log link. Anyway, the primary focus in this model is the &lt;code&gt;sigma_factortime0&lt;/code&gt; and &lt;code&gt;sigma_factortime1&lt;/code&gt; lines in the ‘Population-Level Effects’ section of the output. Those are the summaries for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters, conditional on whether &lt;code&gt;time == 0&lt;/code&gt; or &lt;code&gt;time == 1&lt;/code&gt;. Though is might be difficult to evaluate parameters on the log scale, we can simply exponentiate them to convert them back to their natural metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m17)[2:3, c(1, 3:4)] %&amp;gt;% exp()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   Estimate      Q2.5    Q97.5
## sigma_factortime0 1.013928 0.8352872 1.223336
## sigma_factortime1 1.032290 0.8446723 1.247435&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, it looks like the two parameters are largely overlapping. If we work directly with the posterior draws, we can compute a formal difference score and plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m17) %&amp;gt;% 
  mutate(`sigma[time==0]` = exp(b_sigma_factortime0),
         `sigma[time==1]` = exp(b_sigma_factortime1),
         `sigma[time==1]-sigma[time==0]` = exp(b_sigma_factortime1) - exp(b_sigma_factortime0)) %&amp;gt;% 
  pivot_longer(`sigma[time==0]`:`sigma[time==1]-sigma[time==0]`) %&amp;gt;% 
  
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye(.width = .95) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(ylim = c(1.5, 3.1)) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-62-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, it looks like there was little difference between &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{time} = 0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{time} = 1}\)&lt;/span&gt;. This shouldn’t be a surprise; we simulated that data that way. However, it won’t always be like this in real-world data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_18-colon-the-simple-liner-model-with-robust-variance-for-linear-time.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{18} \colon\)&lt;/span&gt; The simple liner model with robust variance for linear time.&lt;/h4&gt;
&lt;p&gt;Our cross-classified approach treated &lt;code&gt;time&lt;/code&gt; as a factor. Here we’ll treat it as a continuous variable in the models of both &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. This will be a straight extension of the simple linear model, &lt;code&gt;m12&lt;/code&gt;, following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} \\
\log(\sigma_{ti}) &amp;amp; = \eta_0 + \eta_1 \text{time}_{ti},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected value at the first timepoint and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; captures the change in &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; for the final timepoint. Now &lt;span class=&#34;math inline&#34;&gt;\(\log(\sigma_{ti})\)&lt;/span&gt; has a similar linear model, where &lt;span class=&#34;math inline&#34;&gt;\(\eta_0\)&lt;/span&gt; is the expected log of the standard deviation at the first timepoint and &lt;span class=&#34;math inline&#34;&gt;\(\eta_1\)&lt;/span&gt; captures the change standard deviation for the final timepoint.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m18 &amp;lt;-
  brm(data = small_data_long,
      bf(y ~ 1 + time, 
         sigma ~ 1 + time),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m18)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: y ~ 1 + time 
##          sigma ~ 1 + time
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept          -0.15      0.11    -0.38     0.06 1.00    11620     6673
## sigma_Intercept     0.14      0.07     0.00     0.28 1.00    11090     7442
## time                1.17      0.16     0.85     1.49 1.00    11137     7818
## sigma_time          0.01      0.10    -0.18     0.21 1.00    11157     7620
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even though this model looks very different from the last one, we can wrangle the posterior draws a little to make a similar plot comparing &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; at the two timepoints.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m18) %&amp;gt;% 
  mutate(`sigma[time==0]` = exp(b_sigma_Intercept),
         `sigma[time==1]` = exp(b_sigma_Intercept + b_sigma_time * 1),
         `sigma[time==1]-sigma[time==0]` = exp(b_sigma_Intercept) - exp(b_sigma_Intercept + b_sigma_time)) %&amp;gt;% 
  pivot_longer(`sigma[time==0]`:`sigma[time==1]-sigma[time==0]`) %&amp;gt;% 
  
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye(.width = .95) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(ylim = c(1.5, 3.1)) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  theme(axis.text.y = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-64-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though this model is robust to differences in &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; based on timepoint, it still ignores systemic differences across participants. The next model tackles that that limitation in spades.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_19-colon-the-liner-model-with-correlated-random-intercepts-for-mu-and-sigma.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{19} \colon\)&lt;/span&gt; The liner model with correlated random intercepts for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma.\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Here we return to the multilevel model framework to accommodate participant-level differences for both &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} + u_{0i}  \\
\log(\sigma_{ti}) &amp;amp; = \eta_0  + u_{2i} \\
\begin{bmatrix} u_{0i} \\ u_{2i} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf \Sigma \right) \\
\mathbf \Sigma &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_2 \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the grand mean for the intercepts and &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; are the participant-level deviations around that grand mean. &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the time slope, which is invariant across all participants in this model. The &lt;span class=&#34;math inline&#34;&gt;\(\eta_0\)&lt;/span&gt; parameter is the grand mean for the log standard deviations and &lt;span class=&#34;math inline&#34;&gt;\(u_{2i}\)&lt;/span&gt; captures the participant-level deviations around that grand mean. In the fourth line, we learn that &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{2i}\)&lt;/span&gt; are multivariate normal, with a mean vector of two zeros and a variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt;. As is typical within the &lt;strong&gt;brms&lt;/strong&gt; framework, we decompose &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; into a variance matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S\)&lt;/span&gt; and correlation matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R\)&lt;/span&gt;. Of particular interest is the &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; parameter, which captures the correlation in the participant-level intercepts and participant-level standard deviations. Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m19 &amp;lt;-
  brm(data = small_data_long,
      bf(y ~ 1 + time + (1 |x| id),
         sigma ~ 1 + (1 |x| id)),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .9))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may have noticed the &lt;code&gt;|x|&lt;/code&gt; parts in the &lt;code&gt;formula&lt;/code&gt; lines for &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt;. What that did was tell &lt;strong&gt;brms&lt;/strong&gt; we wanted those parameters to be correlated. That is, that’s how we estimated the &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; parameter. There was nothing special about including &lt;code&gt;x&lt;/code&gt; between the vertical lines. We could have used any other character. The important thing is that we used the same character in both. Anyway, here’s the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m19)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: y ~ 1 + time + (1 | x | id) 
##          sigma ~ 1 + (1 | x | id)
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)                      0.52      0.16     0.14     0.78 1.00     1496     1443
## sd(sigma_Intercept)                0.11      0.08     0.01     0.29 1.00     3032     4589
## cor(Intercept,sigma_Intercept)    -0.34      0.49    -0.97     0.80 1.00     5949     6097
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept          -0.15      0.12    -0.38     0.08 1.00    10102     7923
## sigma_Intercept     0.02      0.08    -0.13     0.17 1.00     2344     3931
## time                1.17      0.15     0.88     1.45 1.00    14868     8009
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once again, &lt;strong&gt;brms&lt;/strong&gt; used the log link for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. If you want to see &lt;span class=&#34;math inline&#34;&gt;\(\eta_0\)&lt;/span&gt; in its natural &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; metric, exponentiate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(m19)[&amp;quot;sigma_Intercept&amp;quot;, c(1, 3:4)] %&amp;gt;% exp()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate      Q2.5     Q97.5 
## 1.0171275 0.8759026 1.1849572&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;sd(sigma_Intercept)&lt;/code&gt; row in the ‘Group-Level Effects’ section shows the variation in those &lt;span class=&#34;math inline&#34;&gt;\(\log \sigma\)&lt;/span&gt;’s. It might be easier to appreciate them in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(m19) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_id__sigma&amp;quot;)) %&amp;gt;% 
  mutate(sigma_i = exp(sd_id__sigma_Intercept + value)) %&amp;gt;% 
  
  ggplot(aes(x = sigma_i, y = reorder(name, sigma_i))) +
  stat_pointinterval(point_interval = mean_qi, .width = .95, size = 1/6) +
  scale_y_discrete(expression(italic(i)), breaks = NULL) +
  labs(subtitle = expression(sigma[2]~summarizes~the~variation~across~these),
       x = expression(sigma[italic(i)]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-67-1.png&#34; width=&#34;336&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, there was not a lot of variation in &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; across participants. This is because we simulated the data that way. Though it may be hard to model participant-level variances with 2-timepoint data, I have found it comes in handy in real-world data sets based on more measurement occasions.&lt;/p&gt;
&lt;p&gt;Finally, it might be useful to consider our &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; parameter, which suggested a mild negative correlation between the &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{2i}\)&lt;/span&gt; deviations. Here’s how you might visualize that in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  posterior_samples(m19) %&amp;gt;% select(starts_with(&amp;quot;r_id[&amp;quot;)) %&amp;gt;% set_names(1:100),
  posterior_samples(m19) %&amp;gt;% select(starts_with(&amp;quot;r_id__sigma&amp;quot;) %&amp;gt;% set_names(1:100))
) %&amp;gt;% 
  mutate(iter = rep(1:c(n() / 2), times = 2),
         type = rep(c(&amp;quot;intercept&amp;quot;, &amp;quot;log_sigma&amp;quot;), each = n() / 2)) %&amp;gt;% 
  pivot_longer(-c(iter, type)) %&amp;gt;% 
  pivot_wider(names_from = type, values_from = value) %&amp;gt;% 
  
  ggplot(aes(x = intercept, y = log_sigma, group = name)) +
  stat_ellipse(geom = &amp;quot;polygon&amp;quot;, level = .01, alpha = 1/4) +
  labs(x = expression(italic(u)[0][italic(i)]),
       y = expression(log(italic(u)[2][italic(i)])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-68-1.png&#34; width=&#34;336&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each of the ovals is a 1% ellipse of the bivariate posterior for &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\log(u_{2i})\)&lt;/span&gt;. Notice how using ellipses helps reveal the differences in the between- and within-person patterns.&lt;/p&gt;
&lt;p&gt;This approach where residual variance parameters vary across participants has its origins in the work of &lt;a href=&#34;https://health.uchicago.edu/faculty/donald-hedeker-phd&#34;&gt;Donald Hedeker&lt;/a&gt; and colleagues &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hedekerApplicationMixedeffectsLocation2008&#34; role=&#34;doc-biblioref&#34;&gt;Hedeker et al., 2008&lt;/a&gt;, &lt;a href=&#34;#ref-hedekerModelingWithinsubjectVariance2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;. More recently, &lt;a href=&#34;https://twitter.com/rastlab&#34;&gt;Philippe Rast&lt;/a&gt; and colleagues (particularly graduate student, &lt;a href=&#34;wdonald_1985&#34;&gt;Donald Williams&lt;/a&gt;) have adapted this approach for use within the Stan/&lt;strong&gt;brms&lt;/strong&gt; ecosystem &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-williamsBayesianMultivariateMixedeffects2019a&#34; role=&#34;doc-biblioref&#34;&gt;Williams, Liu, et al., 2019&lt;/a&gt;; &lt;a href=&#34;#ref-williamsSurfaceUnearthingWithinperson2019&#34; role=&#34;doc-biblioref&#34;&gt;Williams, Rouder, et al., 2019&lt;/a&gt;; &lt;a href=&#34;#ref-williamsPuttingIndividualReliability2019&#34; role=&#34;doc-biblioref&#34;&gt;Williams, Martin, et al., 2019&lt;/a&gt;; &lt;a href=&#34;#ref-williamsBayesianNonlinearMixedeffects2019a&#34; role=&#34;doc-biblioref&#34;&gt;Williams, Zimprich, et al., 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathcal-m_20-colon-the-liner-model-with-a-random-slope-for-mu-and-uncorrelated-random-intercept-for-sigma.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal M_{20} \colon\)&lt;/span&gt; The liner model with a random slope for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and uncorrelated random intercept for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/h4&gt;
&lt;p&gt;Though we can find interesting things when we allow the random components in the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; models, we don’t have to think of them as covarying. Here we fit an extension of the linear model with a random time slope, where we add an orthogonal random intercept for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{ti} &amp;amp; \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} &amp;amp; = \beta_0 + \beta_1 \text{time}_{ti} + u_{1i} \\
\log(\sigma_{ti}) &amp;amp; = \eta_0  + u_{2i}  \\
u_{1i} &amp;amp; \sim \operatorname{Normal}(0, \sigma_1) \\
u_{2i} &amp;amp; \sim \operatorname{Normal}(0, \sigma_2),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the two random components, &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{2i}\)&lt;/span&gt;, are now modeled with separate normal distributions, &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, \sigma_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, \sigma_2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m20 &amp;lt;-
  brm(data = small_data_long,
      bf(y ~ 1 + time + (0 + time | id),
         sigma ~ 1 + (1 | id)),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in sharp contrast with our syntax for the previous model, this time we did not employ the &lt;code&gt;|x|&lt;/code&gt; syntax in the &lt;code&gt;formula&lt;/code&gt; lines for &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt;. By omitting the &lt;code&gt;|x|&lt;/code&gt; syntax, we omitted the correlation among those two random effects. Here’s the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: y ~ 1 + time + (0 + time | id) 
##          sigma ~ 1 + (1 | id)
##    Data: small_data_long (Number of observations: 200) 
## Samples: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;
##          total post-warmup samples = 10000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(time)                0.33      0.21     0.02     0.75 1.00     2031     4346
## sd(sigma_Intercept)     0.12      0.09     0.00     0.32 1.00     2463     3519
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept          -0.15      0.11    -0.37     0.07 1.00    13451     7317
## sigma_Intercept     0.11      0.06    -0.02     0.23 1.00     4612     4412
## time                1.18      0.16     0.86     1.49 1.00    12600     7054
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2\)&lt;/span&gt; summaries in the ‘Group-Level Effects’ section. It’s hard to compare them directly, because one is based on parameters in the log metric. But we can at least get a sense of what these parameters are summarizing by plotting the bivariate posterior for &lt;span class=&#34;math inline&#34;&gt;\(u_{1i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\log(u_{2i})\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  posterior_samples(m20) %&amp;gt;% select(starts_with(&amp;quot;r_id[&amp;quot;)) %&amp;gt;% set_names(1:100),
  posterior_samples(m20) %&amp;gt;% select(starts_with(&amp;quot;r_id__sigma&amp;quot;) %&amp;gt;% set_names(1:100))
) %&amp;gt;% 
  mutate(iter = rep(1:c(n() / 2), times = 2),
         type = rep(c(&amp;quot;slope&amp;quot;, &amp;quot;log_sigma&amp;quot;), each = n() / 2)) %&amp;gt;% 
  pivot_longer(-c(iter, type)) %&amp;gt;% 
  pivot_wider(names_from = type, values_from = value) %&amp;gt;% 
  
  ggplot(aes(x = slope, y = log_sigma, group = name)) +
  stat_ellipse(geom = &amp;quot;polygon&amp;quot;, level = .01, alpha = 1/4) +
  labs(x = expression(italic(u)[1][italic(i)]),
       y = expression(log(italic(u)[2][italic(i)])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-regression-models-for-2-timepoint-non-experimental-data/index_files/figure-html/unnamed-chunk-70-1.png&#34; width=&#34;336&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how, this time, the 1% ellipses suggest no clear association between these two dimensions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;As promised, here I recommend some resources for understanding the models in this post.&lt;/p&gt;
&lt;div id=&#34;books-focusing-on-longutidinal-data-analysis.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Books focusing on longutidinal data analysis.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;My introduction to longitudinal data analysis was through Singer and Willett &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, &lt;a href=&#34;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;&lt;em&gt;Applied longitudinal data analysis: Modeling change and event occurrence&lt;/em&gt;&lt;/a&gt;. Their focus was on the multilevel growth model and on survival analysis, primary from a maximum-likelihood frequentist framework. However, they generally avoided 2-timepoint data analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hoffman’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text, &lt;a href=&#34;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&#34;&gt;&lt;em&gt;Longitudinal analysis: Modeling within-person fluctuation and change&lt;/em&gt;&lt;/a&gt; is another thorough introduction to the multilevel growth model, from a frequentist perspective. Hoffman covered 2-timepoint data analysis and variants from the ANOVA family. The text comes with a companion website, &lt;a href=&#34;https://www.pilesofvariance.com/&#34;&gt;https://www.pilesofvariance.com/&lt;/a&gt;, which contains extensive data and code files for reproducing the material.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Newsom’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-newsom2015longitudinal&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text, &lt;a href=&#34;http://www.longitudinalsem.com/&#34;&gt;&lt;em&gt;Longitudinal structural equation modeling: A comprehensive introduction&lt;/em&gt;&lt;/a&gt;, covers longitudinal data analysis from a structural equation modeling (SEM) perspective. Even for those not interested in SEM, his Chapter 4 does a nice job introducing the autoregressive and change-score models. The companion website, &lt;a href=&#34;http://www.longitudinalsem.com/&#34;&gt;http://www.longitudinalsem.com/&lt;/a&gt;, contains data and script files for most of the problems in the text.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;books-introducing-regression.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Books introducing regression.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Gelman, Hill, and Vehtari’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://www.cambridge.org/core/books/regression-and-other-stories/DD20DD6C9057118581076E54E40C372C&#34;&gt;&lt;em&gt;Regression and other stories&lt;/em&gt;&lt;/a&gt; contains a thorough introduction to single-level regression, primarily from a Bayesian framework. The text is not oriented around longitudinal analyses, per se, but it does include several chapters on causal inference. Vehtari hosts a GitHub repo, &lt;a href=&#34;https://github.com/avehtari/ROS-Examples&#34;&gt;https://github.com/avehtari/ROS-Examples&lt;/a&gt;, where you can download the data files and &lt;strong&gt;R&lt;/strong&gt; scripts for many of the examples.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Both editions of McElreath’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;&lt;em&gt;Statistical rethinking: A Bayesian course with examples in R and Stan&lt;/em&gt;&lt;/a&gt; provide a thorough introduction to Bayesian regression, both single-level and multilevel. McElreath also touched on causal inference and included a few examples of longitudinal data analysis. His text includes extensive examples of &lt;strong&gt;R&lt;/strong&gt; code and his website, &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;, contains information about the accompanying statistical software.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] patchwork_1.1.1 tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0  
##  [7] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3  
## [13] tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] splines_4.0.4        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [49] htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [53] ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3      loo_2.4.1           
##  [57] dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2       tidyselect_1.1.0    
##  [61] rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [65] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [69] broom_0.7.5          ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1       
##  [73] yaml_2.2.1           processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [77] nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [81] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [85] gamm4_0.2-6          curl_4.3             reprex_0.3.0         statmod_1.4.35      
##  [89] stringi_1.5.3        highr_0.8            ps_1.6.0             blogdown_1.3        
##  [93] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [97] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1        
## [101] lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [105] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [109] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [113] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [117] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [125] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-agrestiFoundationsLinearGeneralized2015&#34; class=&#34;csl-entry&#34;&gt;
Agresti, A. (2015). &lt;em&gt;Foundations of linear and generalized linear models&lt;/em&gt;. &lt;span&gt;John Wiley &amp;amp; Sons&lt;/span&gt;. &lt;a href=&#34;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&#34;&gt;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Bürkner2021Distributional&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021a). &lt;em&gt;Estimating distributional models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Bürkner2021Multivariate&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021b). &lt;em&gt;Estimating multivariate models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Bürkner2021Non_linear&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021c). &lt;em&gt;Estimating non-linear models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-debruineUnderstandingMixedEffects2020&#34; class=&#34;csl-entry&#34;&gt;
DeBruine, L. M., &amp;amp; Barr, D. J. (2020). &lt;em&gt;Understanding mixed effects models through data simulation&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1177/2515245920965119&#34;&gt;https://doi.org/10.1177/2515245920965119&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hedekerApplicationMixedeffectsLocation2008&#34; class=&#34;csl-entry&#34;&gt;
Hedeker, D., Mermelstein, R. J., &amp;amp; Demirtas, H. (2008). An application of a mixed-effects location scale model for analysis of ecological momentary assessment (&lt;span&gt;EMA&lt;/span&gt;) data. &lt;em&gt;Biometrics&lt;/em&gt;, &lt;em&gt;64&lt;/em&gt;(2), 627–634. &lt;a href=&#34;https://doi.org/10.1111/j.1541-0420.2007.00924.x&#34;&gt;https://doi.org/10.1111/j.1541-0420.2007.00924.x&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hedekerModelingWithinsubjectVariance2012&#34; class=&#34;csl-entry&#34;&gt;
Hedeker, D., Mermelstein, R. J., &amp;amp; Demirtas, H. (2012). Modeling between- and within-subject variance in ecological momentary assessment (&lt;span&gt;EMA&lt;/span&gt;) data using mixed-effects location scale models. &lt;em&gt;Statistics in Medicine&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(27). &lt;a href=&#34;https://doi.org/10.1002/sim.5338&#34;&gt;https://doi.org/10.1002/sim.5338&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hoffmanLongitudinalAnalysisModeling2015&#34; class=&#34;csl-entry&#34;&gt;
Hoffman, L. (2015). &lt;em&gt;Longitudinal analysis: &lt;span&gt;Modeling&lt;/span&gt; within-person fluctuation and change&lt;/em&gt; (1 edition). &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&#34;&gt;https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kisbu2013monte&#34; class=&#34;csl-entry&#34;&gt;
Kisbu-Sakarya, Y., MacKinnon, D. P., &amp;amp; Aiken, L. S. (2013). A &lt;span&gt;Monte Carlo&lt;/span&gt; comparison study of the power of the analysis of covariance, simple difference, and residual change scores in testing two-wave data. &lt;em&gt;Educational and Psychological Measurement&lt;/em&gt;, &lt;em&gt;73&lt;/em&gt;(1), 47–62. &lt;a href=&#34;https://doi.org/10.1177/0013164412450574&#34;&gt;https://doi.org/10.1177/0013164412450574&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020). &lt;em&gt;Statistical rethinking with brms, Ggplot2, and the tidyverse: &lt;span&gt;Second&lt;/span&gt; edition&lt;/em&gt; (version 0.1.1). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lordStatisticalTheoriesMental1968&#34; class=&#34;csl-entry&#34;&gt;
Lord, F. M., &amp;amp; Novick, M. R. (1968). &lt;em&gt;Statistical theories of mental test scores&lt;/em&gt;. &lt;span&gt;Addison-Wesley&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-newsom2015longitudinal&#34; class=&#34;csl-entry&#34;&gt;
Newsom, J. T. (2015). &lt;em&gt;Longitudinal structural equation modeling: &lt;span&gt;A&lt;/span&gt; comprehensive introduction&lt;/em&gt;. &lt;span&gt;Routledge&lt;/span&gt;. &lt;a href=&#34;http://www.longitudinalsem.com/&#34;&gt;http://www.longitudinalsem.com/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-patchwork&#34; class=&#34;csl-entry&#34;&gt;
Pedersen, T. L. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;patchwork&lt;/span&gt;: &lt;span&gt;The&lt;/span&gt; composer of plots&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;https://CRAN.R-project.org/package=patchwork&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rogosaGrowthCurveApproach1982&#34; class=&#34;csl-entry&#34;&gt;
Rogosa, D., Brandt, D., &amp;amp; Zimowski, M. (1982). A growth curve approach to the measurement of change. &lt;em&gt;Psychological Bulletin&lt;/em&gt;, &lt;em&gt;92&lt;/em&gt;(3), 726–748. &lt;a href=&#34;https://doi.org/10.1037/0033-2909.92.3.726&#34;&gt;https://doi.org/10.1037/0033-2909.92.3.726&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-singerAppliedLongitudinalData2003&#34; class=&#34;csl-entry&#34;&gt;
Singer, J. D., &amp;amp; Willett, J. B. (2003). &lt;em&gt;Applied longitudinal data analysis: &lt;span&gt;Modeling&lt;/span&gt; change and event occurrence&lt;/em&gt;. &lt;span&gt;Oxford University Press, USA&lt;/span&gt;. &lt;a href=&#34;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-walkerElementsOfStatisticalModeling2018&#34; class=&#34;csl-entry&#34;&gt;
Walker, J. A. (2018). &lt;em&gt;Elements of statistical modeling for experimental biology&lt;/em&gt; (&#34;2020–11th–22&#34; ed.). &lt;a href=&#34;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/&#34;&gt;https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsBayesianMultivariateMixedeffects2019a&#34; class=&#34;csl-entry&#34;&gt;
Williams, D. R., Liu, S., Martin, S. R., &amp;amp; Rast, P. (2019). &lt;em&gt;Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/4kfjp&#34;&gt;https://doi.org/10.31234/osf.io/4kfjp&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsPuttingIndividualReliability2019&#34; class=&#34;csl-entry&#34;&gt;
Williams, D. R., Martin, S. R., &amp;amp; Rast, P. (2019). &lt;em&gt;Putting the individual into reliability: &lt;span&gt;Bayesian&lt;/span&gt; testing of homogeneous within-person variance in hierarchical models&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/hpq7w&#34;&gt;https://doi.org/10.31234/osf.io/hpq7w&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsSurfaceUnearthingWithinperson2019&#34; class=&#34;csl-entry&#34;&gt;
Williams, D. R., Rouder, J., &amp;amp; Rast, P. (2019). &lt;em&gt;Beneath the surface: &lt;span&gt;Unearthing&lt;/span&gt; within-&lt;span&gt;Person&lt;/span&gt; variability and mean relations with &lt;span&gt;Bayesian&lt;/span&gt; mixed models&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/gwatq&#34;&gt;https://doi.org/10.31234/osf.io/gwatq&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsBayesianNonlinearMixedeffects2019a&#34; class=&#34;csl-entry&#34;&gt;
Williams, D. R., Zimprich, D. R., &amp;amp; Rast, P. (2019). A &lt;span&gt;Bayesian&lt;/span&gt; nonlinear mixed-effects location scale model for learning. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(5), 1968–1986. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01255-9&#34;&gt;https://doi.org/10.3758/s13428-019-01255-9&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Throughout this post, my statistical notation will be a blend of sensibilities from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;Singer &amp;amp; Willett&lt;/a&gt; (&lt;a href=&#34;#ref-singerAppliedLongitudinalData2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;Hoffman&lt;/a&gt; (&lt;a href=&#34;#ref-hoffmanLongitudinalAnalysisModeling2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;, and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath&lt;/a&gt; (&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Though I’m no fan of the null-hypothesis significance testing paradigm, it might be helpful to point out if one were to focus on whether zero is within the 95% interval bounds of our &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; parameter, you be viewing this model through the lens of the repeated-measures &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test. For more on that connection, see Chapter 3 in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-newsom2015longitudinal&#34; role=&#34;doc-biblioref&#34;&gt;Newsom&lt;/a&gt; (&lt;a href=&#34;#ref-newsom2015longitudinal&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;There’s some debate over how to think about the repeated measures ANOVA and what its closest multilevel analogue might be. For a nice collection of perspectives, check out &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1342645143082594304&#34;&gt;this Twitter thread&lt;/a&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multilevel models and the index-variable approach</title>
      <link>/post/2020-12-09-multilevel-models-and-the-index-variable-approach/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-12-09-multilevel-models-and-the-index-variable-approach/</guid>
      <description>
&lt;script src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The set-up&lt;/h2&gt;
&lt;p&gt;PhD candidate Huaiyu Liu recently reached out with a question about how to analyze clustered data. Liu’s basic setup was an experiment with four conditions. The dependent variable was binary, where success = 1, fail = 0. Each participant completed multiple trials under each of the four conditions. The catch was Liu wanted to model those four conditions with a multilevel model using the index-variable approach McElreath advocated for in the second edition of his text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Like any good question, this one got my gears turning. Thanks, Liu! The purpose of this post will be to show how to model data like this two different ways.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;In this post, I’m presuming you are familiar with Bayesian multilevel models and with logistic regression. All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. The statistical models will be fit with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. We’ll also make a little use of the &lt;strong&gt;tidybayes&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;rethinking&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-rethinking&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020b&lt;/a&gt;)&lt;/span&gt; packages. If you need to shore up, I list some educational resources at the &lt;a href=&#34;#next-steps&#34;&gt;end of the post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Load the primary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)
library(tidybayes)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for Liu’s question had the same basic structure as the &lt;code&gt;chimpanzees&lt;/code&gt; data from the &lt;strong&gt;rethinking&lt;/strong&gt; package. Happily, it’s also the case that Liu wanted to fit a model that was very similar to model &lt;code&gt;m14.3&lt;/code&gt; from Chapter 14 of McElreath’s text. Here we’ll load the data and wrangle a little.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(chimpanzees, package = &amp;quot;rethinking&amp;quot;)
d &amp;lt;- chimpanzees
rm(chimpanzees)

# wrangle
d &amp;lt;-
  d %&amp;gt;% 
  mutate(actor = factor(actor),
         treatment = factor(1 + prosoc_left + 2 * condition),
         # this will come in handy, later
         labels    = factor(treatment,
                            levels = 1:4,
                            labels = c(&amp;quot;r/n&amp;quot;, &amp;quot;l/n&amp;quot;, &amp;quot;r/p&amp;quot;, &amp;quot;l/p&amp;quot;)))

glimpse(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 504
## Columns: 10
## $ actor        &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ recipient    &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ condition    &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ block        &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5,…
## $ trial        &amp;lt;int&amp;gt; 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 4…
## $ prosoc_left  &amp;lt;int&amp;gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,…
## $ chose_prosoc &amp;lt;int&amp;gt; 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,…
## $ pulled_left  &amp;lt;int&amp;gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,…
## $ treatment    &amp;lt;fct&amp;gt; 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1,…
## $ labels       &amp;lt;fct&amp;gt; r/n, r/n, l/n, r/n, l/n, l/n, l/n, l/n, r/n, r/n, r/n, l/n, r/n, l/n, r/n, l/…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The focal variable will be &lt;code&gt;pulled_left&lt;/code&gt;, which is binary and coded yes = 1, no = 0. We have four experimental conditions, which are indexed &lt;code&gt;1&lt;/code&gt; through &lt;code&gt;4&lt;/code&gt; in the &lt;code&gt;treatment&lt;/code&gt; variable. The shorthand labels for those conditions are saved as &lt;code&gt;labels&lt;/code&gt;. These data are simple in that there are only seven participants, who are indexed in the &lt;code&gt;actor&lt;/code&gt; column.&lt;/p&gt;
&lt;p&gt;Within the generalized linear model framework, we typically model binary variables with binomial likelihood&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. When you use the conventional link function, you can call this &lt;em&gt;logistic regression&lt;/em&gt;. When you have a binary variable, the parameter of interest is the probability of a 1 in your criterion variable. When you want a quick sample statistic, you can estimate those probabilities with the mean. To get a sense of the data, here are the sample probabilities &lt;code&gt;pulled_left == 1&lt;/code&gt; for each of our seven participants, by the four levels of &lt;code&gt;treatment&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  mutate(treatment = str_c(&amp;quot;treatment &amp;quot;, treatment)) %&amp;gt;% 
  group_by(actor, treatment) %&amp;gt;% 
  summarise(p = mean(pulled_left) %&amp;gt;% round(digits = 2)) %&amp;gt;% 
  pivot_wider(values_from = p, names_from = treatment) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;actor&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;treatment 4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models&lt;/h2&gt;
&lt;p&gt;We are going to analyze these data two kinds of multilevel models. The first way is the direct analogue to McElreath’s model &lt;code&gt;m14.3&lt;/code&gt;; it’ll be a multilevel model using the index-variable approach for the population-level intercepts. The second way is a multilevel Bayesian alternative to the ANOVA, based on Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text.&lt;/p&gt;
&lt;p&gt;However, some readers might benefit from a review of what I even mean by the “index-variable” approach. This approach is uncommon in my field of clinical psychology, for example. So before we get down to business, we’ll clear that up by contrasting it with the widely-used dummy-variable approach.&lt;/p&gt;
&lt;div id=&#34;warm-up-with-the-simple-index-variable-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Warm-up with the simple index-variable model.&lt;/h3&gt;
&lt;p&gt;Let’s forget the multilevel model for a moment. One of the more popular ways to use a categorical predictor variable is with the dummy-variable approach. Say we wanted to predict our criterion variable &lt;code&gt;pulled_left&lt;/code&gt; with &lt;code&gt;treatment&lt;/code&gt;, which is a four-category nominal variable. If we denote the number of categories &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;, &lt;code&gt;treatment&lt;/code&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(K = 4\)&lt;/span&gt; nominal variable. The dummy-variable approach would be to break &lt;code&gt;treatment&lt;/code&gt; into &lt;span class=&#34;math inline&#34;&gt;\(K - 1\)&lt;/span&gt; binary variables, which we’d simultaneously enter into the model. Say we broke &lt;code&gt;treatment&lt;/code&gt; into three dummies with the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;% 
  mutate(d2 = if_else(treatment == 2, 1, 0),
         d3 = if_else(treatment == 3, 1, 0),
         d4 = if_else(treatment == 4, 1, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dummy variables &lt;code&gt;d2&lt;/code&gt;, &lt;code&gt;d3&lt;/code&gt;, and &lt;code&gt;d4&lt;/code&gt; would capture the four levels of &lt;code&gt;treatment&lt;/code&gt; like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  distinct(treatment, d2, d3, d4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   treatment d2 d3 d4
## 1         1  0  0  0
## 2         2  1  0  0
## 3         3  0  1  0
## 4         4  0  0  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;d2 == 1&lt;/code&gt; only when &lt;code&gt;treatment == 2&lt;/code&gt;. Similarly, &lt;code&gt;d3 == 1&lt;/code&gt; only when &lt;code&gt;treatment == 3&lt;/code&gt; and &lt;code&gt;d4 == 1&lt;/code&gt; only when &lt;code&gt;treatment == 4&lt;/code&gt;. When &lt;code&gt;treatment == 1&lt;/code&gt;, all three dummies are &lt;code&gt;0&lt;/code&gt;, which makes &lt;code&gt;treatment == 1&lt;/code&gt; the reference category.&lt;/p&gt;
&lt;p&gt;You can write out the statistical model using these &lt;span class=&#34;math inline&#34;&gt;\(K - 1\)&lt;/span&gt; dummies as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \beta_0 + \beta_1 \text{d2}_i + \beta_2 \text{d3}_i + \beta_3 \text{d4}_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is both the “intercept” and the expected value for the first level of &lt;code&gt;treatment&lt;/code&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the expected change in value, relative to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, for the second level of &lt;code&gt;treatment&lt;/code&gt;. In the same way, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; are changes relative to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; for the third and fourth levels of &lt;code&gt;treatment&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;The index-variable approach takes a different stance. Rather than dividing &lt;code&gt;treatment&lt;/code&gt; into dummies, one simply allows each level of &lt;code&gt;treatment&lt;/code&gt; to have its own intercept. You can write that in statistical notation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \gamma_{\text{treatment}[i]},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\text{treatment}[i]\)&lt;/span&gt; subscript indicates the different levels of &lt;code&gt;treatment&lt;/code&gt;, which vary across cases &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, each get their own &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; parameter. Because &lt;code&gt;treatment&lt;/code&gt; has four levels, we end up with four &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;’s: &lt;span class=&#34;math inline&#34;&gt;\(\gamma_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_3\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_4\)&lt;/span&gt;. When you model intercepts in this way, none of the levels of &lt;code&gt;treatment&lt;/code&gt; end up as the reference category and none of the other levels of &lt;code&gt;treatment&lt;/code&gt; are parameterized in terms of deviations from the reference category. Each intercept is estimated in its own terms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick note on notation&lt;/strong&gt;: There’s nothing special about using the letter &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; for our index variable. We could just as easily have used &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;, or whatever. The only reason I’m using &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;, here, is because that’s what McElreath used for his model &lt;code&gt;m14.3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you’d like more practice with dummy variables, McElreath lectured on them &lt;a href=&#34;https://www.youtube.com/watch?v=e0tO64mtYMU&amp;amp;feature=youtu.be&amp;amp;t=3360&#34;&gt;here&lt;/a&gt;. If you’d like to hear McElreath walk out index variables a bit more, you can find that lecture &lt;a href=&#34;https://youtu.be/l_7yIUqWBmE?t=83&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mcelreaths-approach.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;McElreath’s approach.&lt;/h3&gt;
&lt;p&gt;Okay, now we’re up to speed on what Liu meant by wanting to fit a model with the index-variable approach, let’s see what that looks like in a multilevel model.&lt;/p&gt;
&lt;div id=&#34;the-statistical-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The statistical model.&lt;/h4&gt;
&lt;p&gt;Here’s how we might express McElreath’s index-variable approach to these data in statistical notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \gamma_{\text{treatment}[i]} + \alpha_{\text{actor}[i], \text{treatment}[i]} \\
\gamma_j &amp;amp; \sim \operatorname{Normal}(0, 1), \;\;\; \text{for } j = 1, \dots, 4 \\
\begin{bmatrix} \alpha_{j, 1} \\ \alpha_{j, 2} \\ \alpha_{j, 3} \\ \alpha_{j, 4} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \begin{pmatrix} \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \mathbf \Sigma_\text{actor} \end{pmatrix} \\
\mathbf \Sigma_\text{actor} &amp;amp; = \mathbf{S_\alpha R_\alpha S_\alpha} \\
\sigma_{\alpha, [1]}, \dots, \sigma_{\alpha, [4]} &amp;amp; \sim \operatorname{Exponential}(1) \\
\mathbf R_\alpha &amp;amp; \sim \operatorname{LKJ}(2).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this model, we have four population-level intercepts, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_1, \dots, \gamma_4\)&lt;/span&gt;, one for each of the four levels of &lt;code&gt;treatment&lt;/code&gt;. This is one of the critical features required by Liu’s question. &lt;code&gt;actor&lt;/code&gt; is our higher-level grouping variable. The third line spells out the priors for those four &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;’s. Though they all get the same prior in this model, you could use different priors for each, if you wanted.&lt;/p&gt;
&lt;p&gt;Going back to the second line, the term &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{actor}[i], \text{treatment}[i]}\)&lt;/span&gt; is meant to convey that each of the &lt;code&gt;treatment&lt;/code&gt; effects can vary by &lt;code&gt;actor&lt;/code&gt;. We can–and should–do this because each of our participants experienced each of the four levels of &lt;code&gt;treatment&lt;/code&gt; many times. The fourth line containing the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{MVNormal}(\cdot)\)&lt;/span&gt; operator might look intimidating. The vector on the left is just a way to list those four &lt;code&gt;actor&lt;/code&gt;-level deviations we just mentioned. We’ll be treating them much the same way you might treat a random intercept and slope in a multilevel growth model. That is, we presume they follow a multivariate normal distribution. Since these are all deviations, the 4-dimensional mean vector in our multivariate normal distribution contains four zeros. The spread around those zeros are controlled by the variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_\text{actor}\)&lt;/span&gt;. In the next line, we learn that &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_\text{actor}\)&lt;/span&gt; can be decomposed into two terms, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S_\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. It may not yet be clear by the notation, but &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S_\alpha\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; diagonal matrix of standard deviations,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf S_\alpha = \begin{bmatrix} \sigma_{\alpha, [1]} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_{\alpha, [2]} &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; \sigma_{\alpha, [3]} &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \sigma_{\alpha, [4]} \end{bmatrix}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a similar way, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; correlation matrix,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf R_\alpha = \begin{bmatrix} 1 &amp;amp; \rho_{\alpha, [1, 2]} &amp;amp; \rho_{\alpha, [1, 3]} &amp;amp; \rho_{\alpha, [1, 4]} \\ \rho_{\alpha, [2, 1]} &amp;amp; 1 &amp;amp; \rho_{\alpha, [2, 3]} &amp;amp; \rho_{\alpha, [2, 4]} \\ \rho_{\alpha, [3, 1]} &amp;amp; \rho_{\alpha, [3, 2]} &amp;amp; 1 &amp;amp; \rho_{\alpha, [3, 4]} \\ \rho_{\alpha, [4, 1]} &amp;amp; \rho_{\alpha, [4, 2]} &amp;amp; \rho_{\alpha, [4, 3]} &amp;amp; 1 \end{bmatrix}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As we see in the sixth line, all the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\alpha\)&lt;/span&gt; parameters have individual &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Exponential}(1)\)&lt;/span&gt; priors. The final line shows the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt; matrix has the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{LKJ}(2)\)&lt;/span&gt; prior. Though you could certainly use different priors, here we’re sticking close to those McElreath used in his text.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fit the model.&lt;/h4&gt;
&lt;p&gt;Though the statistical model might look intimidating, we can fit it pretty easily with &lt;code&gt;brms::brm()&lt;/code&gt;. We’ll call this &lt;code&gt;fit1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- 
  brm(data = d, 
      family = binomial,
      pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor),
      prior = c(prior(normal(0, 1), class = b),
                prior(exponential(1), class = sd),
                prior(lkj(2), class = cor)),
      cores = 4, seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From a syntax perspective, the important parts were the two occurrences of &lt;code&gt;0 + treatment&lt;/code&gt; in the model &lt;code&gt;formula&lt;/code&gt; line. The first occurrence was how we told &lt;strong&gt;brms&lt;/strong&gt; we wanted our population-level intercept to be indexed by the four levels of &lt;code&gt;treatment&lt;/code&gt;. The second occurrence was where we told &lt;strong&gt;brms&lt;/strong&gt; we wanted those to vary across our seven levels of &lt;code&gt;actor&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor) 
##    Data: d (Number of observations: 504) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~actor (Number of levels: 7) 
##                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(treatment1)                 1.36      0.48     0.69     2.52 1.00     1979     2494
## sd(treatment2)                 0.90      0.40     0.34     1.89 1.00     2188     2451
## sd(treatment3)                 1.84      0.56     1.00     3.15 1.00     2999     3036
## sd(treatment4)                 1.55      0.60     0.73     2.97 1.00     2550     2518
## cor(treatment1,treatment2)     0.42      0.28    -0.21     0.87 1.00     2511     2461
## cor(treatment1,treatment3)     0.52      0.25    -0.07     0.90 1.00     2313     2619
## cor(treatment2,treatment3)     0.48      0.27    -0.12     0.89 1.00     2989     3261
## cor(treatment1,treatment4)     0.44      0.27    -0.17     0.86 1.00     2515     3034
## cor(treatment2,treatment4)     0.44      0.28    -0.17     0.87 1.00     3205     3217
## cor(treatment3,treatment4)     0.57      0.24     0.00     0.92 1.00     3165     3234
## 
## Population-Level Effects: 
##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## treatment1     0.23      0.46    -0.66     1.16 1.00     1871     2504
## treatment2     0.66      0.36    -0.06     1.39 1.00     2780     2687
## treatment3    -0.02      0.56    -1.14     1.07 1.00     3017     2966
## treatment4     0.69      0.51    -0.32     1.72 1.00     3006     2575
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look at the lower level of the output, the four levels in the ‘Population-Level Effects’ section are the four levels of &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{\text{treatment}[i]}\)&lt;/span&gt; from our statistical formula. If you look above at the ‘Group-Level Effects’ section, the four lines beginning with “sd” correspond to our four &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\alpha, [1]}, \dots, \sigma_{\alpha, [4]}\)&lt;/span&gt; parameters. The correlations among those are depicted in the six rows beginning with “cor,” which correspond to the elements within the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R_\alpha\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;p&gt;It might help if we visualized the model in a plot. Here are the results depicted in a streamlined version of McElreath’s Figure 14.7 &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020a, p. 452&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# for annotation
text &amp;lt;-
  distinct(d, labels) %&amp;gt;% 
  mutate(actor = &amp;quot;actor[1]&amp;quot;,
         prop  = c(.07, .8, .08, .795))

# define the new data
nd &amp;lt;-
  d %&amp;gt;% 
  distinct(actor, condition, labels, prosoc_left, treatment)

# get the fitted draws
fitted(fit1,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  mutate(actor     = str_c(&amp;quot;actor[&amp;quot;, actor, &amp;quot;]&amp;quot;),
         condition = factor(condition)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = labels)) +
  geom_hline(yintercept = .5, color = &amp;quot;white&amp;quot;, linetype = 2) +
  # posterior predictions
  geom_line(aes(y = Estimate, group = prosoc_left),
            size = 3/4) +
  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, shape = condition),
                  fill = &amp;quot;transparent&amp;quot;, fatten = 10, size = 1/3, show.legend = F) + 
  # annotation for the conditions
  geom_text(data = text,
            aes(y = prop, label = labels), 
            size = 3) +
  scale_shape_manual(values = c(21, 19)) +
  scale_x_discrete(NULL, breaks = NULL) +
  scale_y_continuous(&amp;quot;proportion left lever&amp;quot;, breaks = 0:2 / 2, labels = c(&amp;quot;0&amp;quot;, &amp;quot;.5&amp;quot;, &amp;quot;1&amp;quot;)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~actor, nrow = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig1-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s an alternative version, this time faceting by treatment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit1,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  # add the gamma summaries
  left_join(
    tibble(treatment = as.character(1:4),
       gamma = inv_logit_scaled(fixef(fit1)[, 1])),
    by = &amp;quot;treatment&amp;quot;
  )  %&amp;gt;% 
  mutate(treatment = str_c(&amp;quot;treatment[&amp;quot;, treatment, &amp;quot;]&amp;quot;)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = reorder(actor, Estimate), y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(aes(yintercept = gamma),
             color = &amp;quot;white&amp;quot;) +
  geom_pointrange(size = 1/3) +
  scale_x_discrete(breaks = NULL) +
  labs(x = &amp;quot;actor, rank orderred by their average probability&amp;quot;,
       y = &amp;quot;probability of pulling the lever&amp;quot;) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~treatment, nrow = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig2-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The horizontal white lines mark off the posterior means for the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{\text{treatment}[i]}\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kruschkes-approach.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kruschke’s approach.&lt;/h3&gt;
&lt;p&gt;One way to think about our &lt;code&gt;pulled_left&lt;/code&gt; data is they are grouped by two factors. The first factor is the experimental condition, &lt;code&gt;treatment&lt;/code&gt;. The second factor is participant, &lt;code&gt;actor&lt;/code&gt;. Now imagine you arrange the number of times &lt;code&gt;pulled_left == 1&lt;/code&gt; within the cells of a &lt;span class=&#34;math inline&#34;&gt;\(2 \times 2\)&lt;/span&gt; contingency table where the four levels of the &lt;code&gt;treatment&lt;/code&gt; factor are in the rows and the seven levels of &lt;code&gt;actor&lt;/code&gt; are in the columns. Here’s what that might look like in a tile plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  group_by(actor, treatment) %&amp;gt;% 
  summarise(count = sum(pulled_left)) %&amp;gt;% 
  mutate(treatment = factor(treatment, levels = 4:1)) %&amp;gt;% 
  
  ggplot(aes(x = actor, y = treatment, fill = count, label = count)) +
  geom_tile() +
  geom_text(aes(color = count &amp;gt; 6)) +
  scale_color_viridis_d(option = &amp;quot;E&amp;quot;, direction = -1, breaks = NULL) +
  scale_fill_viridis_c(option = &amp;quot;E&amp;quot;, limits = c(0, 18), breaks = NULL) +
  scale_x_discrete(position = &amp;quot;top&amp;quot;, expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  theme(axis.ticks = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig3-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this arrangement, we can model &lt;span class=&#34;math inline&#34;&gt;\(\text{left_pull}_i \sim \operatorname{Binomial}(n_i = 1, p_i)\)&lt;/span&gt;, with three hierarchical grouping factors. The first will be &lt;code&gt;actor&lt;/code&gt;, the second will be &lt;code&gt;treatment&lt;/code&gt;, and the third will be their interaction. Kruschke gave a general depiction of this kind of statistical model in Figure 20.2&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; of his text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke, 2015, p. 588&lt;/a&gt;)&lt;/span&gt;. However, I generally prefer expressing my models using statistical notation similar to McElreath. Though I’m not exactly sure how McElreath would express a model like this, here’s my best attempt using his style of notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{left_pull}_i &amp;amp; \sim \operatorname{Binomial}(n_i = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \gamma + \alpha_{\text{actor}[i]} + \alpha_{\text{treatment}[i]} + \alpha_{\text{actor}[i] \times \text{treatment}[i]} \\
\gamma &amp;amp; \sim \operatorname{Normal}(0, 1) \\
\alpha_\text{actor}  &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{actor}) \\
\alpha_\text{treatment}  &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{treatment}) \\
\alpha_{\text{actor} \times \text{treatment}} &amp;amp; \sim \operatorname{Normal}(0, \sigma_{\text{actor} \times \text{treatment}}) \\
\sigma_\text{actor} &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_\text{treatment} &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_{\text{actor} \times \text{treatment}} &amp;amp; \sim \operatorname{Exponential}(1).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is our overall intercept and the three &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{&amp;lt;group&amp;gt;}[i]}\)&lt;/span&gt; terms are our multilevel deviations around that overall intercept. Notice that because &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; nas no &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; index, we are not technically using the index variable approach we discussed earlier in this post. But we are still indexing the four levels of &lt;code&gt;treatment&lt;/code&gt; by way of higher-level deviations depicted by the &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{treatment}[i]}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{actor}[i] \times \text{treatment}[i]}\)&lt;/span&gt; parameters in the second line. In contrast to our first model based on McElreath’s work, notice our three &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{\text{&amp;lt;group&amp;gt;}[i]}\)&lt;/span&gt; term are all modeled as &lt;em&gt;univariate&lt;/em&gt; normal. This makes this model an extension of the cross-classified model.&lt;/p&gt;
&lt;div id=&#34;fit-the-second-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fit the second model.&lt;/h4&gt;
&lt;p&gt;Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;. We’ll call it &lt;code&gt;fit2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- 
  brm(data = d, 
      family = binomial,
      pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | treatment) + (1 | actor:treatment),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(exponential(1), class = sd)),
      cores = 4, seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | treatment) + (1 | actor:treatment) 
##    Data: d (Number of observations: 504) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~actor (Number of levels: 7) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     2.00      0.66     1.07     3.68 1.00     1270     1894
## 
## ~actor:treatment (Number of levels: 28) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.25      0.18     0.01     0.70 1.00     1296     1810
## 
## ~treatment (Number of levels: 4) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.53      0.36     0.07     1.46 1.00     1144     1032
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.46      0.63    -0.81     1.71 1.00      989     1969
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a model like this, a natural first question is: &lt;em&gt;Where is the variance at?&lt;/em&gt; We can answer that by comparing the three lines in the output from the ‘Group-Level Effects’ section. It might be easier if we plotted the posteriors for those &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{&amp;lt;group&amp;gt;}\)&lt;/span&gt; parameters, instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

posterior_samples(fit2) %&amp;gt;% 
  select(starts_with(&amp;quot;sd&amp;quot;)) %&amp;gt;% 
  set_names(str_c(&amp;quot;sigma[&amp;quot;, c(&amp;quot;actor&amp;quot;, &amp;quot;actor~X~treatment&amp;quot;, &amp;quot;treatment&amp;quot;), &amp;quot;]&amp;quot;)) %&amp;gt;% 
  pivot_longer(everything()) %&amp;gt;% 
  mutate(name = factor(name,
                       levels = str_c(&amp;quot;sigma[&amp;quot;, c(&amp;quot;actor~X~treatment&amp;quot;, &amp;quot;treatment&amp;quot;, &amp;quot;actor&amp;quot;), &amp;quot;]&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = value, y = name)) +
  stat_halfeye(.width = .95, size = 1/2) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  xlab(&amp;quot;marginal posterior (log-odds scale)&amp;quot;) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig4-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like most of the action was between the seven actors. But there was some variation among the four levels of &lt;code&gt;treatment&lt;/code&gt; and even the interaction between the two factors wasn’t completely pushed against zero.&lt;/p&gt;
&lt;p&gt;Okay, here’s an alternative version of the first plot from &lt;code&gt;fit1&lt;/code&gt;, above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit2,
       newdata = nd) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  mutate(actor     = str_c(&amp;quot;actor[&amp;quot;, actor, &amp;quot;]&amp;quot;),
         condition = factor(condition)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = labels)) +
  geom_hline(yintercept = .5, color = &amp;quot;white&amp;quot;, linetype = 2) +
  # posterior predictions
  geom_line(aes(y = Estimate, group = prosoc_left),
            size = 3/4) +
  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, shape = condition),
                  fill = &amp;quot;transparent&amp;quot;, fatten = 10, size = 1/3, show.legend = F) + 
  scale_shape_manual(values = c(21, 19)) +
  scale_x_discrete(NULL, breaks = NULL) +
  scale_y_continuous(&amp;quot;proportion left lever&amp;quot;, limits = 0:1,
                     breaks = 0:2 / 2, labels = c(&amp;quot;0&amp;quot;, &amp;quot;.5&amp;quot;, &amp;quot;1&amp;quot;)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~actor, nrow = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-09-multilevel-models-and-the-index-variable-approach/index_files/figure-html/fig5-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The two models made similar predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-not-make-the-horse-race-official&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why not make the horse race official?&lt;/h3&gt;
&lt;p&gt;Just for kicks and giggles, we’ll compare the two models with the LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- add_criterion(fit1, criterion = &amp;quot;loo&amp;quot;)
fit2 &amp;lt;- add_criterion(fit2, criterion = &amp;quot;loo&amp;quot;)

# LOO differences
loo_compare(fit1, fit2) %&amp;gt;% print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic
## fit2    0.0       0.0  -266.8      9.5        12.2    0.6    533.7   19.1  
## fit1   -4.5       3.0  -271.3      9.7        19.3    1.2    542.6   19.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# LOO weights
model_weights(fit1, fit2, weights = &amp;quot;loo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       fit1       fit2 
## 0.01111874 0.98888126&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like there’s a little bit of an edge for the Kruschke’s multilevel ANOVA model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-whats-the-difference-anyway&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;But what’s the difference, anyway?&lt;/h3&gt;
&lt;p&gt;Rather than attempt to chose one model based on information criteria, we might back up and focus on the conceptual differences between the two models.&lt;/p&gt;
&lt;p&gt;Our first model, based on McElreath’s index-variable approach, explicitly emphasized the four levels of &lt;code&gt;treatment&lt;/code&gt;. Each one got its own &lt;span class=&#34;math inline&#34;&gt;\(\gamma_j\)&lt;/span&gt;. By modeling those &lt;span class=&#34;math inline&#34;&gt;\(\gamma_j\)&lt;/span&gt;’s with the multivariate normal distribution, we also got an explicit accounting of the &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; correlation structure for those parameters.&lt;/p&gt;
&lt;p&gt;Our second model, based on Kruschke’s multilevel ANOVA approach, took a more general perspective. By modeling &lt;code&gt;actor&lt;/code&gt;, &lt;code&gt;treatment&lt;/code&gt; and their interaction as higher-level grouping factors, &lt;code&gt;fit2&lt;/code&gt; conceptualized both participants and experimental conditions as coming from populations of potential participants and conditions, respectively. No longer are those four &lt;code&gt;treatment&lt;/code&gt; levels inherently special. They’re just the four we happen to have in this iteration of the experiment. Were we to run the experiment again, after all, we might want to alter them a little. The &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{treatment}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{actor} \times \text{treatment}}\)&lt;/span&gt; parameters can help give us a sense of how much variation we’d expect among other similar experimental conditions.&lt;/p&gt;
&lt;p&gt;Since I’m not a chimpanzee researcher, I’m in no position to say which perspective is better for these data. At a predictive level, the models perform similarly. But if I were a researcher wanting to analyze these data or others with a similar structure, I’d want to think clearly about what kinds of points I’d want to make to my target audience. Would I want to make focused points about the four levels of &lt;code&gt;treatment&lt;/code&gt;, or would it make sense to generalize from those four levels to other similar conditions? Each model has its rhetorical strengths and weaknesses.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;If you’re new to the Bayesian multilevel model, I recommend the introductory text by either McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020a&lt;/a&gt;)&lt;/span&gt; or Kruschke &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke, 2015&lt;/a&gt;)&lt;/span&gt;. I have ebook versions of both wherein I translated their code into the &lt;strong&gt;tidyverse&lt;/strong&gt; style and fit their models with &lt;strong&gt;brms&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;, &lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2020&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt;. Both McElreath and Kruschke have blogs (&lt;a href=&#34;https://elevanth.org/blog/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://doingbayesiandataanalysis.blogspot.com/&#34;&gt;here&lt;/a&gt;). Also, though it doesn’t cover the multilevel model, you can get a lot of practice with Bayesian regression with the new book by Gelman, Hill, and Vehtari &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. And for more hot Bayesian regression talk, you always have the Stan forums, which even have a &lt;a href=&#34;https://discourse.mc-stan.org/c/interfaces/brms/36&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; section&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5    
##  [7] purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] splines_4.0.4        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] viridisLite_0.3.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3       
##  [53] arrayhelpers_1.1-0   ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3     
##  [57] loo_2.4.1            dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2      
##  [61] tidyselect_1.1.0     rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1       
##  [65] munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [69] generics_0.1.0       broom_0.7.5          ggridges_0.5.2       evaluate_0.14       
##  [73] fastmap_1.0.1        yaml_2.2.1           processx_3.4.5       knitr_1.31          
##  [77] fs_1.5.0             nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [81] xml2_1.3.2           compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [85] rstudioapi_0.13      gamm4_0.2-6          curl_4.3             reprex_0.3.0        
##  [89] statmod_1.4.35       stringi_1.5.3        highr_0.8            ps_1.6.0            
##  [93] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2        
##  [97] nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
## [101] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3    
## [105] httpuv_1.5.4         R6_2.5.0             bookdown_0.21        promises_1.1.1      
## [109] gridExtra_2.3        codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0  
## [113] MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1     withr_2.4.1         
## [117] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [125] rmarkdown_2.7        shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3     
## [129] dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-Bürkner2021Parameterization&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Parameterization of response distributions in brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzDoingBayesianData2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis in brms and the tidyverse&lt;/em&gt; (version 0.3.0). &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Statistical rethinking with brms, Ggplot2, and the tidyverse: &lt;span&gt;Second&lt;/span&gt; edition&lt;/em&gt; (version 0.1.1). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020a). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-rethinking&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;rethinking&lt;/span&gt; &lt;span&gt;R&lt;/span&gt; package&lt;/em&gt;. &lt;a href=&#34;https://xcelab.net/rm/software/&#34;&gt;https://xcelab.net/rm/software/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Given the data are coded 0/1, one could also use the Bernoulli likelihood &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Parameterization&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2021&lt;/a&gt;, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_families.html#binary-and-count-data-models&#34; role=&#34;doc-biblioref&#34;&gt;&lt;em&gt;Binary and count data models&lt;/em&gt;&lt;/a&gt;)&lt;/span&gt;. I’m just partial to the binomial.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This is the typical parameterization for multilevel models fit with &lt;strong&gt;brms&lt;/strong&gt;. Though he used different notation, Bürkner spelled this all out in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; overview paper, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_overview.pdf&#34;&gt;&lt;em&gt;brms: An R package for Bayesian multilevel models using Stan&lt;/em&gt;&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The careful reader might notice that the models Kruschke focused on in Chapter 20 were all based on the Gaussian likelihood. So in the most technical sense, the model in Figure 20.2 is not a perfect match to our &lt;code&gt;fit2&lt;/code&gt;. I’m hoping my readers might look past those details to see the more general point. For more practice, &lt;a href=&#34;https://bookdown.org/content/3686/count-predicted-variable.html#example-hair-eye-go-again&#34;&gt;Section 24.2&lt;/a&gt; and &lt;a href=&#34;https://bookdown.org/content/3686/count-predicted-variable.html#example-interaction-contrasts-shrinkage-and-omnibus-test&#34;&gt;Section 24.3&lt;/a&gt; of my translation of Kruschke’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;)&lt;/span&gt; show variants of this model type using the Poisson likelihood. In Section &lt;a href=&#34;https://bookdown.org/content/3686/count-predicted-variable.html#log-linear-models-for-contingency-tables-bonus-alternative-parameterization&#34;&gt;24.4&lt;/a&gt; you can even find a variant using the aggregated binomial likelihood.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian meta-analysis in brms-II</title>
      <link>/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/</guid>
      <description>
&lt;script src=&#34;/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://bookdown.org/content/3890/missing-data-and-other-opportunities.html#summary-bonus-meta-analysis&#34;&gt;Section 14.3&lt;/a&gt; of my &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingBrms2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; translation of the first edition of McElreath’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;em&gt;Statistical rethinking&lt;/em&gt;, I included a bonus section covering Bayesian meta-analysis. For my &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2020&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt; translation of the second edition of the text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;McElreath, 2020&lt;/a&gt;)&lt;/span&gt;, I’d like to include another section on the topic, but from a different perspective. The first time around, we focused on standardized mean differences. This time, I’d like to tackle odds ratios and, while we’re at it, give a little bit of a plug for open science practices.&lt;/p&gt;
&lt;p&gt;The purpose of this post is to present a rough draft of the section. I intend to tack this section onto the end of Chapter 15 (&lt;em&gt;Missing Data and Other Opportunities&lt;/em&gt;), which covers measurement error. If you have any constrictive criticisms, please pass them along either in the &lt;a href=&#34;https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/issues&#34;&gt;GitHub issues for the ebook&lt;/a&gt; or on &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1317854064839958531&#34;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here’s the rough draft:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-bonus-bayesian-meta-analysis-with-odds-ratios&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;del&gt;Summary&lt;/del&gt; Bonus: Bayesian meta-analysis with odds ratios&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these packages and setting alterations will already have been 
# opened and made before this section
library(tidyverse)
library(brms)
library(ggdark)
library(viridis)
library(broom)
library(tidybayes)

theme_set(
  dark_theme_bw() +
    theme(legend.position = &amp;quot;none&amp;quot;,
          panel.grid = element_blank())
  )

# to reset the default ggplot2 theme to its default parameters,
# execute `ggplot2::theme_set(theme_gray())` and `ggdark::invert_geom_defaults()`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use &lt;code&gt;brms::brm()&lt;/code&gt; to fit Bayesian meta-analyses, too.&lt;/p&gt;
&lt;p&gt;Before we proceed, I should acknowledge that this section is heavily influenced by &lt;a href=&#34;https://mvuorre.github.io/#about&#34;&gt;Matti Vourre&lt;/a&gt;’s great blog post, &lt;a href=&#34;https://mvuorre.github.io/post/2016/09/29/meta-analysis-is-a-special-case-of-bayesian-multilevel-modeling/&#34;&gt;&lt;em&gt;Meta-analysis is a special case of Bayesian multilevel modeling&lt;/em&gt;&lt;/a&gt;. Since neither editions of McElreath’s text directly address meta-analyses, we’ll also have to borrow a bit from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelman2013bayesian&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://stat.columbia.edu/~gelman/book/&#34;&gt;&lt;em&gt;Bayesian data analysis, Third edition&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;how-do-meta-analyses-fit-into-the-picture&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How do meta-analyses fit into the picture?&lt;/h3&gt;
&lt;p&gt;Let Gelman and colleagues introduce the topic:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Discussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected &lt;em&gt;a priori&lt;/em&gt; to have predictable effects favoring one study over another…. this third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis…&lt;/p&gt;
&lt;p&gt;The first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125–126, &lt;em&gt;emphasis&lt;/em&gt; in the original)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The basic version of a Bayesian meta-analysis follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_j \sim \operatorname{Normal}(\theta_j, \sigma_j),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; = the point estimate for the effect size of a single study, &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, which is presumed to have been a draw from a Normal distribution centered on &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;. The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. The standard error from study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is specified &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j\)&lt;/span&gt;, which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j\)&lt;/span&gt;, here. Those values we take directly from the original studies.&lt;/p&gt;
&lt;p&gt;Building on the model, we further presume that study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is itself just one draw from a population of related studies, each of which have their own effect sizes. As such, we presume &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; itself has a distribution following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_j \sim \operatorname{Normal}(\mu, \tau),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the meta-analytic effect (i.e., the population mean) and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; is the variation around that mean, what you might also think of as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\tau\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-some-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need some data.&lt;/h3&gt;
&lt;p&gt;Our data in this section come from the second large-scale replication project by the Many Labs team &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kleinManyLabsInvestigating2018&#34; role=&#34;doc-biblioref&#34;&gt;Klein et al., 2018&lt;/a&gt;)&lt;/span&gt;. Of the 28 studies replicated in the study, we will focus on the replication of the trolley experiment from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hauserDissociationMoralJudgments2007&#34; role=&#34;doc-biblioref&#34;&gt;Hauser et al.&lt;/a&gt; (&lt;a href=&#34;#ref-hauserDissociationMoralJudgments2007&#34; role=&#34;doc-biblioref&#34;&gt;2007&lt;/a&gt;)&lt;/span&gt;. Here’s how the study was described by Klein and colleagues:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;According to the principle of double effect, an act that harms other people is more morally permissible if the act is a foreseen side effect rather than the means to the greater good. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hauserDissociationMoralJudgments2007&#34; role=&#34;doc-biblioref&#34;&gt;Hauser et al.&lt;/a&gt; (&lt;a href=&#34;#ref-hauserDissociationMoralJudgments2007&#34; role=&#34;doc-biblioref&#34;&gt;2007&lt;/a&gt;)&lt;/span&gt; compared participants’ reactions to two scenarios to test whether their judgments followed this principle. In the &lt;em&gt;foreseen-side-effect&lt;/em&gt; scenario, a person on an out-of-control train changed the train’s trajectory so that the train killed one person instead of five. In the &lt;em&gt;greater-good&lt;/em&gt; scenario, a person pushed a fat man in front of a train, killing him, to save five people. Whereas &lt;span class=&#34;math inline&#34;&gt;\(89\%\)&lt;/span&gt; of participants judged the action in the foreseen-side-effect scenario as permissible &lt;span class=&#34;math inline&#34;&gt;\((95 \% \; \text{CI} = [87\%, 91\%]),\)&lt;/span&gt; only &lt;span class=&#34;math inline&#34;&gt;\(11\%\)&lt;/span&gt; of participants in the greater-good scenario judged it as permissible &lt;span class=&#34;math inline&#34;&gt;\((95 \% \; \text{CI} = [9\%, 13\%])\)&lt;/span&gt;. The difference between the percentages was significant&lt;span class=&#34;math inline&#34;&gt;\(, \chi^2(1, N = 2,646) = 1,615.96,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .001,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(w = .78,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(d = 2.50,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(95 \% \; \text{CI} = [2.22, 2.86]\)&lt;/span&gt;. Thus, the results provided evidence for the principle of double effect. (p. 459, &lt;em&gt;emphasis&lt;/em&gt; in the original)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can find supporting materials for the replication project on the Open Science Framework at &lt;a href=&#34;https://osf.io/8cd4r/&#34;&gt;https://osf.io/8cd4r/&lt;/a&gt;. The relevant subset of the data for the replication of Hauser et al. come from the &lt;code&gt;Trolley Dilemma 1 (Hauser et al., 2007)&lt;/code&gt; folder within the &lt;code&gt;OSFdata.zip&lt;/code&gt; (&lt;a href=&#34;https://osf.io/ag2pd/&#34;&gt;https://osf.io/ag2pd/&lt;/a&gt;). I’ve downloaded the file and saved it on GitHub.&lt;/p&gt;
&lt;p&gt;Here we load the data and call it &lt;code&gt;h&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h &amp;lt;- 
  readr::read_csv(&amp;quot;https://raw.githubusercontent.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/master/data/Hauser_1_study_by_order_all_CLEAN_CASE.csv&amp;quot;)

h &amp;lt;- 
  h %&amp;gt;% 
  mutate(y   = ifelse(variable == &amp;quot;Yes&amp;quot;, 1, 0),
         loc = factor(Location,
                      levels = distinct(h, Location) %&amp;gt;% pull(Location),
                      labels = 1:59))

glimpse(h)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 6,842
## Columns: 29
## $ uID              &amp;lt;dbl&amp;gt; 65, 68, 102, 126, 145, 263, 267, 298, 309, 318, 350, 356, 376, 431, 438, …
## $ variable         &amp;lt;chr&amp;gt; &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;…
## $ factor           &amp;lt;chr&amp;gt; &amp;quot;SideEffect&amp;quot;, &amp;quot;SideEffect&amp;quot;, &amp;quot;SideEffect&amp;quot;, &amp;quot;SideEffect&amp;quot;, &amp;quot;SideEffect&amp;quot;, &amp;quot;Si…
## $ .id              &amp;lt;chr&amp;gt; &amp;quot;ML2_Slate1_Brazil__Portuguese_execution_illegal_r.csv&amp;quot;, &amp;quot;ML2_Slate1_Braz…
## $ source           &amp;lt;chr&amp;gt; &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;ubc&amp;quot;, …
## $ haus1.1          &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1…
## $ haus1.1t_1       &amp;lt;dbl&amp;gt; 39.054, 36.792, 56.493, 21.908, 25.635, 50.633, 58.661, 50.137, 51.717, 2…
## $ haus2.1          &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ haus2.1t_1       &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ Source.Global    &amp;lt;chr&amp;gt; &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;ubc&amp;quot;, …
## $ Source.Primary   &amp;lt;chr&amp;gt; &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;ubc&amp;quot;, …
## $ Source.Secondary &amp;lt;chr&amp;gt; &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;brasilia&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;wilfredlaur&amp;quot;, &amp;quot;ubc&amp;quot;, …
## $ Country          &amp;lt;chr&amp;gt; &amp;quot;Brazil&amp;quot;, &amp;quot;Brazil&amp;quot;, &amp;quot;Brazil&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Ca…
## $ Location         &amp;lt;chr&amp;gt; &amp;quot;Social and Work Psychology Department, University of Brasilia, DF, Brazi…
## $ Language         &amp;lt;chr&amp;gt; &amp;quot;Portuguese&amp;quot;, &amp;quot;Portuguese&amp;quot;, &amp;quot;Portuguese&amp;quot;, &amp;quot;English&amp;quot;, &amp;quot;English&amp;quot;, &amp;quot;English&amp;quot;…
## $ Weird            &amp;lt;dbl&amp;gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ Execution        &amp;lt;chr&amp;gt; &amp;quot;illegal&amp;quot;, &amp;quot;illegal&amp;quot;, &amp;quot;illegal&amp;quot;, &amp;quot;illegal&amp;quot;, &amp;quot;illegal&amp;quot;, &amp;quot;illegal&amp;quot;, &amp;quot;illega…
## $ SubjectPool      &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;,…
## $ Setting          &amp;lt;chr&amp;gt; &amp;quot;In a classroom&amp;quot;, &amp;quot;In a classroom&amp;quot;, &amp;quot;In a classroom&amp;quot;, &amp;quot;In a lab&amp;quot;, &amp;quot;In a l…
## $ Tablet           &amp;lt;chr&amp;gt; &amp;quot;Computers&amp;quot;, &amp;quot;Computers&amp;quot;, &amp;quot;Computers&amp;quot;, &amp;quot;Computers&amp;quot;, &amp;quot;Computers&amp;quot;, &amp;quot;Compute…
## $ Pencil           &amp;lt;chr&amp;gt; &amp;quot;No, the whole study was on the computer (except maybe consent/debriefing…
## $ StudyOrderN      &amp;lt;chr&amp;gt; &amp;quot;Hauser|Ross.Slate1|Rottenstrich|Graham|Kay|Inbar|Anderson|VanLange|Huang…
## $ IDiffOrderN      &amp;lt;chr&amp;gt; &amp;quot;ID: Global self-esteem SISE|ID: Mood|ID: Subjective wellbeing|ID: Disgus…
## $ study.order      &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ analysis.type    &amp;lt;chr&amp;gt; &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;Order&amp;quot;, &amp;quot;…
## $ subset           &amp;lt;chr&amp;gt; &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;al…
## $ case.include     &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…
## $ y                &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1…
## $ loc              &amp;lt;fct&amp;gt; 1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The total sample size is &lt;span class=&#34;math inline&#34;&gt;\(N = 6,842\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h %&amp;gt;% 
  distinct(uID) %&amp;gt;% 
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##       n
##   &amp;lt;int&amp;gt;
## 1  6842&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All cases are to be included.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h %&amp;gt;% 
  count(case.include)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   case.include     n
##   &amp;lt;lgl&amp;gt;        &amp;lt;int&amp;gt;
## 1 TRUE          6842&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data were collected in 59 locations with sample sizes ranging from 34 to 325.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h %&amp;gt;% 
  count(Location) %&amp;gt;% 
  arrange(desc(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 59 x 2
##    Location                                                                                        n
##    &amp;lt;chr&amp;gt;                                                                                       &amp;lt;int&amp;gt;
##  1 University of Toronto, Scarborough                                                            325
##  2 MTurk India Workers                                                                           308
##  3 MTurk US Workers                                                                              304
##  4 University of Illinois at Urbana-Champaign, Champaign, IL                                     198
##  5 Eotvos Lorand University, in Budapest, Hungary                                                180
##  6 Department of Social Psychology, Tilburg University, P.O. Box 90153, Tilburg, 5000 LE, Net…   173
##  7 Department of Psychology, San Diego State University, San Diego, CA 92182                     171
##  8 Department of Psychology, Pennsylvania State University Abington, Abington, PA 19001          166
##  9 American University of Sharjah, United Arab Emirates                                          162
## 10 University of British Columbia, Vancouver, Canada                                             147
## # … with 49 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;our-effect-size-will-be-an-odds-ratio.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Our effect size will be an odds ratio.&lt;/h3&gt;
&lt;p&gt;Here’s how Klein and colleagues summarized their primary results:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the aggregate replication sample &lt;span class=&#34;math inline&#34;&gt;\((N = 6,842\)&lt;/span&gt; after removing participants who responded in less than &lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt; s&lt;span class=&#34;math inline&#34;&gt;\(), 71\%\)&lt;/span&gt; of participants judged the action in the foreseen-side-effect scenario as permissible, but only &lt;span class=&#34;math inline&#34;&gt;\(17\%\)&lt;/span&gt; of participants in the greater-good scenario judged it as permissible. The difference between the percentages was significant, &lt;span class=&#34;math inline&#34;&gt;\(p = 2.2 \text e^{-16},\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\text{OR} = 11.54,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(d = 1.35,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(95\% \; \text{CI} = [1.28, 1.41]\)&lt;/span&gt;. The replication results were consistent with the double-effect hypothesis, and the effect was about half the magnitude of the original &lt;span class=&#34;math inline&#34;&gt;\((d = 1.35,\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(95\% \; \text{CI} = [1.28, 1.41],\)&lt;/span&gt; vs. original &lt;span class=&#34;math inline&#34;&gt;\(d = 2.50)\)&lt;/span&gt;. (p. 459)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here is the breakdown of the outcome and primary experimental condition, which will confirm the two empirical percentages mentioned, above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h %&amp;gt;% 
  count(variable, factor) %&amp;gt;% 
  group_by(factor) %&amp;gt;% 
  mutate(percent = 100 * n / sum(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
## # Groups:   factor [2]
##   variable factor          n percent
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 No       GreaterGood  2781    82.8
## 2 No       SideEffect   1026    29.4
## 3 Yes      GreaterGood   577    17.2
## 4 Yes      SideEffect   2458    70.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though the authors presented their overall effect size with a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value, an odds-ratio (OR), and a Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; (i.e., a kind of standardized mean difference), we will focus on the OR. The primary data are binomial counts, which are well-handled with logistic regression. When you perform a logistic regression where a control condition is compared with some experimental condition, the difference between those conditions may be expressed as an OR. To get a sense of what that is, we’ll first practice fitting a logistic regression model with the frequentist &lt;code&gt;glm()&lt;/code&gt; function. Here are the results based on the subset of data from the first location.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm0 &amp;lt;- glm(y ~ factor, family = binomial(logit), data = h %&amp;gt;% filter(loc == 1))

summary(glm0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ factor, family = binomial(logit), data = h %&amp;gt;% 
##     filter(loc == 1))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5227  -0.6231  -0.6231   0.8677   1.8626  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)       -1.5404     0.3673  -4.194 2.74e-05 ***
## factorSideEffect   2.3232     0.4754   4.887 1.02e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 139.47  on 101  degrees of freedom
## Residual deviance: 110.98  on 100  degrees of freedom
## AIC: 114.98
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like with &lt;strong&gt;brms&lt;/strong&gt;, the base-&lt;strong&gt;R&lt;/strong&gt; &lt;code&gt;glm()&lt;/code&gt; function returns the results of a logistic regression model in the log-odds metric. The intercept is the log-odds probability of selecting &lt;em&gt;yes&lt;/em&gt; in the study for participants in the &lt;code&gt;GreaterGood&lt;/code&gt; condition. The ‘factorSideEffect’ parameter is the difference in log-odds probability for participants in the &lt;code&gt;SideEffect&lt;/code&gt; condition. Here’s what happens when you exponentiate that coefficient.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(glm0)[2] %&amp;gt;% exp()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## factorSideEffect 
##         10.20833&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That, my friends, is an odds ratio (OR). &lt;strong&gt;Odds ratios are simply exponentiated logistic regression coefficients&lt;/strong&gt;. The implication of this particular OR is that those in the &lt;code&gt;SideEffect&lt;/code&gt; condition have about 10 times the odds of selecting &lt;em&gt;yes&lt;/em&gt; compared to those in the &lt;code&gt;GreaterGood&lt;/code&gt; condition. In the case of this subset of the data, that’s 18% yeses versus 69%, which seems like a large difference, to me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h %&amp;gt;% 
  filter(loc == 1) %&amp;gt;% 
  count(variable, factor) %&amp;gt;% 
  group_by(factor) %&amp;gt;% 
  mutate(percent = 100 * n / sum(n)) %&amp;gt;% 
  filter(variable == &amp;quot;Yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
## # Groups:   factor [2]
##   variable factor          n percent
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Yes      GreaterGood     9    17.6
## 2 Yes      SideEffect     35    68.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;log-odds-odds-ratios-and-modeling-effect-sizes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Log-odds, odds ratios, and modeling effect sizes.&lt;/h3&gt;
&lt;p&gt;Though it’s common for researchers to express their effect sizes as odds ratios, we don’t want to work directly with odds ratios in a meta-analysis. &lt;em&gt;Why?&lt;/em&gt; Well, think back on why we model binomial data with the logit link. The logit link transforms a bounded &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt; parameter space into an unbounded parameter space ranging from negative to positive infinity. For us Bayesians, it also provides a context in which our &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters are approximately Gaussian. However, when we exponentiate those approximately Gaussian log-odds coefficients, the resulting odds ratios aren’t so Gaussian any more. This is why, even if our ultimate goal is to express a meta-analytic effect as an OR, we want to work with effect sizes in the log-odds metric. It allows us to use the Bayesian meta-analytic framework outlined by Gelman et al, above,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
y_j      &amp;amp; \sim \operatorname{Normal}(\theta_j, \sigma_j) \\
\theta_j &amp;amp; \sim \operatorname{Normal}(\mu, \tau),
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; is the point estimate in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;th study still in the log-odds scale. After fitting the model, we can then exponentiate the meta-analytic parameter &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; into the OR metric.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-the-study-specific-effect-sizes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compute the study-specific effect sizes.&lt;/h3&gt;
&lt;p&gt;Our &lt;code&gt;h&lt;/code&gt; data from the Klein et al replication study includes the un-aggregated data from all of the study locations combined. Before we compute our meta-analysis, we’ll need to compute the study-specific effect sizes and standard errors. Here we do so within a nested tibble.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glms &amp;lt;-
  h %&amp;gt;% 
  select(loc, y, factor) %&amp;gt;% 
  nest(data = c(y, factor)) %&amp;gt;% 
  mutate(glm = map(data, ~update(glm0, data = .))) %&amp;gt;% 
  mutate(coef = map(glm, tidy)) %&amp;gt;% 
  select(-data, -glm) %&amp;gt;% 
  unnest(coef) %&amp;gt;% 
  filter(term == &amp;quot;factorSideEffect&amp;quot;)

# what did we do?
glms %&amp;gt;% 
  mutate_if(is.double, round, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 59 x 6
##    loc   term             estimate std.error statistic p.value
##    &amp;lt;fct&amp;gt; &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 1     factorSideEffect     2.32     0.475      4.89       0
##  2 2     factorSideEffect     3.64     0.644      5.64       0
##  3 3     factorSideEffect     2.37     0.399      5.96       0
##  4 4     factorSideEffect     2.24     0.263      8.54       0
##  5 5     factorSideEffect     2.02     0.505      4.00       0
##  6 6     factorSideEffect     2.49     0.571      4.36       0
##  7 7     factorSideEffect     2.53     0.658      3.84       0
##  8 8     factorSideEffect     1.78     0.459      3.87       0
##  9 9     factorSideEffect     1.81     0.378      4.79       0
## 10 10    factorSideEffect     2.37     0.495      4.79       0
## # … with 49 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;estimate&lt;/code&gt; column we have all the &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; values and &lt;code&gt;std.error&lt;/code&gt; contains the corresponding &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j\)&lt;/span&gt; values. Here they are in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color &amp;lt;- viridis_pal(option = &amp;quot;C&amp;quot;)(7)[5]

glms %&amp;gt;% 
  ggplot(aes(x = std.error, y = estimate)) +
  geom_point(color = color) +
  labs(x = expression(sigma[italic(j)]~(&amp;quot;log-odds&amp;quot;)),
       y = expression(italic(y[j])~(&amp;quot;log-odds&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-bayesian-meta-analysis.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the Bayesian meta-analysis.&lt;/h3&gt;
&lt;p&gt;Now are data are ready, we can express our first Bayesian meta-analysis with the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\text{estimate}_j &amp;amp; \sim \operatorname{Normal}(\theta_j, \; \text{std.error}_j) \\
\theta_j   &amp;amp; \sim \operatorname{Normal}(\mu, \tau) \\
\mu        &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\tau       &amp;amp; \sim \operatorname{Exponential}(1),
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the last two lines spell out our priors. As we learned in &lt;a href=&#34;https://bookdown.org/content/4857/god-spiked-the-integers.html#binomial-regression&#34;&gt;Section 11.1&lt;/a&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, 1.5)\)&lt;/span&gt; prior in the log-odds space is just about flat on the probability space. If you wanted to be more conservative, consider something like &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, 1)\)&lt;/span&gt;. Here’s how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;me0 &amp;lt;- 
  brm(data = glms, 
      family = gaussian,
      estimate | se(std.error) ~ 1 + (1 | loc),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(exponential(1), class = sd)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;se()&lt;/code&gt; is one of the &lt;strong&gt;brms&lt;/strong&gt; helper functions designed to provide additional information about the criterion variable. Here it informs &lt;code&gt;brm()&lt;/code&gt; that each &lt;code&gt;estimate&lt;/code&gt; value has an associated measurement error defined in the &lt;code&gt;std.error&lt;/code&gt; column. Unlike the &lt;code&gt;mi()&lt;/code&gt; function, which we used earlier in the chapter to accommodate measurement error and the Bayesian imputation of missing data, the &lt;code&gt;se()&lt;/code&gt; function is specially designed to handle meta-analyses. &lt;code&gt;se()&lt;/code&gt; contains a &lt;code&gt;sigma&lt;/code&gt; argument which is set to &lt;code&gt;FALSE&lt;/code&gt; by default. This will return a model with no estimate for sigma, which is what we want. The uncertainty around the &lt;code&gt;estimate&lt;/code&gt;-value for each study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; has already been encoded in the data as &lt;code&gt;std.error&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s look at the model results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(me0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: estimate | se(std.error) ~ 1 + (1 | loc) 
##    Data: glms (Number of observations: 59) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~loc (Number of levels: 59) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.43      0.09     0.26     0.62 1.00     1956     2389
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.55      0.09     2.38     2.72 1.00     3443     2631
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our estimate for heterogeneity across studies, &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, is about 0.4, suggesting modest differences across the studies. The meta-analytic effect, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, is about 2.5. Both, recall, are in the log-odds metric. Here we exponentiate &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; to get our odds ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(me0) %&amp;gt;% exp()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Estimate Est.Error     Q2.5    Q97.5
## Intercept 12.79272  1.091829 10.85899 15.25431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look back up to the results reported by Klein and colleagues, you’ll see this is rather close to their OR estimate of 11.54.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-bayesian-muiltilevel-alternative.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the Bayesian muiltilevel alternative.&lt;/h3&gt;
&lt;p&gt;We said earlier that meta-analysis is just a special case of the multilevel model, applied to summary data. We typically perform meta-analyses on data summaries because historically it has not been the norm among researchers to make their data publicly available. So effect size summaries were the best we typically had for aggregating study results. However, times are changing (e.g., &lt;a href=&#34;https://www.apa.org/monitor/2017/11/trends-open-science.aspx&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://www.blog.google/products/search/making-it-easier-discover-datasets/&#34;&gt;here&lt;/a&gt;). In this case, Klein and colleagues engaged in open-science practices and reported all their data. Thus we can just directly fit the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\text{y}_{ij} &amp;amp; \sim \operatorname{Binomial}(n = 1, p_{ij}) \\
\operatorname{logit}(p_{ij}) &amp;amp; \sim \alpha + \beta \text{factor}_{ij} + u_{\alpha j} + u_{\beta j} \text{factor}_{ij} \\

\begin{bmatrix} u_{\alpha j} \\ u_{\beta j} \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal} \begin{pmatrix} \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf{SRS} \end{pmatrix} \\

\mathbf S &amp;amp; = \begin{bmatrix} \sigma_\alpha &amp;amp; 0 \\ 0 &amp;amp; \sigma_\beta \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 0 &amp;amp; \rho_{\alpha \beta} \\ \rho_{\beta \alpha} &amp;amp; 0 \end{bmatrix} \\

\alpha &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\beta  &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\sigma_\alpha &amp;amp; \sim \operatorname{Exponential}(1) \\
\sigma_\beta  &amp;amp; \sim \operatorname{Exponential}(1) \\
\mathbf R &amp;amp; \sim \operatorname{LKJ}(2),
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the criterion variable, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, is nested in &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; participants within &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; locations. The &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter is analogous to the meta-analytic effect (&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\beta\)&lt;/span&gt; is analogous to the expression of heterogeneity in the meta-analytic effect (&lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;). Here is how to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;me1 &amp;lt;- 
  brm(data = h, 
      family = binomial,
      y | trials(1) ~ 0 + Intercept + factor + (1 + factor | loc),
      prior = c(prior(normal(0, 1.5), class = b),
                prior(exponential(1), class = sd),
                prior(lkj(2), class = cor)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results for the focal parameters are very similar to those from &lt;code&gt;me0&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(me1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: y | trials(1) ~ 0 + Intercept + factor + (1 + factor | loc) 
##    Data: h (Number of observations: 6842) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~loc (Number of levels: 59) 
##                                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)                       0.42      0.07     0.30     0.57 1.00     2120     2870
## sd(factorSideEffect)                0.48      0.09     0.32     0.66 1.01     1107     2010
## cor(Intercept,factorSideEffect)    -0.31      0.19    -0.62     0.08 1.00     1487     2276
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept           -1.66      0.08    -1.82    -1.52 1.00     2012     2675
## factorSideEffect     2.57      0.09     2.39     2.76 1.00     2044     2623
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the multilevel version of the effect size as an odds ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(me1)[2, -2] %&amp;gt;% exp()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate     Q2.5    Q97.5 
## 13.02704 10.93772 15.73129&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we compare the study specific effect sizes, &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;, by our two modeling approaches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color &amp;lt;- viridis_pal(option = &amp;quot;C&amp;quot;)(7)[3]

# how many levels are there?
n_loc &amp;lt;- distinct(h, loc) %&amp;gt;% count() %&amp;gt;% pull(n)

# rank by meta-analysis
ranks &amp;lt;-
  tibble(Estimate = coef(me0)$loc[, 1, &amp;quot;Intercept&amp;quot;],
         index    = 1:n_loc) %&amp;gt;% 
  arrange(Estimate) %&amp;gt;% 
  mutate(rank = 1:n_loc)

rbind(coef(me0)$loc[, , &amp;quot;Intercept&amp;quot;],
      coef(me1)$loc[, , &amp;quot;factorSideEffect&amp;quot;]) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  mutate(index = rep(1:n_loc, times = 2),
         type  = rep(c(&amp;quot;meta-analysis&amp;quot;, &amp;quot;multilevel model&amp;quot;), each = n_loc)) %&amp;gt;% 
  left_join(select(ranks, -Estimate), 
            by = &amp;quot;index&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +
  geom_pointrange(fatten = 1, color = color) +
  scale_x_continuous(expression(log-odds~effect~size~(theta[italic(j)])), limits = c(0, 4.5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~type)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results are very similar. You might be curious how to show these results in a more conventional looking forest plot where the names of the groups (typically studies) for the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; values are listed on the left, the point estimate and 95% interval summaries are listed on the right, and the summary for the population level effect, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, is listed beneath all all the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;’s. That’ll require some prep work. First we’ll need to reformat the location names. I’ll save the results in an object called &lt;code&gt;labs&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labs &amp;lt;-
  h %&amp;gt;% 
  mutate(lab = case_when(
    Location == &amp;quot;Social and Work Psychology Department, University of Brasilia, DF, Brazil&amp;quot; ~ &amp;quot;University of Brasilia&amp;quot;,
    Location == &amp;quot;Wilfrid Laurier University, Waterloo, Ontario, Canada&amp;quot; ~ &amp;quot;Wilfrid Laurier University&amp;quot;,
    Location == &amp;quot;University of British Columbia, Vancouver, Canada&amp;quot; ~ &amp;quot;University of British Columbia&amp;quot;,
    Location == &amp;quot;University of Toronto, Scarborough&amp;quot; ~ &amp;quot;University of Toronto&amp;quot;,
    Location == &amp;quot;Division of Social Science, The Hong Kong University of Science and Technology, Hong Kong, China&amp;quot; ~ &amp;quot;Hong Kong University of Science and Technology&amp;quot;,
    Location == &amp;quot;Chinese Academy of Science, Beijing, China&amp;quot; ~ &amp;quot;Chinese Academy of Science&amp;quot;,
    Location == &amp;quot;Shanghai International Studies University, SISU Intercultural Institute, Shanghai, China&amp;quot; ~ &amp;quot;Shanghai International Studies University&amp;quot;,
    Location == &amp;quot;Guangdong Literature &amp;amp; Art Vocational College, Guangzhou, China&amp;quot; ~ &amp;quot;Guangdong Literature &amp;amp; Art Vocational College&amp;quot;,
    Location == &amp;quot;The University of J. E. Purkyně, Ústí nad Labem, Czech Republic&amp;quot; ~ &amp;quot;The University of J. E. Purkyně&amp;quot;,
    Location == &amp;quot;University of Leuven, Belgium&amp;quot; ~ &amp;quot;University of Leuven&amp;quot;,
    Location == &amp;quot;Department of Experimental and Applied Psychology, VU Amsterdam, 1081BT, Amsterdam, The Netherlands&amp;quot; ~ &amp;quot;VU Amsterdam&amp;quot;,
    Location == &amp;quot;Department of Social Psychology, Tilburg University, P.O. Box 90153, Tilburg, 5000 LE, Netherlands&amp;quot; ~ &amp;quot;Department of Social Psychology, Tilburg University&amp;quot;,
    Location == &amp;quot;Eindhoven University of Technology, Eindhoven, Netherlands&amp;quot; ~ &amp;quot;Eindhoven University of Technology&amp;quot;,
    Location == &amp;quot;Department of Communication and Information Sciences, P.O. Box 90153, Tilburg, 5000 LE, Netherlands&amp;quot; ~ &amp;quot;Department of Communication and Information Sciences, Tilburg University&amp;quot;,
    Location == &amp;quot;University of Navarra, Spain&amp;quot; ~ &amp;quot;University of Navarra&amp;quot;,
    Location == &amp;quot;University of Lausanne, Switzerland&amp;quot; ~ &amp;quot;University of Lausanne&amp;quot;,
    Location == &amp;quot;Université de Poitiers, France&amp;quot; ~ &amp;quot;Université de Poitiers&amp;quot;,
    Location == &amp;quot;Eotvos Lorand University, in Budapest, Hungary&amp;quot; ~ &amp;quot;Eotvos Lorand University&amp;quot;,
    Location == &amp;quot;MTurk India Workers&amp;quot; ~ &amp;quot;MTurk India Workers&amp;quot;,
    Location == &amp;quot;University of Winchester, Winchester, Hampshire, England&amp;quot; ~ &amp;quot;University of Winchester&amp;quot;,
    Location == &amp;quot;Doshisha University, Kyoto, Japan&amp;quot; ~ &amp;quot;Doshisha University&amp;quot;,
    Location == &amp;quot;Victoria University of Wellington, New Zealand&amp;quot; ~ &amp;quot;Victoria University of Wellington&amp;quot;,
    Location == &amp;quot;University of Social Sciences and Humanities, Wroclaw, Poland&amp;quot; ~ &amp;quot;University of Social Sciences and Humanities&amp;quot;,
    Location == &amp;quot;Department of Psychology, SWPS University of Social Sciences and Humanities Campus Sopot, Sopot, Poland&amp;quot; ~ &amp;quot;SWPS University of Social Sciences and Humanities Campus Sopot&amp;quot;,
    Location == &amp;quot;badania.net&amp;quot; ~ &amp;quot;badania.net&amp;quot;,
    Location == &amp;quot;Universidade do Porto, Portugal&amp;quot; ~ &amp;quot;Universidade do Porto&amp;quot;,
    Location == &amp;quot;University of Belgrade, Belgrade, Serbia&amp;quot; ~ &amp;quot;University of Belgrade&amp;quot;,
    Location == &amp;quot;University of Johannesburg, Johanneburg, South Africa&amp;quot; ~ &amp;quot;University of Johannesburg&amp;quot;,
    Location == &amp;quot;Santiago, Chile&amp;quot; ~ &amp;quot;Santiago, Chile&amp;quot;,
    Location == &amp;quot;Universidad de Costa Rica, Costa Rica&amp;quot; ~ &amp;quot;Universidad de Costa Rica&amp;quot;,
    Location == &amp;quot;National Autonomous University of Mexico in Mexico City&amp;quot; ~ &amp;quot;National Autonomous University of Mexico&amp;quot;,
    Location == &amp;quot;University of the Republic, Montevideo, Uruguay&amp;quot; ~ &amp;quot;University of the Republic&amp;quot;,
    Location == &amp;quot;Lund University, Lund, Sweden&amp;quot; ~ &amp;quot;Lund University&amp;quot;,
    Location == &amp;quot;Academia Sinica, Taiwan National Taiwan Normal University, Taiwan&amp;quot; ~ &amp;quot;Taiwan National Taiwan Normal University&amp;quot;,
    Location == &amp;quot;Bilgi University, Istanbul, Turkey&amp;quot; ~ &amp;quot;Bilgi University&amp;quot;,
    Location == &amp;quot;Koç University, Istanbul, Turkey&amp;quot; ~ &amp;quot;Koç University&amp;quot;,
    Location == &amp;quot;American University of Sharjah, United Arab Emirates&amp;quot; ~ &amp;quot;American University of Sharjah&amp;quot;,
    Location == &amp;quot;University of Hawaii, Honolulu, HI&amp;quot; ~ &amp;quot;University of Hawaii&amp;quot;,
    Location == &amp;quot;Social Science and Policy Studies Department, Worcester Polytechnic Institute, Worcester, MA 01609&amp;quot; ~ &amp;quot;Worcester Polytechnic Institute&amp;quot;,
    Location == &amp;quot;Department of Psychology, Washington and Lee University, Lexington, VA 24450&amp;quot; ~ &amp;quot;Washington and Lee University&amp;quot;,
    Location == &amp;quot;Department of Psychology, San Diego State University, San Diego, CA 92182&amp;quot; ~ &amp;quot;San Diego State University&amp;quot;,
    Location == &amp;quot;Tufts&amp;quot; ~ &amp;quot;Tufts&amp;quot;,
    Location == &amp;quot;University of Florida, Florida&amp;quot; ~ &amp;quot;University of Florida&amp;quot;,
    Location == &amp;quot;University of Illinois at Urbana-Champaign, Champaign, IL&amp;quot; ~ &amp;quot;University of Illinois at Urbana-Champaign&amp;quot;,
    Location == &amp;quot;Pacific Lutheran University, Tacoma, WA&amp;quot; ~ &amp;quot;Pacific Lutheran University&amp;quot;,
    Location == &amp;quot;University of Virginia, VA&amp;quot; ~ &amp;quot;University of Virginia&amp;quot;,
    Location == &amp;quot;Marian University, Indianapolis, IN&amp;quot; ~ &amp;quot;Marian University&amp;quot;,
    Location == &amp;quot;Department of Psychology, Ithaca College, Ithaca, NY 14850&amp;quot; ~ &amp;quot;Ithaca College&amp;quot;,
    Location == &amp;quot;University of Michigan&amp;quot; ~ &amp;quot;University of Michigan&amp;quot;,
    Location == &amp;quot;Department of Psychology, Pennsylvania State University Abington, Abington, PA 19001&amp;quot; ~ &amp;quot;Pennsylvania State University Abington&amp;quot;,
    Location == &amp;quot;Department of Psychology, Texas A&amp;amp;M University, College Station, TX 77843&amp;quot; ~ &amp;quot;Texas A&amp;amp;M University&amp;quot;,
    Location == &amp;quot;William Paterson University, Wayne, NJ&amp;quot; ~ &amp;quot;William Paterson University&amp;quot;,
    Location == &amp;quot;Department of Cognitive Science, Occidental College, Los Angeles, CA&amp;quot; ~ &amp;quot;Occidental College&amp;quot;,
    Location == &amp;quot;The Pennsylvania State University&amp;quot; ~ &amp;quot;The Pennsylvania State University&amp;quot;,
    Location == &amp;quot;MTurk US Workers&amp;quot; ~ &amp;quot;MTurk US Workers&amp;quot;,
    Location == &amp;quot;University of Graz AND the Universty of Vienna&amp;quot; ~ &amp;quot;University of Graz and the Universty of Vienna&amp;quot;,
    Location == &amp;quot;University of Potsdam, Germany&amp;quot; ~ &amp;quot;University of Potsdam&amp;quot;,
    Location == &amp;quot;Open University of Hong Kong&amp;quot; ~ &amp;quot;Open University of Hong Kong&amp;quot;,
    Location == &amp;quot;Concepción, Chile&amp;quot; ~ &amp;quot;Concepción&amp;quot;
  )) %&amp;gt;% 
  distinct(loc, lab)

# what is this?
labs %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 59
## Columns: 2
## $ loc &amp;lt;fct&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,…
## $ lab &amp;lt;chr&amp;gt; &amp;quot;University of Brasilia&amp;quot;, &amp;quot;Wilfrid Laurier University&amp;quot;, &amp;quot;University of British Columbi…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ll do some tricky wrangling with the output from &lt;code&gt;coef()&lt;/code&gt; and &lt;code&gt;fixef()&lt;/code&gt; to arrange the odds ratio summaries for the population average and the location-specific results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this will help us format the labels on the secondary y-axis
my_format &amp;lt;- function(number) {
  formatC(number, digits = 2, format = &amp;quot;f&amp;quot;)
}

# grab the theta_j summaries
groups &amp;lt;-
  coef(me1)$loc[, , &amp;quot;factorSideEffect&amp;quot;] %&amp;gt;% 
  data.frame() %&amp;gt;% 
  mutate(loc = distinct(h, loc) %&amp;gt;% pull()) %&amp;gt;% 
  arrange(Estimate)

# grat the mu summary
average &amp;lt;-
  fixef(me1) %&amp;gt;% 
  data.frame() %&amp;gt;% 
  slice(2) %&amp;gt;% 
  mutate(loc = &amp;quot;Average&amp;quot;)

# combine and wrangle
post &amp;lt;-
  bind_rows(groups, average) %&amp;gt;% 
  mutate(rank     = c(1:59, 0),
         Estimate = exp(Estimate),
         Q2.5     = exp(Q2.5),
         Q97.5    = exp(Q97.5)) %&amp;gt;% 
  left_join(labs, by = &amp;quot;loc&amp;quot;) %&amp;gt;% 
  arrange(rank) %&amp;gt;% 
  mutate(label   = ifelse(is.na(lab), &amp;quot;POPULATION AVERAGE&amp;quot;, lab),
         summary = str_c(my_format(Estimate), &amp;quot; [&amp;quot;, my_format(Q2.5), &amp;quot;, &amp;quot;, my_format(Q97.5), &amp;quot;]&amp;quot;))

# what have we done?
post %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 60
## Columns: 9
## $ Estimate  &amp;lt;dbl&amp;gt; 13.027040, 5.994537, 7.225509, 7.894728, 7.896201, 7.989348, 8.158148, 8.425675,…
## $ Est.Error &amp;lt;dbl&amp;gt; 0.09183827, 0.23712115, 0.35418533, 0.32549107, 0.35978096, 0.23125168, 0.343829…
## $ Q2.5      &amp;lt;dbl&amp;gt; 10.937724, 3.752456, 3.537577, 4.170577, 3.898701, 5.147465, 4.109451, 4.488119,…
## $ Q97.5     &amp;lt;dbl&amp;gt; 15.731289, 9.501053, 14.080042, 15.016368, 15.701244, 12.588834, 16.272517, 15.8…
## $ loc       &amp;lt;chr&amp;gt; &amp;quot;Average&amp;quot;, &amp;quot;19&amp;quot;, &amp;quot;38&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;32&amp;quot;, &amp;quot;55&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;34&amp;quot;, &amp;quot;22&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;58&amp;quot;, &amp;quot;24&amp;quot;, &amp;quot;…
## $ rank      &amp;lt;dbl&amp;gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22…
## $ lab       &amp;lt;chr&amp;gt; NA, &amp;quot;MTurk India Workers&amp;quot;, &amp;quot;University of Hawaii&amp;quot;, &amp;quot;Guangdong Literature &amp;amp; Art V…
## $ label     &amp;lt;chr&amp;gt; &amp;quot;POPULATION AVERAGE&amp;quot;, &amp;quot;MTurk India Workers&amp;quot;, &amp;quot;University of Hawaii&amp;quot;, &amp;quot;Guangdong …
## $ summary   &amp;lt;chr&amp;gt; &amp;quot;13.03 [10.94, 15.73]&amp;quot;, &amp;quot;5.99 [3.75, 9.50]&amp;quot;, &amp;quot;7.23 [3.54, 14.08]&amp;quot;, &amp;quot;7.89 [4.17, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s our custom forest plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +
  geom_interval(aes(color = label == &amp;quot;POPULATION AVERAGE&amp;quot;),
                size = 1/2) +
  geom_point(aes(size = 1 - Est.Error, color = label == &amp;quot;POPULATION AVERAGE&amp;quot;),
             shape = 15) +
  scale_color_viridis_d(option = &amp;quot;C&amp;quot;, begin = .33, end = .67) +
  scale_size_continuous(range = c(1, 3.5)) +
  scale_x_continuous(&amp;quot;odds ratio&amp;quot;, breaks = 1:6 * 10, expand = expansion(mult = c(0.005, 0.005))) +
  scale_y_continuous(NULL, breaks = 0:59, limits = c(-1, 60), expand = c(0, 0),
                     labels = pull(post, label),
                     sec.axis = dup_axis(labels = pull(post, summary))) +
  theme(text = element_text(family = &amp;quot;Times&amp;quot;),
        axis.text.y = element_text(hjust = 0, color = &amp;quot;white&amp;quot;, size = 7),
        axis.text.y.right = element_text(hjust = 1, size = 7),
        axis.ticks.y = element_blank(),
        panel.background = element_rect(fill = &amp;quot;grey8&amp;quot;),
        panel.border = element_rect(color = &amp;quot;transparent&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You may have noticed this plot is based on the results of our multilevel model, &lt;code&gt;me1&lt;/code&gt;. We could have done the same basic thing with the results from the more conventional meta-analysis model, &lt;code&gt;me0&lt;/code&gt;, too.&lt;/p&gt;
&lt;p&gt;I’m not aware this it typical in random effect meta-analyses, but it might be useful to further clarify the meaning of the two primary parameters, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;. Like with the forest plot, above, we could examine these with either &lt;code&gt;me0&lt;/code&gt; or &lt;code&gt;me1&lt;/code&gt;. For kicks, we’ll use &lt;code&gt;me0&lt;/code&gt; (the conventional Bayesian meta-analysis). In the output from &lt;code&gt;posterior_samples(me0)&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; are in the columns named &lt;code&gt;b_Intercept&lt;/code&gt; and &lt;code&gt;sd_loc__Intercept&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(me0)

post %&amp;gt;% 
  select(b_Intercept:sd_loc__Intercept) %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_Intercept sd_loc__Intercept
## 1    2.378526         0.4688289
## 2    2.562858         0.4555103
## 3    2.435846         0.3252279
## 4    2.658129         0.3895584
## 5    2.451356         0.3583352
## 6    2.672061         0.5595212&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you scroll back above, you’ll see our random effect meta-analysis explicitly presumed our empirical effect-size estimates &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; are approximations of the true effect sizes &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;, which are themselves normally distributed in the population of possible effect sizes from similar studies: &lt;span class=&#34;math inline&#34;&gt;\(\theta_j \sim \operatorname{Normal}(\mu, \tau)\)&lt;/span&gt;. Why not use our posterior samples to simulate draws from &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(\mu, \tau)\)&lt;/span&gt; to get a sense of what this distribution might look like? Recall that the parameters are in the log-odds metric. We’ll present the distribution in that metric and as odds ratios.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color &amp;lt;- viridis_pal(option = &amp;quot;C&amp;quot;)(7)[6]
set.seed(15)

post %&amp;gt;% 
  transmute(lo = rnorm(n(), mean = b_Intercept, sd = sd_loc__Intercept),
            or = rnorm(n(), mean = b_Intercept, sd = sd_loc__Intercept) %&amp;gt;% exp()) %&amp;gt;% 
  slice(1:1e3) %&amp;gt;% 
  pivot_longer(lo:or, values_to = &amp;quot;effect size&amp;quot;) %&amp;gt;% 
  mutate(name = factor(name, labels = c(&amp;quot;log-odds&amp;quot;, &amp;quot;odds ratio&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = `effect size`, y = 0)) +
  geom_dots(color = color, fill = color) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(Normal(mu*&amp;#39;, &amp;#39;*tau))) +
  theme(text = element_text(family = &amp;quot;Times&amp;quot;),
        strip.background = element_rect(color = &amp;quot;transparent&amp;quot;)) +
  facet_wrap(~name, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-16-bayesian-meta-analysis-in-brms-ii/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both panels show 1,000 draws, each of which is depicted by a single dot. If we were to run this experiment 1,000 times and compute the effect size separately for each one, this is what we’d expect those distributions of effect sizes to look like. Seems like there’s a lot of variation in there, eh? The next time you observe your fellow scientists debating over whether a study replicated or not, keep these distributions in mind. Once you start thinking about distributions, replication becomes a tricky notion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parting-thoughts.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parting thoughts.&lt;/h3&gt;
&lt;p&gt;There are other things you might do with these data. For example, you might inspect how much the effect size varies between those from WEIRD and non-WEIRD countries. You might also model the data as clustered by &lt;code&gt;Language&lt;/code&gt; rather than by &lt;code&gt;Location&lt;/code&gt;. But I think we’ve gone far enough to get you started.&lt;/p&gt;
&lt;p&gt;If you’d like to learn more about these methods, do check out Vourre’s &lt;a href=&#34;https://mvuorre.github.io/post/2016/09/29/meta-analysis-is-a-special-case-of-bayesian-multilevel-modeling/&#34;&gt;&lt;em&gt;Meta-analysis is a special case of Bayesian multilevel modeling&lt;/em&gt;&lt;/a&gt;. You might also read Williams, Rast, and Bürkner’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-williamsBayesianMetaanalysisWeakly2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; manuscript, &lt;a href=&#34;https://psyarxiv.com/7tbrm/&#34;&gt;&lt;em&gt;Bayesian meta-analysis with weakly informative prior distributions&lt;/em&gt;&lt;/a&gt;. For an alternative workflow, consider the &lt;a href=&#34;https://github.com/wwiecek/baggr&#34;&gt;&lt;strong&gt;baggr&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-baggr&#34; role=&#34;doc-biblioref&#34;&gt;Wiecek &amp;amp; Meager, 2020&lt;/a&gt;)&lt;/span&gt;, which is designed to fit hierarchical Bayesian meta-analyses with Stan under the hood.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1   broom_0.7.5       viridis_0.5.1     viridisLite_0.3.0 ggdark_0.2.1     
##  [6] brms_2.15.0       Rcpp_1.0.6        forcats_0.5.1     stringr_1.4.0     dplyr_1.0.5      
## [11] purrr_0.3.4       readr_1.4.0       tidyr_1.1.3       tibble_3.1.0      ggplot2_3.3.3    
## [16] tidyverse_1.3.0  
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] svUnit_1.0.3         splines_4.0.4        crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [49] htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [53] ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3      loo_2.4.1           
##  [57] dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2       tidyselect_1.1.0    
##  [61] rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [65] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [69] ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1        yaml_2.2.1          
##  [73] processx_3.4.5       knitr_1.31           fs_1.5.0             nlme_3.1-152        
##  [77] mime_0.10            projpred_2.0.2       xml2_1.3.2           compiler_4.0.4      
##  [81] bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13      gamm4_0.2-6         
##  [85] curl_4.3             reprex_0.3.0         statmod_1.4.35       stringi_1.5.3       
##  [89] highr_0.8            ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6   
##  [93] lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1        
##  [97] shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1         lifecycle_1.0.0     
## [101] bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4         R6_2.5.0            
## [105] bookdown_0.21        promises_1.1.1       gridExtra_2.3        codetools_0.2-18    
## [109] boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [113] assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0      multcomp_1.4-16     
## [117] mgcv_1.8-33          parallel_4.0.4       hms_0.5.3            grid_4.0.4          
## [121] coda_0.19-4          minqa_1.2.4          rmarkdown_2.7        shiny_1.5.0         
## [125] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp;amp; Rubin, D. B. (2013). &lt;em&gt;Bayesian data analysis&lt;/em&gt; (Third Edition). &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://stat.columbia.edu/~gelman/book/&#34;&gt;https://stat.columbia.edu/~gelman/book/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hauserDissociationMoralJudgments2007&#34; class=&#34;csl-entry&#34;&gt;
Hauser, M., Cushman, F., Young, L., Jin, R. K.-X., &amp;amp; Mikhail, J. (2007). A dissociation between moral judgments and justifications. &lt;em&gt;Mind &amp;amp; Language&lt;/em&gt;, &lt;em&gt;22&lt;/em&gt;(1), 1–21. &lt;a href=&#34;https://doi.org/10.1111/j.1468-0017.2006.00297.x&#34;&gt;https://doi.org/10.1111/j.1468-0017.2006.00297.x&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kleinManyLabsInvestigating2018&#34; class=&#34;csl-entry&#34;&gt;
Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A. (2018). Many &lt;span&gt;Labs&lt;/span&gt; 2: &lt;span&gt;Investigating&lt;/span&gt; variation in replicability across samples and settings. &lt;em&gt;Advances in Methods and Practices in Psychological Science&lt;/em&gt;, &lt;em&gt;1&lt;/em&gt;(4), 443–490. &lt;a href=&#34;https://doi.org/10.1177/2515245918810225&#34;&gt;https://doi.org/10.1177/2515245918810225&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingBrms2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Statistical rethinking with brms, &lt;span class=&#34;nocase&#34;&gt;ggplot2&lt;/span&gt;, and the tidyverse&lt;/em&gt; (version 1.2.0). &lt;a href=&#34;https://doi.org/10.5281/zenodo.3693202&#34;&gt;https://doi.org/10.5281/zenodo.3693202&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Statistical rethinking with brms, Ggplot2, and the tidyverse: &lt;span&gt;Second&lt;/span&gt; edition&lt;/em&gt; (version 0.1.1). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-baggr&#34; class=&#34;csl-entry&#34;&gt;
Wiecek, W., &amp;amp; Meager, R. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;baggr&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; aggregate treatment effects&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=baggr&#34;&gt;https://CRAN.R-project.org/package=baggr&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsBayesianMetaanalysisWeakly2018&#34; class=&#34;csl-entry&#34;&gt;
Williams, D. R., Rast, P., &amp;amp; Bürkner, P.-C. (2018). &lt;em&gt;Bayesian meta-analysis with weakly informative prior distributions&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/7tbrm&#34;&gt;https://doi.org/10.31234/osf.io/7tbrm&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Time-varying covariates in longitudinal multilevel models contain state- and trait-level information: This includes binary variables, too</title>
      <link>/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/</guid>
      <description>
&lt;script src=&#34;/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;When you have a time-varying covariate you’d like to add to a multilevel growth model, it’s important to break that variable into two. One part of the variable will account for within-person variation. The other part will account for between person variation. Keep reading to learn how you might do so when your time-varying covariate is binary.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;i-assume-things.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I assume things.&lt;/h2&gt;
&lt;p&gt;For this post, I’m presuming you are familiar with longitudinal multilevel models and vaguely familiar with the basic differences between frequentist and Bayesian statistics. All code in is &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;&lt;strong&gt;R&lt;/strong&gt;&lt;/a&gt;, with a heavy use of the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt;–which you might learn a lot about &lt;a href=&#34;http://r4ds.had.co.nz&#34;&gt;here&lt;/a&gt;, especially &lt;a href=&#34;http://r4ds.had.co.nz/transform.html&#34;&gt;chapter 5&lt;/a&gt;–, and the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; package&lt;/a&gt; for Bayesian regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;context&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Context&lt;/h2&gt;
&lt;p&gt;In my applied work, one of my collaborators collects longitudinal behavioral data. They are in the habit of analyzing their focal dependent variables (DVs) with variants of the longitudinal multilevel model, which is great. Though they often collect their primary independent variables (IVs) at all time points, they typically default to only using the baseline values for their IVs to predict the random intercepts and slopes of the focal DVs.&lt;/p&gt;
&lt;p&gt;It seems like we’re making inefficient use of the data. At first I figured we’d just use the IVs at all time points, which would be treating them as time-varying covariates. But time varying covariates don’t allow one to predict variation in the random intercepts and slopes, which I and my collaborator would like to do. So while using the IVs at all time points as time-varying covariates makes use of more of the available data, it requires us to trade one substantive focus for another, which seems frustrating.&lt;/p&gt;
&lt;p&gt;After low-key chewing on this for a while, I recalled that it’s possible to decompose time-varying covariates into measures of traits and states. Consider the simple case where your time-varying covariate, &lt;span class=&#34;math inline&#34;&gt;\(x_{ij}\)&lt;/span&gt; is continuous. In this notation, the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; values vary across persons &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and time points &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. If we compute the person level mean, &lt;span class=&#34;math inline&#34;&gt;\(\overline x_i\)&lt;/span&gt;, that would be a time-invariant covariate and would, conceptually, be a measure of a person’s trait level for &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Even if you do this, it’s still okay to include both &lt;span class=&#34;math inline&#34;&gt;\(\overline x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_{ij}\)&lt;/span&gt; in the model equation. The former would be the time-&lt;em&gt;invariant&lt;/em&gt; covariate that might predict the variation in the random intercepts and slopes. The latter would still serve as a time-&lt;em&gt;varying&lt;/em&gt; covariate that might account for the within-person variation in the DV over time.&lt;/p&gt;
&lt;p&gt;There, of course, are technicalities about how one might center &lt;span class=&#34;math inline&#34;&gt;\(\overline x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_{ij}\)&lt;/span&gt; that one should carefully consider for these kinds of models. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.928.9848&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Enders &amp;amp; Tofighi (2007)&lt;/a&gt; covered the issue from a cross-sectional perspective. &lt;a href=&#34;http://www.pilesofvariance.com/index.html&#34;&gt;Hoffman (2015)&lt;/a&gt; covered it from a longitudinal perspective. But in the grand scheme of things, those are small potatoes. The main deal is that I can use our IVs as both time-varying and time-invariant predictors.&lt;/p&gt;
&lt;p&gt;I was pretty excited once I remembered all this.&lt;/p&gt;
&lt;p&gt;But then I realized that some of my collaborator’s IVs are binary, which initially seemed baffling, to me. Would it be sensible to compute &lt;span class=&#34;math inline&#34;&gt;\(\overline x_i\)&lt;/span&gt; for a binary time-varying covariate? What would that mean for the time-varying version of the variable? So I did what any responsible postdoctoral researcher would do. I posed the issue on Twitter.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Context: multilevel growth models.&lt;br&gt;&lt;br&gt;When you have a continuous time-varying covariate, you can decompose it into two variables, an id-level grand mean and the time-specific deviations from that mean. Is anyone aware of a complimentary approach for binary time-varying covariates?&lt;/p&gt;&amp;mdash; Solomon Kurz (@SolomonKurz) &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1188185892332150789?ref_src=twsrc%5Etfw&#34;&gt;October 26, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;My initial thoughts on the topic were a little confused. I wasn’t differentiating well between issues about the variance decomposition and centering and I’m a little embarrassed over that gaff. But I’m still glad I posed the question to Twitter. My virtual colleagues came through in spades! In particular, I’d like to give major shout outs to Andrea Howard (&lt;a href=&#34;https://twitter.com/DrAndreaHoward&#34;&gt;@DrAndreaHoward&lt;/a&gt;), Mattan Ben-Shachar (&lt;a href=&#34;https://twitter.com/mattansb&#34;&gt;@mattansb&lt;/a&gt;), and Aidan Wright (&lt;a href=&#34;https://twitter.com/aidangcw&#34;&gt;@aidangcw&lt;/a&gt;), who collectively pointed me to the solution. It was detailed in the references I listed, above: Enders &amp;amp; Tofighi (2007) and Hoffman (2015). Thank you, all!&lt;/p&gt;
&lt;p&gt;Here’s the deal: Yes, you simply take the person-level means for the binary covariate &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. That will create a vector of time-invariant IVs ranging continuously from 0 to 1. They’ll be in a probability metric and they conceptually index a person’s probability of endorsing 1 over time. It’s basically the same as a batting average in baseball. You are at liberty to leave the time-invariant covariate in this metric, or you could center it by standardizing or some other sensible transformation. As for the state version of the IV, &lt;span class=&#34;math inline&#34;&gt;\(x_{ij}\)&lt;/span&gt;, you’d just leave it exactly as it is. [There are other ways to code binary data, such as effects coding. I’m not a fan and will not be covering that in detail, here. But yes, you could recode your time-varying binary covariate that way, too.]&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;break-out-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Break out the data&lt;/h2&gt;
&lt;p&gt;We should practice this with some data. I’ve been chipping away at working through Singer and Willett’s classic (2003) text, &lt;a href=&#34;https://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968&#34;&gt;&lt;em&gt;Applied longitudinal data analysis: Modeling change and event occurrence&lt;/em&gt;&lt;/a&gt; with &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt; code. You can find the working files in this &lt;a href=&#34;https://github.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse&#34;&gt;GitHub repository&lt;/a&gt;. In chapter 5, Singer and Willett worked through a series of examples with a data set with a continuous DV and a binary IV. Here are those data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

d &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse/master/data/unemployment_pp.csv&amp;quot;)

glimpse(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 674
## Columns: 4
## $ id     &amp;lt;dbl&amp;gt; 103, 103, 103, 641, 641, 641, 741, 846, 846, 846, 937, 937, 111…
## $ months &amp;lt;dbl&amp;gt; 1.149897, 5.946612, 12.911704, 0.788501, 4.862423, 11.827515, 1…
## $ cesd   &amp;lt;dbl&amp;gt; 25, 16, 33, 27, 7, 25, 40, 2, 22, 0, 3, 8, 3, 0, 5, 7, 18, 26, …
## $ unemp  &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, …&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;set-the-stage-with-descriptive-plots.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set the stage with descriptive plots.&lt;/h3&gt;
&lt;p&gt;The focal DV is &lt;code&gt;cesd&lt;/code&gt;, a continuous variable measuring depression. Singer and Willett (2003):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Each time participants completed the Center for Epidemiologic Studies’ Depression (CES-D) scale (&lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/014662167700100306?casa_token=igspo7W_9SUAAAAA%3AhnRVqiDEM-b6nNh_-8VQ6tx1PukP8nsqyo4yd4m_inspjhH-3aeShEGodUxux8GuInG9AYbP1D2GLA&amp;amp;journalCode=apma&#34;&gt;Radloff, 1977&lt;/a&gt;), which asks them to rate, on a four-point scale, the frequency with which they experience each of the 20 depressive symptoms. The CES-D scores can vary from a low or 0 for someone with no symptoms to a high of 80 for someone in serious distress. (p. 161)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here’s what the &lt;code&gt;cesd&lt;/code&gt; scores look like, collapsing over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_gray() +
            theme(panel.grid = element_blank()))

d %&amp;gt;% 
  ggplot(aes(x = cesd)) +
  geom_histogram(fill = &amp;quot;grey50&amp;quot;, binwidth = 1) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since these are longutdnial data, our fundamental IV is a measure of time. That’s captured in the &lt;code&gt;months&lt;/code&gt; column. Most participants have data on just three occasions and the &lt;code&gt;months&lt;/code&gt; values range from about 0 to 15.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  ggplot(aes(x = months)) +
  geom_histogram(fill = &amp;quot;grey50&amp;quot;, binwidth = 1) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The main research question we’ll be addressing is: &lt;em&gt;What do participants’ &lt;code&gt;cesd&lt;/code&gt; scores look like over time and to what extent does their employment/unemployment status help explain their depression?&lt;/em&gt; So our substantive IV of interest is &lt;code&gt;unemp&lt;/code&gt;, which is coded 0 = employed and 1 = unemployed. Since participants were recruited from local unemployment offices, everyone started off as &lt;code&gt;unemp == 1&lt;/code&gt;. The values varied after that. Here’s a look at the data from a random sample of 25 of the participants.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this makes `sample_n()` reproducible
set.seed(5)

# wrangle the data a little
d %&amp;gt;% 
  nest(data = c(months, cesd, unemp)) %&amp;gt;% 
  sample_n(size = 25) %&amp;gt;% 
  unnest(data) %&amp;gt;% 
  mutate(id = str_c(&amp;quot;id: &amp;quot;, id),
         e  = if_else(unemp == 0, &amp;quot;employed&amp;quot;, &amp;quot;unemployed&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = months, y = cesd)) +
  geom_line(aes(group = id),
            size = 1/4) +
  geom_point(aes(color = e),
             size = 7/4) +
  scale_color_manual(NULL, values = c(&amp;quot;blue3&amp;quot;, &amp;quot;red3&amp;quot;)) +
  theme(panel.grid      = element_blank(),
        legend.position = &amp;quot;top&amp;quot;) +
  facet_wrap(~id, nrow = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;embrace-the-hate.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Embrace the hate.&lt;/h3&gt;
&lt;p&gt;To be honest, I kinda hate these data. There are too few measurement occasions within participants for my liking and the assessment schedule just seems bazar. As we’ll see in a bit, these data are also un-ideal to address exactly the kinds of models this blog is centered on.&lt;/p&gt;
&lt;p&gt;Yet it’s for just these reasons I love these data. Real-world data analysis is ugly. The data are never what you want or expected them to be. So it seems the data we use in our educational materials should be equally terrible.&lt;/p&gt;
&lt;p&gt;Much like we do for our most meaningful relationships, let’s embrace our hate/love ambivalence for our data with wide-open eyes and tender hearts. 🖤&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;time-to-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Time to model.&lt;/h3&gt;
&lt;p&gt;Following Singer and Willett, we can define our first model using a level-1/level-2 specification. The level-1 model would be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{cesd}_{ij} = \pi_{0i} + \pi_{1i} \text{months}_{ij} + \pi_{2i} \text{unemp}_{ij} + \epsilon_{ij},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\pi_{0i}\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(\pi_{1i}\)&lt;/span&gt; is the effect of &lt;code&gt;months&lt;/code&gt; on &lt;code&gt;cesd&lt;/code&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\pi_{2i}\)&lt;/span&gt; is the effect of &lt;code&gt;unemp&lt;/code&gt; on &lt;code&gt;cesd&lt;/code&gt;. The final term, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij}\)&lt;/span&gt;, is the within-person variation not accounted for by the model–sometimes called error or residual variance. Our &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij}\)&lt;/span&gt; term follows the usual distribution of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\epsilon_{ij} \sim \operatorname{Normal} (0, \sigma_\epsilon),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which, in words, means that the within-person variance estimates are normally distributed with a mean of zero and a standard deviation that’s estimated from the data. The corresponding level-2 model follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\pi_{0i} &amp;amp; = \gamma_{00} + \zeta_{0i} \\
\pi_{1i} &amp;amp; = \gamma_{10} + \zeta_{1i} \\
\pi_{2i} &amp;amp; = \gamma_{20},
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; is the grand mean for the intercept, which varies by person, as captured by the level-2 variance term &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{0i}\)&lt;/span&gt;. Similarly, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{10}\)&lt;/span&gt; is the grand mean for the effect of &lt;code&gt;months&lt;/code&gt;, which varies by person, as captured by the second level-2 variance term &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{1i}\)&lt;/span&gt;. With this parameterization, it turns out &lt;span class=&#34;math inline&#34;&gt;\(\pi_{2i}\)&lt;/span&gt; does not vary by person and so its &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{20}\)&lt;/span&gt; terms does not get a corresponding level-2 variance coefficient. If we wanted the effects of the time-varying covariate &lt;code&gt;unemp&lt;/code&gt; to vary across individuals, we’d expand the definition of &lt;span class=&#34;math inline&#34;&gt;\(\pi_{2i}\)&lt;/span&gt; to be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\pi_{2i} = \gamma_{20} + \zeta_{2i}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Within our &lt;strong&gt;brms&lt;/strong&gt; paradigm, the two level-2 variance parameters follow the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\begin{bmatrix} 
\zeta_{0i} \\ \zeta_{1i} \\
\end{bmatrix} &amp;amp; \sim \operatorname{Normal} 
\left ( 
\begin{bmatrix} 0 \\ 0 \end{bmatrix},
\mathbf{D} \mathbf{\Omega} \mathbf{D}&amp;#39;
\right ), \text{where} \\

\mathbf{D}    &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_1 \end{bmatrix} \text{and} \\

\mathbf{\Omega}  &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho_{01} \\ \rho_{01} &amp;amp; 1 \end{bmatrix}.

\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I’ll be using a weakly-regularizing approach for the model priors in this post. I detail how I came to these in the &lt;a href=&#34;https://github.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse/blob/master/05.md&#34;&gt;Chapter 5 file from my GitHub repo&lt;/a&gt;. If you check that file, you’ll see this model is a simplified version of &lt;code&gt;fit10&lt;/code&gt;. Here are our priors:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\gamma_{00}     &amp;amp; \sim \operatorname{Normal}(14.5, 20) \\
\gamma_{10} \text{ and }  \gamma_{20}  &amp;amp; \sim \operatorname{Normal}(0, 10) \\
\sigma_\epsilon, \sigma_0,  \text{ and } \sigma_1 &amp;amp; \sim \operatorname{Student-t} (3, 0, 10) \\
\Omega          &amp;amp; \sim \operatorname{LKJ} (4).
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Feel free to explore different priors on your own. But now we’re done spelling our our first model, it’s time to fire up our main statistical package, &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can fit the model with &lt;code&gt;brms::brm()&lt;/code&gt;, like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d, 
      family = gaussian,
      cesd ~ 0 + intercept + months + unemp + (1 + months | id),
      prior = c(prior(normal(14.5, 20), class = b, coef = &amp;quot;intercept&amp;quot;),
                prior(normal(0, 10),    class = b),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(4), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = .95),
      seed = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we explore the results from this model, we should point out that we only included &lt;code&gt;unemp&lt;/code&gt; as a level-1 time-varying predictor. As Hoffman pointed out in her (2015) text, the flaw in this approach is that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;time-varying predictors contain both between-person and within-person information&lt;/em&gt;…&lt;/p&gt;
&lt;p&gt;[Thus,] time-varying predictors will need to be represented by two separate predictors that distinguish their between-person and within-person sources of variance in order to properly distinguish their potential between-person and within-person effects on a longitudinal outcome. (pp. 329, 333, &lt;em&gt;emphasis&lt;/em&gt; in the original)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The simplest way to separate the between-person variance in &lt;code&gt;unemp&lt;/code&gt; from the pure within-person variation is to compute a new variable capturing &lt;span class=&#34;math inline&#34;&gt;\(\overline{\text{unemp}}_i\)&lt;/span&gt;, the person-level means for their unemployment status. Here we compute that variable, which we’ll call &lt;code&gt;unemp_id_mu&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;% 
  group_by(id) %&amp;gt;% 
  mutate(unemp_id_mu = mean(unemp)) %&amp;gt;% 
  ungroup()

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##      id months  cesd unemp unemp_id_mu
##   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1   103  1.15     25     1       1    
## 2   103  5.95     16     1       1    
## 3   103 12.9      33     1       1    
## 4   641  0.789    27     1       0.333
## 5   641  4.86      7     0       0.333
## 6   641 11.8      25     0       0.333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because &lt;code&gt;umemp&lt;/code&gt; is binary, &lt;span class=&#34;math inline&#34;&gt;\(\overline{\text{unemp}}_i\)&lt;/span&gt; can only take on values ranging from 0 to 1. Here are the unique values we have for &lt;code&gt;unemp_id_mu&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d %&amp;gt;% 
  distinct(unemp_id_mu)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 1
##   unemp_id_mu
##         &amp;lt;dbl&amp;gt;
## 1       1    
## 2       0.333
## 3       0.667
## 4       0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because each participant’s &lt;span class=&#34;math inline&#34;&gt;\(\overline{\text{unemp}}_i\)&lt;/span&gt; was based on 3 or fewer measurement occasions, basic algebra limited the variability in our &lt;code&gt;unemp_id_mu&lt;/code&gt; values. You’ll also note that there were no 0s. This, recall, is because participants were recruited at local unemployment offices, leaving all participants with at least one starting value of &lt;code&gt;unemp == 1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We should rehearse how we might interpret the &lt;code&gt;unemp_id_mu&lt;/code&gt; values. First recall they are considered level-2 variables; they are between-participant variables. Since they are averages of binary data, they are in a probability metric. In this instance, they are each participants overall probability of being unemployed–their trait-level propensity toward unemployment. No doubt these values would be more reliable if they were computed from data on a greater number of assessment occasions. But with three measurement occasions, we at least have a sense of stability.&lt;/p&gt;
&lt;p&gt;Since our new &lt;span class=&#34;math inline&#34;&gt;\(\overline{\text{unemp}}_i\)&lt;/span&gt; variable is a level-2 predictor, the level-1 equation for our next model is the same as before:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{cesd}_{ij} = \pi_{0i} + \pi_{1i} \text{months}_{ij} + \pi_{2i} \text{unemp}_{ij} + \epsilon_{ij}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, there are two new terms in our level-2 model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\pi_{0i} &amp;amp; = \gamma_{00} + \gamma_{01} (\overline{\text{unemp}}_i) + \zeta_{0i} \\
\pi_{1i} &amp;amp; = \gamma_{10} + \gamma_{11} (\overline{\text{unemp}}_i) + \zeta_{1i} \\
\pi_{2i} &amp;amp; = \gamma_{20},
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is meant to convey that &lt;span class=&#34;math inline&#34;&gt;\(\overline{\text{unemp}}_i\)&lt;/span&gt; is allowed to explain variability in both initial status on CES-D scores (i.e., the random intercepts) and change in CES-D scores over time (i.e., the random &lt;code&gt;months&lt;/code&gt; slopes). Our variance parameters are all the same:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\epsilon_{ij} &amp;amp; \sim \operatorname{Normal} (0, \sigma_\epsilon) \text{ and} \\
\begin{bmatrix} 
\zeta_{0i} \\ \zeta_{1i} \\
\end{bmatrix} &amp;amp; \sim \operatorname{Normal} 
\left ( 
\begin{bmatrix} 0 \\ 0 \end{bmatrix},
\mathbf{D} \mathbf{\Omega} \mathbf{D}&amp;#39;
\right ), \text{where} \\

\mathbf{D}    &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_1 \end{bmatrix} \text{and} \\

\mathbf{\Omega}  &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho_{01} \\ \rho_{01} &amp;amp; 1 \end{bmatrix}.

\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Our priors also follow the same basic specification as before:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\gamma_{00}     &amp;amp; \sim \operatorname{Normal}(14.5, 20) \\
\gamma_{01}, \gamma_{10}, \gamma_{11}, \text{ and }  \gamma_{20}  &amp;amp; \sim \operatorname{Normal}(0, 10) \\
\sigma_\epsilon, \sigma_0,  \text{ and } \sigma_1 &amp;amp; \sim \operatorname{Student-t} (3, 0, 10) \\
\Omega          &amp;amp; \sim \operatorname{LKJ} (4).
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note, however, that the inclusion of our new level-2 predictor, &lt;span class=&#34;math inline&#34;&gt;\((\overline{\text{unemp}}_i)\)&lt;/span&gt;, changes the meaning of the intercept, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt;. The intercept is now the expected value for a person for whom &lt;code&gt;unemp_id_mu == 0&lt;/code&gt; at the start of the study (i.e., &lt;code&gt;months == 0&lt;/code&gt;). I still think our intercept prior from the first model is fine for this example. But do think carefully about the priors you use in your real-world data analyses.&lt;/p&gt;
&lt;p&gt;Here’s how to fit the udpdate model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;-
  brm(data = d, 
      family = gaussian,
      cesd ~ 0 + intercept + months + unemp + unemp_id_mu + unemp_id_mu:months + (1 + months | id),
      prior = c(prior(normal(14.5, 20), class = b, coef = &amp;quot;intercept&amp;quot;),
                prior(normal(0, 10),    class = b),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(4), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We should fit one more model before we look at the parameters. If you were paying close attention, above, you may have noticed how it’s odd that we kept &lt;code&gt;unemp_id_mu&lt;/code&gt; in it’s natural metric. Sure, it’s fine in principle–sensible even–to use a variable in a probability metric. But in this particular study, none of the participants had a value of &lt;code&gt;unemp_id_mu == 0&lt;/code&gt; because all of them were unemployed at the first time point. Though it is mathematically kosher to fit a model with an intercept based on &lt;code&gt;unemp_id_mu == 0&lt;/code&gt;, it’s awkward to interpret. So in this case, it makes sense to transform the metric of our level-2 predictor. Perhaps the simplest way is to standardize the variable. That would then give an intercept based on the average &lt;code&gt;unemp_id_mu&lt;/code&gt; value and a &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{01}\)&lt;/span&gt; coefficient that was the expected change in intercept based on a one-standard-deviation higher value in &lt;code&gt;unemp_id_mu&lt;/code&gt;. Let’s compute that new standardized variable, which we’ll call &lt;code&gt;unemp_id_mu_s&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;% 
  nest(data = c(months:unemp)) %&amp;gt;% 
  mutate(unemp_id_mu_s = (unemp_id_mu - mean(unemp_id_mu)) / sd(unemp_id_mu)) %&amp;gt;% 
  unnest(data)

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##      id unemp_id_mu months  cesd unemp unemp_id_mu_s
##   &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
## 1   103       1      1.15     25     1         0.873
## 2   103       1      5.95     16     1         0.873
## 3   103       1     12.9      33     1         0.873
## 4   641       0.333  0.789    27     1        -1.58 
## 5   641       0.333  4.86      7     0        -1.58 
## 6   641       0.333 11.8      25     0        -1.58&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model formula is the same as before with the exception that we replace &lt;code&gt;unemp_id_mu&lt;/code&gt; with &lt;code&gt;unemp_id_mu_s&lt;/code&gt;. For simplicity, I’m leaving the priors the way they were.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;-
  brm(data = d, 
      family = gaussian,
      cesd ~ 0 + intercept + months + unemp + unemp_id_mu_s + unemp_id_mu_s:months + (1 + months | id),
      prior = c(prior(normal(14.5, 20), class = b, coef = &amp;quot;intercept&amp;quot;),
                prior(normal(0, 10),    class = b),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(4), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = .9),
      seed = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of examining each of the model summaries one by one, we’ll condense the information into a series of coefficient plots. For simplicity, we’ll restrict our focus to the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the `fit1` summaries
fixef(fit1) %&amp;gt;%
  data.frame() %&amp;gt;%
  rownames_to_column(&amp;quot;par&amp;quot;) %&amp;gt;%
  mutate(fit = &amp;quot;fit1&amp;quot;) %&amp;gt;% 
  bind_rows(
    # add the `fit2` summaries
    fixef(fit2) %&amp;gt;%
      data.frame() %&amp;gt;% 
      rownames_to_column(&amp;quot;par&amp;quot;) %&amp;gt;%
      mutate(fit = &amp;quot;fit2&amp;quot;),
    # add the `fit2` summaries
    fixef(fit3) %&amp;gt;%
      data.frame() %&amp;gt;% 
      rownames_to_column(&amp;quot;par&amp;quot;) %&amp;gt;%
      mutate(fit = &amp;quot;fit3&amp;quot;)
  ) %&amp;gt;% 
  # rename the parameters
  mutate(gamma = case_when(
    par == &amp;quot;intercept&amp;quot;     ~ &amp;quot;gamma[0][0]&amp;quot;,
    par == &amp;quot;months&amp;quot;        ~ &amp;quot;gamma[1][0]&amp;quot;,
    par == &amp;quot;unemp&amp;quot;         ~ &amp;quot;gamma[2][0]&amp;quot;,
    str_detect(par, &amp;quot;:&amp;quot;)   ~ &amp;quot;gamma[1][1]&amp;quot;,
    par == &amp;quot;unemp_id_mu&amp;quot;   ~ &amp;quot;gamma[0][1]&amp;quot;,
    par == &amp;quot;unemp_id_mu_s&amp;quot; ~ &amp;quot;gamma[0][1]&amp;quot;
  )) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = fit, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_pointrange(fatten = 3) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ gamma, nrow = 3, scale = &amp;quot;free_x&amp;quot;, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-31-time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In case you’re not familiar with the output from the &lt;code&gt;brms::fixef()&lt;/code&gt; function, each of the parameter estimates are summarized by their posterior means (i.e,. the dots) and percentile-based 95% intervals (i.e., the horizontal lines).&lt;/p&gt;
&lt;p&gt;Recall how earlier I complained that these data weren’t particularly good for demonstrating this method? Well, here you finally get to see why. Regardless of the model, the estimates didn’t change much. In these data, the predictive utility of our between-level variable, &lt;code&gt;unemp_id_mu&lt;/code&gt;–standardized or not–, was just about zilch. This is summarized by the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{01}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{11}\)&lt;/span&gt; parameters. Both are centered around zero for both models containing them. Thus adding in an inconsequential level-2 predictor had little effect on its level-1 companion, &lt;code&gt;unemp&lt;/code&gt;, which was expressed by &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{20}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Depressing as these results are, the practice was still worthwhile. Had we not decomposed our time-varying &lt;code&gt;unemp&lt;/code&gt; variable into its within- and between-level components, we would never had known that the trait levels of &lt;code&gt;umemp&lt;/code&gt; were inconsequential for these analyses. Now we know. For these models, all the action for &lt;code&gt;unemp&lt;/code&gt; was at the within-person level.&lt;/p&gt;
&lt;p&gt;This is also the explanation for why we focused on the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;s to the neglect of the variance parameters. Because our &lt;code&gt;unemp_id_mu&lt;/code&gt; variables were poor predictors of the random effects, there was no reason to expect they’d differ meaningfully across models. And because &lt;code&gt;unemp_id_mu&lt;/code&gt; is only a level-2 predictor, it never had any hope for changing the estimates for &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-centering-umemp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What about centering &lt;code&gt;umemp&lt;/code&gt;?&lt;/h3&gt;
&lt;p&gt;If you look through our primary two references for this post, Enders &amp;amp; Tofighi (2007) and Hoffman (2015), you’ll see both works spend a lot of time on discussing how one might center the level-1 versions of the time-varying covariates. If &lt;code&gt;unemp&lt;/code&gt; was a continuous variable, we would have had to contend with that issue, too. But this just isn’t necessary with binary variables. They have a sensible interpretation when left in the typical 0/1 format. So my recommendation is when you’re decomposing your binary time-varying covariates, put your focus on meaningfully centering the level-2 version of the variable. Leave the level-1 version alone. However, if you’re really interested in playing around with alternatives like effects coding, Enders and Tofighi provided several recommendations.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
##  [9] tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         splines_4.0.4        crosstalk_1.1.0.1   
##   [7] TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17       
##  [10] digest_0.6.27        htmltools_0.5.1.1    rsconnect_0.8.16    
##  [13] fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [16] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1          
##  [19] sandwich_3.0-0       prettyunits_1.1.1    colorspace_2.0-0    
##  [22] rvest_0.3.6          haven_2.3.1          xfun_0.22           
##  [25] callr_3.5.1          crayon_1.4.1         jsonlite_1.7.2      
##  [28] lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [31] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1     
##  [34] V8_3.4.0             pkgbuild_1.2.0       rstan_2.21.2        
##  [37] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [40] emo_0.0.0.9000       DBI_1.1.0            miniUI_0.1.1.1      
##  [43] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [46] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2          
##  [49] threejs_0.3.3        ellipsis_0.3.1       pkgconfig_2.0.3     
##  [52] loo_2.4.1            farver_2.0.3         dbplyr_2.0.0        
##  [55] utf8_1.1.4           tidyselect_1.1.0     labeling_0.4.2      
##  [58] rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1       
##  [61] munsell_0.5.0        cellranger_1.1.0     tools_4.0.4         
##  [64] cli_2.3.1            generics_0.1.0       broom_0.7.5         
##  [67] ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1       
##  [70] yaml_2.2.1           processx_3.4.5       knitr_1.31          
##  [73] fs_1.5.0             nlme_3.1-152         mime_0.10           
##  [76] projpred_2.0.2       xml2_1.3.2           compiler_4.0.4      
##  [79] bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [82] curl_4.3             gamm4_0.2-6          reprex_0.3.0        
##  [85] statmod_1.4.35       stringi_1.5.3        highr_0.8           
##  [88] ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6   
##  [91] lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [94] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
##  [97] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0
## [100] estimability_1.3     httpuv_1.5.4         R6_2.5.0            
## [103] bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [106] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0  
## [109] MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1    
## [112] withr_2.4.1          shinystan_2.5.0      multcomp_1.4-16     
## [115] mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [118] grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [121] rmarkdown_2.7        shiny_1.5.0          lubridate_1.7.9.2   
## [124] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian power analysis: Part III.b. What about 0/1 data?</title>
      <link>/post/bayesian-power-analysis-part-iii-b/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/bayesian-power-analysis-part-iii-b/</guid>
      <description>
&lt;script src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;version-1.1.0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Version 1.1.0&lt;/h2&gt;
&lt;p&gt;Edited on April 21, 2021, to fix a few code breaks and add a Reference section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;orientation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Orientation&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&#34;https://solomonkurz.netlify.com/post/bayesian-power-analysis-part-iii-a/&#34;&gt;last post&lt;/a&gt;, we covered how the Poisson distribution is handy for modeling count data. Binary data are even weirder than counts. They typically only take on two values: 0 and 1. Sometimes 0 is a stand-in for “no” and 1 for “yes” (e.g., &lt;em&gt;Are you an expert in Bayesian power analysis?&lt;/em&gt; For me that would be &lt;code&gt;0&lt;/code&gt;). You can also have data of this kind if you asked people whether they’d like to choose option A or B. With those kinds of data, you might arbitrarily code A as 0 and B as 1. Binary data also often stand in for trials where 0 = “fail” and 1 = “success.” For example, if you answered “Yes” to the question &lt;em&gt;Are all data normally distributed?&lt;/em&gt; we’d mark your answer down as a &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Though 0’s and 1’s are popular, sometimes binary data appear in their aggregated form. Let’s say I gave you 10 algebra questions and you got 7 of them right. Here’s one way to encode those data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 10
z &amp;lt;- 7

rep(0:1, times = c(n - z, z))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0 0 0 1 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In that example, &lt;code&gt;n&lt;/code&gt; stood for the total number of trials and &lt;code&gt;z&lt;/code&gt; was the number you got correct (i.e., the number of times we encoded your response as a 1). A more compact way to encode that data is with two columns, one for &lt;code&gt;z&lt;/code&gt; and the other for &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

tibble(z = z,
       n = n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##       z     n
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     7    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So then if you gave those same 10 questions to four of your friends, we could encode the results like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(3)

tibble(id = letters[1:5],
       z  = rpois(n = 5, lambda = 5),
       n  = n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 3
##   id        z     n
##   &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 a         3    10
## 2 b         7    10
## 3 c         4    10
## 4 d         4    10
## 5 e         5    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you were &lt;code&gt;b&lt;/code&gt;, you’d be the smart one in the group.&lt;/p&gt;
&lt;p&gt;Anyway, whether working with binary or aggregated binary data, we’re interested in the probability a given trial will be 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression-with-unaggregated-binary-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic regression with unaggregated binary data&lt;/h2&gt;
&lt;p&gt;Taking unaggregated binary data as a starting point, given &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; data that includes a variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; where the value in the &lt;span class=&#34;math inline&#34;&gt;\(i^\text{th}\)&lt;/span&gt; row is a 0 or a 1, we’d like to know the probability a given trial would be 1, given &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; [i.e., &lt;span class=&#34;math inline&#34;&gt;\(p(y_i = 1 | d)\)&lt;/span&gt;]. The binomial distribution will help us get that estimate for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We’ll do so within the context of a logistic regression model following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i                        &amp;amp; \sim \text{Binomial} (n = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \beta_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;were the logit function is defined as the log odds&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\operatorname{logit} (p_i) = \log \left (\frac{p_i}{1 - p_i} \right ),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which also means that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\log \left (\frac{p_i}{1 - p_i} \right ) = \beta_0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In those formulas, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the intercept. In a binomial model with no predictors&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is just the estimate for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, but in the log-odds metric. So yes, similar to the Poisson models from the last post, we typically use a link function with our binomial models. Instead of the log link, we use the logit because it constrains the posterior for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to values between 0 and 1. Just as the null value for a probability is .5, the null value for the parameters within a logistic regression model is typically 0.&lt;/p&gt;
&lt;p&gt;As with the Poisson, I’m not going to go into a full-blown tutorial on the binomial distribution or on logistic regression. For more thorough introductions, check out chapters 9 through 10 in McElreath’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;&lt;em&gt;Statistical rethinking&lt;/em&gt;&lt;/a&gt; or Agresti’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-agrestiFoundationsLinearGeneralized2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&#34;&gt;&lt;em&gt;Foundations of linear and generalized linear models&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;Time to simulate some data. Let’s say we’d like to estimate the probability someone will hit a ball in a baseball game. Nowadays, batting averages for professional baseball players tend around .25 (see &lt;a href=&#34;http://www.baseball-almanac.com/hitting/hibavg4.shtml&#34;&gt;here&lt;/a&gt;). So if we wanted to simulate 50 at-bats, we might do so like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(3)

d &amp;lt;- tibble(y = rbinom(n = 50, size = 1, prob = .25))

str(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [50 × 1] (S3: tbl_df/tbl/data.frame)
##  $ y: int [1:50] 0 1 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are what those data look like in a bar plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_gray() + theme(panel.grid = element_blank()))

d %&amp;gt;% 
  mutate(y = factor(y)) %&amp;gt;% 
  
  ggplot(aes(x = y)) +
  geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;time-to-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Time to model.&lt;/h3&gt;
&lt;p&gt;To practice modeling those data, we’ll want to fire up the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;get_prior()&lt;/code&gt; function to get the &lt;strong&gt;brms&lt;/strong&gt; default for our intercept-only logistic regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_prior(data = d, 
          family = binomial,
          y | trials(1) ~ 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Intercept ~ student_t(3, 0, 2.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it turns out, that’s a really liberal prior. We might step up a bit and put a more skeptical &lt;code&gt;normal(0, 2)&lt;/code&gt; prior on that intercept. With the context of our logit link, that still puts a 95% probability that the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is between .02 and .98, which is almost the entire parameter space. Here’s how to fit the model with the &lt;code&gt;brm()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d, 
      family = binomial,
      y | trials(1) ~ 1,
      prior(normal(0, 2), class = Intercept),
      seed = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;brm()&lt;/code&gt; formula syntax, including a &lt;code&gt;|&lt;/code&gt; bar on the left side of a formula indicates we have extra supplementary information about our criterion variable. In this case, that information is that each &lt;code&gt;y&lt;/code&gt; value corresponds to a single trial [i.e., &lt;code&gt;trials(1)&lt;/code&gt;], which itself corresponds to the &lt;span class=&#34;math inline&#34;&gt;\(n = 1\)&lt;/span&gt; portion of the statistical formula, above. Here are the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: y | trials(1) ~ 1 
##    Data: d (Number of observations: 50) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -1.39      0.36    -2.12    -0.71 1.00     1622     1434
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that that intercept is on the scale of the logit link, the log odds. We can transform it with the &lt;code&gt;brms::inv_logit_scaled()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;Intercept&amp;quot;, 1] %&amp;gt;% 
  inv_logit_scaled()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1995929&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we’d like to view the full posterior distribution, we’ll need to work with the posterior draws themselves. Then we’ll plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws
posterior_samples(fit1) %&amp;gt;% 
  # transform from the log-odds to a probability metric
  transmute(p = inv_logit_scaled(b_Intercept)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = p)) +
  geom_density(fill = &amp;quot;grey25&amp;quot;, size = 0) +
  scale_x_continuous(&amp;quot;probability of a hit&amp;quot;, limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Method &amp;#39;posterior_samples&amp;#39; is deprecated. Please see ?as_draws for
## recommended alternatives.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like the null hypothesis of &lt;span class=&#34;math inline&#34;&gt;\(p = .5\)&lt;/span&gt; is not credible for this simulation. If we’d like the posterior median and percentile-based 95% intervals, we might use the &lt;code&gt;median_qi()&lt;/code&gt; function from the handy &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

posterior_samples(fit1) %&amp;gt;% 
  transmute(p = inv_logit_scaled(b_Intercept)) %&amp;gt;% 
  median_qi()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 6
##       p .lower .upper .width .point .interval
##   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
## 1 0.201  0.108  0.330   0.95 median qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yep, .5 was not within those intervals.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-what-about-power&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;But what about power?&lt;/h3&gt;
&lt;p&gt;That’s enough preliminary work. Let’s see what happens when we do a mini power analysis with 100 iterations. First we set up our simulation function using the same methods we introduced in earlier blog posts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_data_fit &amp;lt;- function(seed, n_player) {
  
  n_trials &amp;lt;- 1
  prob_hit &amp;lt;- .25
  
  set.seed(seed)
  
  d &amp;lt;- tibble(y = rbinom(n    = n_player, 
                         size = n_trials, 
                         prob = prob_hit))
  
  update(fit1,
         newdata = d,
         seed = seed) %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  transmute(p = inv_logit_scaled(b_Intercept)) %&amp;gt;% 
  median_qi() %&amp;gt;% 
    select(.lower:.upper)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n_player = 50)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might plot the intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 %&amp;gt;% 
  ggplot(aes(x = seed, ymin = .lower, ymax = .upper)) +
  geom_hline(yintercept = c(.25, .5), color = &amp;quot;white&amp;quot;) +
  geom_linerange() +
  xlab(&amp;quot;seed (i.e., simulation index)&amp;quot;) +
  scale_y_continuous(&amp;quot;probability of hitting the ball&amp;quot;, limits = c(0, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Like one of my old coworkers used to say: &lt;em&gt;Purtier ’n a hog!&lt;/em&gt; Here we’ll summarize the results both in terms of their conventional power, their mean width, and the proportion of widths more narrow than .25. &lt;em&gt;Why .25?&lt;/em&gt; I don’t know. Without a substantively-informed alternative, it’s as good a criterion as any.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 %&amp;gt;% 
  mutate(width = .upper - .lower) %&amp;gt;% 
  summarise(`conventional power` = mean(.upper &amp;lt; .5),
            `mean width`         = mean(width),
            `width below .25`    = mean(width &amp;lt; .25))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 3
##   `conventional power` `mean width` `width below .25`
##                  &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;
## 1                 0.95        0.231              0.78&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Depending on your study needs, you’d adjust your sample size accordingly, do a mini simulation or two first, and then follow up with a proper power simulation with 1000+ iterations.&lt;/p&gt;
&lt;p&gt;I should point out that whereas in the last post we evaluated the power of the Poisson model with the parameters on the scale of the link function, here we evaluated the power for our logistic regression model after transforming the intercept back into the probability metric. Both methods are fine. I recommend you run your power simulation based on how you want to interpret and report your results.&lt;/p&gt;
&lt;p&gt;We should also acknowledge that this was our first example of a power simulation that wasn’t based on some group comparison. Comparing groups is fine and normal and important. And it’s also the case that we can care about power and/or parameter precision for more than group-based analyses. Our simulation-based approach is fine for both.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregated-binomial-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Aggregated binomial regression&lt;/h2&gt;
&lt;p&gt;It’s no more difficult to simulate and work with aggregated binomial data. But since the mechanics for &lt;code&gt;brms::brm()&lt;/code&gt; and thus the down-the-road simulation setup are a little different, we should practice. With our new setup, we’ll consider a new example. Since .25 is the typical batting average, it might better sense to define the null hypothesis like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0 \text{: } p = .25.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Consider a case where we had some intervention where we expected a new batting average of .35. How many trials would we need, then, to either reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; or perhaps estimate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; with a satisfactory degree of precision? Here’s what the statistical formula for the implied aggregated binomial model might look like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i                        &amp;amp; \sim \text{Binomial} (n, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \beta_0.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The big change is we no longer defined &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; as 1. Let’s say we wanted our aggregated binomial data set to contain the summary statistics for &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt; trials. Here’s what that might look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_trials &amp;lt;- 100
prob_hit &amp;lt;- .35

set.seed(3)

d &amp;lt;- tibble(n_trials = n_trials,
            y = rbinom(n    = 1, 
                       size = n_trials, 
                       prob = prob_hit))

d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   n_trials     y
##      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1      100    32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have two columns. The first, &lt;code&gt;n_trials&lt;/code&gt;, indicates how many cases or trials we’re summarizing. The second, &lt;code&gt;y&lt;/code&gt;, indicates how many successes/1’s/hits we might expect given &lt;span class=&#34;math inline&#34;&gt;\(p = .35\)&lt;/span&gt;. This is the aggregated binomial equivalent of if we had a 100-row vector composed of 32 1s and 68 0s.&lt;/p&gt;
&lt;p&gt;Now, before we discuss fitting the model with &lt;strong&gt;brms&lt;/strong&gt;, let’s talk priors. Since we’ve updated our definition of &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;, it might make sense to update the prior for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;. As it turns out, setting that prior to &lt;code&gt;normal(-1, 0.5)&lt;/code&gt; puts the posterior mode at about .25 on the probability space, but with fairly wide 95% intervals ranging from about .12 to .5. Though centered on our updated null value, this prior is still quite permissive given our hypothesized &lt;span class=&#34;math inline&#34;&gt;\(p = .35\)&lt;/span&gt;. Let’s give it a whirl.&lt;/p&gt;
&lt;p&gt;To fit an aggregated binomial model with the &lt;code&gt;brm()&lt;/code&gt; function, we augment the &lt;code&gt;&amp;lt;criterion&amp;gt; | trials()&lt;/code&gt; syntax where the value that goes in &lt;code&gt;trials()&lt;/code&gt; is either a fixed number or variable in the data indexing &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Our approach will be the latter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;-
  brm(data = d, 
      family = binomial,
      y | trials(n_trials) ~ 1,
      prior(normal(-1, 0.5), class = Intercept),
      seed = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inspect the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: y | trials(n_trials) ~ 1 
##    Data: d (Number of observations: 1) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.80      0.20    -1.19    -0.42 1.00     1524     1697
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After a transformation, here’s what that looks like in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit2) %&amp;gt;% 
  transmute(p = inv_logit_scaled(b_Intercept)) %&amp;gt;% 
  
  ggplot(aes(x = p, y = 0)) +
  stat_halfeye(.width = c(.5, .95)) +
  scale_x_continuous(&amp;quot;probability of a hit&amp;quot;, limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Method &amp;#39;posterior_samples&amp;#39; is deprecated. Please see ?as_draws for
## recommended alternatives.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on a single simulation, it looks like &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt; won’t quite be enough to reject &lt;span class=&#34;math inline&#34;&gt;\(H_0 \text{: } p = .25\)&lt;/span&gt; with a conventional 2-sided 95% interval. But it does look like we’re in the ballpark and that our basic data + model setup will work for a larger-scale simulation. Here’s an example of how you might update our custom simulation function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_data_fit &amp;lt;- function(seed, n_trials) {
  
  prob_hit &amp;lt;- .35
  
  set.seed(seed)
  
  d &amp;lt;- tibble(y = rbinom(n    = 1, 
                         size = n_trials, 
                         prob = prob_hit),
              n_trials = n_trials)
  
  update(fit2,
         newdata = d,
         seed = seed) %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  transmute(p = inv_logit_scaled(b_Intercept)) %&amp;gt;% 
  median_qi() %&amp;gt;% 
    select(.lower:.upper)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate, this time trying out &lt;span class=&#34;math inline&#34;&gt;\(n = 120\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim2 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n_trials = 120)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot the intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim2 %&amp;gt;% 
  ggplot(aes(x = seed, ymin = .lower, ymax = .upper)) +
  geom_hline(yintercept = c(.25, .35), color = &amp;quot;white&amp;quot;) +
  geom_linerange() +
  xlab(&amp;quot;seed (i.e., simulation index)&amp;quot;) +
  scale_y_continuous(&amp;quot;probability of hitting the ball&amp;quot;,
                     limits = c(0, 1), breaks = c(0, .25, .35, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Overall, those intervals look pretty good. They’re fairly narrow and are hovering around the data generating &lt;span class=&#34;math inline&#34;&gt;\(p = .35\)&lt;/span&gt;. But many are still crossing the .25 threshold. Let’s see the results of a formal summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim2 %&amp;gt;% 
  mutate(width = .upper - .lower) %&amp;gt;% 
  summarise(`conventional power` = mean(.lower &amp;gt; .25),
            `mean width`         = mean(width),
            `width below .2`     = mean(width &amp;lt; .2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 3
##   `conventional power` `mean width` `width below .2`
##                  &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1                 0.54        0.155                1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All widths were narrower than .2 and the mean width was about .16. In the abstract that might seem reasonably precise. But we’re still not precise enough to reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; with a conventional power level. Depending on your needs, adjust the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; accordingly and simulate again.&lt;/p&gt;
&lt;p&gt;Now you’ve got a sense of how to work with the binomial likelihood for (aggregated)binary data, next time we’ll play with Likert-type data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.1 (2021-08-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_3.0.1 brms_2.16.2     Rcpp_1.0.7      forcats_0.5.1  
##  [5] stringr_1.4.0   dplyr_1.0.7     purrr_0.3.4     readr_2.0.1    
##  [9] tidyr_1.1.3     tibble_3.1.4    ggplot2_3.3.5   tidyverse_1.3.1
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.6         splines_4.1.1       
##   [7] crosstalk_1.1.1      TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.19        digest_0.6.27        htmltools_0.5.2     
##  [13] rsconnect_0.8.24     fansi_0.5.0          magrittr_2.0.1      
##  [16] checkmate_2.0.0      tzdb_0.1.2           modelr_0.1.8        
##  [19] RcppParallel_5.1.4   matrixStats_0.60.1   sandwich_3.0-1      
##  [22] xts_0.12.1           prettyunits_1.1.1    colorspace_2.0-2    
##  [25] rvest_1.0.1          ggdist_3.0.0         haven_2.4.3         
##  [28] xfun_0.25            callr_3.7.0          crayon_1.4.1        
##  [31] jsonlite_1.7.2       lme4_1.1-27.1        survival_3.2-11     
##  [34] zoo_1.8-9            glue_1.4.2           gtable_0.3.0        
##  [37] emmeans_1.6.3        V8_3.4.2             distributional_0.2.2
##  [40] pkgbuild_1.2.0       rstan_2.26.3         abind_1.4-5         
##  [43] scales_1.1.1         mvtnorm_1.1-2        DBI_1.1.1           
##  [46] miniUI_0.1.1.1       xtable_1.8-4         stats4_4.1.1        
##  [49] StanHeaders_2.26.3   DT_0.19              htmlwidgets_1.5.3   
##  [52] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [55] posterior_1.0.1      ellipsis_0.3.2       pkgconfig_2.0.3     
##  [58] loo_2.4.1            farver_2.1.0         sass_0.4.0          
##  [61] dbplyr_2.1.1         utf8_1.2.2           tidyselect_1.1.1    
##  [64] labeling_0.4.2       rlang_0.4.11         reshape2_1.4.4      
##  [67] later_1.3.0          munsell_0.5.0        cellranger_1.1.0    
##  [70] tools_4.1.1          cli_3.0.1            generics_0.1.0      
##  [73] broom_0.7.9          ggridges_0.5.3       evaluate_0.14       
##  [76] fastmap_1.1.0        yaml_2.2.1           processx_3.5.2      
##  [79] knitr_1.33           fs_1.5.0             nlme_3.1-152        
##  [82] mime_0.11            projpred_2.0.2       xml2_1.3.2          
##  [85] compiler_4.1.1       bayesplot_1.8.1      shinythemes_1.2.0   
##  [88] rstudioapi_0.13      gamm4_0.2-6          curl_4.3.2          
##  [91] reprex_2.0.1         bslib_0.3.0          stringi_1.7.4       
##  [94] highr_0.9            ps_1.6.0             blogdown_1.5        
##  [97] Brobdingnag_1.2-6    lattice_0.20-44      Matrix_1.3-4        
## [100] nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0       
## [103] tensorA_0.36.2       vctrs_0.3.8          pillar_1.6.2        
## [106] lifecycle_1.0.0      jquerylib_0.1.4      bridgesampling_1.1-2
## [109] estimability_1.3     httpuv_1.6.2         R6_2.5.1            
## [112] bookdown_0.23        promises_1.2.0.1     gridExtra_2.3       
## [115] codetools_0.2-18     boot_1.3-28          colourpicker_1.1.0  
## [118] MASS_7.3-54          gtools_3.9.2         assertthat_0.2.1    
## [121] withr_2.4.2          shinystan_2.5.0      multcomp_1.4-17     
## [124] mgcv_1.8-36          parallel_4.1.1       hms_1.1.0           
## [127] grid_4.1.1           coda_0.19-4          minqa_1.2.4         
## [130] rmarkdown_2.10       shiny_1.6.0          lubridate_1.7.10    
## [133] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-agrestiFoundationsLinearGeneralized2015&#34; class=&#34;csl-entry&#34;&gt;
Agresti, A. (2015). &lt;em&gt;Foundations of linear and generalized linear models&lt;/em&gt;. &lt;span&gt;John Wiley &amp;amp; Sons&lt;/span&gt;. &lt;a href=&#34;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&#34;&gt;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In case this is all new to you and you and you had the question in your mind: Yes, you can add predictors to the logistic regression model. Say we had a model with two predictors, &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. Our statistical model would then follow the form &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit} (p_i) = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}\)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian power analysis: Part III.a. Counts are special.</title>
      <link>/post/bayesian-power-analysis-part-iii-a/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/bayesian-power-analysis-part-iii-a/</guid>
      <description>
&lt;script src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;version-1.1.0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Version 1.1.0&lt;/h2&gt;
&lt;p&gt;Edited on April 21, 2021, to remove the &lt;code&gt;broom::tidy()&lt;/code&gt; portion of the workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;orientation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Orientation&lt;/h2&gt;
&lt;p&gt;So far we’ve covered Bayesian power simulations from both a null hypothesis orientation (see &lt;a href=&#34;https://solomonkurz.netlify.com/post/bayesian-power-analysis-part-i/&#34;&gt;part I&lt;/a&gt;) and a parameter width perspective (see &lt;a href=&#34;https://solomonkurz.netlify.com/post/bayesian-power-analysis-part-ii/&#34;&gt;part II&lt;/a&gt;). In both instances, we kept things simple and stayed with Gaussian (i.e., normally distributed) data. But not all data follow that form, so it might do us well to expand our skill set a bit. In the next few posts, we’ll cover how we might perform power simulations with other kinds of data. In this post, we’ll focus on how to use the Poisson likelihood to model counts. In follow-up posts, we’ll explore how to model binary and Likert-type data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-poisson-distribution-is-handy-for-counts.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Poisson distribution is handy for counts.&lt;/h2&gt;
&lt;p&gt;In the social sciences, count data arise when we ask questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How many sexual partners have you had?&lt;/li&gt;
&lt;li&gt;How many pets do you have at home?&lt;/li&gt;
&lt;li&gt;How many cigarettes did you smoke, yesterday?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The values these data will take are discrete&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; in that you’ve either slept with 9 or 10 people, but definitely not 9.5. The values cannot go below zero in that even if you quit smoking cold turkey 15 years ago and have been a health nut since, you still could not have smoked -3 cigarettes, yesterday. Zero is as low as it goes.&lt;/p&gt;
&lt;p&gt;The canonical distribution for data of this type–non-negative integers–is the Poisson. It’s named after the French mathematician Siméon Denis Poisson, &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e8/E._Marcellot_Siméon-Denis_Poisson_1804.jpg&#34;&gt;who had quite the confident stare in his youth&lt;/a&gt;. The Poisson distribution has one parameter, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, which controls both its mean and variance. Although the numbers the Poisson describes are counts, the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter does not need to be an integer. For example, here’s the plot of 1,000 draws from a Poisson for which &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 3.2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

theme_set(theme_gray() + theme(panel.grid = element_blank()))

tibble(x = rpois(n = 1e3, lambda = 3.2)) %&amp;gt;% 
  mutate(x = factor(x)) %&amp;gt;% 
  
  ggplot(aes(x = x)) +
  geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In case you missed it, the key function for generating those data was &lt;code&gt;rpois()&lt;/code&gt; (see &lt;code&gt;?rpois&lt;/code&gt;). I’m not going to go into a full-blown tutorial on the Poisson distribution or on count regression. For more thorough introductions, check out Atkins et al’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-atkinsTutorialOnCount2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3513584/pdf/nihms396181.pdf&#34;&gt;&lt;em&gt;A tutorial on count regression and zero-altered count models for longitudinal substance use data&lt;/em&gt;&lt;/a&gt;, chapters 9 through 11 in McElreath’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt;&lt;/a&gt;, or, if you really want to dive in, Agresti’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-agrestiFoundationsLinearGeneralized2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&#34;&gt;&lt;em&gt;Foundations of linear and generalized linear models&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For our power example, let’s say you were interested in drinking. Using data from &lt;a href=&#34;https://pubs.niaaa.nih.gov/publications/AA70/AA70.htm&#34;&gt;the National Epidemiologic Survey on Alcohol and Related Conditions&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-niaaaNationalEpidemiologicSurvey2006&#34; role=&#34;doc-biblioref&#34;&gt;{{National Institute on Alcohol Abuse and Alcoholism}}, 2006&lt;/a&gt;)&lt;/span&gt;, Christopher Ingraham &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ingrahamThinkYouDrink2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; presented &lt;a href=&#34;https://www.washingtonpost.com/news/wonk/wp/2014/09/25/think-you-drink-a-lot-this-chart-will-tell-you/?utm_term=.b81599bbbe25&#34;&gt;a data visualization&lt;/a&gt; of the average number of alcoholic drinks American adults consume, per week. By decile, the numbers were:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;0.00&lt;/li&gt;
&lt;li&gt;0.00&lt;/li&gt;
&lt;li&gt;0.00&lt;/li&gt;
&lt;li&gt;0.02&lt;/li&gt;
&lt;li&gt;0.14&lt;/li&gt;
&lt;li&gt;0.63&lt;/li&gt;
&lt;li&gt;2.17&lt;/li&gt;
&lt;li&gt;6.25&lt;/li&gt;
&lt;li&gt;15.28&lt;/li&gt;
&lt;li&gt;73.85&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s say you wanted to run a study where you planned on comparing two demographic groups by their weekly drinking levels. Let’s further say you suspected one of those groups drank like the American adults in the 7&lt;sup&gt;th&lt;/sup&gt; decile and the other drank like American adults in the 8&lt;sup&gt;th&lt;/sup&gt;. We’ll call them low and high drinkers, respectively. For convenience, let’s further presume you’ll be able to recruit equal numbers of participants from both groups. The objective for our power analysis–or sample size analysis if you prefer to avoid the language of &lt;em&gt;power&lt;/em&gt;–is to determine how many you’d need per group to detect reliable differences. Using &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt; as a starting point, here’s what the data for our hypothetical groups might look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu_7 &amp;lt;- 2.17
mu_8 &amp;lt;- 6.25

n &amp;lt;- 50

set.seed(3)

d &amp;lt;-
  tibble(low  = rpois(n = n, lambda = mu_7),
         high = rpois(n = n, lambda = mu_8)) %&amp;gt;% 
  gather(group, count) 

d %&amp;gt;%
  mutate(count = factor(count)) %&amp;gt;% 
  
  ggplot(aes(x = count)) +
  geom_bar() +
  facet_wrap(~group, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This will be our primary data type. Our next step is to determine how to express our research question as a regression model. Like with our two-group Gaussian models, we can predict counts in terms of an intercept (i.e., standing for the expected value on the reference group) and slope (i.e., standing for the expected difference between the reference group and the comparison group). If we coded our two groups by a &lt;code&gt;high&lt;/code&gt; variable for which 0 stood for low drinkers and 1 stood for high drinkers, the basic model would follow the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{drinks_per_week}_i         &amp;amp; \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i)   &amp;amp; = \beta_0 + \beta_1 \text{high}_i.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s how to set the data up for that model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;% 
  mutate(high = ifelse(group == &amp;quot;low&amp;quot;, 0, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you were attending closely to our model formula, you noticed we ran into a detail. Count regression, such as with the Poisson likelihood, tends to use the log link. &lt;em&gt;Why?&lt;/em&gt; you ask. Recall that counts need to be 0 and above. Same deal for our &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter. In order to make sure our models don’t yield silly estimates for &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, like -2 or something, we typically use the log link. You don’t have to, of course. The world is your playground. But this is the method most of your colleagues are likely to use and it’s the one I suggest you use until you have compelling reasons to do otherwise.&lt;/p&gt;
&lt;p&gt;So then since we’re now fitting a model with a log link, it might seem challenging to pick good priors. As a place to start, we can use the &lt;code&gt;brms::get_prior()&lt;/code&gt; function to see the &lt;strong&gt;brms&lt;/strong&gt; defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)

get_prior(data = d,
          family = poisson,
          count ~ 0 + Intercept + high)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   prior class      coef group resp dpar nlpar bound       source
##  (flat)     b                                            default
##  (flat)     b      high                             (vectorized)
##  (flat)     b Intercept                             (vectorized)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hopefully two things popped out. First, there’s no prior of &lt;code&gt;class = sigma&lt;/code&gt;. Since the Poisson distribution only has one parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, we don’t need to set a prior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Our model won’t have one. Second, because we’re continuing to use the &lt;code&gt;0 + Intercept&lt;/code&gt; syntax for our model intercept, both our intercept and slope are of prior &lt;code&gt;class = b&lt;/code&gt; and those currently have default flat priors with &lt;strong&gt;brms&lt;/strong&gt;. To be sure, flat priors aren’t the best. But maybe if this was your first time playing around with a Poisson model, default flat priors might seem like a safe place to start. &lt;a href=&#34;https://xkcd.com/386/&#34;&gt;Feel free to disagree&lt;/a&gt;. In the meantime, here’s how to fit that default Poisson model with &lt;code&gt;brms::brm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d,
      family = poisson,
      count ~ 0 + Intercept + high,
      seed = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: count ~ 0 + Intercept + high 
##    Data: d (Number of observations: 100) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.59      0.11     0.38     0.79 1.01      917     1133
## high          1.27      0.12     1.03     1.51 1.01      935     1182
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we used the log link, our model results are in the log metric, too. If you’d like them in the metric of the data, you’d work directly with the poster samples and exponentiate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- 
  posterior_samples(fit1) %&amp;gt;% 
  mutate(`beta_0 (i.e., low)`                       = exp(b_Intercept),
         `beta_1 (i.e., difference score for high)` = exp(b_high))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then just summarize our parameters of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  select(starts_with(&amp;quot;beta_&amp;quot;)) %&amp;gt;% 
  gather() %&amp;gt;% 
  group_by(key) %&amp;gt;% 
  summarise(mean  = mean(value),
            lower = quantile(value, prob = .025),
            upper = quantile(value, prob = .975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
##   key                                       mean lower upper
##   &amp;lt;chr&amp;gt;                                    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 beta_0 (i.e., low)                        1.81  1.46  2.21
## 2 beta_1 (i.e., difference score for high)  3.58  2.81  4.53&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of simulation, it’ll be easier if we press on with evaluating the parameters on the log metric, though. If you’re working within a null-hypothesis oriented power paradigm, you’ll be happy to know zero is still the number to beat for evaluating our 95% intervals for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, even when that parameter is in the log metric. Here it is, again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;high&amp;quot;, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
## 1.2690437 0.1211455 1.0330613 1.5108894&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So our first fit suggests we’re on good footing to run a quick power simulation holding &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt;. As in the prior blog posts, our lives will be simpler if we set up a custom simulation function. Since we’ll be using it to simulate the data and fit the model in one step, let’s call it &lt;code&gt;sim_data_fit()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_data_fit &amp;lt;- function(seed, n) {
  
  # define our mus in the function
  mu_7 &amp;lt;- 2.17
  mu_8 &amp;lt;- 6.25

  # make your results reproducible
  set.seed(seed)
  
  # simulate the data
  d &amp;lt;-
    tibble(high  = rep(0:1, each = n),
           count = c(rpois(n = n, lambda = mu_7),
                     rpois(n = n, lambda = mu_8)))
  
  # fit and summarize
  update(fit1,
         newdata = d,
         seed = seed) %&amp;gt;% 
    fixef() %&amp;gt;% 
    data.frame() %&amp;gt;% 
    rownames_to_column(&amp;quot;parameter&amp;quot;) %&amp;gt;% 
    filter(parameter == &amp;quot;high&amp;quot;) %&amp;gt;% 
    select(Q2.5:Q97.5 )
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the simulation for a simple 100 iterations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n = 50)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That went quick–just a little over a minute on my laptop. Here’s what those 100 &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; intervals look like in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 %&amp;gt;% 
  ggplot(aes(x = seed, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_linerange() +
  labs(x = &amp;quot;seed (i.e., simulation index)&amp;quot;,
       y = expression(beta[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;None of them are anywhere near the null value 0. So it appears we’re well above .8 power to reject the typical &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt;. Switching to the precision orientation, here’s the distribution of their widths.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 %&amp;gt;% 
  mutate(width = Q97.5 - Q2.5) %&amp;gt;% 
  
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = 0.01) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What if we wanted a mean width of 0.25 on the log scale? We might try the simulation with &lt;span class=&#34;math inline&#34;&gt;\(n = 150\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim2 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n = 150)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we’ll summarize the widths both in terms of their mean and what proportion were smaller than 0.25.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim2 %&amp;gt;% 
  mutate(width = Q97.5 - Q2.5) %&amp;gt;% 
  summarise(`mean width` = mean(width),
            `below 0.25` = mean(width &amp;lt; 0.25))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   `mean width` `below 0.25`
##          &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1        0.252         0.43&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to focus on the mean, we did pretty good. Perhaps set the &lt;span class=&#34;math inline&#34;&gt;\(n = 155\)&lt;/span&gt; and simulate a full 1,000+ iterations for a serious power analysis. But if we wanted to make the stricter criteria of all below 0.25, we’d need to up the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; quite a bit more. And of course, once you have a little experience working with Poisson models, you might do the power simulations with more ambitious priors. For example, if your count values are lower than like 1,000, there’s a good chance a &lt;code&gt;normal(0, 6)&lt;/code&gt; prior on your &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters will be nearly flat within the reasonable neighborhoods of the parameter space.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-logs-are-hard.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;But logs are hard.&lt;/h2&gt;
&lt;p&gt;If we approach our Bayesian power analysis from a precision perspective, it can be difficult to settle on a reasonable interval width when they’re on the log scale. So let’s modify our simulation flow so it converts the width summaries back into the natural metric. Before we go big, let’s practice with a single iteration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seed &amp;lt;- 0
set.seed(seed)

# simulate the data
d &amp;lt;-
  tibble(high  = rep(0:1, each = n),
         count = c(rpois(n = n, lambda = mu_7),
                   rpois(n = n, lambda = mu_8)))

# fit the model
fit2 &amp;lt;-
  update(fit1,
         newdata = d,
         seed = seed) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now summarize.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

fit2 %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  transmute(`beta_1` = exp(b_high)) %&amp;gt;% 
  mean_qi()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     beta_1  .lower   .upper .width .point .interval
## 1 2.705404 2.16512 3.341729   0.95   mean        qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we used the &lt;code&gt;fixef()&lt;/code&gt; function to extract our intervals, which took the &lt;strong&gt;brms&lt;/strong&gt; fit object as input. Here we took a different approach. Because we are transforming &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, we used the &lt;code&gt;posterior_samples()&lt;/code&gt; function to work directly with the posterior draws. We then exponentiated within &lt;code&gt;transmute()&lt;/code&gt;, which returned a single-column tibble, not a &lt;strong&gt;brms&lt;/strong&gt; fit object. So instead of &lt;code&gt;fixef()&lt;/code&gt;, it’s easier to get our summary statistics with the &lt;code&gt;tidybayes::mean_qi()&lt;/code&gt; function. Do note that now our lower and upper levels are named &lt;code&gt;.lower&lt;/code&gt; and &lt;code&gt;.upper&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Now we’ve practiced with the new flow, let’s redefine our simulation function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_data_fit &amp;lt;- function(seed, n) {
  
  # define our mus in the function
  mu_7 &amp;lt;- 2.17
  mu_8 &amp;lt;- 6.25

  # make your results reproducible
  set.seed(seed)
  
  # simulate the data
  d &amp;lt;-
    tibble(high  = rep(0:1, each = n),
           count = c(rpois(n = n, lambda = mu_7),
                     rpois(n = n, lambda = mu_8)))
  
  # fit and summarize
  update(fit1,
         newdata = d,
         seed = seed) %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  transmute(`beta_1` = exp(b_high)) %&amp;gt;% 
  mean_qi()
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim3 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n = 50)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what those 100 &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; intervals look like in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim3 %&amp;gt;% 
  ggplot(aes(x = seed, y = beta_1, ymin = .lower, ymax = .upper)) +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_pointrange(fatten = 1) +
  labs(x = &amp;quot;seed (i.e., simulation index)&amp;quot;,
       y = expression(beta[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Inspect the distribution of their widths.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim3 %&amp;gt;% 
  mutate(width = .upper - .lower) %&amp;gt;% 
  
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = 0.05) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What if we wanted a mean 95% interval width of 1? Let’s run the simulation again, this time with &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim4 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n = 100)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  mutate(width = .upper - .lower)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the new width distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim4 %&amp;gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = 0.05) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-a/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And the mean width is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim4 %&amp;gt;% 
  summarise(mean_width = mean(width))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   mean_width
##        &amp;lt;dbl&amp;gt;
## 1      0.913&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice! If we want a mean width of 1, it looks like we’re a little &lt;em&gt;overpowered&lt;/em&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt;. The next step would be to up your iterations to 1,000 or so to do a proper simulation.&lt;/p&gt;
&lt;p&gt;Now you’ve got a sense of how to work with the Poisson likelihood, &lt;a href=&#34;https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-iii-b/&#34;&gt;next time&lt;/a&gt; we’ll play with binary data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1  
##  [5] stringr_1.4.0   dplyr_1.0.6     purrr_0.3.4     readr_1.4.0    
##  [9] tidyr_1.1.3     tibble_3.1.2    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.3         splines_4.0.4       
##   [7] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0  
##  [19] xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [22] colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.23            callr_3.7.0         
##  [28] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2          
##  [34] gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2        
##  [40] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [43] DBI_1.1.0            miniUI_0.1.1.1       xtable_1.8-4        
##  [46] stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [49] htmlwidgets_1.5.3    httr_1.4.2           threejs_0.3.3       
##  [52] arrayhelpers_1.1-0   ellipsis_0.3.2       pkgconfig_2.0.3     
##  [55] loo_2.4.1            farver_2.1.0         sass_0.3.1          
##  [58] dbplyr_2.0.0         utf8_1.2.1           tidyselect_1.1.1    
##  [61] labeling_0.4.2       rlang_0.4.11         reshape2_1.4.4      
##  [64] later_1.2.0          munsell_0.5.0        cellranger_1.1.0    
##  [67] tools_4.0.4          cli_2.5.0            generics_0.1.0      
##  [70] broom_0.7.6          ggridges_0.5.3       evaluate_0.14       
##  [73] fastmap_1.1.0        yaml_2.2.1           processx_3.5.2      
##  [76] knitr_1.33           fs_1.5.0             nlme_3.1-152        
##  [79] mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [82] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [85] rstudioapi_0.13      gamm4_0.2-6          curl_4.3            
##  [88] reprex_0.3.0         statmod_1.4.35       bslib_0.2.4         
##  [91] stringi_1.6.2        highr_0.9            ps_1.6.0            
##  [94] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41     
##  [97] Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1        
## [100] shinyjs_2.0.0        vctrs_0.3.8          pillar_1.6.1        
## [103] lifecycle_1.0.0      jquerylib_0.1.4      bridgesampling_1.0-0
## [106] estimability_1.3     httpuv_1.6.0         R6_2.5.0            
## [109] bookdown_0.22        promises_1.2.0.1     gridExtra_2.3       
## [112] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0  
## [115] MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1    
## [118] withr_2.4.2          shinystan_2.5.0      multcomp_1.4-16     
## [121] mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [124] grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [127] rmarkdown_2.8        shiny_1.6.0          lubridate_1.7.9.2   
## [130] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-agrestiFoundationsLinearGeneralized2015&#34; class=&#34;csl-entry&#34;&gt;
Agresti, A. (2015). &lt;em&gt;Foundations of linear and generalized linear models&lt;/em&gt;. &lt;span&gt;John Wiley &amp;amp; Sons&lt;/span&gt;. &lt;a href=&#34;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&#34;&gt;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-atkinsTutorialOnCount2013&#34; class=&#34;csl-entry&#34;&gt;
Atkins, D. C., Baldwin, S. A., Zheng, C., Gallop, R. J., &amp;amp; Neighbors, C. (2013). A tutorial on count regression and zero-altered count models for longitudinal substance use data. &lt;em&gt;Psychology of Addictive Behaviors&lt;/em&gt;, &lt;em&gt;27&lt;/em&gt;(1), 166. &lt;a href=&#34;https://doi.org/10.1037/a0029508&#34;&gt;https://doi.org/10.1037/a0029508&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ingrahamThinkYouDrink2014&#34; class=&#34;csl-entry&#34;&gt;
Ingraham, C. (2014). Think you drink a lot? &lt;span&gt;This&lt;/span&gt; chart will tell you. &lt;em&gt;Wonkblog. The Washington Post&lt;/em&gt;. &lt;a href=&#34;https://www.washingtonpost.com/news/wonk/wp/2014/09/25/think-you-drink-a-lot-this-chart-will-tell-you/?utm_term=.b81599bbbe25&#34;&gt;https://www.washingtonpost.com/news/wonk/wp/2014/09/25/think-you-drink-a-lot-this-chart-will-tell-you/?utm_term=.b81599bbbe25&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-niaaaNationalEpidemiologicSurvey2006&#34; class=&#34;csl-entry&#34;&gt;
{{National Institute on Alcohol Abuse and Alcoholism}}. (2006). &lt;em&gt;National epidemiologic survey on alcohol and related conditions&lt;/em&gt;. &lt;a href=&#34;https://pubs.niaaa.nih.gov/publications/AA70/AA70.htm&#34;&gt;https://pubs.niaaa.nih.gov/publications/AA70/AA70.htm&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Yes, one can smoke half a cigarette or drink 1/3 of a drink. Ideally, we’d have the exact amount of nicotine in your blood at a given moment and over time and the same for the amount of alcohol in your system relative to your blood volume and such. But in practice, substance use researchers just don’t tend to have access to data of that quality. Instead, we’re typically stuck with simple counts. And I look forward to the day the right team of engineers, computer scientists, and substance use researchers (and whoever else I forgot to mention) release the cheap, non-invasive technology we need to passively measure these things. Until then: &lt;em&gt;How many standard servings of alcohol did you drink, last night?&lt;/em&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian power analysis: Part II. Some might prefer precision to power</title>
      <link>/post/bayesian-power-analysis-part-ii/</link>
      <pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/bayesian-power-analysis-part-ii/</guid>
      <description>
&lt;script src=&#34;/post/bayesian-power-analysis-part-ii/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;version-1.1.0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Version 1.1.0&lt;/h2&gt;
&lt;p&gt;Edited on April 21, 2021, to remove the &lt;code&gt;broom::tidy()&lt;/code&gt; portion of the workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;When researchers decide on a sample size for an upcoming project, there are more things to consider than null-hypothesis-oriented power. Bayesian researchers might like to frame their concerns in terms of precision. Stick around to learn what and how.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;are-bayesians-doomed-to-refer-to-h_0-1-with-sample-size-planning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Are Bayesians doomed to refer to &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; with sample-size planning?&lt;/h2&gt;
&lt;p&gt;If you read the first post in this series (click &lt;a href=&#34;https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-i/&#34;&gt;here&lt;/a&gt; for a refresher), you may have found yourself thinking: &lt;em&gt;Sure, last time you avoided computing &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values with your 95% Bayesian credible intervals. But weren’t you still operating like a NHSTesting frequentist with all that &lt;span class=&#34;math inline&#34;&gt;\(H_0 / H_1\)&lt;/span&gt; talk?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Solid criticism. We didn’t even bother discussing all the type-I versus type-II error details. Yet they too were lurking in the background the way we just chose the typical .8 power benchmark. That’s not to say that a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value oriented approach isn’t legitimate. It’s certainly congruent with what most reviewers would expect.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; But this all seems at odds with a model-oriented Bayesian approach, which is what I generally prefer. Happily, we have other options to explore.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-just-pick-up-where-we-left-off.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let’s just pick up where we left off.&lt;/h2&gt;
&lt;p&gt;Load our primary statistical packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a recap, here’s how we performed the last simulation-based Bayesian power analysis from part I. First, we simulated a single data set and fit an initial model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the means
mu_c &amp;lt;- 0
mu_t &amp;lt;- 0.5

# determine the group size
n &amp;lt;- 50

# simulate the data
set.seed(1)
d &amp;lt;-
  tibble(group     = rep(c(&amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;), each = n)) %&amp;gt;% 
  mutate(treatment = ifelse(group == &amp;quot;control&amp;quot;, 0, 1),
         y         = ifelse(group == &amp;quot;control&amp;quot;, 
                            rnorm(n, mean = mu_c, sd = 1),
                            rnorm(n, mean = mu_t, sd = 1)))
# fit the model
fit &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 0 + intercept + treatment,
      prior = c(prior(normal(0, 2), class = b),
                prior(student_t(3, 1, 1), class = sigma)),
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we made a custom function that both simulated data sets and used the &lt;code&gt;update()&lt;/code&gt; function to update that initial fit in order to avoid additional compilation time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_d_and_fit &amp;lt;- function(seed, n) {
  
  mu_c &amp;lt;- 0
  mu_t &amp;lt;- 0.5
  
  set.seed(seed)
  
  d &amp;lt;-
    tibble(group = rep(c(&amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;), each = n)) %&amp;gt;% 
    mutate(treatment = ifelse(group == &amp;quot;control&amp;quot;, 0, 1),
           y         = ifelse(group == &amp;quot;control&amp;quot;, 
                              rnorm(n, mean = mu_c, sd = 1),
                              rnorm(n, mean = mu_t, sd = 1)))
  
  update(fit,
         newdata = d, 
         seed = seed) %&amp;gt;% 
    fixef() %&amp;gt;% 
    data.frame() %&amp;gt;% 
    rownames_to_column(&amp;quot;parameter&amp;quot;) %&amp;gt;% 
    filter(parameter == &amp;quot;treatment&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we finally iterated over &lt;code&gt;n_sim &amp;lt;- 100&lt;/code&gt; times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_sim &amp;lt;- 100

s3 &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(b1 = map(seed, sim_d_and_fit, n = 50)) %&amp;gt;% 
  unnest(b1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results looked like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_grey() +
            theme(panel.grid = element_blank()))

s3 %&amp;gt;% 
  ggplot(aes(x = seed, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = c(0, .5), color = &amp;quot;white&amp;quot;) +
  geom_pointrange(fatten = 1/2) +
  labs(x = &amp;quot;seed (i.e., simulation index)&amp;quot;,
       y = expression(beta[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s time to build on the foundation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-might-evaluate-power-by-widths.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We might evaluate “power” by widths.&lt;/h2&gt;
&lt;p&gt;Instead of just ordering the point-ranges by their &lt;code&gt;seed&lt;/code&gt; values, we might instead arrange them by the &lt;code&gt;lower&lt;/code&gt; levels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s3 %&amp;gt;%
  ggplot(aes(x = reorder(seed, Q2.5), y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = c(0, .5), color = &amp;quot;white&amp;quot;) +
  geom_pointrange(fatten = 1/2) +
  scale_x_discrete(&amp;quot;reordered by the lower level of the 95% intervals&amp;quot;, breaks = NULL) +
  ylab(expression(beta[1])) +
  coord_cartesian(ylim = c(-.5, 1.3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how this arrangement highlights the differences in widths among the intervals. The wider the interval, the less precise the estimate. Some intervals were wider than others, but all tended to hover in a similar range. We might quantify those ranges by computing a &lt;code&gt;width&lt;/code&gt; variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s3 &amp;lt;-
  s3 %&amp;gt;% 
  mutate(width = Q97.5 - Q2.5)

head(s3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
##    seed parameter Estimate Est.Error    Q2.5 Q97.5 width
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 treatment    0.514     0.185  0.159  0.898 0.739
## 2     2 treatment    0.307     0.239 -0.143  0.782 0.925
## 3     3 treatment    0.643     0.171  0.310  0.975 0.666
## 4     4 treatment    0.224     0.182 -0.128  0.574 0.702
## 5     5 treatment    0.429     0.189  0.0596 0.792 0.733
## 6     6 treatment    0.304     0.208 -0.114  0.711 0.825&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the &lt;code&gt;width&lt;/code&gt; distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s3 %&amp;gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The widths of our 95% intervals range from 0.6 to 0.95, with the bulk sitting around 0.8. Let’s focus a bit and take a random sample from one of the simulation iterations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

s3 %&amp;gt;% 
  sample_n(1) %&amp;gt;% 
  mutate(seed = seed %&amp;gt;% as.character()) %&amp;gt;% 

  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = seed)) +
  geom_vline(xintercept = c(0, .5), color = &amp;quot;white&amp;quot;) +
  geom_pointrange() +
  labs(x = expression(beta[1]),
       y = &amp;quot;seed #&amp;quot;) +
  xlim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though the posterior mean suggests the most probable value for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is about 0.6, the intervals suggest values from about 0.2 to almost 1 are within the 95% probability range. That’s a wide spread. Within psychology, a standardized mean difference of 0.2 would typically be considered small, whereas a difference of 1 would be large enough to raise a skeptical eyebrow or two.&lt;/p&gt;
&lt;p&gt;So instead of focusing on rejecting a null hypothesis like &lt;span class=&#34;math inline&#34;&gt;\(\mu_\text{control} = \mu_\text{treatment}\)&lt;/span&gt;, we might instead use our simulation skills to determine the sample size we need to have most of our 95% intervals come in at a certain level of precision. This has been termed the accuracy in parameter estimation [AIPE; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-maxwellSampleSizePlanning2008&#34; role=&#34;doc-biblioref&#34;&gt;Maxwell et al.&lt;/a&gt; (&lt;a href=&#34;#ref-maxwellSampleSizePlanning2008&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;; see also &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke&lt;/a&gt; (&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;] approach to sample size planning.&lt;/p&gt;
&lt;p&gt;Thinking in terms of AIPE, in terms of precision, let’s say we wanted widths of 0.7 or smaller. Here’s how we did with &lt;code&gt;s3&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s3 %&amp;gt;% 
  mutate(check = ifelse(width &amp;lt; .7, 1, 0)) %&amp;gt;% 
  summarise(`width power` = mean(check))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   `width power`
##           &amp;lt;dbl&amp;gt;
## 1           0.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We did terrible. I’m not sure the term “width power” is even a thing. But hopefully you get the point. Our baby 100-iteration simulation suggests we have about a .08 probability of achieving 95% CI widths of 0.7 or smaller with &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt; per group. Though we’re pretty good at excluding zero, we don’t tend to do so with precision above that.&lt;/p&gt;
&lt;p&gt;That last bit about excluding zero brings up an important point. Once we’re concerned about width size, about precision, the null hypothesis is no longer of direct relevance. And since we’re no longer wed to thinking in terms of the null hypothesis, there’s no real need to stick with a .8 threshold for evaluating width power (okay, I’ll stop using that term). Now if we wanted to stick with .8, we could. Though a little nonsensical, the .8 criterion would give our AIPE analyses a sense of familiarity with traditional power analyses, which some reviewers might appreciate. But in his text, Kruschke mentioned several other alternatives. One would be to set maximum value for our CI widths and simulate to find the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; necessary so all our simulations pass that criterion. Another would follow Joseph, Wolfson, and du Berger &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-josephSampleSizeCalculations1995&#34; role=&#34;doc-biblioref&#34;&gt;1995a&lt;/a&gt;, &lt;a href=&#34;#ref-josephCommentsBayesianSample1995&#34; role=&#34;doc-biblioref&#34;&gt;1995b&lt;/a&gt;)&lt;/span&gt;, who suggested we shoot for an &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; that produces widths that pass that criterion on average. Here’s how we did based on the average-width criterion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s3 %&amp;gt;% 
  summarise(`average width` = mean(width))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   `average width`
##             &amp;lt;dbl&amp;gt;
## 1           0.783&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Close. Let’s see how increasing our sample size to 75 per group effects these metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s4 &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(b1 = map(seed, sim_d_and_fit, n = 75)) %&amp;gt;% 
  unnest(b1) %&amp;gt;%
  mutate(width = Q97.5 - Q2.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what our new batch of 95% intervals looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s4 %&amp;gt;% 
  ggplot(aes(x = reorder(seed, Q2.5), y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = c(0, .5), color = &amp;quot;white&amp;quot;) +
  geom_pointrange(fatten = 1/2) +
  scale_x_discrete(&amp;quot;reordered by the lower level of the 95% intervals&amp;quot;, breaks = NULL) +
  ylab(expression(beta[1])) +
  # this kept the scale on the y-axis the same as the simulation with n = 50
  coord_cartesian(ylim = c(-.5, 1.3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some of the intervals are still more precise than others, but they all now hover more tightly around their true data-generating value of 0.5. Here’s our updated “power” for producing interval widths smaller than 0.7.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s4 %&amp;gt;% 
  mutate(check = ifelse(width &amp;lt; .7, 1, 0)) %&amp;gt;% 
  summarise(`proportion below 0.7` = mean(check),
            `average width`        = mean(width))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   `proportion below 0.7` `average width`
##                    &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;
## 1                   0.94           0.639&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we hold to the NHST-oriented .8 threshold, we did great and are even “overpowered.” We didn’t quite meet Kruschke’s strict limiting-worst-precision threshold, but we got close enough we’d have a good sense of what range of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; values we might evaluate over next. As far as the mean-precision criterion, we did great by that one and even beat it.&lt;/p&gt;
&lt;p&gt;Here’s a look at how this batch of widths is distributed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s4 %&amp;gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .02) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s see if we can nail down the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;s for our three AIPE criteria. Since we’re so close to fulfilling Kruschke’s limiting-worst-precision criterion, we’ll start there. I’m thinking &lt;span class=&#34;math inline&#34;&gt;\(n = 85\)&lt;/span&gt; should just about do it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s5 &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(b1 = map(seed, sim_d_and_fit, n = 85)) %&amp;gt;% 
  unnest(b1) %&amp;gt;%
  mutate(width = Q97.5 - Q2.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Did we pass?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s5 %&amp;gt;% 
  mutate(check = ifelse(width &amp;lt; .7, 1, 0)) %&amp;gt;% 
  summarise(`proportion below 0.7` = mean(check))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   `proportion below 0.7`
##                    &amp;lt;dbl&amp;gt;
## 1                      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Success! We might look at how they’re distributed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s5 %&amp;gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A few of our simulated widths were approaching the 0.7 boundary. If we were to do a proper simulation with 1,000+ iterations, I’d worry one or two would creep over that boundary. So perhaps &lt;span class=&#34;math inline&#34;&gt;\(n = 90\)&lt;/span&gt; would be a better candidate for a large-scale simulation.&lt;/p&gt;
&lt;p&gt;If we just wanted to meet the mean-precision criterion, we might look at something like &lt;span class=&#34;math inline&#34;&gt;\(n = 65\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s6 &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(b1 = map(seed, sim_d_and_fit, n = 65)) %&amp;gt;% 
  unnest(b1) %&amp;gt;%
  mutate(width = Q97.5 - Q2.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Did we pass the mean-precision criterion?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s6 %&amp;gt;% 
  summarise(`average width` = mean(width))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   `average width`
##             &amp;lt;dbl&amp;gt;
## 1           0.688&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We got it! It looks like something like &lt;span class=&#34;math inline&#34;&gt;\(n = 65\)&lt;/span&gt; would be a good candidate for a larger-scale simulation. Here’s the distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s6 %&amp;gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .02) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For our final possible criterion, just get .8 of the widths below the threshold, we’ll want an &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; somewhere between 65 and 85. 70, perhaps?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s7 &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(b1 = map(seed, sim_d_and_fit, n = 70)) %&amp;gt;% 
  unnest(b1) %&amp;gt;%
  mutate(width = Q97.5 - Q2.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Did we pass the .8-threshold criterion?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s7 %&amp;gt;% 
  mutate(check = ifelse(width &amp;lt; .7, 1, 0)) %&amp;gt;% 
  summarise(`proportion below 0.7` = mean(check))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   `proportion below 0.7`
##                    &amp;lt;dbl&amp;gt;
## 1                   0.82&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yep. Here’s the distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s7 %&amp;gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .02) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-are-we-defining-our-widths&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How are we defining our widths?&lt;/h2&gt;
&lt;p&gt;In frequentist analyses, we typically work with 95% confidence intervals because of their close connection to the conventional &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt; threshold. Another consequence of dropping our focus on rejecting &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is that it no longer seems necessary to evaluate our posteriors with 95% intervals. And as it turns out, some Bayesians aren’t fans of the 95% interval. McElreath, for example, defiantly used 89% intervals in both editions of his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;http://xcelab.net/rm/statistical-rethinking/&#34;&gt;text&lt;/a&gt;. In contrast, Gelman has &lt;a href=&#34;https://statmodeling.stat.columbia.edu/2016/11/05/why-i-prefer-50-to-95-intervals/&#34;&gt;blogged&lt;/a&gt; on his fondness for 50% intervals. Just for kicks, let’s follow Gelman’s lead and practice evaluating an &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; based on 50% intervals. This will require us to update our &lt;code&gt;sim_d_and_fit()&lt;/code&gt; function to allow us to change the &lt;code&gt;probs&lt;/code&gt; setting in the &lt;code&gt;fixef()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_d_and_fit &amp;lt;- function(seed, n, probs = c(.25, .75)) {
  
  mu_c &amp;lt;- 0
  mu_t &amp;lt;- 0.5
  
  set.seed(seed)
  
  d &amp;lt;-
    tibble(group = rep(c(&amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;), each = n)) %&amp;gt;% 
    mutate(treatment = ifelse(group == &amp;quot;control&amp;quot;, 0, 1),
           y         = ifelse(group == &amp;quot;control&amp;quot;, 
                              rnorm(n, mean = mu_c, sd = 1),
                              rnorm(n, mean = mu_t, sd = 1)))
  
  update(fit,
         newdata = d, 
         seed = seed) %&amp;gt;% 
    fixef(probs = probs) %&amp;gt;% 
    data.frame() %&amp;gt;% 
    rownames_to_column(&amp;quot;parameter&amp;quot;) %&amp;gt;% 
    filter(parameter == &amp;quot;treatment&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make things simple, we just set the default &lt;code&gt;probs&lt;/code&gt; settings to return 50% intervals. Now we simulate to examine those 50% intervals. We’ll start with the original &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_sim &amp;lt;- 100

s8 &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(b1 = map(seed, sim_d_and_fit, n = 50)) %&amp;gt;% 
  unnest(b1) %&amp;gt;% 
  # notice the change to this line of code
  mutate(width = Q75 - Q25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the distribution of our 50% interval widths.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s8 %&amp;gt;% 
  mutate(width = Q75 - Q25) %&amp;gt;% 
  
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since we’ve gone from 95% to 50% intervals, it should be no surprise that their widths are narrower. Accordingly, we should evaluate then with a higher standard. Perhaps it’s more reasonable to ask for an average width of 0.1. Let’s see how close &lt;span class=&#34;math inline&#34;&gt;\(n = 150\)&lt;/span&gt; gets us.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s9 &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(b1 = map(seed, sim_d_and_fit, n = 150)) %&amp;gt;% 
  unnest(b1) %&amp;gt;% 
  mutate(width = Q75 - Q25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at the distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s9 %&amp;gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .0025) +
  geom_rug(size = 1/6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-ii/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nope, we’re not there yet. Perhaps &lt;span class=&#34;math inline&#34;&gt;\(n = 200\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(250\)&lt;/span&gt; is the ticket. This is an iterative process. Anyway, once we’re talking that AIPE/precision/interval-width talk, we can get all kinds of creative with which intervals we’re even interested in. As far as I can tell, the topic is wide open for fights and collaborations between statisticians, methodologists, and substantive researchers to find sensible ways forward.&lt;/p&gt;
&lt;p&gt;Maybe you should write a dissertation on it.&lt;/p&gt;
&lt;p&gt;Regardless, get ready for &lt;a href=&#34;https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-iii-a/&#34;&gt;part III&lt;/a&gt; where we’ll liberate ourselves from the tyranny of the Gauss.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
##  [9] tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         splines_4.0.4        crosstalk_1.1.0.1   
##   [7] TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17       
##  [10] digest_0.6.27        htmltools_0.5.1.1    rsconnect_0.8.16    
##  [13] fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [16] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1          
##  [19] sandwich_3.0-0       prettyunits_1.1.1    colorspace_2.0-0    
##  [22] rvest_0.3.6          haven_2.3.1          xfun_0.22           
##  [25] callr_3.5.1          crayon_1.4.1         jsonlite_1.7.2      
##  [28] lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [31] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1     
##  [34] V8_3.4.0             pkgbuild_1.2.0       rstan_2.21.2        
##  [37] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [40] DBI_1.1.0            miniUI_0.1.1.1       xtable_1.8-4        
##  [43] stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [46] htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3       
##  [49] ellipsis_0.3.1       pkgconfig_2.0.3      loo_2.4.1           
##  [52] farver_2.0.3         dbplyr_2.0.0         utf8_1.1.4          
##  [55] tidyselect_1.1.0     labeling_0.4.2       rlang_0.4.10        
##  [58] reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [61] cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [64] generics_0.1.0       broom_0.7.5          ggridges_0.5.2      
##  [67] evaluate_0.14        fastmap_1.0.1        yaml_2.2.1          
##  [70] processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [73] nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [76] xml2_1.3.2           compiler_4.0.4       bayesplot_1.8.0     
##  [79] shinythemes_1.1.2    rstudioapi_0.13      gamm4_0.2-6         
##  [82] curl_4.3             reprex_0.3.0         statmod_1.4.35      
##  [85] stringi_1.5.3        highr_0.8            ps_1.6.0            
##  [88] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41     
##  [91] Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1        
##  [94] shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1        
##  [97] lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3    
## [100] httpuv_1.5.4         R6_2.5.0             bookdown_0.21       
## [103] promises_1.1.1       gridExtra_2.3        codetools_0.2-18    
## [106] boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [109] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1         
## [112] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33         
## [115] parallel_4.0.4       hms_0.5.3            grid_4.0.4          
## [118] coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [121] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3     
## [124] dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-josephSampleSizeCalculations1995&#34; class=&#34;csl-entry&#34;&gt;
Joseph, L., Wolfson, D. B., &amp;amp; Berger, R. D. (1995a). Sample size calculations for binomial proportions via highest posterior density intervals. &lt;em&gt;Journal of the Royal Statistical Society: Series D (The Statistician)&lt;/em&gt;, &lt;em&gt;44&lt;/em&gt;(2), 143–154. &lt;a href=&#34;https://doi.org/10.2307/2348439&#34;&gt;https://doi.org/10.2307/2348439&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-josephCommentsBayesianSample1995&#34; class=&#34;csl-entry&#34;&gt;
Joseph, L., Wolfson, D. B., &amp;amp; Berger, R. D. (1995b). Some comments on &lt;span&gt;Bayesian&lt;/span&gt; sample size determination. &lt;em&gt;Journal of the Royal Statistical Society: Series D (The Statistician)&lt;/em&gt;, &lt;em&gt;44&lt;/em&gt;(2), 167–171. &lt;a href=&#34;https://doi.org/10.2307/2348442&#34;&gt;https://doi.org/10.2307/2348442&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-maxwellSampleSizePlanning2008&#34; class=&#34;csl-entry&#34;&gt;
Maxwell, S. E., Kelley, K., &amp;amp; Rausch, J. R. (2008). Sample size planning for statistical power and accuracy in parameter estimation. &lt;em&gt;Annual Review of Psychology&lt;/em&gt;, &lt;em&gt;59&lt;/em&gt;(1), 537–563. &lt;a href=&#34;https://doi.org/10.1146/annurev.psych.59.103006.093735&#34;&gt;https://doi.org/10.1146/annurev.psych.59.103006.093735&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-moreyBayesFactorApproaches2011&#34; class=&#34;csl-entry&#34;&gt;
Morey, R. D., &amp;amp; Rouder, J. N. (2011). Bayes factor approaches for testing interval null hypotheses. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;(4), 406–419. &lt;a href=&#34;https://doi.org/10.1037/a0024377&#34;&gt;https://doi.org/10.1037/a0024377&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rouderBayesianTestsAccepting2009&#34; class=&#34;csl-entry&#34;&gt;
Rouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., &amp;amp; Iverson, G. (2009). Bayesian t tests for accepting and rejecting the null hypothesis. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;(2), 225–237. &lt;a href=&#34;https://doi.org/10.3758/PBR.16.2.225&#34;&gt;https://doi.org/10.3758/PBR.16.2.225&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wassersteinMovingWorld052019&#34; class=&#34;csl-entry&#34;&gt;
Wasserstein, R. L., Schirm, A. L., &amp;amp; Lazar, N. A. (2019). Moving to a &lt;span&gt;World Beyond&lt;/span&gt; &lt;span&gt;“p &lt;span&gt;&lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;\)&lt;/span&gt;&lt;/span&gt; 0.05.”&lt;/span&gt; &lt;em&gt;The American Statistician&lt;/em&gt;, &lt;em&gt;73&lt;/em&gt;(sup1), 1–19. &lt;a href=&#34;https://doi.org/10.1080/00031305.2019.1583913&#34;&gt;https://doi.org/10.1080/00031305.2019.1583913&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;To be clear, one can consider the null hypothesis within the Bayesian paradigm. I don’t tend to take this approach, but it’d be unfair not to at least mention some resources. Kurschke covered the topic in chapters 11 and 12 in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text, &lt;a href=&#34;http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/&#34;&gt;&lt;em&gt;Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan&lt;/em&gt;&lt;/a&gt;. You might also check out &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-rouderBayesianTestsAccepting2009&#34; role=&#34;doc-biblioref&#34;&gt;Rouder et al.&lt;/a&gt; (&lt;a href=&#34;#ref-rouderBayesianTestsAccepting2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;, &lt;a href=&#34;https://link.springer.com/content/pdf/10.3758/PBR.16.2.225.pdf&#34;&gt;&lt;em&gt;Bayesian t tests for accepting and rejecting the null hypothesis&lt;/em&gt;&lt;/a&gt;, or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-moreyBayesFactorApproaches2011&#34; role=&#34;doc-biblioref&#34;&gt;Morey &amp;amp; Rouder&lt;/a&gt; (&lt;a href=&#34;#ref-moreyBayesFactorApproaches2011&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;, &lt;a href=&#34;https://d1wqtxts1xzle7.cloudfront.net/45416179/Bayes_Factor_Approaches_for_Testing_Inte20160506-23207-1t89l96.pdf?1462571611=&amp;amp;response-content-disposition=inline%3B+filename%3DBayes_factor_approaches_for_testing_inte.pdf&amp;amp;Expires=1597530412&amp;amp;Signature=QAJQOISIvwxUlHd2uTfzgOMzf2TRcuWTcfwgki7JL4AIoYDziVCAfmDFOgUDi-h1mMEViTKFhOLTJF0-9u2IEyF2lR7-yhM67CYdKhqs8EEJOnhT9iK9MaaM2FBwZM8QoVtOXkOUaOXRHIt7C76UV5dbErTUx0r5Y1yym4a~-hDClb0696a6EB~dj0arYeDdylP7a3tfczmSxbIvrH8pOE4kQeHwsZXoANSh-eKXKYIYf6VD1yed~CSVPRkqlhMq6udOjg4INPZ33QBv3QQqYCk2esRC2DxxNmDF~rRVrIp0ebr6VMZkuMflVaj2~I2BFz7WS32Lb2hGFHT3jHskDA__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA&#34;&gt;&lt;em&gt;Bayes factor approaches for testing interval null hypotheses&lt;/em&gt;&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;For a contemporary discussion of the uses and misuses of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values, see &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-wassersteinMovingWorld052019&#34; role=&#34;doc-biblioref&#34;&gt;Wasserstein et al.&lt;/a&gt; (&lt;a href=&#34;#ref-wassersteinMovingWorld052019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and the other articles contained in that &lt;a href=&#34;https://www.tandfonline.com/toc/utas20/73/sup1?nav=tocList&#34;&gt;special issue of &lt;em&gt;The American Statistician&lt;/em&gt;&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian power analysis: Part I. Prepare to reject $H_0$ with simulation.</title>
      <link>/post/bayesian-power-analysis-part-i/</link>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/bayesian-power-analysis-part-i/</guid>
      <description>
&lt;script src=&#34;/post/bayesian-power-analysis-part-i/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;version-1.1.0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Version 1.1.0&lt;/h2&gt;
&lt;p&gt;Edited on April 21, 2021, to remove the &lt;code&gt;broom::tidy()&lt;/code&gt; portion of the workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;If you’d like to learn how to do Bayesian power calculations using &lt;strong&gt;brms&lt;/strong&gt;, stick around for this multi-part blog series. Here with part I, we’ll set the foundation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power-is-hard-especially-for-bayesians.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power is hard, especially for Bayesians.&lt;/h2&gt;
&lt;p&gt;Many journals, funding agencies, and dissertation committees require power calculations for your primary analyses. Frequentists have a variety of tools available to perform these calculations (e.g., &lt;a href=&#34;https://rpsychologist.com/analytical-and-simulation-based-power-analyses-for-mixed-design-anovas&#34;&gt;here&lt;/a&gt;). Bayesians, however, have a more difficult time of it. Most of our research questions and data issues are sufficiently complicated that we cannot solve the problems by hand. We need Markov chain Monte Carlo methods to iteratively sample from the posterior to summarize the parameters from our models. Same deal for power. If you’d like to compute the power for a given combination of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, likelihood &lt;span class=&#34;math inline&#34;&gt;\(p(\text{data} | \theta)\)&lt;/span&gt;, and set of priors &lt;span class=&#34;math inline&#34;&gt;\(p (\theta)\)&lt;/span&gt;, you’ll need to simulate.&lt;/p&gt;
&lt;p&gt;It’s been one of my recent career goals to learn how to do this. You know how they say: &lt;em&gt;The best way to learn is to teach&lt;/em&gt;. This series of blog posts is the evidence of me learning by teaching. It will be an exploration of what a Bayesian power simulation workflow might look like. The overall statistical framework will be within &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with an emphasis on code style based on the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. We’ll be fitting our Bayesian models with Bürkner’s &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;What this series is not, however, is an introduction to statistical power itself. Keep reading if you’re ready to roll up your sleeves, put on your applied hat, and learn how to get things done. If you’re more interested in introductions to power, see the references in the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I make assumptions.&lt;/h2&gt;
&lt;p&gt;For this series, I’m presuming you are familiar with linear regression, familiar with the basic differences between frequentist and Bayesian approaches to statistics, and have a basic sense of what we mean by statistical power. Here are some resources if you’d like to shore up.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you’re unfamiliar with statistical power, Kruschke covered it in chapter 13 of his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/&#34;&gt;text&lt;/a&gt;. You might also check out the &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-maxwellSampleSizePlanning2008&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://www3.nd.edu/~kkelley/publications/articles/Maxwell_Kelley_Rausch_2008.pdf&#34;&gt;review paper&lt;/a&gt; by Maxwell, Kelley, and Rausch. There’s always, of course, the original work by Cohen &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-cohenStatisticalPowerAnalysis1988a&#34; role=&#34;doc-biblioref&#34;&gt;Cohen, 1988&lt;/a&gt;)&lt;/span&gt;. You might also like this &lt;a href=&#34;https://www.khanacademy.org/math/ap-statistics/tests-significance-ap/error-probabilities-power/v/introduction-to-power-in-significance-tests&#34;&gt;Khan Academy video&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;To learn about Bayesian regression, I recommend the introductory text books by either McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke&lt;/a&gt; (&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;. Both authors host blogs (&lt;a href=&#34;http://doingbayesiandataanalysis.blogspot.com&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://elevanth.org/blog/&#34;&gt;here&lt;/a&gt;, respectively). If you go with McElreath, do check out his &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;online lectures&lt;/a&gt; and my &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingBrms2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;, &lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2020&#34; role=&#34;doc-biblioref&#34;&gt;2020c&lt;/a&gt;)&lt;/span&gt; ebooks translating his text to &lt;strong&gt;brms&lt;/strong&gt; and &lt;strong&gt;tidyverse&lt;/strong&gt; code. I have an ebook for Kruschke’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020b&lt;/a&gt;)&lt;/span&gt;, too.&lt;/li&gt;
&lt;li&gt;For even more &lt;strong&gt;brms&lt;/strong&gt;-related resources, you can find vignettes and documentation at &lt;a href=&#34;https://cran.r-project.org/package=brms/index.html&#34;&gt;https://cran.r-project.org/package=brms/index.html&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;tidyverse&lt;/strong&gt; introductions, your best bets are Grolemund and Wickham’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-grolemundDataScience2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R for data science&lt;/em&gt;&lt;/a&gt; and Wickham’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wickhamTidyverseStyleGuide2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://style.tidyverse.org&#34;&gt;&lt;em&gt;The tidyverse style guide&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;We’ll be simulating data. If that’s new to you, both Kruschke and McElreath cover that a little in their texts. You can find nice online tutorials &lt;a href=&#34;https://debruine.github.io/tutorials/sim-data.html&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/&#34;&gt;here&lt;/a&gt;, too.&lt;/li&gt;
&lt;li&gt;We’ll also be making a couple custom functions. If that’s new, you might check out &lt;a href=&#34;https://r4ds.had.co.nz/functions.html&#34;&gt;&lt;em&gt;R4DS&lt;/em&gt;, chapter 19&lt;/a&gt; or &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/functions.html&#34;&gt;chapter 14&lt;/a&gt; of Roger Peng’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-pengProgrammingDataScience2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; &lt;em&gt;R Programming for Data Science&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-to-warm-up-before-jumping-into-power.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need to warm up before jumping into power.&lt;/h2&gt;
&lt;p&gt;Let’s load our primary packages. The &lt;strong&gt;tidyverse&lt;/strong&gt; helps organize data and we model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Consider a case where you have some dependent variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; that you’d like to compare between two groups, which we’ll call treatment and control. Here we presume &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is continuous and, for the sake of simplicity, is in a standardized metric for the control condition. Letting &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; stand for control and &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; index the data row for a given case, we might write that as &lt;span class=&#34;math inline&#34;&gt;\(y_{i, c} \sim \operatorname{Normal} (0, 1)\)&lt;/span&gt;. The mean for our treatment condition is 0.5, with the standard deviation still in the standardized metric. In the social sciences a standardized mean difference of 0.5 would typically be considered a medium effect size. Here’s what that’d look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set our theme because, though I love the default ggplot theme, I hate gridlines
theme_set(theme_grey() +
            theme(panel.grid = element_blank()))

# define the means
mu_c &amp;lt;- 0
mu_t &amp;lt;- 0.5

# set up the data
tibble(x = seq(from = -4, to = 5, by = .01)) %&amp;gt;%
  mutate(c = dnorm(x, mean = mu_c, sd = 1),
         t = dnorm(x, mean = mu_t, sd = 1)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = x)) +
  geom_area(aes(y = c),
            size = 0, alpha = 1/3, fill = &amp;quot;grey25&amp;quot;) +
  geom_area(aes(y = t),
            size = 0, alpha = 1/3, fill = &amp;quot;blue2&amp;quot;) +
 annotate(geom = &amp;quot;text&amp;quot;,
           x = c(-.5, 1), y = .385,
           label = c(&amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;),
           hjust = 1:0,
           size = 5) +
  scale_x_continuous(NULL, breaks = -4:5) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_color_manual(values = c(&amp;quot;grey25&amp;quot;, &amp;quot;blue2&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-i/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sure, those distributions have a lot of overlap. But their means are clearly different and we’d like to make sure we plan on collecting enough data to do a good job showing that. A power analysis will help.&lt;/p&gt;
&lt;p&gt;Within the conventional frequentist paradigm, power is the probability of rejecting the null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; in favor of the alternative hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;, given the alternative hypothesis is “true.” In this case, the typical null hypothesis is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0\text{: } \mu_c = \mu_t,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or put differently,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0\text{: } \mu_t - \mu_c = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And the alternative hypothesis is often just&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_1\text{: } \mu_c \neq \mu_t,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or otherwise put,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_1\text{: } \mu_t - \mu_c \neq 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Within the regression framework, we’ll be comparing &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;s using the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 \text{treatment}_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{treatment}\)&lt;/span&gt; is a dummy variable coded 0 = control 1 = treatment and varies across cases indexed by &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. In this setup, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the estimate for &lt;span class=&#34;math inline&#34;&gt;\(\mu_c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the estimate of the difference between condition means, &lt;span class=&#34;math inline&#34;&gt;\(\mu_t - \mu_c\)&lt;/span&gt;. Thus our focal parameter, the one we care about the most in our power analysis, will be &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Within the frequentist paradigm, we typically compare these hypotheses using a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value for &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; with the critical value, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, set to .05. Thus, power is the probability we’ll have &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt; when it is indeed the case that &lt;span class=&#34;math inline&#34;&gt;\(\mu_c \neq \mu_t\)&lt;/span&gt;. We won’t be computing &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values in this project, but we will use 95% intervals. Recall that the result of a Bayesian analysis, the posterior distribution, is the probability of the parameters, given the data &lt;span class=&#34;math inline&#34;&gt;\(p (\theta | \text{data})\)&lt;/span&gt;. With our 95% Bayesian credible intervals, we’ll be able to describe the parameter space over which our estimate of &lt;span class=&#34;math inline&#34;&gt;\(\mu_t - \mu_c\)&lt;/span&gt; is 95% probable. That is, for our power analysis, we’re interested in the probability our 95% credible intervals for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; contain zero within their bounds when we know a priori &lt;span class=&#34;math inline&#34;&gt;\(\mu_c \neq \mu_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The reason we know &lt;span class=&#34;math inline&#34;&gt;\(\mu_c \neq \mu_t\)&lt;/span&gt; is because we’ll be simulating the data that way. What our power analysis will help us determine is how many cases we’ll need to achieve a predetermined level of power. The conventional threshold is .8.&lt;/p&gt;
&lt;div id=&#34;dry-run-number-1.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dry run number 1.&lt;/h3&gt;
&lt;p&gt;To make this all concrete, let’s start with a simple example. We’ll simulate a single set of data, fit a Bayesian regression model, and examine the results for the critical parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;. For the sake of simplicity, let’s keep our two groups, treatment and control, the same size. We’ll start with &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt; for each.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already decided above that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{i, c} &amp;amp; \sim \operatorname{Normal}(0, 1) \text{ and} \\
y_{i, t} &amp;amp; \sim \operatorname{Normal}(0.5, 1).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s how we might simulate data along those lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

d &amp;lt;-
  tibble(group = rep(c(&amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;), each = n)) %&amp;gt;% 
  mutate(treatment = ifelse(group == &amp;quot;control&amp;quot;, 0, 1),
         y         = ifelse(group == &amp;quot;control&amp;quot;, 
                            rnorm(n, mean = mu_c, sd = 1),
                            rnorm(n, mean = mu_t, sd = 1)))

glimpse(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 100
## Columns: 3
## $ group     &amp;lt;chr&amp;gt; &amp;quot;control&amp;quot;, &amp;quot;control&amp;quot;, &amp;quot;control&amp;quot;, &amp;quot;control&amp;quot;, &amp;quot;control&amp;quot;, &amp;quot;cont…
## $ treatment &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ y         &amp;lt;dbl&amp;gt; -0.62645381, 0.18364332, -0.83562861, 1.59528080, 0.32950777…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case it wasn’t clear, the two variables &lt;code&gt;group&lt;/code&gt; and &lt;code&gt;treatment&lt;/code&gt; are redundant. Whereas the former is composed of names, the latter is the dummy-variable equivalent (i.e., control = 0, treatment = 1). The main event was how we used the &lt;code&gt;rnorm()&lt;/code&gt; function to simulate the normally-distributed values for &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before we fit our model, we need to decide on priors. To give us ideas, here are the &lt;strong&gt;brms&lt;/strong&gt; defaults for our model and data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_prior(data = d,
          family = gaussian,
          y ~ 0 + Intercept + treatment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 prior class      coef group resp dpar nlpar bound       source
##                (flat)     b                                            default
##                (flat)     b Intercept                             (vectorized)
##                (flat)     b treatment                             (vectorized)
##  student_t(3, 0, 2.5) sigma                                            default&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A few things: Notice that here we’re using the &lt;code&gt;0 + Intercept&lt;/code&gt; syntax. This is because &lt;strong&gt;brms&lt;/strong&gt; handles the priors for the default intercept under the presumption you’ve mean-centered all your predictor variables. However, since our &lt;code&gt;treatment&lt;/code&gt; variable is a dummy, that assumption won’t fly. The &lt;code&gt;0 + Intercept&lt;/code&gt; allows us to treat the model intercept as just another &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter, which makes no assumptions about centering. Along those lines, you’ll notice &lt;strong&gt;brms&lt;/strong&gt; currently defaults to flat priors for the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters (i.e., those for which &lt;code&gt;class = b&lt;/code&gt;). And finally, the default prior on &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is moderately wide &lt;code&gt;student_t(3, 0, 2.5)&lt;/code&gt;. By default, &lt;strong&gt;brms&lt;/strong&gt; also sets the left bounds for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters at zero, making that a folded-&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution. If you’re confused by these details, spend some time with the &lt;a href=&#34;https://cran.r-project.org/package=brms/brms.pdf&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; reference manual&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brms2020RM&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2020b&lt;/a&gt;)&lt;/span&gt;, particularly the &lt;code&gt;brm&lt;/code&gt; and &lt;code&gt;brmsformula&lt;/code&gt; sections.&lt;/p&gt;
&lt;p&gt;In this project, we’ll be primarily using two kinds of priors: default flat priors and weakly-regularizing priors. Hopefully flat priors are self-explanatory. They let the likelihood (data) dominate the posterior and tend to produce results similar to those from frequentist estimators.&lt;/p&gt;
&lt;p&gt;As for weakly-regularizing priors, McElreath covered them in his text. They’re mentioned a bit in the &lt;strong&gt;Stan&lt;/strong&gt; team’s &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;&lt;em&gt;Prior Choice Recommendations&lt;/em&gt;&lt;/a&gt; wiki, and you can learn even more from Gelman, Simpson, and Betancourt’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanPriorCanOften2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/published/entropy-19-00555-v2.pdf&#34;&gt;&lt;em&gt;The prior can only be understood in the context of the likelihood&lt;/em&gt;&lt;/a&gt;. These priors aren’t strongly informative and aren’t really representative of our research hypotheses. But they’re not as absurd as flat priors, either. Rather, with just a little bit of knowledge about the data, these priors are set to keep the MCMC chains on target. Since our &lt;code&gt;y&lt;/code&gt; variable has a mean near zero and a standard deviation near one and since our sole predictor, &lt;code&gt;treatment&lt;/code&gt; is a dummy, setting &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, 2)\)&lt;/span&gt; as the prior for both &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters might be a good place to start. The prior is permissive enough that it will let likelihood dominate the posterior, but it also rules out ridiculous parts of the parameter space (e.g., a standardized mean difference of 20, an intercept of -93). And since we know the data are on the unit scale, we might just center our folded-Student-&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; prior on one and add a gentle scale setting of one.&lt;/p&gt;
&lt;p&gt;Feel free to disagree and use your own priors. The great thing about priors is that they can be proposed, defended, criticized and improved. The point is to settle on the priors you can defend with written reasons. Select ones you’d feel comfortable defending to a skeptical reviewer.&lt;/p&gt;
&lt;p&gt;Here’s how we might fit the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 0 + Intercept + treatment,
      prior = c(prior(normal(0, 2), class = b),
                prior(student_t(3, 1, 1), class = sigma)),
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we look at the summary, we might check the chains in a trace plot. We’re looking for “stuck” chains that don’t appear to come from a normal distribution (the chains are a profile-like view rather than histogram, allowing for inspection of dependence between samples).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-i/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yep, the chains all look good. Here’s the parameter summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 0 + Intercept + treatment 
##    Data: d (Number of observations: 100) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.10      0.13    -0.16     0.36 1.00     2094     2048
## treatment     0.51      0.18     0.16     0.90 1.00     2079     1989
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.92      0.07     0.80     1.06 1.00     2684     2122
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 95% credible intervals for our &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; parameter, termed &lt;code&gt;treatment&lt;/code&gt; in the output, are well above zero.&lt;/p&gt;
&lt;p&gt;Another way to look at the parameter summary is with the &lt;code&gt;brms::fixef()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Estimate Est.Error       Q2.5     Q97.5
## Intercept 0.1033326 0.1312632 -0.1556192 0.3640170
## treatment 0.5142275 0.1847559  0.1587896 0.8976941&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;you-can-reuse-a-fit.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;You can reuse a fit.&lt;/h3&gt;
&lt;p&gt;Especially with simple models like this, a lot of the time we spend waiting for &lt;code&gt;brms::brm()&lt;/code&gt; to return the model is wrapped up in compilation. This is because &lt;strong&gt;brms&lt;/strong&gt; is a collection of user-friendly functions designed to fit models with &lt;a href=&#34;https://mc-stan.org&#34;&gt;&lt;strong&gt;Stan&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-standevelopmentteamRStanInterfaceStan2020&#34; role=&#34;doc-biblioref&#34;&gt;Stan Development Team, 2020&lt;/a&gt;, &lt;a href=&#34;#ref-standevelopmentteamStanReferenceManual2021&#34; role=&#34;doc-biblioref&#34;&gt;2021a&lt;/a&gt;, &lt;a href=&#34;#ref-standevelopmentteamStanUserGuide2021&#34; role=&#34;doc-biblioref&#34;&gt;2021b&lt;/a&gt;)&lt;/span&gt;. With each new model, &lt;code&gt;brm()&lt;/code&gt; translates your model into &lt;strong&gt;Stan&lt;/strong&gt; code, which then gets translated to C++ and is compiled afterwards (see &lt;a href=&#34;https://cran.r-project.org/package=brms/vignettes/brms_overview.pdf&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://cran.r-project.org/package=brms/brms.pdf&#34;&gt;here&lt;/a&gt;). However, we can use the &lt;code&gt;update()&lt;/code&gt; function to update a previously-compiled fit object with new data. This cuts out the compilation time and allows us to get directly to sampling. Here’s how to do it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set a new seed
set.seed(2)

# simulate new data based on that new seed
d &amp;lt;-
  tibble(group = rep(c(&amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;), each = n)) %&amp;gt;% 
  mutate(treatment = ifelse(group == &amp;quot;control&amp;quot;, 0, 1),
         y         = ifelse(group == &amp;quot;control&amp;quot;, 
                            rnorm(n, mean = mu_c, sd = 1),
                            rnorm(n, mean = mu_t, sd = 1)))

updated_fit &amp;lt;-
  update(fit,
         newdata = d,
         seed = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Behold the &lt;code&gt;fixef()&lt;/code&gt; parameter summary for our updated model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(updated_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Estimate Est.Error       Q2.5     Q97.5
## Intercept 0.06410605 0.1692172 -0.2700568 0.3995847
## treatment 0.30654690 0.2387028 -0.1434469 0.7815116&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well how about that? In this case, our 95% credible intervals for the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; &lt;code&gt;treatment&lt;/code&gt; coefficient did include zero within their bounds. Though the posterior mean, 0.30, is still well away from zero, here we’d fail to reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; at the conventional level. This is why we simulate.&lt;/p&gt;
&lt;p&gt;To recap, we’ve&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;determined our primary data type,&lt;/li&gt;
&lt;li&gt;cast our research question in terms of a regression model,&lt;/li&gt;
&lt;li&gt;identified the parameter of interest,&lt;/li&gt;
&lt;li&gt;settled on defensible priors,&lt;/li&gt;
&lt;li&gt;picked an initial sample size,&lt;/li&gt;
&lt;li&gt;fit an initial model with a single simulated data set, and&lt;/li&gt;
&lt;li&gt;practiced reusing that fit with &lt;code&gt;update()&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We’re more than half way there! It’s time to do our first power simulation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simulate-to-determine-power.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulate to determine power.&lt;/h2&gt;
&lt;p&gt;In this post, we’ll play with three ways to do a Bayesian power simulation. They’ll all be similar, but hopefully you’ll learn a bit as we transition from one to the next. Though if you’re impatient and all this seems remedial, you could probably just skip down to the final example, &lt;a href=&#34;#version-3-still-talking-about-memory-we-can-be-even-stingier.&#34;&gt;Version 3&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;version-1-lets-introduce-making-a-custom-model-fitting-function.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Version 1: Let’s introduce making a custom model-fitting function.&lt;/h3&gt;
&lt;p&gt;For our power analysis, we’ll need to simulate a large number of data sets, each of which we’ll fit a model to. Here we’ll make a custom function, &lt;code&gt;sim_d()&lt;/code&gt;, that will simulate new data sets just like before. Our function will have two parameters: we’ll set our seeds with &lt;code&gt;seed&lt;/code&gt; and determine how many cases we’d like per group with &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_d &amp;lt;- function(seed, n) {
  
  mu_t &amp;lt;- .5
  mu_c &amp;lt;- 0

  set.seed(seed)
  
  tibble(group = rep(c(&amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;), each = n)) %&amp;gt;% 
  mutate(treatment = ifelse(group == &amp;quot;control&amp;quot;, 0, 1),
         y         = ifelse(group == &amp;quot;control&amp;quot;, 
                            rnorm(n, mean = mu_c, sd = 1),
                            rnorm(n, mean = mu_t, sd = 1)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a quick example of how our function works.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_d(seed = 123, n = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 3
##   group     treatment      y
##   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 control           0 -0.560
## 2 control           0 -0.230
## 3 treatment         1  2.06 
## 4 treatment         1  0.571&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to get down to business. We’re going to be saving our simulation results in a nested data frame, &lt;code&gt;s&lt;/code&gt;. Initially, &lt;code&gt;s&lt;/code&gt; will have one column of &lt;code&gt;seed&lt;/code&gt; values. These will serve a dual function. First, they are the values we’ll be feeding into the &lt;code&gt;seed&lt;/code&gt; argument of our custom data-generating function, &lt;code&gt;sim_d()&lt;/code&gt;. Second, since the &lt;code&gt;seed&lt;/code&gt; values serially increase, they also stand in as iteration indexes.&lt;/p&gt;
&lt;p&gt;For our second step, we add the data simulations and save them in a nested column, &lt;code&gt;d&lt;/code&gt;. In the first argument of the &lt;code&gt;purrr::map()&lt;/code&gt; function, we indicate we want to iterate over the values in &lt;code&gt;seed&lt;/code&gt;. In the second argument, we indicate we want to serially plug those &lt;code&gt;seed&lt;/code&gt; values into the first argument within the &lt;code&gt;sim_d()&lt;/code&gt; function. That argument, recall, is the well-named &lt;code&gt;seed&lt;/code&gt; argument. With the final argument in &lt;code&gt;map()&lt;/code&gt;, &lt;code&gt;n = 50&lt;/code&gt;, we hard code 50 into the &lt;code&gt;n&lt;/code&gt; argument of &lt;code&gt;sim_d()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the third step, we expand our &lt;code&gt;purrr::map()&lt;/code&gt; skills from above to &lt;code&gt;purrr::map2()&lt;/code&gt;, which allows us to iteratively insert two arguments into a function. Within this paradigm, the two arguments are generically termed &lt;code&gt;.x&lt;/code&gt; and &lt;code&gt;.y&lt;/code&gt;. Thus our approach will be &lt;code&gt;.x = d, .y = seed&lt;/code&gt;. For our function, we specify &lt;code&gt;~update(fit, newdata = .x, seed = .y)&lt;/code&gt;. Thus we’ll be iteratively inserting our simulated &lt;code&gt;d&lt;/code&gt; data into the &lt;code&gt;newdata&lt;/code&gt; argument and will be simultaneously inserting our &lt;code&gt;seed&lt;/code&gt; values into the &lt;code&gt;seed&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;Also notice that the number of iterations we’ll be working with is determined by the number of rows in the &lt;code&gt;seed&lt;/code&gt; column. We are defining that number as &lt;code&gt;n_sim&lt;/code&gt;. Since this is just a blog post, I’m going to take it easy and use 100. But if this was a real power analysis for one of your projects, something like 1,000 would be better.&lt;/p&gt;
&lt;p&gt;Finally, you don’t have to do this, but I’m timing my simulation by saving &lt;code&gt;Sys.time()&lt;/code&gt; values at the beginning and end of the simulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many simulations would you like?
n_sim &amp;lt;- 100

# this will help us track time
t1 &amp;lt;- Sys.time()

# here&amp;#39;s the main event!
s &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(d = map(seed, sim_d, n = 50)) %&amp;gt;% 
  mutate(fit = map2(d, seed, ~update(fit, newdata = .x, seed = .y)))

t2 &amp;lt;- Sys.time()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The entire simulation took just about a minute on my &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1250193047096299520&#34;&gt;new laptop&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t2 - t1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 44.20048 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Your mileage may vary.&lt;/p&gt;
&lt;p&gt;Let’s take a look at what we’ve done.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##    seed d                  fit      
##   &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;             &amp;lt;list&amp;gt;   
## 1     1 &amp;lt;tibble [100 × 3]&amp;gt; &amp;lt;brmsfit&amp;gt;
## 2     2 &amp;lt;tibble [100 × 3]&amp;gt; &amp;lt;brmsfit&amp;gt;
## 3     3 &amp;lt;tibble [100 × 3]&amp;gt; &amp;lt;brmsfit&amp;gt;
## 4     4 &amp;lt;tibble [100 × 3]&amp;gt; &amp;lt;brmsfit&amp;gt;
## 5     5 &amp;lt;tibble [100 × 3]&amp;gt; &amp;lt;brmsfit&amp;gt;
## 6     6 &amp;lt;tibble [100 × 3]&amp;gt; &amp;lt;brmsfit&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our 100-row nested tibble, we have all our simulated data sets in the &lt;code&gt;d&lt;/code&gt; column and all of our &lt;strong&gt;brms&lt;/strong&gt; fit objects nested in the &lt;code&gt;fit&lt;/code&gt; column. Next we’ll use &lt;code&gt;fixef()&lt;/code&gt; and a little wrangling to extract the parameter of interest, &lt;code&gt;treatment&lt;/code&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;), from each simulation. We’ll save the results as &lt;code&gt;parameters&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parameters &amp;lt;-
  s %&amp;gt;% 
  mutate(treatment = map(fit, ~ fixef(.) %&amp;gt;% 
                           data.frame() %&amp;gt;% 
                           rownames_to_column(&amp;quot;parameter&amp;quot;))) %&amp;gt;% 
  unnest(treatment)

parameters %&amp;gt;% 
  select(-d, -fit) %&amp;gt;% 
  filter(parameter == &amp;quot;treatment&amp;quot;) %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##    seed parameter Estimate Est.Error    Q2.5 Q97.5
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 treatment    0.514     0.185  0.159  0.898
## 2     2 treatment    0.307     0.239 -0.143  0.782
## 3     3 treatment    0.643     0.171  0.310  0.975
## 4     4 treatment    0.224     0.182 -0.128  0.574
## 5     5 treatment    0.429     0.189  0.0596 0.792
## 6     6 treatment    0.304     0.208 -0.114  0.711&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an aside, I know I’m moving kinda fast with all this wacky &lt;code&gt;purrr::map()&lt;/code&gt;/&lt;code&gt;purrr::map2()&lt;/code&gt; stuff. If you’re new to using the &lt;strong&gt;tidyverse&lt;/strong&gt; for iterating and saving the results in nested data structures, I recommend fixing an adult beverage and cozying up with Hadley Wickham’s presentation, &lt;a href=&#34;https://www.youtube.com/watch?v=rz3_FDVt9eg&#34;&gt;&lt;em&gt;Managing many models&lt;/em&gt;&lt;/a&gt;. And if you really hate it, both Kruschke and McElreath texts contain many examples of how to iterate in a more base &lt;strong&gt;R&lt;/strong&gt; sort of way.&lt;/p&gt;
&lt;p&gt;Anyway, here’s what those 100 &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; summaries look like in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parameters %&amp;gt;% 
  filter(parameter == &amp;quot;treatment&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = seed, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = c(0, .5), color = &amp;quot;white&amp;quot;) +
  geom_pointrange(fatten = 1/2) +
  labs(x = &amp;quot;seed (i.e., simulation index)&amp;quot;,
       y = expression(beta[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-i/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The horizontal lines show the idealized effect size (0.5) and the null hypothesis (0). Already, it’s apparent that most of our intervals indicate there’s more than a 95% probability the null hypothesis is not credible. Several do. Here’s how to quantify that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parameters %&amp;gt;% 
  filter(parameter == &amp;quot;treatment&amp;quot;) %&amp;gt;% 
  mutate(check = ifelse(Q2.5 &amp;gt; 0, 1, 0)) %&amp;gt;% 
  summarise(power = mean(check))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   power
##   &amp;lt;dbl&amp;gt;
## 1  0.66&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the second &lt;code&gt;mutate()&lt;/code&gt; line, we used a logical statement within &lt;code&gt;ifelse()&lt;/code&gt; to code all instances where the lower limit of the 95% interval (&lt;code&gt;Q2.5&lt;/code&gt;) was greater than 0 as a 1, with the rest as 0. That left us with a vector of 1’s and 0’s, which we saved as &lt;code&gt;check&lt;/code&gt;. In the &lt;code&gt;summarise()&lt;/code&gt; line, we took the mean of that column, which returned our Bayesian power estimate.&lt;/p&gt;
&lt;p&gt;That is, in 66 of our 100 simulations, an &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt; per group was enough to produce a 95% Bayesian credible interval that did not straddle 0.&lt;/p&gt;
&lt;p&gt;I should probably point out that a 95% interval for which &lt;code&gt;Q97.5 &amp;lt; 0&lt;/code&gt; would have also been consistent with the alternative hypothesis of &lt;span class=&#34;math inline&#34;&gt;\(\mu_c \neq \mu_t\)&lt;/span&gt;. However, I didn’t bother to work that option into the definition of our &lt;code&gt;check&lt;/code&gt; variable because I knew from the outset that that would be a highly unlikely result. But if you’d like to work more rigor into your checks, by all means do.&lt;/p&gt;
&lt;p&gt;And if you’ve gotten this far and have been following along with code of your own, congratulations! You did it! You’ve estimated the power of a Bayesian model with a given &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Now let’s refine our approach.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;version-2-we-might-should-be-more-careful-with-memory.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Version 2: We might should be more careful with memory.&lt;/h3&gt;
&lt;p&gt;I really like it that our &lt;code&gt;s&lt;/code&gt; object contains all our &lt;code&gt;brm()&lt;/code&gt; fits. It makes it really handy to do global diagnostics like making sure our &lt;span class=&#34;math inline&#34;&gt;\(\widehat R\)&lt;/span&gt; values are all within a respectable range.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s %&amp;gt;% 
  mutate(rhat = map(fit, rhat)) %&amp;gt;% 
  unnest(rhat) %&amp;gt;% 
  
  ggplot(aes(x = rhat)) +
  geom_histogram(bins = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-i/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Man those &lt;span class=&#34;math inline&#34;&gt;\(\widehat R\)&lt;/span&gt; values look sweet. It’s great to have a workflow that lets you check them. But holding on to all those fits can take up a lot of memory. If the only thing you’re interested in are the parameter summaries, a better approach might be to do the model refitting and parameter extraction in one step. That way you only save the parameter summaries. Here’s how you might do that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t3 &amp;lt;- Sys.time()

s2 &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(d = map(seed, sim_d, n = 50)) %&amp;gt;% 
  # here&amp;#39;s the new part
  mutate(b1 = map2(d, seed, ~update(fit, newdata = .x, seed = .y) %&amp;gt;% 
                     fixef() %&amp;gt;% 
                     data.frame() %&amp;gt;% 
                     rownames_to_column(&amp;quot;parameter&amp;quot;) %&amp;gt;% 
                     filter(parameter == &amp;quot;treatment&amp;quot;)))

t4 &amp;lt;- Sys.time()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like before, this only about a minute.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t4 - t3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 48.12622 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a point of comparison, here are the sizes of the results from our first approach to those from the second.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;object.size(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 79822320 bytes&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;object.size(s2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 503120 bytes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s a big difference. Hopefully you get the idea. With more complicated models and 10+ times the number of simulations, size will eventually matter.&lt;/p&gt;
&lt;p&gt;Anyway, here are the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s2 %&amp;gt;% 
  unnest(b1) %&amp;gt;% 

  ggplot(aes(x = seed, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = c(0, .5), color = &amp;quot;white&amp;quot;) +
  geom_pointrange(fatten = 1/2) +
  labs(x = &amp;quot;seed (i.e., simulation index)&amp;quot;,
       y = expression(beta[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-i/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Same parameter summaries, lower memory burden.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;version-3-still-talking-about-memory-we-can-be-even-stingier.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Version 3: Still talking about memory, we can be even stingier.&lt;/h3&gt;
&lt;p&gt;So far, both of our simulation attempts resulted in our saving the simulated data sets. It’s a really nice option if you ever want to go back and take a look at those simulated data. For example, you might want to inspect a random subset of the data simulations with box plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

s2 %&amp;gt;% 
  sample_n(12) %&amp;gt;% 
  unnest(d) %&amp;gt;% 
  
  ggplot(aes(x = group, y = y)) +
  geom_boxplot(aes(fill = group), 
               alpha = 2/3, show.legend = F) +
  scale_fill_manual(values = c(&amp;quot;grey25&amp;quot;, &amp;quot;blue2&amp;quot;)) +
  xlab(NULL) +
  facet_wrap(~ seed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-i/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, it’s no big deal if we keep the data around or not. The data sets are fairly small and we’re only simulating 100 of them. But in cases where the data are larger and you’re doing thousands of simulations, keeping the data could become a memory drain.&lt;/p&gt;
&lt;p&gt;If you’re willing to forgo the luxury of inspecting your data simulations, it might make sense to run our power analysis in a way that avoids saving them. One way to do so would be to just wrap the data simulation and model fitting all in one function. We’ll call it &lt;code&gt;sim_d_and_fit()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_d_and_fit &amp;lt;- function(seed, n) {
  
  mu_t &amp;lt;- .5
  mu_c &amp;lt;- 0
  
  set.seed(seed)
  
  d &amp;lt;-
    tibble(group     = rep(c(&amp;quot;control&amp;quot;, &amp;quot;treatment&amp;quot;), each = n)) %&amp;gt;% 
    mutate(treatment = ifelse(group == &amp;quot;control&amp;quot;, 0, 1),
           y         = ifelse(group == &amp;quot;control&amp;quot;, 
                              rnorm(n, mean = mu_c, sd = 1),
                              rnorm(n, mean = mu_t, sd = 1)))
  
  update(fit,
         newdata = d, 
         seed = seed) %&amp;gt;% 
    fixef() %&amp;gt;% 
    data.frame() %&amp;gt;% 
    rownames_to_column(&amp;quot;parameter&amp;quot;) %&amp;gt;% 
    filter(parameter == &amp;quot;treatment&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now iterate 100 times once more.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t5 &amp;lt;- Sys.time()

s3 &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(b1 = map(seed, sim_d_and_fit, n = 50)) %&amp;gt;% 
  unnest(b1)

t6 &amp;lt;- Sys.time()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That was pretty quick.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t6 - t5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 48.48654 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what it returned.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(s3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##    seed parameter Estimate Est.Error    Q2.5 Q97.5
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 treatment    0.514     0.185  0.159  0.898
## 2     2 treatment    0.307     0.239 -0.143  0.782
## 3     3 treatment    0.643     0.171  0.310  0.975
## 4     4 treatment    0.224     0.182 -0.128  0.574
## 5     5 treatment    0.429     0.189  0.0596 0.792
## 6     6 treatment    0.304     0.208 -0.114  0.711&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By wrapping our data simulation, model fitting, and parameter extraction steps all in one function, we simplified the output such that we’re no longer holding on to the data simulations or the &lt;strong&gt;brms&lt;/strong&gt; fit objects. We just have the parameter summaries and the &lt;code&gt;seed&lt;/code&gt;, making the product even smaller.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(object = c(&amp;quot;s&amp;quot;, &amp;quot;s2&amp;quot;, &amp;quot;s3&amp;quot;)) %&amp;gt;% 
  mutate(bytes = map_dbl(object, ~get(.) %&amp;gt;% object.size()))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   object    bytes
##   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 s      79822320
## 2 s2       503120
## 3 s3         5952&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But the primary results are the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s3 %&amp;gt;% 
  ggplot(aes(x = seed, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = c(0, .5), color = &amp;quot;white&amp;quot;) +
  geom_pointrange(fatten = 1/2) +
  labs(x = &amp;quot;seed (i.e., simulation index)&amp;quot;,
       y = expression(beta[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-i/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We still get the same power estimate, too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s3 %&amp;gt;% 
  mutate(check = ifelse(Q2.5 &amp;gt; 0, 1, 0)) %&amp;gt;% 
  summarise(power = mean(check))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   power
##   &amp;lt;dbl&amp;gt;
## 1  0.66&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;But my goal was to figure out what &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; will get me power of .8 or more!&lt;/em&gt;, you say. Fair enough. Try increasing &lt;code&gt;n&lt;/code&gt; to 65 or something.&lt;/p&gt;
&lt;p&gt;If that seems unsatisfying, welcome to the world of simulation. Since our Bayesian models are complicated, we don’t have the luxury of plugging a few values into some quick power formula. Just as simulation is an iterative process, determining on the right values to simulate over might well be an iterative process, too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrap-up&lt;/h2&gt;
&lt;p&gt;Anyway, that’s the essence of the &lt;strong&gt;brms/tidyverse&lt;/strong&gt; workflow for Bayesian power analysis. You follow these steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Determine your primary data type.&lt;/li&gt;
&lt;li&gt;Determine your primary regression model and parameter(s) of interest.&lt;/li&gt;
&lt;li&gt;Pick defensible priors for all parameters–the kinds of priors you intend to use once you have the real data in hand.&lt;/li&gt;
&lt;li&gt;Select a sample size.&lt;/li&gt;
&lt;li&gt;Fit an initial model and save the fit object.&lt;/li&gt;
&lt;li&gt;Simulate some large number of data sets all following your prechosen form and use the &lt;code&gt;update()&lt;/code&gt; function to iteratively fit the models.&lt;/li&gt;
&lt;li&gt;Extract the parameter(s) of interest.&lt;/li&gt;
&lt;li&gt;Summarize.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In addition, we played with a few approaches based on logistical concerns like memory. In the next post, &lt;a href=&#34;https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-ii/&#34;&gt;part II&lt;/a&gt;, we’ll see how the precision-oriented approach to sample-size planning is a viable alternative to power focused on rejecting null hypotheses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;i-had-help.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I had help.&lt;/h2&gt;
&lt;p&gt;Special thanks to Christopher Peters (&lt;a href=&#34;https://github.com/statwonk&#34;&gt;@statwonk&lt;/a&gt;) for the helpful edits and suggestions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
##  [9] tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         splines_4.0.4        crosstalk_1.1.0.1   
##   [7] TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17       
##  [10] digest_0.6.27        htmltools_0.5.1.1    rsconnect_0.8.16    
##  [13] fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [16] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1          
##  [19] sandwich_3.0-0       prettyunits_1.1.1    colorspace_2.0-0    
##  [22] rvest_0.3.6          haven_2.3.1          xfun_0.22           
##  [25] callr_3.5.1          crayon_1.4.1         jsonlite_1.7.2      
##  [28] lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [31] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1     
##  [34] V8_3.4.0             pkgbuild_1.2.0       rstan_2.21.2        
##  [37] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [40] DBI_1.1.0            miniUI_0.1.1.1       xtable_1.8-4        
##  [43] stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [46] htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3       
##  [49] ellipsis_0.3.1       pkgconfig_2.0.3      loo_2.4.1           
##  [52] farver_2.0.3         dbplyr_2.0.0         utf8_1.1.4          
##  [55] tidyselect_1.1.0     labeling_0.4.2       rlang_0.4.10        
##  [58] reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [61] cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [64] generics_0.1.0       broom_0.7.5          ggridges_0.5.2      
##  [67] evaluate_0.14        fastmap_1.0.1        yaml_2.2.1          
##  [70] processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [73] nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [76] xml2_1.3.2           compiler_4.0.4       bayesplot_1.8.0     
##  [79] shinythemes_1.1.2    rstudioapi_0.13      gamm4_0.2-6         
##  [82] curl_4.3             reprex_0.3.0         statmod_1.4.35      
##  [85] stringi_1.5.3        highr_0.8            ps_1.6.0            
##  [88] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41     
##  [91] Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1        
##  [94] shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1        
##  [97] lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3    
## [100] httpuv_1.5.4         R6_2.5.0             bookdown_0.21       
## [103] promises_1.1.1       gridExtra_2.3        codetools_0.2-18    
## [106] boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [109] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1         
## [112] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33         
## [115] parallel_4.0.4       hms_0.5.3            grid_4.0.4          
## [118] coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [121] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3     
## [124] dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# for the hard-core scrollers:
# if you increase n to 65, the power becomes about .85
n_sim &amp;lt;- 100

t7 &amp;lt;- Sys.time()

s4 &amp;lt;-
  tibble(seed = 1:n_sim) %&amp;gt;% 
  mutate(b1 = map(seed, sim_d_and_fit, n = 65))

t8 &amp;lt;- Sys.time()

t8 - t7

object.size(s4)

s4 %&amp;gt;% 
  unnest(b1) %&amp;gt;% 
  mutate(check = ifelse(Q2.5 &amp;gt; 0, 1, 0)) %&amp;gt;% 
  summarise(power = mean(check))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020a). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brms2020RM&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt; reference manual, &lt;span&gt;Version&lt;/span&gt; 2.14.4&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/brms.pdf&#34;&gt;https://CRAN.R-project.org/package=brms/brms.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cohenStatisticalPowerAnalysis1988a&#34; class=&#34;csl-entry&#34;&gt;
Cohen, J. (1988). &lt;em&gt;Statistical power analysis for the behavioral sciences&lt;/em&gt;. &lt;span&gt;L. Erlbaum Associates&lt;/span&gt;. &lt;a href=&#34;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&#34;&gt;https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanPriorCanOften2017&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Simpson, D., &amp;amp; Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood. &lt;em&gt;Entropy&lt;/em&gt;, &lt;em&gt;19&lt;/em&gt;(10), 555. &lt;a href=&#34;https://doi.org/10.3390/e19100555&#34;&gt;https://doi.org/10.3390/e19100555&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-grolemundDataScience2017&#34; class=&#34;csl-entry&#34;&gt;
Grolemund, G., &amp;amp; Wickham, H. (2017). &lt;em&gt;R for data science&lt;/em&gt;. &lt;span&gt;O’Reilly&lt;/span&gt;. &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;https://r4ds.had.co.nz&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingBrms2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Statistical rethinking with brms, &lt;span class=&#34;nocase&#34;&gt;ggplot2&lt;/span&gt;, and the tidyverse&lt;/em&gt; (version 1.2.0). &lt;a href=&#34;https://doi.org/10.5281/zenodo.3693202&#34;&gt;https://doi.org/10.5281/zenodo.3693202&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzDoingBayesianData2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis in brms and the tidyverse&lt;/em&gt; (version 0.3.0). &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020c). &lt;em&gt;Statistical rethinking with brms, Ggplot2, and the tidyverse: &lt;span&gt;Second&lt;/span&gt; edition&lt;/em&gt; (version 0.1.1). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-maxwellSampleSizePlanning2008&#34; class=&#34;csl-entry&#34;&gt;
Maxwell, S. E., Kelley, K., &amp;amp; Rausch, J. R. (2008). Sample size planning for statistical power and accuracy in parameter estimation. &lt;em&gt;Annual Review of Psychology&lt;/em&gt;, &lt;em&gt;59&lt;/em&gt;(1), 537–563. &lt;a href=&#34;https://doi.org/10.1146/annurev.psych.59.103006.093735&#34;&gt;https://doi.org/10.1146/annurev.psych.59.103006.093735&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pengProgrammingDataScience2019&#34; class=&#34;csl-entry&#34;&gt;
Peng, R. D. (2019). &lt;em&gt;R programming for data science&lt;/em&gt;. &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/&#34;&gt;https://bookdown.org/rdpeng/rprogdatascience/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-standevelopmentteamRStanInterfaceStan2020&#34; class=&#34;csl-entry&#34;&gt;
Stan Development Team. (2020). &lt;em&gt;&lt;span&gt;RStan&lt;/span&gt;: The &lt;span&gt;R&lt;/span&gt; interface to &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;a href=&#34;https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html&#34;&gt;https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-standevelopmentteamStanReferenceManual2021&#34; class=&#34;csl-entry&#34;&gt;
Stan Development Team. (2021a). &lt;em&gt;Stan reference manual, &lt;span&gt;Version&lt;/span&gt; 2.26&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/docs/2_26/reference-manual/&#34;&gt;https://mc-stan.org/docs/2_26/reference-manual/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-standevelopmentteamStanUserGuide2021&#34; class=&#34;csl-entry&#34;&gt;
Stan Development Team. (2021b). &lt;em&gt;Stan user’s guide, &lt;span&gt;Version&lt;/span&gt; 2.26&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/docs/2_26/stan-users-guide/index.html&#34;&gt;https://mc-stan.org/docs/2_26/stan-users-guide/index.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamTidyverseStyleGuide2020&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2020). &lt;em&gt;The tidyverse style guide&lt;/em&gt;. &lt;a href=&#34;https://style.tidyverse.org/&#34;&gt;https://style.tidyverse.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Would you like all your posteriors in one plot?</title>
      <link>/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/</guid>
      <description>
&lt;script src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A colleague reached out to me earlier this week with a plotting question. They had fit a series of Bayesian models, all containing a common parameter of interest. They knew how to plot their focal parameter one model at a time, but were stumped on how to combine the plots across models into a seamless whole. It reminded me a bit of this gif&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/Bqn8Z7xdPCFy0/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;which I originally got from &lt;a href=&#34;https://twitter.com/JennyBryan&#34;&gt;Jenny Bryan&lt;/a&gt;’s great talk, &lt;a href=&#34;https://www.youtube.com/watch?v=4MfUCX_KpdE&#34;&gt;&lt;em&gt;Behind every great plot there’s a great deal of wrangling&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The goal of this post is to provide solutions. We’ll practice a few different ways you can combine the posterior samples from your Bayesian models into a single plot. As usual, we’ll be fitting our models with &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt;, wrangling with packages from the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt;, and getting a little help from the &lt;a href=&#34;https://mjskay.github.io/tidybayes/index.html&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I make assumptions.&lt;/h2&gt;
&lt;p&gt;For this post, I’m presuming you are familiar Bayesian regression using &lt;strong&gt;brms.&lt;/strong&gt; I’m also assuming you’ve coded using some of the foundational functions from the &lt;strong&gt;tidyverse.&lt;/strong&gt; If you’d like to firm up your foundations a bit, check out these resources.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To learn about Bayesian regression, I recommend the introductory text books by either McElreath (&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;here&lt;/a&gt;) or Kruschke (&lt;a href=&#34;http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/&#34;&gt;here&lt;/a&gt;). Both authors host blogs (&lt;a href=&#34;http://doingbayesiandataanalysis.blogspot.com&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://elevanth.org/blog/&#34;&gt;here&lt;/a&gt;, respectively). If you go with McElreath, do check out his &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;online lectures&lt;/a&gt; and my ebooks where I translated his text to &lt;strong&gt;brms&lt;/strong&gt; and &lt;strong&gt;tidyverse&lt;/strong&gt; code (&lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;here&lt;/a&gt;). I have a similar ebook translation for Kruschke’s text (&lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For even more &lt;strong&gt;brms&lt;/strong&gt;-related resources, you can find vignettes and documentation &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/index.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;tidyverse&lt;/strong&gt; introductions, your best bets are &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R4DS&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://style.tidyverse.org&#34;&gt;&lt;em&gt;The tidyverse style guide&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;same-parameter-different-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Same parameter, different models&lt;/h2&gt;
&lt;p&gt;Let’s load our primary statistical packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)
library(tidybayes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate &lt;span class=&#34;math inline&#34;&gt;\(n = 150\)&lt;/span&gt; draws from the standard normal distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 150

set.seed(1)
d &amp;lt;-
  tibble(y = rnorm(n, mean = 0, sd = 1))

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##        y
##    &amp;lt;dbl&amp;gt;
## 1 -0.626
## 2  0.184
## 3 -0.836
## 4  1.60 
## 5  0.330
## 6 -0.820&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we’ll fit three intercept-only models for &lt;code&gt;y&lt;/code&gt;. Each will follow the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i     &amp;amp; \sim \text{Normal} (\mu, \sigma) \\
\mu     &amp;amp; = \beta_0 \\
\beta_0 &amp;amp; \sim \text{Normal} (0, x) \\
\sigma  &amp;amp; \sim \text{Student-t}(3, 0, 10)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the unconditional intercept (i.e., an intercept not conditioned on any predictors). We will be fitting three alternative models. All will have the same prior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{Student-t}(3, 0, 10)\)&lt;/span&gt;, which is the &lt;strong&gt;brms&lt;/strong&gt; default in this case. [If you’d like to check, use the &lt;code&gt;get_prior()&lt;/code&gt; function.] The only way the models will differ is by their prior on the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;. By model, those priors will be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fit1&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, 10)\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit2&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, 1)\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit3&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, 0.1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So if you were wondering, the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in the &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, x)\)&lt;/span&gt; line, above, was a stand-in for the varying &lt;a href=&#34;https://en.wikipedia.org/wiki/Hyperparameter&#34;&gt;hyperparameter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here we fit the models in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 1,
      prior(normal(0, 10), class = Intercept),
      seed = 1)

fit2 &amp;lt;-
  update(fit1,
         prior = prior(normal(0, 1), class = Intercept),
         seed = 1)

fit3 &amp;lt;-
  update(fit1,
         prior = prior(normal(0, 0.1), class = Intercept),
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Normally we’d use &lt;code&gt;plot()&lt;/code&gt; to make sure the chains look good and then use something like &lt;code&gt;print()&lt;/code&gt; or &lt;code&gt;posterior_summary()&lt;/code&gt; to summarize the models’ results. I’ve checked and they’re all fine. For the sake of space, let’s press forward.&lt;/p&gt;
&lt;p&gt;If you were going to plot the results of an individual fit using something like the &lt;code&gt;tidybayes::stat_halfeye()&lt;/code&gt; function, the next step would be extracting the posterior draws. Here we’ll do so with the &lt;code&gt;brms::posterior_samples()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post1 &amp;lt;- posterior_samples(fit1)
post2 &amp;lt;- posterior_samples(fit2)
post3 &amp;lt;- posterior_samples(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Focusing on &lt;code&gt;fit1&lt;/code&gt;, here’s how we’d plot the results for the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this part is unnecessary; it just adjusts some theme defaults to my liking
theme_set(theme_gray() +
            theme(axis.text.y  = element_text(hjust = 0),
                  axis.ticks.y = element_blank(),
                  panel.grid   = element_blank()))

# plot!
post1 %&amp;gt;% 
  ggplot(aes(x = b_Intercept, y = 0)) +
  stat_halfeye() +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But how might we get the posterior draws from all three fits into one plot?&lt;/em&gt; The answer is by somehow combining the posterior draws from each into one data frame. There are many ways to do this. Perhaps the simplest is with the &lt;code&gt;bind_rows()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  bind_rows(
    post1,
    post2,
    post3
  ) %&amp;gt;% 
  mutate(prior = str_c(&amp;quot;normal(0, &amp;quot;, c(10, 1, 0.1), &amp;quot;)&amp;quot;) %&amp;gt;% rep(., each = 4000))

head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_Intercept     sigma      lp__         prior
## 1  0.06440413 0.9408454 -202.2537 normal(0, 10)
## 2  0.02603356 0.9416735 -202.1114 normal(0, 10)
## 3 -0.02122717 0.8967501 -202.0446 normal(0, 10)
## 4  0.02620046 0.9521795 -202.2594 normal(0, 10)
## 5  0.02620046 0.9521795 -202.2594 normal(0, 10)
## 6  0.08025366 0.9101939 -202.1808 normal(0, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;bind_rows()&lt;/code&gt; function worked well, here, because all three post objects had the same number of columns of the same names. So we just stacked them three high. That is, we went from three data objects of 4,000 rows and 3 columns to one data object with 12,000 rows and 3 columns. But with the &lt;code&gt;mutate()&lt;/code&gt; function we did add a fourth column, &lt;code&gt;prior&lt;/code&gt;, that indexed which model each row came from. Now our data are ready, we can plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  ggplot(aes(x = b_Intercept, y = prior)) +
  stat_halfeye()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our plot arrangement made it easy to compare the results of tightening the prior on &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;; the narrower the prior, the narrower the posterior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-if-my-posterior_samples-arent-of-the-same-dimensions-across-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What if my &lt;code&gt;posterior_samples()&lt;/code&gt; aren’t of the same dimensions across models?&lt;/h2&gt;
&lt;p&gt;For the next examples, we need new data. Here we’ll simulate three predictors–&lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, and &lt;code&gt;x3&lt;/code&gt;. We then simulate our criterion &lt;code&gt;y&lt;/code&gt; as a linear additive function of those predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
d &amp;lt;-
  tibble(x1 = rnorm(n, mean = 0, sd = 1),
         x2 = rnorm(n, mean = 0, sd = 1),
         x3 = rnorm(n, mean = 0, sd = 1)) %&amp;gt;% 
  mutate(y  = rnorm(n, mean = 0 + x1 * 0 + x2 * 0.2 + x3 * -0.4))

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##       x1      x2     x3      y
##    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 -0.626  0.450   0.894  0.694
## 2  0.184 -0.0186 -1.05  -0.189
## 3 -0.836 -0.318   1.97  -1.61 
## 4  1.60  -0.929  -0.384 -1.59 
## 5  0.330 -1.49    1.65  -2.41 
## 6 -0.820 -1.08    1.51  -0.764&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to work with these data in two ways. For the first example, we’ll fit a series of univariable models following the same basic form, but each with a different predictor. For the second example, we’ll fit a series of multivariable models with various combinations of the predictors. Each requires its own approach.&lt;/p&gt;
&lt;div id=&#34;same-form-different-predictors.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Same form, different predictors.&lt;/h3&gt;
&lt;p&gt;This time we’re just using the &lt;strong&gt;brms&lt;/strong&gt; default priors. As such, the models all follow the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i     &amp;amp; \sim \text{Normal} (\mu_i, \sigma) \\
\mu_i   &amp;amp; = \beta_0 + \beta_n x_n\\
\beta_0 &amp;amp; \sim \text{Student-t}(3, 0, 10) \\
\sigma  &amp;amp; \sim \text{Student-t}(3, 0, 10)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You may be wondering &lt;em&gt;What about the prior for&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\beta_n\)&lt;/span&gt;&lt;em&gt;?&lt;/em&gt; The &lt;strong&gt;brms&lt;/strong&gt; defaults for those are improper flat priors. We define &lt;span class=&#34;math inline&#34;&gt;\(\beta_n x_n\)&lt;/span&gt; for the next three models as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fit4&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 x_1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit5&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_2 x_2\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit5&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_3 x_3\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s fit the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit4 &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 1 + x1,
      seed = 1)

fit5 &amp;lt;-
  update(fit4,
         newdata = d,
         y ~ 1 + x2,
         seed = 1)

fit6 &amp;lt;-
  update(fit4,
         newdata = d,
         y ~ 1 + x3,
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like before, save the posterior draws for each as separate data frames.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post4 &amp;lt;- posterior_samples(fit4)
post5 &amp;lt;- posterior_samples(fit5)
post6 &amp;lt;- posterior_samples(fit6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, our simple &lt;code&gt;bind_rows()&lt;/code&gt; trick won’t work well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  post4,
  post5,
  post6
) %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_Intercept        b_x1    sigma      lp__ b_x2 b_x3
## 1 -0.26609646 -0.07795464 1.249694 -242.9716   NA   NA
## 2 -0.11933443 -0.03143494 1.251379 -240.4618   NA   NA
## 3 -0.10952301  0.02739295 1.278072 -241.2102   NA   NA
## 4 -0.08785528 -0.01065453 1.443157 -245.2715   NA   NA
## 5 -0.22020421 -0.16635358 1.185220 -241.7569   NA   NA
## 6  0.02973246 -0.13106488 1.123438 -239.2940   NA   NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We don’t want separate columns for &lt;code&gt;b_x1&lt;/code&gt;, &lt;code&gt;b_x2&lt;/code&gt;, and &lt;code&gt;b_x3&lt;/code&gt;. We want them all stacked atop one another. One simple solution is a two-step wherein we (1) select the relevant columns from each and bind them together with &lt;code&gt;bind_cols()&lt;/code&gt; and then (2) stack them atop one another with the &lt;code&gt;gather()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  bind_cols(
    post4 %&amp;gt;% select(b_x1),
    post5 %&amp;gt;% select(b_x2),
    post6 %&amp;gt;% select(b_x3)
  ) %&amp;gt;% 
  gather() %&amp;gt;% 
  mutate(predictor = str_remove(key, &amp;quot;b_&amp;quot;))

head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    key       value predictor
## 1 b_x1 -0.07795464        x1
## 2 b_x1 -0.03143494        x1
## 3 b_x1  0.02739295        x1
## 4 b_x1 -0.01065453        x1
## 5 b_x1 -0.16635358        x1
## 6 b_x1 -0.13106488        x1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That &lt;code&gt;mutate()&lt;/code&gt; line at the end wasn’t necessary, but it will make the plot more attractive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  ggplot(aes(x = value, y = predictor)) +
  stat_halfeye()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;different-combinations-of-predictors-in-different-forms.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Different combinations of predictors in different forms.&lt;/h3&gt;
&lt;p&gt;Now we fit a series of multivariable models. The first three will have combinations of two of the predictors. The final model will have all three. For simplicity, we continue to use the &lt;strong&gt;brms&lt;/strong&gt; default priors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit7 &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 1 + x1 + x2,
      seed = 1)

fit8 &amp;lt;-
  update(fit7,
         newdata = d,
         y ~ 1 + x1 + x3,
         seed = 1)

fit9 &amp;lt;-
  update(fit7,
         newdata = d,
         y ~ 1 + x2 + x3,
         seed = 1)

fit10 &amp;lt;-
  update(fit7,
         newdata = d,
         y ~ 1 + x1 + x2 + x3,
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Individually extract the posterior draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post7  &amp;lt;- posterior_samples(fit7)
post8  &amp;lt;- posterior_samples(fit8)
post9  &amp;lt;- posterior_samples(fit9)
post10 &amp;lt;- posterior_samples(fit10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a look at what happens this time when we use the &lt;code&gt;bind_rows()&lt;/code&gt; approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  bind_rows(
    post7,
    post8,
    post9,
    post10
  ) 

glimpse(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 16,000
## Columns: 6
## $ b_Intercept &amp;lt;dbl&amp;gt; -0.034398871, 0.008116322, 0.109134954, -0.134114504, -0.148230448, 0.04629622…
## $ b_x1        &amp;lt;dbl&amp;gt; -0.018887709, -0.156024614, -0.248414749, 0.057442787, 0.241874229, -0.3504998…
## $ b_x2        &amp;lt;dbl&amp;gt; 0.23847261, 0.27500306, 0.37294396, 0.20640317, 0.15437136, 0.28201317, 0.1538…
## $ sigma       &amp;lt;dbl&amp;gt; 1.250134, 1.065501, 1.029253, 1.220301, 1.206074, 1.114755, 1.180636, 1.266597…
## $ lp__        &amp;lt;dbl&amp;gt; -236.9970, -236.7477, -241.3055, -237.9540, -242.0909, -239.3407, -237.2902, -…
## $ b_x3        &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still have the various data frames stacked atop another, with the data from &lt;code&gt;post7&lt;/code&gt; in the first 4,000 rows. See how the values in the &lt;code&gt;b_x3&lt;/code&gt; column are all missing (i.e., filled with &lt;code&gt;NA&lt;/code&gt; values)? That’s because &lt;code&gt;fit7&lt;/code&gt; didn’t contain &lt;code&gt;x3&lt;/code&gt; as a predictor. Similarly, if we were to look at rows 4,001 through 8,000, we’d see column &lt;code&gt;b_x2&lt;/code&gt; would be the one filled with &lt;code&gt;NA&lt;/code&gt;s. This behavior is a good thing, here. After a little more wrangling, we’ll plot and it should be become clear why. Here’s the wrangling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  posts %&amp;gt;% 
  select(starts_with(&amp;quot;b_x&amp;quot;)) %&amp;gt;% 
  mutate(contains = rep(c(&amp;quot;&amp;lt;1, 1, 0&amp;gt;&amp;quot;, &amp;quot;&amp;lt;1, 0, 1&amp;gt;&amp;quot;, &amp;quot;&amp;lt;0, 1, 1&amp;gt;&amp;quot;, &amp;quot;&amp;lt;1, 1, 1&amp;gt;&amp;quot;), each = 4000)) %&amp;gt;% 
  gather(key, value, -contains) %&amp;gt;% 
  mutate(coefficient = str_remove(key, &amp;quot;b_x&amp;quot;) %&amp;gt;% str_c(&amp;quot;beta[&amp;quot;, ., &amp;quot;]&amp;quot;))

head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    contains  key       value coefficient
## 1 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.01888771     beta[1]
## 2 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.15602461     beta[1]
## 3 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.24841475     beta[1]
## 4 &amp;lt;1, 1, 0&amp;gt; b_x1  0.05744279     beta[1]
## 5 &amp;lt;1, 1, 0&amp;gt; b_x1  0.24187423     beta[1]
## 6 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.35049990     beta[1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;code&gt;contains&lt;/code&gt; variable, we indexed which fit the draws came from. The 1’s and 0’s within the angle brackets indicate which of the three predictors were present within the model with the 1’s indicating they were and the 0’s indicating they were not. For example, &lt;code&gt;&amp;lt;1, 1, 0&amp;gt;&lt;/code&gt; in the first row indicated this was the model including &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt;. Importantly, we also added a &lt;code&gt;coefficient&lt;/code&gt; index. This is just a variant of &lt;code&gt;key&lt;/code&gt; that’ll make the strip labels in our plot more attractive. Behold:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  drop_na(value) %&amp;gt;% 
  ggplot(aes(x = value, y = contains)) +
  stat_halfeye() +
  ylab(NULL) +
  facet_wrap(~coefficient, ncol = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hopefully now it’s clear why it was good to save those cells with the &lt;code&gt;NA&lt;/code&gt;s.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bonus-you-can-streamline-your-workflow.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bonus: You can streamline your workflow.&lt;/h2&gt;
&lt;p&gt;The workflows above are generally fine. But they’re a little inefficient. If you’d like to reduce the amount of code you’re writing and the number of objects you have floating around in your environment, you might consider a more streamlined workflow where you work with your fit objects in bulk. Here we’ll demonstrate a nested tibble approach with the first three fits.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  tibble(name  = str_c(&amp;quot;fit&amp;quot;, 1:3),
         prior = str_c(&amp;quot;normal(0, &amp;quot;, c(10, 1, 0.1), &amp;quot;)&amp;quot;)) %&amp;gt;% 
  mutate(fit = map(name, get)) %&amp;gt;% 
  mutate(post = map(fit, posterior_samples))
  
head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 4
##   name  prior          fit       post                
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;              
## 1 fit1  normal(0, 10)  &amp;lt;brmsfit&amp;gt; &amp;lt;df[,3] [4,000 × 3]&amp;gt;
## 2 fit2  normal(0, 1)   &amp;lt;brmsfit&amp;gt; &amp;lt;df[,3] [4,000 × 3]&amp;gt;
## 3 fit3  normal(0, 0.1) &amp;lt;brmsfit&amp;gt; &amp;lt;df[,3] [4,000 × 3]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a 3-row nested tibble. The first column, &lt;code&gt;name&lt;/code&gt; is just a character vector with the names of the fits. The next column isn’t necessary, but it nicely explicates the main difference in the models: the prior we used on the intercept. It’s in the &lt;code&gt;map()&lt;/code&gt; functions within the two &lt;code&gt;mutate()&lt;/code&gt;lines where all the magic happens. With the first, we used the &lt;code&gt;get()&lt;/code&gt; function to snatch up the &lt;strong&gt;brms&lt;/strong&gt; fit objects matching the names in the &lt;code&gt;name&lt;/code&gt; column. In the second, we used the &lt;code&gt;posterior_samples()&lt;/code&gt; function to extract the posterior draws from each of the fits saved in &lt;code&gt;fit&lt;/code&gt;. Do you see how each for in the &lt;code&gt;post&lt;/code&gt; column contains an entire &lt;span class=&#34;math inline&#34;&gt;\(4,000 \times 3\)&lt;/span&gt; data frame? That’s why we refer to this as a nested tibble. We have data frames compressed within data frames. If you’d like to access the data within the &lt;code&gt;post&lt;/code&gt; column, just &lt;code&gt;unnest()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  select(-fit) %&amp;gt;% 
  unnest(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12,000 x 5
##    name  prior         b_Intercept sigma  lp__
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 fit1  normal(0, 10)     0.0644  0.941 -202.
##  2 fit1  normal(0, 10)     0.0260  0.942 -202.
##  3 fit1  normal(0, 10)    -0.0212  0.897 -202.
##  4 fit1  normal(0, 10)     0.0262  0.952 -202.
##  5 fit1  normal(0, 10)     0.0262  0.952 -202.
##  6 fit1  normal(0, 10)     0.0803  0.910 -202.
##  7 fit1  normal(0, 10)    -0.00142 0.886 -202.
##  8 fit1  normal(0, 10)     0.0696  0.939 -202.
##  9 fit1  normal(0, 10)    -0.172   0.943 -205.
## 10 fit1  normal(0, 10)     0.0259  0.839 -203.
## # … with 11,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After un-nesting, we can remake the plot from above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  select(-fit) %&amp;gt;% 
  unnest(post) %&amp;gt;% 

  ggplot(aes(x = b_Intercept, y = prior)) +
  stat_halfeye()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To learn more about using the &lt;strong&gt;tidyverse&lt;/strong&gt; for iterating and saving the results in nested tibbles, check out &lt;a href=&#34;https://twitter.com/hadleywickham&#34;&gt;Hadley Wickham&lt;/a&gt;’s great talk, &lt;a href=&#34;https://www.youtube.com/watch?v=rz3_FDVt9eg&#34;&gt;&lt;em&gt;Managing many models&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session information&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5    
##  [7] purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] splines_4.0.4        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [49] htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [53] ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3      loo_2.4.1           
##  [57] dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2       tidyselect_1.1.0    
##  [61] rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [65] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [69] broom_0.7.5          ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1       
##  [73] yaml_2.2.1           processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [77] nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [81] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [85] gamm4_0.2-6          curl_4.3             reprex_0.3.0         statmod_1.4.35      
##  [89] stringi_1.5.3        highr_0.8            ps_1.6.0             blogdown_1.3        
##  [93] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [97] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1        
## [101] lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [105] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [109] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [113] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [117] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [125] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stein&#39;s Paradox and What Partial Pooling Can Do For You</title>
      <link>/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/</link>
      <pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/</guid>
      <description>
&lt;script src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;[edited on January 18, 2021]&lt;/p&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;a href=&#34;https://www.urbandictionary.com/define.php?term=tl%3Bdr&#34;&gt;tl;dr&lt;/a&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Sometimes a mathematical result is strikingly contrary to generally held belief even though an obviously valid proof is given. &lt;a href=&#34;https://en.wikipedia.org/wiki/Charles_M._Stein&#34;&gt;Charles Stein&lt;/a&gt; of Stanford University discovered such a paradox in statistics in 1955. His result undermined a century and a half of work on estimation theory. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 119&lt;/a&gt;)&lt;/span&gt;
The James-Stein estimator leads to better predictions than simple means. Though I don’t recommend you actually use the James-Stein estimator in applied research, understanding why it works might help clarify why it’s time social scientists consider &lt;a href=&#34;http://elevanth.org/blog/2017/08/24/multilevel-regression-as-default/&#34;&gt;defaulting to multilevel models&lt;/a&gt; for their work-a-day projects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;the-james-stein-can-help-us-understand-multilevel-models.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The James-Stein can help us understand multilevel models.&lt;/h2&gt;
&lt;p&gt;I recently noticed someone—I wish I could recall who—tweet about Efron and Morris’s classic paper, &lt;a href=&#34;http://statweb.stanford.edu/~ckirby/brad/other/Article1977.pdf&#34;&gt;&lt;em&gt;Stein’s paradox in statistics&lt;/em&gt;&lt;/a&gt;. At the time, I was vaguely aware of the paper but hadn’t taken the chance to read it. The tweet’s author mentioned how good a read it was. Now I’ve finally given it a look, I concur. I’m not a sports fan, but I really appreciated their primary example using batting averages from baseball players in 1970. It clarified why partial pooling leads to better estimates than taking simple averages.&lt;/p&gt;
&lt;p&gt;In this post, I’ll walk out Efron and Morris’s baseball example and then link it to contemporary Bayesian multilevel models.&lt;/p&gt;
&lt;div id=&#34;i-assume-things.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I assume things.&lt;/h3&gt;
&lt;p&gt;For this project, I’m presuming you are familiar with logistic regression, vaguely familiar with the basic differences between frequentist and Bayesian approaches to fitting regression models, and have heard of multilevel models. All code in is &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;&lt;strong&gt;R&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with a heavy use of the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;, and the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; package&lt;/a&gt; for Bayesian regression &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;behold-the-baseball-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Behold the &lt;code&gt;baseball&lt;/code&gt; data.&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Stein’s paradox concerns the use of observed averages to estimate unobservable quantities. Averaging is the second most basic process in statistics, the first being the simple act of counting. A baseball player who gets seven hits in 20 official times at bat is said to have a batting average of .350. In computing this statistic we are forming an estimate of the payer’s true batting ability in terms of his observed average rate of success. Asked how well the player will do in his next 100 times at bat, we would probably predict 35 more hits. In traditional statistical theory it can be proved that no other estimation rule is uniformly better than the observed average.&lt;/p&gt;
&lt;p&gt;The paradoxical element in Stein’s result is that it sometimes contradicts this elementary law of statistical theory. If we have three or more baseball players, and if we are interested in predicting future batting averages for each of them, then there is a procedure that is better than simply extrapolating from the three separate averages…&lt;/p&gt;
&lt;p&gt;As our primary data we shall consider the batting averages of 18 major-league players as they were recorded after their first 45 times at bat in the 1970 season. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 119&lt;/a&gt;)&lt;/span&gt;
Let’s enter the &lt;code&gt;baseball&lt;/code&gt; data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
baseball &amp;lt;- 
  tibble(player = c(&amp;quot;Clemente&amp;quot;, &amp;quot;F Robinson&amp;quot;, &amp;quot;F Howard&amp;quot;, &amp;quot;Johnstone&amp;quot;, &amp;quot;Berry&amp;quot;, &amp;quot;Spencer&amp;quot;, &amp;quot;Kessinger&amp;quot;, &amp;quot;L Alvarado&amp;quot;, &amp;quot;Santo&amp;quot;, &amp;quot;Swoboda&amp;quot;, &amp;quot;Unser&amp;quot;, &amp;quot;Williams&amp;quot;, &amp;quot;Scott&amp;quot;, &amp;quot;Petrocelli&amp;quot;, &amp;quot;E Rodriguez&amp;quot;, &amp;quot;Campaneris&amp;quot;, &amp;quot;Munson&amp;quot;, &amp;quot;Alvis&amp;quot;),
         hits = c(18:15, 14, 14:12, 11, 11, rep(10, times = 5), 9:7),
         times_at_bat = 45,
         true_ba = c(.346, .298, .276, .222, .273, .27, .263, .21, .269, .23, .264, .256, .303, .264, .226, .286, .316, .2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what those data look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(baseball)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 18
## Columns: 4
## $ player       &amp;lt;chr&amp;gt; &amp;quot;Clemente&amp;quot;, &amp;quot;F Robinson&amp;quot;, &amp;quot;F Howard&amp;quot;, &amp;quot;Johnstone&amp;quot;, &amp;quot;Berry&amp;quot;, &amp;quot;Spencer&amp;quot;, &amp;quot;Kessi…
## $ hits         &amp;lt;dbl&amp;gt; 18, 17, 16, 15, 14, 14, 13, 12, 11, 11, 10, 10, 10, 10, 10, 9, 8, 7
## $ times_at_bat &amp;lt;dbl&amp;gt; 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45
## $ true_ba      &amp;lt;dbl&amp;gt; 0.346, 0.298, 0.276, 0.222, 0.273, 0.270, 0.263, 0.210, 0.269, 0.230, 0.264, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have data from 18 players. The main columns are of the number of &lt;code&gt;hits&lt;/code&gt; for their first 45 &lt;code&gt;times_at_bat&lt;/code&gt;. I got the &lt;code&gt;player&lt;/code&gt;, &lt;code&gt;hits&lt;/code&gt;, and &lt;code&gt;times_at_bat&lt;/code&gt; values directly from the paper. However, Efron and Morris didn’t include the batting averages for the end of the season in the paper. Happily, I was able to find those values in the &lt;a href=&#34;http://statweb.stanford.edu/~ckirby/brad/LSI/chapter1.pdf&#34;&gt;online&lt;/a&gt; posting of the first chapter of one of Effron’s books &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronEmpiricalBayesJamesStein2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; . They’re included in the &lt;code&gt;true_ba&lt;/code&gt; column.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;These were all the players who happened to have batted exactly 45 times the day the data were tabulated. A batting average is defined, of course, simply as the number of hits divided by the number of times at bat; it is always a number between 0 and 1. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 119&lt;/a&gt;)&lt;/span&gt;
I like use a lot of plots to better understand what I’m doing. Before we start plotting, I should point out the color theme in this project comes from &lt;a href=&#34;https://teamcolorcodes.com/seattle-mariners-color-codes/&#34;&gt;here&lt;/a&gt;. [Haters gonna hate.]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;navy_blue &amp;lt;- &amp;quot;#0C2C56&amp;quot;
nw_green  &amp;lt;- &amp;quot;#005C5C&amp;quot;  
silver    &amp;lt;- &amp;quot;#C4CED4&amp;quot;
theme_set(theme_grey() +
            theme(panel.grid = element_blank(),
                  panel.background = element_rect(fill = silver),
                  strip.background = element_rect(fill = silver)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might use a histogram to get a sense of the &lt;code&gt;hits&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  ggplot(aes(x = hits)) +
  geom_histogram(color = nw_green,
                 fill  = navy_blue,
                 size  = 1/10, binwidth = 1) +
  scale_x_continuous(&amp;quot;hits during the first 45 trials&amp;quot;,
                     breaks = 7:18)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is the distribution of the end-of-the-season batting averages, &lt;code&gt;true_ba&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)
baseball %&amp;gt;% 
  ggplot(aes(x = true_ba, y = 0)) +
  stat_halfeye(point_interval = median_qi, .width = .5,
               color = navy_blue, fill = alpha(nw_green, 2/3)) +
  geom_rug(color = navy_blue, size = 1/3, alpha = 1/2) +
  ggtitle(NULL, 
          subtitle = &amp;quot;The dot and horizontal line are the median and\ninterquartile range, respectively.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;james-stein-will-help-us-achieve-our-goal.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;James-Stein will help us achieve our goal.&lt;/h3&gt;
&lt;p&gt;For each of the 18 players in the data, our goal is to the best job possible to use the data for their first 45 times at bat (i.e., &lt;code&gt;hits&lt;/code&gt; and &lt;code&gt;times_at_bat&lt;/code&gt;) to predict their batting averages at the end of the season (i.e., &lt;code&gt;true_ba&lt;/code&gt;). Before Charles Stein, the conventional reasoning was their initial batting averages (i.e., &lt;code&gt;hits / times_at_bat&lt;/code&gt;) are the best way to do this. It turns out that would be naïve. To see why, let&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) = the batting average for the first 45 times at bat,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y_bar&lt;/code&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\overline y\)&lt;/span&gt;) = the grand mean for the first 45 times at bat,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;) = shrinking factor,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;z&lt;/code&gt; (i.e., &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;) = James-Stein estimate, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;true_ba&lt;/code&gt; (i.e., &lt;code&gt;theta&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) = the batting average at the end of the season.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The first step in applying Stein’s method is to determine the average of the averages. Obviously this grand average, which we give the symbol &lt;span class=&#34;math inline&#34;&gt;\(\overline y\)&lt;/span&gt;, must also lie between 0 and 1. The essential process in Stein’s method is the “shrinking” of all the individual averages toward this grand average. If a player’s hitting record is better than the grand average, then it must be reduced; if he is not hitting as well as the grand average, then his hitting record must be increased. The resulting shrunken value for each player we designate &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 119&lt;/a&gt;)&lt;/span&gt;
As such, the James-Stein estimator is&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[z = \overline y + c(y - \overline y),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, in the paper, &lt;span class=&#34;math inline&#34;&gt;\(c = .212\)&lt;/span&gt;. Let’s get some of those values into the &lt;code&gt;baseball&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
  baseball &amp;lt;-
  baseball %&amp;gt;% 
  mutate(y = hits / times_at_bat) %&amp;gt;% 
  mutate(y_bar = mean(y),
         c     = .212) %&amp;gt;% 
  mutate(z     = y_bar + c * (y - y_bar),
         theta = true_ba)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18 x 9
##    player       hits times_at_bat true_ba     y y_bar     c     z theta
##    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Clemente       18           45   0.346 0.4   0.265 0.212 0.294 0.346
##  2 F Robinson     17           45   0.298 0.378 0.265 0.212 0.289 0.298
##  3 F Howard       16           45   0.276 0.356 0.265 0.212 0.285 0.276
##  4 Johnstone      15           45   0.222 0.333 0.265 0.212 0.280 0.222
##  5 Berry          14           45   0.273 0.311 0.265 0.212 0.275 0.273
##  6 Spencer        14           45   0.27  0.311 0.265 0.212 0.275 0.27 
##  7 Kessinger      13           45   0.263 0.289 0.265 0.212 0.270 0.263
##  8 L Alvarado     12           45   0.21  0.267 0.265 0.212 0.266 0.21 
##  9 Santo          11           45   0.269 0.244 0.265 0.212 0.261 0.269
## 10 Swoboda        11           45   0.23  0.244 0.265 0.212 0.261 0.23 
## 11 Unser          10           45   0.264 0.222 0.265 0.212 0.256 0.264
## 12 Williams       10           45   0.256 0.222 0.265 0.212 0.256 0.256
## 13 Scott          10           45   0.303 0.222 0.265 0.212 0.256 0.303
## 14 Petrocelli     10           45   0.264 0.222 0.265 0.212 0.256 0.264
## 15 E Rodriguez    10           45   0.226 0.222 0.265 0.212 0.256 0.226
## 16 Campaneris      9           45   0.286 0.2   0.265 0.212 0.252 0.286
## 17 Munson          8           45   0.316 0.178 0.265 0.212 0.247 0.316
## 18 Alvis           7           45   0.2   0.156 0.265 0.212 0.242 0.2&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Which set of values, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, is the better indicator of batting ability for the 18 players in our example? In order to answer that question in a precise way one would have to know the “true batting ability” of each player. This true average we shall designate &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; (the Greek letter theta). Actually it is an unknowable quantity, an abstraction representing the probability that a player will get a hit on any given time at bat. Although &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is unobservable, we have a good approximation to it: the subsequent performance of the batters. It is sufficient to consider just the remainder of the 1970 season, which includes about nine times as much data as the preliminary averages were based on. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 119&lt;/a&gt;)&lt;/span&gt;
Now we have both &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; in the data, let’s compare their distributions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  pivot_longer(cols = c(y, z)) %&amp;gt;% 
  mutate(label = ifelse(name == &amp;quot;z&amp;quot;, 
                        &amp;quot;the James-Stein estimate&amp;quot;, 
                        &amp;quot;early-season batting average&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = value, y = label)) +
  geom_vline(xintercept = 0.2654321, linetype = 2,
             color = &amp;quot;white&amp;quot;) +
  stat_halfeye(point_interval = median_qi, .width = .5,
               color = navy_blue, fill = alpha(nw_green, 2/3),
               height = 4) +
  labs(x = &amp;quot;batting average&amp;quot;, y = NULL) +
  coord_cartesian(ylim = c(1.25, 5.25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As implied in the formula, the James-Stein estimates are substantially shrunken towards the grand mean, &lt;code&gt;y_bar&lt;/code&gt;. To get a sense of which estimate is better, we can subtract the estimate from &lt;code&gt;theta&lt;/code&gt;, the end of the season batting average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball &amp;lt;-
  baseball %&amp;gt;% 
  mutate(y_error = theta - y,
         z_error = theta - z)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since &lt;code&gt;y_error&lt;/code&gt; and &lt;code&gt;y_error&lt;/code&gt; are error distributions, we prefer values to be as close to zero as possible. Let’s take a look.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  pivot_longer(y_error:z_error) %&amp;gt;% 
  
  ggplot(aes(x = value, y = name)) +
  geom_vline(xintercept = 0, linetype = 2,
             color = &amp;quot;white&amp;quot;) +
  stat_halfeye(point_interval = median_qi, .width = .5,
               color = navy_blue, fill = alpha(nw_green, 2/3),
               height = 2.5) +
  labs(x = NULL, y = NULL) +
  coord_cartesian(ylim = c(1.25, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The James-Stein errors (i.e., &lt;code&gt;z_error&lt;/code&gt;) are more concentrated toward zero. In the paper, we read: “One method of evaluating the two estimates is by simply counting their successes and failures. For 16 of the 18 players the James-Stein estimator &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is closer than the observed average &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to the ‘true,’ or seasonal, average &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;” (pp. 119–121). We can compute that with a little &lt;code&gt;ifelse()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  transmute(closer_to_theta = ifelse(abs(y_error) - abs(z_error) == 0, &amp;quot;equal&amp;quot;,
                                     ifelse(abs(y_error) - abs(z_error) &amp;gt; 0, &amp;quot;z&amp;quot;, &amp;quot;y&amp;quot;))) %&amp;gt;% 
  count(closer_to_theta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   closer_to_theta     n
##   &amp;lt;chr&amp;gt;           &amp;lt;int&amp;gt;
## 1 y                   2
## 2 z                  16&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;A more quantitative way of comparing the two techniques is through the total squared error of estimation… The observed averages &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; have a total squared error of .077, whereas the squared error of the James-Stein estimators is only .022. By this comparison, then, Stein’s method is 3.5 times as accurate. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 121&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  pivot_longer(y_error:z_error) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  summarise(total_squared_error = sum(value * value))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   name    total_squared_error
##   &amp;lt;chr&amp;gt;                 &amp;lt;dbl&amp;gt;
## 1 y_error              0.0755
## 2 z_error              0.0214&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get the 3.5 value with simple division.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.07548795 / 0.02137602&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.531431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So it does indeed turn out that shrinking each player’s initial estimate toward the grand mean of those initial estimates does a better job of predicting their end-of-the-season batting averages than using their individual batting averages. To get a sense of what this looks like, let’s make our own version of the figure on page 121.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  baseball %&amp;gt;% 
    select(y, z, theta, player) %&amp;gt;% 
    gather(key, value, -player) %&amp;gt;% 
    mutate(time = ifelse(key == &amp;quot;theta&amp;quot;, &amp;quot;theta&amp;quot;, &amp;quot;estimate&amp;quot;)),
  baseball %&amp;gt;% 
    select(player, theta) %&amp;gt;% 
    rename(value = theta) %&amp;gt;% 
    mutate(key  = &amp;quot;theta&amp;quot;, 
           time = &amp;quot;theta&amp;quot;)
) %&amp;gt;% 
  mutate(facet = rep(c(&amp;quot;estimate = y&amp;quot;, &amp;quot;estimate = z&amp;quot;), each = n() / 4) %&amp;gt;% rep(., times = 2)) %&amp;gt;% 
  
  ggplot(aes(x = time, y = value, group = player)) +
  geom_hline(yintercept = 0.2654321, linetype = 2,
             color = &amp;quot;white&amp;quot;) +
  geom_line(alpha = 1/2,
            color = nw_green) +
  geom_point(alpha = 1/2,
             color = navy_blue) +
  labs(x = NULL,
       y = &amp;quot;batting average&amp;quot;) +
  theme(axis.ticks.x = element_blank()) +
  facet_wrap(~facet)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The James-Stein estimator works because of its shrinkage, and the shrinkage factor is called &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. Though in the first parts of the paper, Efron and Morris just told us &lt;span class=&#34;math inline&#34;&gt;\(c = .212\)&lt;/span&gt;, they gave the actual formula for &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; a little later on. If you let &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; be the number of means (i.e., the number of clusters), then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[c = 1 - \frac{(k - 3)\sigma^2}{\sum (y - \overline y)^2}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The difficulty of that formula is we don’t know the value for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. It’s not the sample variance of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; (i.e., &lt;code&gt;var(y)&lt;/code&gt;). An &lt;a href=&#34;https://stats.stackexchange.com/questions/5727/james-stein-estimator-how-did-efron-and-morris-calculate-sigma2-in-shrinkag&#34;&gt;answer to this stackexchange question&lt;/a&gt; helped clarify Efron and Morris were using the formula for the standard error of the estimate,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sqrt{\hat p(1 - \hat p) / n},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which, in the variance metric, is simply&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat p(1 - \hat p) / n.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Following along, we can compute &lt;code&gt;sigma_squared&lt;/code&gt; like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sigma_squared &amp;lt;- mean(baseball$y) * (1 - mean(baseball$y))  / 45)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004332842&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can reproduce the &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; value from the paper.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  select(player, y:c) %&amp;gt;% 
  mutate(squared_deviation = (y - y_bar)^2) %&amp;gt;%
  summarise(c_by_hand = 1 - ((n() - 3) * sigma_squared / sum(squared_deviation)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   c_by_hand
##       &amp;lt;dbl&amp;gt;
## 1     0.212&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-go-bayesian.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let’s go Bayesian.&lt;/h2&gt;
&lt;p&gt;This has been fun. But I don’t recommend you actually use the James-Stein estimator in your research.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The James-Stein estimator is not the only one that is known to be better than the sample averages…&lt;/p&gt;
&lt;p&gt;The search for new estimators continues. Recent efforts [in the 1970s, that is] have been concentrated on achieving results like those obtained with Stein’s method for problems involving distributions other than the normal distribution. Several lines of work, including Stein’s and Robbins’ and more formal &lt;em&gt;Bayesian methods&lt;/em&gt; seem to be converging on a powerful general theory of parameter estimation. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977, p. 127&lt;/a&gt;, &lt;em&gt;emphasis&lt;/em&gt; added)&lt;/span&gt;
The James-Stein estimator is not Bayesian, but it is a precursor to the kind of analyses we now do with Bayesian multilevel models, which pool cluster-level means toward a grand mean. To get a sense of this, we’ll fit a couple models. First, let’s load the &lt;strong&gt;brms&lt;/strong&gt; package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I typically work with the linear regression paradigm. If we were to analyze the &lt;code&gt;baseball&lt;/code&gt; data, we’d use an aggregated binomial mode, which is a particular kind of logistic regression. You can learn more about it &lt;a href=&#34;https://www.youtube.com/watch?v=DyrUkqK9Tj4&amp;amp;t=1581s&amp;amp;frags=pl%2Cwn&#34;&gt;here&lt;/a&gt;. If we wanted a model that corresponded to the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; estimates, above, we’d use &lt;code&gt;hits&lt;/code&gt; as the criterion and allow each player to get his own &lt;em&gt;separate&lt;/em&gt; estimate. Since we’re working within the Bayesian paradigm, we also need to assign priors. In this case, we’ll use a weakly-regularizing &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, 1.5)\)&lt;/span&gt; on the intercepts. See &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;this wiki&lt;/a&gt; for more on weakly-regularizing priors.&lt;/p&gt;
&lt;p&gt;Here’s the code to fit the model with &lt;strong&gt;brms&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_y &amp;lt;-
  brm(data = baseball, 
      family = binomial,
      hits | trials(45) ~ 0 + player,
      prior(normal(0, 1.5), class = b),
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you were curious, that model followed the statistical formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{hits}_i &amp;amp; \sim \operatorname{Binomial} (n = 45, p_i) \\
\operatorname{logit}(p_i) &amp;amp; = \alpha_\text{player} \\
\alpha_\text{player}      &amp;amp; \sim \operatorname{Normal}(0, 1.5),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; is the probability of player &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_\text{player}\)&lt;/span&gt; is a vector of &lt;span class=&#34;math inline&#34;&gt;\(\text{player}\)&lt;/span&gt;-specific intercepts from within the logistic regression model, and each of those intercepts are given a &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, 1.5)\)&lt;/span&gt; prior on the log-odds scale. (If this is all new and confusing, don’t worry. I’ll recommended some resources at the end of this post.)&lt;/p&gt;
&lt;p&gt;For our analogue to the James-Stein estimate &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, we’ll fit the multilevel version of that last model. While each player still gets his own estimate, those estimates are now partially-pooled toward the grand mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_z &amp;lt;-
  brm(data = baseball, 
      family = binomial,
      hits | trials(45) ~ 1 + (1 | player),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = sd)),
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That model followed the statistical formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{hits}_i &amp;amp; \sim \operatorname{Binomial}(n = 45, p_i) \\
\operatorname{logit}(p_i) &amp;amp; = \alpha + \alpha_\text{player} \\
\alpha               &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\ 
\alpha_\text{player} &amp;amp; \sim \operatorname{Normal}(0, \sigma_\text{player}) \\
\sigma_\text{player} &amp;amp; \sim \operatorname{HalfNormal}(0, 1.5),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is the grand mean among the &lt;span class=&#34;math inline&#34;&gt;\(\text{player}\)&lt;/span&gt;-specific intercepts, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_\text{player}\)&lt;/span&gt; is the vector of &lt;span class=&#34;math inline&#34;&gt;\(\text{player}\)&lt;/span&gt;-specific deviations from the grand mean, which are Normally distributed with a mean of zero and a standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\text{player}\)&lt;/span&gt;, which is estimated from the data.&lt;/p&gt;
&lt;p&gt;Here are the model summaries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_y$fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: c40df95ccb9776814804fa2fdc850e74.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##                      mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b_playerAlvis       -1.62    0.00 0.39  -2.42  -1.86  -1.61  -1.35  -0.89  8659 1.00
## b_playerBerry       -0.77    0.00 0.31  -1.40  -0.98  -0.77  -0.56  -0.19  8866 1.00
## b_playerCampaneris  -1.35    0.00 0.36  -2.08  -1.58  -1.33  -1.11  -0.67  9745 1.00
## b_playerClemente    -0.40    0.00 0.31  -1.02  -0.60  -0.40  -0.21   0.20 11477 1.00
## b_playerERodriguez  -1.22    0.00 0.35  -1.92  -1.45  -1.20  -0.99  -0.57  8931 1.00
## b_playerFHoward     -0.58    0.00 0.31  -1.20  -0.78  -0.57  -0.37   0.01  8294 1.00
## b_playerFRobinson   -0.49    0.00 0.30  -1.09  -0.70  -0.49  -0.28   0.09  8854 1.00
## b_playerJohnstone   -0.68    0.00 0.32  -1.31  -0.89  -0.67  -0.46  -0.09  7975 1.00
## b_playerKessinger   -0.89    0.00 0.33  -1.54  -1.11  -0.88  -0.66  -0.26  8191 1.00
## b_playerLAlvarado   -0.99    0.00 0.32  -1.64  -1.20  -0.98  -0.76  -0.39  8147 1.00
## b_playerMunson      -1.48    0.00 0.37  -2.25  -1.72  -1.47  -1.23  -0.78  9573 1.00
## b_playerPetrocelli  -1.22    0.00 0.36  -1.95  -1.45  -1.21  -0.97  -0.55  9342 1.00
## b_playerSanto       -1.09    0.00 0.32  -1.75  -1.31  -1.09  -0.88  -0.49  8616 1.00
## b_playerScott       -1.22    0.00 0.35  -1.92  -1.44  -1.20  -0.98  -0.58 10266 1.00
## b_playerSpencer     -0.77    0.00 0.32  -1.42  -0.98  -0.77  -0.56  -0.16  9060 1.00
## b_playerSwoboda     -1.10    0.00 0.33  -1.79  -1.31  -1.09  -0.87  -0.47  9435 1.00
## b_playerUnser       -1.21    0.00 0.34  -1.89  -1.44  -1.21  -0.97  -0.57  8065 1.00
## b_playerWilliams    -1.22    0.00 0.36  -1.94  -1.46  -1.21  -0.98  -0.56  7829 1.00
## lp__               -73.41    0.08 3.01 -80.13 -75.25 -73.08 -71.26 -68.43  1285 1.01
## 
## Samples were drawn using NUTS(diag_e) at Wed Apr 21 16:39:19 2021.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_z$fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: 668801c962bccfca26059b8ea6970ac4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##                                   mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b_Intercept                      -1.02    0.00 0.09  -1.20  -1.08  -1.02  -0.96  -0.83  3915    1
## sd_player__Intercept              0.17    0.00 0.11   0.01   0.08   0.15   0.24   0.42  1542    1
## r_player[Alvis,Intercept]        -0.12    0.00 0.19  -0.62  -0.21  -0.06   0.00   0.14  2305    1
## r_player[Berry,Intercept]         0.04    0.00 0.16  -0.27  -0.04   0.02   0.12   0.42  4742    1
## r_player[Campaneris,Intercept]   -0.07    0.00 0.17  -0.51  -0.14  -0.04   0.02   0.22  3840    1
## r_player[Clemente,Intercept]      0.14    0.00 0.19  -0.13   0.00   0.08   0.23   0.62  2526    1
## r_player[E.Rodriguez,Intercept]  -0.05    0.00 0.16  -0.43  -0.13  -0.02   0.03   0.24  3733    1
## r_player[F.Howard,Intercept]      0.09    0.00 0.17  -0.19  -0.01   0.05   0.17   0.53  3200    1
## r_player[F.Robinson,Intercept]    0.11    0.00 0.18  -0.15   0.00   0.07   0.20   0.55  3344    1
## r_player[Johnstone,Intercept]     0.07    0.00 0.17  -0.23  -0.02   0.03   0.14   0.49  3796    1
## r_player[Kessinger,Intercept]     0.02    0.00 0.16  -0.30  -0.06   0.01   0.09   0.37  4818    1
## r_player[L.Alvarado,Intercept]    0.00    0.00 0.15  -0.34  -0.07   0.00   0.07   0.32  4687    1
## r_player[Munson,Intercept]       -0.10    0.00 0.18  -0.56  -0.19  -0.05   0.01   0.18  3442    1
## r_player[Petrocelli,Intercept]   -0.05    0.00 0.16  -0.44  -0.13  -0.02   0.03   0.26  5131    1
## r_player[Santo,Intercept]        -0.03    0.00 0.16  -0.39  -0.10  -0.01   0.05   0.30  4883    1
## r_player[Scott,Intercept]        -0.05    0.00 0.17  -0.45  -0.12  -0.02   0.04   0.26  4392    1
## r_player[Spencer,Intercept]       0.05    0.00 0.16  -0.24  -0.03   0.02   0.13   0.42  4149    1
## r_player[Swoboda,Intercept]      -0.02    0.00 0.16  -0.38  -0.10  -0.01   0.05   0.30  5432    1
## r_player[Unser,Intercept]        -0.04    0.00 0.17  -0.45  -0.12  -0.02   0.04   0.26  4419    1
## r_player[Williams,Intercept]     -0.05    0.00 0.16  -0.42  -0.12  -0.02   0.04   0.27  5162    1
## lp__                            -73.87    0.12 4.09 -82.44 -76.44 -73.79 -71.06 -66.48  1112    1
## 
## Samples were drawn using NUTS(diag_e) at Wed Apr 21 16:39:41 2021.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re new to aggregated binomial or logistic regression, those estimates might be confusing. For technical reasons–see &lt;a href=&#34;https://www.youtube.com/watch?v=DyrUkqK9Tj4&amp;amp;t=1430s&amp;amp;frags=pl%2Cwn&#34;&gt;here&lt;/a&gt;–, they’re in the log-odds metric. But we can use the &lt;code&gt;brms::inv_logit_scaled()&lt;/code&gt; function to convert them back to a probability metric. &lt;em&gt;Why would we want a probability metric?&lt;/em&gt;, you might ask. As it turns out, batting average is in a probability metric, too. So you might also think of the &lt;code&gt;inv_logit_scaled()&lt;/code&gt; function as turning the model results into a batting-average metric. For example, if we wanted to get the estimated batting average for E. Rodriguez based on the &lt;code&gt;y_fit&lt;/code&gt; model (i.e., the model corresponding to the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; estimator), we might do something like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit_y)[&amp;quot;playerERodriguez&amp;quot;, 1] %&amp;gt;% 
  inv_logit_scaled()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2282195&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To double check the model returned a sensible estimate, here’s the corresponding &lt;code&gt;y&lt;/code&gt; value from the &lt;code&gt;baseball&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  filter(player == &amp;quot;E Rodriguez&amp;quot;) %&amp;gt;% 
  select(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##       y
##   &amp;lt;dbl&amp;gt;
## 1 0.222&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s a little off, but in the right ballpark. Here is the corresponding estimate from the multilevel model, &lt;code&gt;fit_z&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(fit_z)$player[&amp;quot;E Rodriguez&amp;quot;, 1, ] %&amp;gt;% inv_logit_scaled()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2555493&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And indeed that’s pretty close to the &lt;code&gt;z&lt;/code&gt; value from the &lt;code&gt;baseball&lt;/code&gt; data, too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseball %&amp;gt;% 
  filter(player == &amp;quot;E Rodriguez&amp;quot;) %&amp;gt;% 
  select(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##       z
##   &amp;lt;dbl&amp;gt;
## 1 0.256&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now we have these too competing ways to model the data of the first 45 times at bat, let’s see how well their estimates predict the &lt;code&gt;true_ba&lt;/code&gt; values. We’ll do so with a couple plots. This first one is of the single-level model which did not pool the batting averages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get the `fitted()` draws and wrangle a bit
f_y &amp;lt;-
  baseball %&amp;gt;% 
  distinct(player) %&amp;gt;% 
  add_fitted_draws(fit_y, dpar = &amp;quot;mu&amp;quot;) %&amp;gt;% 
  left_join(baseball %&amp;gt;% 
              select(player, true_ba))
  
# save the plot
p1 &amp;lt;-
  f_y %&amp;gt;% 
  ggplot(aes(x = mu, y = reorder(player, true_ba))) +
  geom_vline(xintercept = mean(baseball$true_ba), color = &amp;quot;white&amp;quot;) +
  stat_interval(.width = .95, alpha = 1/3, color = nw_green) +
  stat_interval(.width = .50, alpha = 1/3, color = nw_green) +
  geom_point(data = baseball,
             aes(x = true_ba),
             size = 2, alpha = 3/4,
             color = navy_blue) +
  labs(subtitle = &amp;quot;fit_y, the no pooling model&amp;quot;,
       x = &amp;quot;batting average&amp;quot;, 
       y = NULL) +
  coord_cartesian(xlim = c(0, .6)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note our use of some handy convenience functions (i.e., &lt;code&gt;add_fitted_draws()&lt;/code&gt; and &lt;code&gt;stat_interval()&lt;/code&gt;) from the &lt;a href=&#34;https://github.com/mjskay/tidybayes&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This second plot is almost the same as the previous one, but this time based on the partial-pooling multilevel model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f_z &amp;lt;-
  baseball %&amp;gt;% 
  distinct(player) %&amp;gt;% 
  add_fitted_draws(fit_z, dpar = &amp;quot;mu&amp;quot;) %&amp;gt;% 
  left_join(baseball %&amp;gt;% 
              select(player, true_ba))
p2 &amp;lt;-
  f_z %&amp;gt;% 
  ggplot(aes(x = mu, y = reorder(player, true_ba))) +
  geom_vline(xintercept = mean(baseball$true_ba), color = &amp;quot;white&amp;quot;) +
  stat_interval(.width = .95, alpha = 1/3, color = nw_green) +
  stat_interval(.width = .50, alpha = 1/3, color = nw_green) +
  geom_point(data = baseball,
             aes(x = true_ba),
             size = 2, alpha = 3/4,
             color = navy_blue) +
  labs(subtitle = &amp;quot;fit_z, the multilevel pooling model&amp;quot;,
       x = &amp;quot;batting average&amp;quot;, 
       y = NULL) +
  coord_cartesian(xlim = c(0, .6)) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we join them together with &lt;strong&gt;patchwork&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-patchwork&#34; role=&#34;doc-biblioref&#34;&gt;Pedersen, 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)
p1 | p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In both panels, the end-of-the-season batting averages (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) are the blue dots. The model-implied estimates are depicted by 95% and 50% interval bands (i.e., the lighter and darker green horizontal lines, respectively). The white line in the background marks off the mean of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Although neither model was perfect, the multilevel model, our analogue to the James-Stein estimates, yielded predictions that appear both more valid and more precise.&lt;/p&gt;
&lt;p&gt;We might also compare the models by their prediction errors. Here we’ll subtract the end-of-the-season batting averages from the model estimates. But unlike with &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt; estimates, above, our &lt;code&gt;fit_y&lt;/code&gt; and &lt;code&gt;fit_z&lt;/code&gt; models yielded entire posterior distributions. Therefore, we’ll express our prediction errors in terms of error distributions, rather than single values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# save the `fit_y` plot
p3 &amp;lt;-
  f_y %&amp;gt;% 
  # the error distribution is just the model-implied values minus 
  # the true end-of-season values
  mutate(error = mu - true_ba)  %&amp;gt;% 
  ggplot(aes(x = error, y = reorder(player, true_ba))) +
  geom_vline(xintercept = c(0, -.2, .2), size = c(1/2, 1/4, 1/4), 
             linetype = c(1, 3, 3), color = &amp;quot;white&amp;quot;) +
  stat_halfeye(point_interval = mean_qi, .width = .95,
               color = navy_blue, fill = alpha(nw_green, 2/3)) +
  coord_cartesian(xlim = c(-.35, .35)) +
  labs(subtitle = &amp;quot;fit_y, the no pooling model&amp;quot;,
       x = &amp;quot;error&amp;quot;, 
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(hjust = .5))
# save the `fit_z` plot
p4 &amp;lt;-
  f_z %&amp;gt;%   
  mutate(error = mu - true_ba)  %&amp;gt;% 
  
  ggplot(aes(x = error, y = reorder(player, true_ba))) +
  geom_vline(xintercept = c(0, -.2, .2), size = c(1/2, 1/4, 1/4), 
             linetype = c(1, 3, 3), color = &amp;quot;white&amp;quot;) +
  stat_halfeye(point_interval = mean_qi, .width = .95,
               color = navy_blue, fill = alpha(nw_green, 2/3)) +
  coord_cartesian(xlim = c(-.35, .35)) +
  labs(subtitle = &amp;quot;fit_z, the multilevel pooling model&amp;quot;,
       x = &amp;quot;error&amp;quot;, 
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(hjust = .5))
# now combine the two and behold
p3 | p4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-stein-s-paradox-and-what-partial-pooling-can-do-for-you/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For consistency, I’ve ordered the players along the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis the same as above. In both panels, we see the prediction error distribution for each player in green and then summarize those distributions in terms of their means and percentile-based 95% intervals. Since these are error distributions, we prefer them to be as close to zero as possible. Although neither model made perfect predictions, the overall errors in the multilevel model were clearly smaller. Much like with the James-Stein estimator, the partial pooling of the multilevel model made for better end-of-the-season estimates.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The paradoxical [consequence of Bayesian multilevel models] is that [they can contradict] this elementary law of statistical theory. If we have [two] or more baseball players, and if we are interested in predicting future batting averages for each of them, then [the Bayesian multilevel model can be better] than simply extrapolating from [the] separate averages. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-efronSteinParadoxStatistics1977&#34; role=&#34;doc-biblioref&#34;&gt;Efron &amp;amp; Morris, 1977&lt;/a&gt;.)&lt;/span&gt;, p. 119]
This is another example of how the &lt;a href=&#34;https://en.wikipedia.org/wiki/KISS_principle&#34;&gt;KISS principle&lt;/a&gt; isn’t always the best bet with data analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;If you’re new to logistic regression, multilevel models or Bayesian statistics, I recommend any of the following texts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;either edition of McElreath’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;&lt;em&gt;Statistical rethinking&lt;/em&gt;&lt;/a&gt;, both editions for which I have &lt;strong&gt;brms&lt;/strong&gt; translations for &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzStatisticalRethinkingBrms2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020a&lt;/a&gt;, &lt;a href=&#34;#ref-kurzStatisticalRethinkingSecondEd2020&#34; role=&#34;doc-biblioref&#34;&gt;2020c&lt;/a&gt;)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;&lt;em&gt;Doing Bayesian data analysis&lt;/em&gt;&lt;/a&gt;, for which I have a &lt;strong&gt;brms&lt;/strong&gt; translation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kurzDoingBayesianData2020&#34; role=&#34;doc-biblioref&#34;&gt;Kurz, 2020b&lt;/a&gt;)&lt;/span&gt;; or&lt;/li&gt;
&lt;li&gt;Gelman and Hill’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanDataAnalysisUsing2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://stat.columbia.edu/~gelman/arm/&#34;&gt;&lt;em&gt;Data analysis using regression and multilevel/hierarchical models&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you choose &lt;em&gt;Statistical rethinking&lt;/em&gt;, do check out &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;these great lectures&lt;/a&gt; on the text.&lt;/p&gt;
&lt;p&gt;Also, don’t miss the provocative &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-davis-stoberWhenAreSample2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; preprint by Davis-Stober, Dana and Rouder, &lt;a href=&#34;https://osf.io/2ukxj/&#34;&gt;&lt;em&gt;When are sample means meaningful? The role of modern estimation in psychological science&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] patchwork_1.1.1 brms_2.15.0     Rcpp_1.0.6      tidybayes_2.3.1 forcats_0.5.1   stringr_1.4.0  
##  [7] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3  
## [13] tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] splines_4.0.4        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   sandwich_3.0-0       xts_0.12.1          
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] viridisLite_0.3.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3       
##  [53] arrayhelpers_1.1-0   ellipsis_0.3.1       pkgconfig_2.0.3      loo_2.4.1           
##  [57] farver_2.0.3         dbplyr_2.0.0         utf8_1.1.4           tidyselect_1.1.0    
##  [61] labeling_0.4.2       rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1       
##  [65] munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [69] generics_0.1.0       broom_0.7.5          ggridges_0.5.2       evaluate_0.14       
##  [73] fastmap_1.0.1        yaml_2.2.1           processx_3.4.5       knitr_1.31          
##  [77] fs_1.5.0             nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [81] xml2_1.3.2           compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [85] rstudioapi_0.13      gamm4_0.2-6          curl_4.3             reprex_0.3.0        
##  [89] statmod_1.4.35       stringi_1.5.3        highr_0.8            ps_1.6.0            
##  [93] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2        
##  [97] nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
## [101] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3    
## [105] httpuv_1.5.4         R6_2.5.0             bookdown_0.21        promises_1.1.1      
## [109] gridExtra_2.3        codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0  
## [113] MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1     withr_2.4.1         
## [117] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [125] rmarkdown_2.7        shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3     
## [129] dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-davis-stoberWhenAreSample2017&#34; class=&#34;csl-entry&#34;&gt;
Davis-Stober, C., Dana, J., &amp;amp; Rouder, J. (2017). &lt;em&gt;When are sample means meaningful? &lt;span&gt;The&lt;/span&gt; role of modern estimation in psychological science&lt;/em&gt;. &lt;span&gt;OSF Preprints&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.31219/osf.io/2ukxj&#34;&gt;https://doi.org/10.31219/osf.io/2ukxj&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-efronEmpiricalBayesJamesStein2010&#34; class=&#34;csl-entry&#34;&gt;
Efron, B. (2010). Empirical &lt;span&gt;Bayes&lt;/span&gt; and the &lt;span&gt;James&lt;/span&gt;-&lt;span&gt;Stein&lt;/span&gt; estimator. In &lt;em&gt;Large-scale inference: &lt;span&gt;Empirical Bayes&lt;/span&gt; methods for estimation, testing, and prediction&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://statweb.stanford.edu/~ckirby/brad/LSI/monograph_CUP.pdf&#34;&gt;https://statweb.stanford.edu/~ckirby/brad/LSI/monograph_CUP.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-efronSteinParadoxStatistics1977&#34; class=&#34;csl-entry&#34;&gt;
Efron, B., &amp;amp; Morris, C. (1977). Stein’s paradox in statistics. &lt;em&gt;Scientific American&lt;/em&gt;, &lt;em&gt;236&lt;/em&gt;(5), 119–127. &lt;a href=&#34;https://doi.org/10.1038/scientificamerican0577-119&#34;&gt;https://doi.org/10.1038/scientificamerican0577-119&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanDataAnalysisUsing2006&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., &amp;amp; Hill, J. (2006). &lt;em&gt;Data analysis using regression and multilevel/hierarchical models&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9780511790942&#34;&gt;https://doi.org/10.1017/CBO9780511790942&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingBrms2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020a). &lt;em&gt;Statistical rethinking with brms, &lt;span class=&#34;nocase&#34;&gt;ggplot2&lt;/span&gt;, and the tidyverse&lt;/em&gt; (version 1.2.0). &lt;a href=&#34;https://doi.org/10.5281/zenodo.3693202&#34;&gt;https://doi.org/10.5281/zenodo.3693202&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzDoingBayesianData2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020b). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis in brms and the tidyverse&lt;/em&gt; (version 0.3.0). &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kurzStatisticalRethinkingSecondEd2020&#34; class=&#34;csl-entry&#34;&gt;
Kurz, A. S. (2020c). &lt;em&gt;Statistical rethinking with brms, Ggplot2, and the tidyverse: &lt;span&gt;Second&lt;/span&gt; edition&lt;/em&gt; (version 0.1.1). &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;https://bookdown.org/content/4857/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-patchwork&#34; class=&#34;csl-entry&#34;&gt;
Pedersen, T. L. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;patchwork&lt;/span&gt;: &lt;span&gt;The&lt;/span&gt; composer of plots&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;https://CRAN.R-project.org/package=patchwork&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Correlations: Let&#39;s Talk Options.</title>
      <link>/post/2019-02-16-bayesian-correlations-let-s-talk-options/</link>
      <pubDate>Sat, 16 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-02-16-bayesian-correlations-let-s-talk-options/</guid>
      <description>
&lt;script src=&#34;/post/2019-02-16-bayesian-correlations-let-s-talk-options/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;[edited Apr 21, 2021]&lt;/p&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;There’s more than one way to fit a Bayesian correlation in brms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;heres-the-deal.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Here’s the deal.&lt;/h2&gt;
&lt;p&gt;In the last post, we considered how we might estimate correlations when our data contain influential outlier values. Our big insight was that if we use variants of Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution as the likelihood rather than the conventional normal distribution, our correlation estimates were less influenced by those outliers. And we mainly did that as Bayesians using the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms package&lt;/a&gt;. Click &lt;a href=&#34;https://solomonkurz.netlify.com/post/bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/&#34;&gt;here&lt;/a&gt; for a refresher.&lt;/p&gt;
&lt;p&gt;Since the brms package is designed to fit regression models, &lt;a href=&#34;https://twitter.com/tjmahr/status/1094808459239981056&#34;&gt;it can be surprising&lt;/a&gt; when you discover it’s handy for correlations, too. In short, you can fit them using a few tricks based on the &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html&#34;&gt;multivariate syntax&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Shortly after uploading the post, it occurred to me we had more options and it might be useful to walk through them a bit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;i-assume-things.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I assume things.&lt;/h2&gt;
&lt;p&gt;For this post, I’m presuming you are vaguely familiar with linear regression–both univariate and multivariate–, have a little background with Bayesian statistics, and have used Paul Bürkner’s brms packge. As you might imagine, all code in is &lt;a href=&#34;https://www.r-bloggers.com/why-use-r-five-reasons/&#34;&gt;R&lt;/a&gt;, with a heavy use of the &lt;a href=&#34;http://style.tidyverse.org&#34;&gt;tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need data.&lt;/h2&gt;
&lt;p&gt;First, we’ll load our main packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mvtnorm)
library(brms)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll use the &lt;a href=&#34;https://cran.r-project.org/web/packages/mvtnorm/index.html&#34;&gt;mvtnorm package&lt;/a&gt; to simulate three positively correlated variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- c(10, 15, 20)  # the means
s &amp;lt;- c(10, 20, 30)  # the sigmas
r &amp;lt;- c(.9, .6, .3)  # the correlations

# here&amp;#39;s the variance/covariance matrix
v &amp;lt;- 
  matrix(c((s[1] * s[1]),        (s[2] * s[1] * r[1]), (s[3] * s[1] * r[2]),
           (s[2] * s[1] * r[1]), (s[2] * s[2]),        (s[3] * s[2] * r[3]),
           (s[3] * s[1] * r[2]), (s[3] * s[2] * r[3]), (s[3] * s[3])),
         nrow = 3, ncol = 3)

# after setting our seed, we&amp;#39;re ready to simulate with `rmvnorm()`
set.seed(1)
d &amp;lt;- 
  rmvnorm(n = 50, mean = m, sigma = v) %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  set_names(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;z&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our data look like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(GGally)
theme_set(theme_gray() +
            theme(panel.grid = element_blank()))

d %&amp;gt;% 
  ggpairs()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-16-bayesian-correlations-let-s-talk-options/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Do note the Pearson’s correlation coefficients in the upper triangle.&lt;/p&gt;
&lt;p&gt;In order to exploit all the methods we’ll cover in this post, we need to standardize our data. Here we do so by hand using the typical formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[z_{x_i} = \frac{x_i - \overline x}{s_x}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\overline x\)&lt;/span&gt; is the observed mean and &lt;span class=&#34;math inline&#34;&gt;\(s_x\)&lt;/span&gt; is the observed standard deviation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;% 
  mutate(x_s = (x - mean(x)) / sd(x),
         y_s = (y - mean(y)) / sd(y),
         z_s = (z - mean(z)) / sd(z))

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##       x     y     z    x_s      y_s    z_s
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1  3.90  11.5 -6.90 -0.723 -0.308   -0.928
## 2 17.7   29.5  4.01  0.758  0.653   -0.512
## 3 20.4   33.8 41.5   1.05   0.886    0.917
## 4 20.3   42.1 34.8   1.04   1.33     0.663
## 5 -3.64 -26.8 43.5  -1.53  -2.36     0.994
## 6 13.9   17.3 47.6   0.347  0.00255  1.15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are at least two broad ways to get correlations out of standardized data in brms. One way uses the typical univariate syntax. The other way is an extension of the multivariate &lt;code&gt;cbind()&lt;/code&gt; approach. Let’s start univariate.&lt;/p&gt;
&lt;p&gt;And for a point of clarification, we’re presuming the Gaussian likelihood for all the examples in this post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Univariate&lt;/h2&gt;
&lt;p&gt;If you fit a simple univariate model with standardized data and a single predictor, the coefficient for the slope will be in a correlation-like metric. Happily, since the data are all standardized, it’s easy to use &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;regularizing priors&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f1 &amp;lt;- 
  brm(data = d, 
      family = gaussian,
      y_s ~ 1 + x_s,
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(normal(0, 1), class = sigma)),
      chains = 4, cores = 4, 
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a look at the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(f1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y_s ~ 1 + x_s 
##    Data: d (Number of observations: 50) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.00      0.06    -0.12     0.12 1.00     3689     2507
## x_s           0.91      0.06     0.79     1.03 1.00     3385     2456
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.42      0.05     0.35     0.52 1.00     3199     2787
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ‘Population-Level Effects’ has the summary information for our intercept and slope. Notice how our &lt;code&gt;x_s&lt;/code&gt; slope is the same as the Pearson’s correlation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(d$x, d$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9119708&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this approach only yields one correlation at a time, we have to fit two more models to get the other two correlations. To do so with haste, we can use the &lt;code&gt;update()&lt;/code&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f2 &amp;lt;-
  update(f1,
         newdata = d,
         formula = z_s ~ 1 + x_s)

f3 &amp;lt;-
  update(f2,
         newdata = d,
         formula = z_s ~ 1 + y_s)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;code&gt;fixef()&lt;/code&gt; function, we can easily isolate the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(f2)[2, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
## 0.5829389 0.1205587 0.3448574 0.8235006&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(f3)[2, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Estimate  Est.Error       Q2.5      Q97.5 
## 0.31107268 0.14237961 0.02815067 0.58747426&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There’s another thing I’d like to point out. Plotting the model results will help make the point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the predictor values you&amp;#39;d like the fitted values for
nd &amp;lt;- tibble(x_s = seq(from = -3, to = 3, length.out = d %&amp;gt;% nrow()))

# wrangle
fitted(f1,
       newdata = nd) %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  
  # plot
  ggplot(aes(x_s)) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_point(data = d,
             aes(y = y_s)) +
  geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = &amp;quot;identity&amp;quot;,
              alpha = 1/4, size = 1/2) +
  coord_cartesian(xlim = range(d$x_s),
                  ylim = range(d$y_s))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-16-bayesian-correlations-let-s-talk-options/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;336&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blue line is the posterior mean and the surrounding gray ribbon depicts the 95% posterior interval. Notice how the data and their respective fitted lines pass through [0, 0]? This is a consequence of modeling standardized data. We should always expect the intercept of a model like this to be 0. Here are the intercept summaries for all three models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(f1)[&amp;quot;Intercept&amp;quot;, ] %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
##     0.000     0.062    -0.123     0.121&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(f2)[&amp;quot;Intercept&amp;quot;, ] %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
##     0.000     0.117    -0.234     0.230&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(f3)[&amp;quot;Intercept&amp;quot;, ] %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Estimate Est.Error      Q2.5     Q97.5 
##     0.002     0.138    -0.267     0.270&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within simulation error, they’re all centered on zero. So instead of estimating the intercept, why not just bake that into the models? Here we refit the models by fixing the intercept for each to zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f4 &amp;lt;-
  update(f1,
         formula = y_s ~ 0 + x_s)

f5 &amp;lt;-
  update(f4,
         newdata = d,
         formula = z_s ~ 0 + x_s)

f6 &amp;lt;-
  update(f4,
         newdata = d,
         formula = z_s ~ 0 + y_s)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the summary for the first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(f4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y_s ~ x_s - 1 
##    Data: d (Number of observations: 50) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## x_s     0.91      0.06     0.79     1.03 1.00     3389     2550
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.42      0.04     0.34     0.52 1.00     3421     2600
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even though it may have seemed like we substantially changed the models by fixing the intercepts to 0, the summaries are essentially the same as when we estimated the intercepts. Here we’ll confirm the summaries with a plot, like above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wrangle
fitted(f4,
       newdata = nd) %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  bind_cols(nd) %&amp;gt;% 
  
  # plot
  ggplot(aes(x_s)) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_point(data = d,
             aes(y = y_s)) +
  geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = &amp;quot;identity&amp;quot;,
              alpha = 1/4, size = 1/2) +
  coord_cartesian(xlim = range(d$x_s),
                  ylim = range(d$y_s))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-16-bayesian-correlations-let-s-talk-options/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;336&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The difference is subtle. By fixing the intercepts at 0, we estimated the slopes (i.e., the correlations) with increased precision as demonstrated by the slightly smaller posterior standard deviations (i.e., the values in the ‘Est.Error’ columns).&lt;/p&gt;
&lt;p&gt;Here are the correlation summaries for those last three models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(f4) %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate Est.Error  Q2.5 Q97.5
## x_s    0.908     0.061 0.788 1.031&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(f5) %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate Est.Error  Q2.5 Q97.5
## x_s    0.581     0.116 0.355 0.801&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(f6) %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate Est.Error  Q2.5 Q97.5
## y_s    0.311     0.135 0.049 0.569&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But anyway, you get the idea. If you want to estimate a correlation in brms using simple univariate syntax, just (a) standardize the data and (b) fit a univariate model with or without an intercept. The slop will be in a correlation-like metric.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-go-multivariate.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let’s go multivariate.&lt;/h2&gt;
&lt;p&gt;If you don’t recall the steps to fit correlations in brms with the multivariate syntax, here they are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;List the variables you’d like correlations for within &lt;code&gt;mvbind()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Place the &lt;code&gt;mvbind()&lt;/code&gt; function within the left side of the model formula.&lt;/li&gt;
&lt;li&gt;On the right side of the model formula, indicate you only want intercepts (i.e., &lt;code&gt;~ 1&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Wrap that whole formula within &lt;code&gt;bf()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Then use the &lt;code&gt;+&lt;/code&gt; operator to append &lt;code&gt;set_rescor(TRUE)&lt;/code&gt;, which will ensure brms fits a model with residual correlations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, you you want to use non-default priors, you’ll want to use the &lt;code&gt;resp&lt;/code&gt; argument to specify which prior is associated with which criterion variable. Here’s what that all looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f7 &amp;lt;- 
  brm(data = d, 
      family = gaussian,
      bf(mvbind(x_s, y_s, z_s) ~ 1) + set_rescor(TRUE),
      prior = c(prior(normal(0, 1), class = Intercept, resp = xs),
                prior(normal(0, 1), class = Intercept, resp = ys),
                prior(normal(0, 1), class = Intercept, resp = zs),
                prior(normal(1, 1), class = sigma, resp = xs),
                prior(normal(1, 1), class = sigma, resp = ys),
                prior(normal(1, 1), class = sigma, resp = zs),
                prior(lkj(2), class = rescor)),
      chains = 4, cores = 4, 
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Behold the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(f7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: x_s ~ 1 
##          y_s ~ 1 
##          z_s ~ 1 
##    Data: d (Number of observations: 50) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## xs_Intercept     0.00      0.14    -0.27     0.27 1.00     2031     2446
## ys_Intercept     0.00      0.14    -0.27     0.28 1.00     2288     2677
## zs_Intercept     0.00      0.15    -0.28     0.29 1.00     2697     2728
## 
## Family Specific Parameters: 
##          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_xs     0.99      0.10     0.82     1.19 1.00     1999     2074
## sigma_ys     1.00      0.10     0.83     1.21 1.00     2265     2463
## sigma_zs     1.03      0.11     0.85     1.26 1.00     3137     2530
## 
## Residual Correlations: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(xs,ys)     0.89      0.03     0.83     0.94 1.00     2458     2815
## rescor(xs,zs)     0.55      0.09     0.35     0.72 1.00     3039     2828
## rescor(ys,zs)     0.26      0.13    -0.00     0.49 1.00     2733     2530
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at the ‘Residual Correlations:’ section at the bottom of the output. Since there are no predictors in the model, the residual correlations are just correlations. Now notice how the intercepts in this model are also hovering around 0, just like in our univariate models. Yep, we can fix those, too. We do this by changing our formula to &lt;code&gt;mvbind(x_s, y_s, z_s) ~ 0&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f8 &amp;lt;- 
  brm(data = d, 
      family = gaussian,
      bf(mvbind(x_s, y_s, z_s) ~ 0) + set_rescor(TRUE),
      prior = c(prior(normal(1, 1), class = sigma, resp = xs),
                prior(normal(1, 1), class = sigma, resp = ys),
                prior(normal(1, 1), class = sigma, resp = zs),
                prior(lkj(2), class = rescor)),
      chains = 4, cores = 4, 
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Without the intercepts, the rest of the model is the same within simulation variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(f8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: x_s ~ 0 
##          y_s ~ 0 
##          z_s ~ 0 
##    Data: d (Number of observations: 50) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Family Specific Parameters: 
##          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_xs     0.98      0.09     0.81     1.18 1.00     1967     2063
## sigma_ys     0.99      0.10     0.82     1.19 1.00     2204     1917
## sigma_zs     1.01      0.10     0.83     1.24 1.00     2690     2817
## 
## Residual Correlations: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(xs,ys)     0.90      0.03     0.83     0.94 1.00     2441     2328
## rescor(xs,zs)     0.55      0.09     0.35     0.71 1.00     2739     2438
## rescor(ys,zs)     0.26      0.12     0.01     0.48 1.00     2608     2468
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you wanna get silly, we can prune even further. Did you notice how the estimates for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; are all hovering around 1? Since we have no predictors, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is just an estimate of the population standard deviation. And since we’re working with standardized data, the population standard deviation has to be 1. Any other estimate would be nonsensical. So why not fix it to 1?&lt;/p&gt;
&lt;p&gt;With brms, we can fix those &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;’s to 1 with a trick of the nonlinear &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html&#34;&gt;distributional modeling syntax&lt;/a&gt;. Recall when you model &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, the brms default is to actually model its log. As is turns out, the log of 1 is zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how to make use of that within &lt;code&gt;brm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f9 &amp;lt;- 
  brm(data = d, 
      family = gaussian,
      bf(mvbind(x_s, y_s, z_s) ~ 0,
         sigma ~ 0) + 
        set_rescor(TRUE),
      prior(lkj(2), class = rescor),
      chains = 4, cores = 4, 
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(f9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian, gaussian) 
##   Links: mu = identity; sigma = log
##          mu = identity; sigma = log
##          mu = identity; sigma = log 
## Formula: x_s ~ 0 
##          sigma ~ 0
##          y_s ~ 0 
##          sigma ~ 0
##          z_s ~ 0 
##          sigma ~ 0
##    Data: d (Number of observations: 50) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Residual Correlations: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(xs,ys)     0.90      0.02     0.87     0.93 1.00     3719     3031
## rescor(xs,zs)     0.57      0.07     0.42     0.69 1.00     3047     2773
## rescor(ys,zs)     0.29      0.09     0.11     0.46 1.00     2839     2615
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlations are the only things left in the model.&lt;/p&gt;
&lt;p&gt;Just to be clear, the multivariate approach does not require standardized data. To demonstrate, here we refit &lt;code&gt;f7&lt;/code&gt;, but with the unstandardized variables. And, since we’re no longer in the standardized metric, we’ll be less certain with our priors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f10 &amp;lt;- 
  brm(data = d, 
      family = gaussian,
      bf(mvbind(x, y, z) ~ 1) + set_rescor(TRUE),
      prior = c(prior(normal(0, 10), class = Intercept, resp = x),
                prior(normal(0, 10), class = Intercept, resp = y),
                prior(normal(0, 10), class = Intercept, resp = z),
                prior(student_t(3, 0, 10), class = sigma, resp = x),
                prior(student_t(3, 0, 10), class = sigma, resp = y),
                prior(student_t(3, 0, 10), class = sigma, resp = z),
                prior(lkj(2), class = rescor)),
      chains = 4, cores = 4, 
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See, the ‘rescor()’ results are about the same as with &lt;code&gt;f7&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(f10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: x ~ 1 
##          y ~ 1 
##          z ~ 1 
##    Data: d (Number of observations: 50) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## x_Intercept     9.63      1.22     7.14    12.02 1.00     1941     2016
## y_Intercept    15.60      2.49    10.63    20.30 1.00     2238     2441
## z_Intercept    14.71      3.58     7.65    21.51 1.00     3021     2316
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_x     8.93      0.84     7.44    10.70 1.00     2263     2515
## sigma_y    18.13      1.73    15.13    21.77 1.00     2553     2793
## sigma_z    26.15      2.58    21.78    31.92 1.00     2626     2198
## 
## Residual Correlations: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(x,y)     0.89      0.03     0.82     0.94 1.00     2540     2630
## rescor(x,z)     0.54      0.09     0.34     0.70 1.00     3122     3224
## rescor(y,z)     0.24      0.12    -0.01     0.47 1.00     2689     2829
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;its-time-to-compare-methods.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;It’s time to compare methods.&lt;/h2&gt;
&lt;p&gt;To recap, we’ve compared several ways to fit correlations in brms. Some of the methods were with univariate syntax, others were with the multivariate syntax. Some of the models had all free parameters, others included fixed intercepts and sigmas. Whereas all the univariate models required standardized data, the multivariate approach can work with unstandardized data, too.&lt;/p&gt;
&lt;p&gt;Now it might be of help to compare the results from each of the methods to get a sense of which ones you might prefer. Before we do so, we’ll define a couple custom functions to streamline the data wrangling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rho &amp;lt;- function(fit) {
  posterior_samples(fit) %&amp;gt;% 
    select(starts_with(&amp;quot;b_&amp;quot;), -contains(&amp;quot;Intercept&amp;quot;)) %&amp;gt;% 
    set_names(&amp;quot;rho&amp;quot;) 
}

get_rescor &amp;lt;- function(fit) {
  posterior_samples(fit) %&amp;gt;% 
    select(starts_with(&amp;quot;rescor&amp;quot;)) %&amp;gt;% 
    set_names(&amp;quot;x with y&amp;quot;, &amp;quot;x with z&amp;quot;, &amp;quot;y with z&amp;quot;) %&amp;gt;% 
    gather(label, rho) %&amp;gt;% 
    select(rho, label)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s put those functions to work and plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

# collect the posteriors from the univariate models
tibble(name = str_c(&amp;quot;f&amp;quot;, 1:6)) %&amp;gt;% 
  mutate(fit = map(name, get)) %&amp;gt;% 
  mutate(rho = map(fit, get_rho)) %&amp;gt;% 
  unnest(rho) %&amp;gt;% 
  mutate(predictor = rep(c(&amp;quot;x&amp;quot;, &amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;), each = 4000) %&amp;gt;% rep(., times = 2),
         criterion = rep(c(&amp;quot;y&amp;quot;, &amp;quot;z&amp;quot;, &amp;quot;z&amp;quot;), each = 4000) %&amp;gt;% rep(., times = 2)) %&amp;gt;% 
  mutate(label = str_c(predictor, &amp;quot; with &amp;quot;, criterion)) %&amp;gt;% 
  select(-c(predictor:criterion)) %&amp;gt;% 
  # add in the posteriors from the multivariate models
  bind_rows(
    tibble(name = str_c(&amp;quot;f&amp;quot;, 7:10)) %&amp;gt;% 
      mutate(fit = map(name, get)) %&amp;gt;% 
      mutate(post = map(fit, get_rescor)) %&amp;gt;% 
      unnest(post)
  ) %&amp;gt;% 
  # wrangle a bit just to make the y axis easier to understand
  mutate(name = factor(name, 
                       levels = c(str_c(&amp;quot;f&amp;quot;, 1:10)),
                       labels = c(&amp;quot;1. standardized, univariate&amp;quot;,
                                  &amp;quot;2. standardized, univariate&amp;quot;,
                                  &amp;quot;3. standardized, univariate&amp;quot;,
                                  &amp;quot;4. standardized, univariate, fixed intercepts&amp;quot;,
                                  &amp;quot;5. standardized, univariate, fixed intercepts&amp;quot;,
                                  &amp;quot;6. standardized, univariate, fixed intercepts&amp;quot;,
                                  &amp;quot;7. standardized, multivariate, fixed intercepts&amp;quot;,
                                  &amp;quot;8. standardized, multivariate, fixed intercepts&amp;quot;,
                                  &amp;quot;9. standardized, multivariate, fixed intercepts/sigmas&amp;quot;,
                                  &amp;quot;10. unstandardized, multivariate&amp;quot;))) %&amp;gt;%
  
  # plot
  ggplot(aes(x = rho, y = name)) +
  geom_vline(data = tibble(label = c(&amp;quot;x with y&amp;quot;, &amp;quot;x with z&amp;quot;, &amp;quot;y with z&amp;quot;),
                           rho   = r),
             aes(xintercept = rho), color = &amp;quot;white&amp;quot;) +
  stat_halfeye(.width = .95, size = 5/4) +
  scale_x_continuous(breaks = c(0, r)) +
  labs(x = expression(rho),
       y = NULL) +
  coord_cartesian(0:1) +
  theme(axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0)) +
  facet_wrap(~ label, ncol = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-16-bayesian-correlations-let-s-talk-options/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To my eye, a few patterns emerged. First, the point estimates were about the same across methods. Second, fixing the intercepts didn’t seem to effect things, much. But, third, it appears that fixing the sigmas in the multivariate models did narrow the posteriors a bit.&lt;/p&gt;
&lt;p&gt;Fourth, and perhaps most importantly, notice how the posteriors for the multivariate models were more asymmetric when they approached 1. Hopefully this makes intuitive sense. Correlations are bound between -1 and 1. However, standardized regression coefficients are not so bound. Accordingly, notice how the posteriors from the univariate models stayed symmetric when approaching 1 and some of their right tails even crossed over 1. So while the univariate approach did a reasonable job capturing the correlation point estimates, their posteriors weren’t quite in a correlation metric. Alternately, the univariate approach did make it convenient to express the correlations with fitted regression lines in scatter plots.&lt;/p&gt;
&lt;p&gt;Both univariate and multivariate approaches appear to have their strengths and weaknesses. Choose which methods seems most appropriate for your correlation needs.&lt;/p&gt;
&lt;p&gt;Happy modeling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 GGally_2.1.1    forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
##  [9] tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0 brms_2.15.0    
## [13] Rcpp_1.0.6      mvtnorm_1.1-1  
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.3         splines_4.0.4       
##   [7] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0  
##  [19] xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [22] colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1         
##  [28] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2          
##  [34] gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2        
##  [40] abind_1.4-5          scales_1.1.1         DBI_1.1.0           
##  [43] miniUI_0.1.1.1       xtable_1.8-4         stats4_4.0.4        
##  [46] StanHeaders_2.21.0-7 DT_0.16              htmlwidgets_1.5.2   
##  [49] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [52] RColorBrewer_1.1-2   ellipsis_0.3.1       farver_2.0.3        
##  [55] reshape_0.8.8        pkgconfig_2.0.3      loo_2.4.1           
##  [58] dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2      
##  [61] tidyselect_1.1.0     rlang_0.4.10         reshape2_1.4.4      
##  [64] later_1.1.0.1        munsell_0.5.0        cellranger_1.1.0    
##  [67] tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [70] broom_0.7.5          ggridges_0.5.2       evaluate_0.14       
##  [73] fastmap_1.0.1        yaml_2.2.1           processx_3.4.5      
##  [76] knitr_1.31           fs_1.5.0             nlme_3.1-152        
##  [79] mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [82] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [85] rstudioapi_0.13      curl_4.3             gamm4_0.2-6         
##  [88] reprex_0.3.0         statmod_1.4.35       stringi_1.5.3       
##  [91] highr_0.8            ps_1.6.0             blogdown_1.3        
##  [94] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2        
##  [97] nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0       
## [100] vctrs_0.3.6          pillar_1.5.1         lifecycle_1.0.0     
## [103] bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [106] R6_2.5.0             bookdown_0.21        promises_1.1.1      
## [109] gridExtra_2.3        codetools_0.2-18     boot_1.3-26         
## [112] colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [115] assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [118] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4         
## [124] minqa_1.2.4          rmarkdown_2.7        shiny_1.5.0         
## [127] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian robust correlations with brms (and why you should love Student&#39;s $t$)</title>
      <link>/post/2019-02-10-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-02-10-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/</guid>
      <description>
&lt;script src=&#34;/post/2019-02-10-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;[edited Apr 21, 2021]&lt;/p&gt;
&lt;p&gt;In this post, we’ll show how Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution can produce better correlation estimates when your data have outliers. As is often the case, we’ll do so as Bayesians.&lt;/p&gt;
&lt;p&gt;This post is a direct consequence of Adrian Baez-Ortega’s great blog, “&lt;a href=&#34;https://baezortega.github.io/2018/05/28/robust-correlation/&#34;&gt;Bayesian robust correlation with Stan in R (and why you should use Bayesian methods)&lt;/a&gt;”. Baez-Ortega worked out the approach and code for direct use with &lt;a href=&#34;http://mc-stan.org&#34;&gt;Stan&lt;/a&gt; computational environment. That solution is great because Stan is free, open source, and very flexible. However, Stan’s interface might be prohibitively technical for non-statistician users. Happily, the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt; package allows users to access the computational power of Stan through a simpler interface. In this post, we show how to extend Baez-Ortega’s method to brms. To pay respects where they’re due, the synthetic data, priors, and other model settings are largely the same as those Baez-Ortega used in his blog.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I make assumptions&lt;/h2&gt;
&lt;p&gt;For this post, I’m presuming you are vaguely familiar with linear regression, know about the basic differences between frequentist and Bayesian approaches to fitting models, and have a sense that the issue of outlier values is a pickle worth contending with. All code in is &lt;a href=&#34;https://www.r-bloggers.com/why-use-r-five-reasons/&#34;&gt;R&lt;/a&gt;, with a heavy use of the &lt;a href=&#34;http://style.tidyverse.org&#34;&gt;tidyverse&lt;/a&gt;–which you might learn a lot about &lt;a href=&#34;http://r4ds.had.co.nzhttp://r4ds.had.co.nz&#34;&gt;here, especially chapter 5&lt;/a&gt;–, and, of course, Bürkner’s &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you’d like a warmup, consider checking out my related post, &lt;a href=&#34;https://solomonkurz.netlify.com/post/robust-linear-regression-with-the-robust-student-s-t-distribution/&#34;&gt;Robust Linear Regression with Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-Distribution&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-the-deal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’s the deal?&lt;/h2&gt;
&lt;p&gt;Pearson’s correlations are designed to quantify the linear relationship between two normally distributed variables. The normal distribution and its multivariate generalization, the multivariate normal distribution, are sensitive to outliers. When you have well-behaved synthetic data, this isn’t an issue. But if you work real-world data, this can be a problem. One can have data for which the vast majority of cases are well-characterized by a nice liner relationship, but have a few odd cases for which that relationship does not hold. And if those odd cases happen to be overly influential–sometimes called leverage points–the resulting Pearson’s correlation coefficient might look off.&lt;/p&gt;
&lt;p&gt;Recall that the normal distribution is a special case of Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution with the &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; parameter (i.e., &lt;em&gt;nu&lt;/em&gt;, degree of freedom) set to infinity. As it turns out, when &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; is small, Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution is more robust to multivariate outliers. It’s less influenced by them. I’m not going to cover why in any detail. For that you’ve got &lt;a href=&#34;https://baezortega.github.io/2018/05/28/robust-correlation/&#34;&gt;Baez-Ortega’s blog&lt;/a&gt;, an even earlier blog from &lt;a href=&#34;http://www.sumsar.net/blog/2013/08/bayesian-estimation-of-correlation/&#34;&gt;Rasmus Bååth&lt;/a&gt;, and textbook treatments on the topic by &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/arm/&#34;&gt;Gelman &amp;amp; Hill (2007, chapter 6)&lt;/a&gt; and &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;Kruschke (2015, chapter 16)&lt;/a&gt;. Here we’ll get a quick sense of how vulnerable Pearson’s correlations–with their reliance on the Gaussian–are to outliers, we’ll demonstrate how fitting correlations within the Bayesian paradigm using the conventional Gaussian likelihood is similarly vulnerable to distortion, and then see how Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution can save the day. And importantly, we’ll do the bulk of this with the brms package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need data&lt;/h2&gt;
&lt;p&gt;To start off, we’ll make a multivariate normal simulated data set using the same steps Baez-Ortega’s used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mvtnorm)
library(tidyverse)

sigma &amp;lt;- c(20, 40)  # the variances
rho   &amp;lt;- -.95       # the desired correlation

# here&amp;#39;s the variance/covariance matrix
cov.mat &amp;lt;- 
  matrix(c(sigma[1] ^ 2,
           sigma[1] * sigma[2] * rho,
           sigma[1] * sigma[2] * rho,
           sigma[2] ^ 2),
         nrow = 2, byrow = T)

# after setting our seed, we&amp;#39;re ready to simulate with `rmvnorm()`
set.seed(210191)
x.clean &amp;lt;- 
  rmvnorm(n = 40, sigma = cov.mat) %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  rename(x = V1,
         y = V2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we make our second data set, &lt;code&gt;x.noisy&lt;/code&gt;, which is identical to our well-behaved &lt;code&gt;x.clean&lt;/code&gt; data, but with the first three cases transformed to outlier values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.noisy &amp;lt;- x.clean

x.noisy[1:3,] &amp;lt;-
  matrix(c(-40, -60,
           20, 100,
           40, 40),
         nrow = 3, byrow = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we’ll add an &lt;code&gt;outlier&lt;/code&gt; index to the data sets, which will help us with plotting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.clean &amp;lt;-
  x.clean %&amp;gt;% 
  mutate(outlier = factor(0))

x.noisy &amp;lt;- 
  x.noisy %&amp;gt;% 
  mutate(outlier = c(rep(1, 3), rep(0, 37)) %&amp;gt;% as.factor(.))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below shows what the &lt;code&gt;x.clean&lt;/code&gt; data look like. I’m a fan of &lt;a href=&#34;http://fivethirtyeight.com&#34;&gt;FiveThirtyEight&lt;/a&gt;, so we’ll use a few convenience functions from the handy &lt;a href=&#34;https://github.com/jrnold/ggthemes&#34;&gt;ggthemes package&lt;/a&gt; to give our plots a FiveThirtyEight-like feel.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggthemes)

x.clean %&amp;gt;% 
  ggplot(aes(x = x, y = y, color = outlier, fill = outlier)) +
  geom_point() +
  stat_ellipse(geom = &amp;quot;polygon&amp;quot;, alpha = .15, size = .15, level = .5) +
  stat_ellipse(geom = &amp;quot;polygon&amp;quot;, alpha = .15, size = .15, level = .95) +
  scale_color_fivethirtyeight() +
  scale_fill_fivethirtyeight() +
  coord_cartesian(xlim = c(-50, 50),
                  ylim = c(-100, 100)) +
  theme_fivethirtyeight() +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-10-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;312&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here are the &lt;code&gt;x.noisy&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.noisy %&amp;gt;% 
  ggplot(aes(x = x, y = y, color = outlier, fill = outlier)) +
  geom_point() +
  stat_ellipse(geom = &amp;quot;polygon&amp;quot;, alpha = .15, size = .15, level = .5) +
  stat_ellipse(geom = &amp;quot;polygon&amp;quot;, alpha = .15, size = .15, level = .95) +
  scale_color_fivethirtyeight() +
  scale_fill_fivethirtyeight() +
  coord_cartesian(xlim = c(-50, 50),
                  ylim = c(-100, 100)) +
  theme_fivethirtyeight() +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-10-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;312&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The three outliers are in red. Even in their presence, the old interocular trauma test suggests there is a pronounced overall trend in the data. I would like a correlation procedure that’s capable of capturing that overall trend. Let’s examine some candidates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-does-old-pearson-hold-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How does old Pearson hold up?&lt;/h2&gt;
&lt;p&gt;A quick way to get a Pearson’s correlation coefficient in R is with the &lt;code&gt;cor()&lt;/code&gt; function, which does a nice job recovering the correlation we simulated the &lt;code&gt;x.clean&lt;/code&gt; data with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x.clean$x, x.clean$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.959702&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, things fall apart if you use &lt;code&gt;cor()&lt;/code&gt; on the &lt;code&gt;x.noisy&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x.noisy$x, x.noisy$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.6365649&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So even though most of the &lt;code&gt;x.noisy&lt;/code&gt; data continue to show a clear strong relation, three outlier values reduced the Pearson’s correlation a third of the way toward zero. Let’s see what happens when we go Bayesian.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-correlations-in-brms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian correlations in brms&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/paulbuerkner&#34;&gt;Bürkner&lt;/a&gt;’s brms is a general purpose interface for fitting all manner of Bayesian regression models with &lt;a href=&#34;https://mc-stan.org&#34;&gt;Stan&lt;/a&gt; as the engine under the hood. It has popular &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/index.html&#34;&gt;lme4&lt;/a&gt;-like syntax and offers a variety of convenience functions for post processing. Let’s load it up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;first-with-the-gaussian-likelihood.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;First with the Gaussian likelihood.&lt;/h3&gt;
&lt;p&gt;I’m not going to spend a lot of time walking through the syntax in the main brms function, &lt;code&gt;brm()&lt;/code&gt;. You can learn all about that &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;here&lt;/a&gt; or with my ebook &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking with brms, ggplot2, and the tidyverse&lt;/em&gt;&lt;/a&gt;. But our particular use of &lt;code&gt;brm()&lt;/code&gt; requires we make a few fine points.&lt;/p&gt;
&lt;p&gt;One doesn’t always think about bivariate correlations within the regression paradigm. But they work just fine. Within brms, you would typically specify the conventional Gaussian likelihood (i.e., &lt;code&gt;family = gaussian&lt;/code&gt;), use the &lt;code&gt;mvbind()&lt;/code&gt; syntax to set up a &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html&#34;&gt;multivariate model&lt;/a&gt;, and fit that model without predictors. For each variable specified in &lt;code&gt;cbind()&lt;/code&gt;, you’ll estimate an intercept (i.e., mean, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) and sigma (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, often called a residual variance). Since there are no predictors in the model, the residual variance is just the variance and the brms default for multivariate models is to allow the residual variances to covary. But since variances are parameterized in the standard deviation metric in brms, the residual variances and their covariance are &lt;em&gt;SD&lt;/em&gt;s and their correlation, respectively.&lt;/p&gt;
&lt;p&gt;Here’s what it looks like in practice.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f0 &amp;lt;- 
  brm(data = x.clean, 
      family = gaussian,
      bf(mvbind(x, y) ~ 1) + set_rescor(TRUE),
      prior = c(prior(normal(0, 100), class = Intercept, resp = x),
                prior(normal(0, 100), class = Intercept, resp = y),
                prior(normal(0, 100), class = sigma, resp = x),
                prior(normal(0, 100), class = sigma, resp = y),
                prior(lkj(1), class = rescor)),
      iter = 2000, warmup = 500, chains = 4, cores = 4, 
      seed = 210191)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a typical Bayesian workflow, you’d examine the quality of the chains with trace plots. The easy way to do that in brms is with &lt;code&gt;plot()&lt;/code&gt;. E.g., to get the trace plots for our first model, you’d code &lt;code&gt;plot(f0)&lt;/code&gt;. Happily, the trace plots look fine for all models in this post. For the sake of space, I’ll leave their inspection as exercises for interested readers.&lt;/p&gt;
&lt;p&gt;Our priors and such mirror those in Baez-Ortega’s blog. Here are the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(f0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: x ~ 1 
##          y ~ 1 
##    Data: x.clean (Number of observations: 40) 
## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1;
##          total post-warmup samples = 6000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## x_Intercept    -2.85      3.40    -9.28     3.81 1.00     2449     2471
## y_Intercept     3.69      6.80    -9.80    16.64 1.00     2428     2368
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_x    21.49      2.60    17.25    27.39 1.00     2051     2251
## sigma_y    43.01      5.18    34.59    54.89 1.00     2102     2226
## 
## Residual Correlations: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(x,y)    -0.95      0.02    -0.98    -0.92 1.00     2146     2715
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Way down there in the last line in the ‘Family Specific Parameters’ section we have &lt;code&gt;rescor(x,y)&lt;/code&gt;, which is our correlation. And indeed, our Gaussian intercept-only multivariate model did a great job recovering the correlation we used to simulate the &lt;code&gt;x.clean&lt;/code&gt; data with. Look at what happens when we try this approach with &lt;code&gt;x.noisy&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f1 &amp;lt;-
  update(f0,
         newdata = x.noisy,
         iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 210191)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(f1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: x ~ 1 
##          y ~ 1 
##    Data: x.noisy (Number of observations: 40) 
## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1;
##          total post-warmup samples = 6000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## x_Intercept    -3.05      3.84   -10.60     4.44 1.00     4935     4170
## y_Intercept     6.71      7.59    -8.26    21.54 1.00     4832     4362
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_x    23.64      2.76    19.01    29.78 1.00     3699     3844
## sigma_y    47.17      5.54    37.86    59.66 1.00     4058     3752
## 
## Residual Correlations: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(x,y)    -0.61      0.10    -0.78    -0.39 1.00     3682     4159
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the correlation estimate is -.61. As it turns out, &lt;code&gt;data = x.noisy&lt;/code&gt; + &lt;code&gt;family = gaussian&lt;/code&gt; in &lt;code&gt;brm()&lt;/code&gt; failed us just like Pearson’s correlation failed us. Time to leave failure behind.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;now-with-students-t-distribution.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Now with Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution.&lt;/h3&gt;
&lt;p&gt;Before we jump into using &lt;code&gt;family = student&lt;/code&gt;, we should talk a bit about &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;. This is our new parameter which is silently fixed to infinity when we use the Gaussian likelihood. The &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; parameter is bound at zero but, as discussed in Baez-Ortega’s blog, is somewhat nonsensical for values below 1. As it turns out, &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; is constrained to be equal to or greater than 1 in brms. So nothing for us to worry about, there. The &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;Stan team currently recommends the gamma(2, 0.1) prior for &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;&lt;/a&gt;, which is also the current brms default. This is what that distribution looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(x = seq(from = 1, to = 120, by = .5)) %&amp;gt;% 
  ggplot(aes(x = x, fill = factor(0))) +
  geom_ribbon(aes(ymin = 0, 
                  ymax = dgamma(x, 2, 0.1))) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_fill_fivethirtyeight() +
  coord_cartesian(xlim = c(0, 100)) +
  ggtitle(&amp;quot;gamma(2, 0.1)&amp;quot;) +
  theme_fivethirtyeight() +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-10-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So gamma(2, 0.1) should gently push the &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; posterior toward low values, but it’s slowly-sloping right tail will allow higher values to emerge.&lt;/p&gt;
&lt;p&gt;Following the Stan team’s recommendation, the brms default and Baez-Ortega’s blog, here’s our robust Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; model for the &lt;code&gt;x.noisy&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f2 &amp;lt;- 
  brm(data = x.noisy, 
      family = student,
      bf(mvbind(x, y) ~ 1) + set_rescor(TRUE),
      prior = c(prior(gamma(2, .1), class = nu),
                prior(normal(0, 100), class = Intercept, resp = x),
                prior(normal(0, 100), class = Intercept, resp = y),
                prior(normal(0, 100), class = sigma, resp = x),
                prior(normal(0, 100), class = sigma, resp = y),
                prior(lkj(1), class = rescor)),
      iter = 2000, warmup = 500, chains = 4, cores = 4, 
      seed = 210191)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(f2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(student, student) 
##   Links: mu = identity; sigma = identity; nu = identity
##          mu = identity; sigma = identity; nu = identity 
## Formula: x ~ 1 
##          y ~ 1 
##    Data: x.noisy (Number of observations: 40) 
## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1;
##          total post-warmup samples = 6000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## x_Intercept    -2.07      3.59    -9.49     4.72 1.00     2412     2651
## y_Intercept     1.93      7.20   -11.31    16.81 1.00     2454     2815
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_x    18.35      2.99    13.12    24.76 1.00     2313     2816
## sigma_y    36.52      5.90    26.13    49.49 1.00     2216     3225
## nu          2.65      0.99     1.36     4.99 1.00     3500     2710
## nu_x        1.00      0.00     1.00     1.00 1.00     6000     6000
## nu_y        1.00      0.00     1.00     1.00 1.00     6000     6000
## 
## Residual Correlations: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(x,y)    -0.93      0.03    -0.97    -0.85 1.00     2974     3366
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whoa, look at that correlation, &lt;code&gt;rescore(x,y)&lt;/code&gt;! It’s right about what we’d hope for. Sure, it’s not a perfect -.95, but that’s way better than -.61.&lt;/p&gt;
&lt;p&gt;While we’re at it, we may as well see what happens when we fit a Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; model when we have perfectly multivariate normal data. Here it is with the &lt;code&gt;x.clean&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f3 &amp;lt;- 
  update(f2,
         newdata = x.clean, 
         iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 210191)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(f3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(student, student) 
##   Links: mu = identity; sigma = identity; nu = identity
##          mu = identity; sigma = identity; nu = identity 
## Formula: x ~ 1 
##          y ~ 1 
##    Data: x.clean (Number of observations: 40) 
## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1;
##          total post-warmup samples = 6000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## x_Intercept    -2.31      3.45    -9.10     4.41 1.00     2819     3208
## y_Intercept     2.63      6.85   -10.82    16.16 1.00     2813     2882
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_x    20.75      2.59    16.29    26.28 1.00     2504     3202
## sigma_y    41.29      5.19    32.31    52.36 1.00     2596     3424
## nu         22.63     14.11     5.42    58.63 1.00     4002     3228
## nu_x        1.00      0.00     1.00     1.00 1.00     6000     6000
## nu_y        1.00      0.00     1.00     1.00 1.00     6000     6000
## 
## Residual Correlations: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(x,y)    -0.96      0.01    -0.98    -0.92 1.00     3147     3684
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So when you don’t need Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, it yields the right answer anyways. That’s a nice feature.&lt;/p&gt;
&lt;p&gt;We should probably compare the posteriors of the correlations across the four models. First we’ll collect the posterior samples into a tibble.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  tibble(model = str_c(&amp;quot;f&amp;quot;, 0:3)) %&amp;gt;% 
  mutate(fit = map(model, get)) %&amp;gt;% 
  mutate(post = map(fit, posterior_samples)) %&amp;gt;% 
  unnest(post)

head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 11
##   model fit       b_x_Intercept b_y_Intercept sigma_x sigma_y rescor__x__y  lp__
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;gt;            &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 f0    &amp;lt;brmsfit&amp;gt;         -6.60        14.2      21.6    46.9       -0.968 -355.
## 2 f0    &amp;lt;brmsfit&amp;gt;         -4.85         8.20     19.5    42.5       -0.941 -353.
## 3 f0    &amp;lt;brmsfit&amp;gt;         -1.35        -0.678    19.8    37.9       -0.955 -352.
## 4 f0    &amp;lt;brmsfit&amp;gt;         -6.02         9.94     22.9    46.0       -0.963 -352.
## 5 f0    &amp;lt;brmsfit&amp;gt;         -9.25        13.8      24.9    45.6       -0.966 -355.
## 6 f0    &amp;lt;brmsfit&amp;gt;         -5.31         6.55     23.3    43.0       -0.955 -353.
## # … with 3 more variables: nu &amp;lt;dbl&amp;gt;, nu_x &amp;lt;dbl&amp;gt;, nu_y &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the posterior draws in hand, we just need to wrangle a bit before showing the correlation posteriors in a coefficient plot. To make things easier, we’ll do so with a couple convenience functions from the &lt;a href=&#34;https://github.com/mjskay/tidybayes&#34;&gt;tidybayes&lt;/a&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

# wrangle
posts %&amp;gt;% 
  group_by(model) %&amp;gt;% 
  median_qi(rescor__x__y, .width = c(.5, .95)) %&amp;gt;% 
  mutate(key = recode(model, 
                      f0 = &amp;quot;Gaussian likelihood with clean data&amp;quot;,
                      f1 = &amp;quot;Gaussian likelihood with noisy data&amp;quot;,
                      f2 = &amp;quot;Student likelihood with noisy data&amp;quot;,
                      f3 = &amp;quot;Student likelihood with clean data&amp;quot;),
         clean = ifelse(model %in% c(&amp;quot;f0&amp;quot;, &amp;quot;f3&amp;quot;), &amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;)) %&amp;gt;%
  
  # plot
  ggplot(aes(x = rescor__x__y, xmin = .lower, xmax = .upper, y = key, 
             color = clean)) +
  geom_pointinterval() +
  scale_color_fivethirtyeight() +
  scale_x_continuous(breaks = -5:0 / 5, limits = -1:0, expand = expansion(mult = c(0, 0.05))) +
  labs(subtitle = expression(paste(&amp;quot;The posterior for &amp;quot;, rho, &amp;quot; depends on the likelihood. Why not go robust and use Student&amp;#39;s &amp;quot;, italic(t), &amp;quot;?&amp;quot;))) +
  theme_fivethirtyeight() +
  theme(axis.text.y = element_text(hjust = 0),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-10-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From our &lt;code&gt;tidybayes::median_qi()&lt;/code&gt; code, the dots are the posterior medians, the thick inner lines the 50% intervals, and the thinner outer lines the 95% intervals. The posteriors for the &lt;code&gt;x.noisy&lt;/code&gt; data are in red and those for the &lt;code&gt;x.clean&lt;/code&gt; data are in blue. If the data are clean multivariate normal Gaussian or if they’re dirty but fit with robust Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, everything is pretty much alright. But whoa, if you fit a correlation with a combination of &lt;code&gt;family = gaussian&lt;/code&gt; and noisy outlier-laden data, man that’s just a mess.&lt;/p&gt;
&lt;p&gt;Don’t let a few overly-influential outliers make a mess of your analyses. Try the robust Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      ggthemes_4.2.4 
##  [5] forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5     purrr_0.3.4    
##  [9] readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3  
## [13] tidyverse_1.3.0 mvtnorm_1.1-1  
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      Hmisc_4.4-2         
##   [4] plyr_1.8.6           igraph_1.2.6         svUnit_1.0.3        
##   [7] splines_4.0.4        crosstalk_1.1.0.1    TH.data_1.0-10      
##  [10] rstantools_2.1.1     inline_0.3.17        digest_0.6.27       
##  [13] htmltools_0.5.1.1    rsconnect_0.8.16     gdata_2.18.0        
##  [16] fansi_0.4.2          checkmate_2.0.0      magrittr_2.0.1      
##  [19] cluster_2.1.0        modelr_0.1.8         RcppParallel_5.0.2  
##  [22] matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [25] prettyunits_1.1.1    jpeg_0.1-8.1         colorspace_2.0-0    
##  [28] rvest_0.3.6          ggdist_2.4.0.9000    haven_2.3.1         
##  [31] xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [34] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10     
##  [37] zoo_1.8-8            glue_1.4.2           gtable_0.3.0        
##  [40] emmeans_1.5.2-1      V8_3.4.0             distributional_0.2.2
##  [43] weights_1.0.1        pkgbuild_1.2.0       rstan_2.21.2        
##  [46] abind_1.4-5          scales_1.1.1         DBI_1.1.0           
##  [49] miniUI_0.1.1.1       htmlTable_2.1.0      xtable_1.8-4        
##  [52] foreign_0.8-81       Formula_1.2-4        stats4_4.0.4        
##  [55] StanHeaders_2.21.0-7 DT_0.16              htmlwidgets_1.5.2   
##  [58] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [61] RColorBrewer_1.1-2   ellipsis_0.3.1       mice_3.13.0         
##  [64] pkgconfig_2.0.3      loo_2.4.1            farver_2.0.3        
##  [67] nnet_7.3-15          dbplyr_2.0.0         utf8_1.1.4          
##  [70] tidyselect_1.1.0     labeling_0.4.2       rlang_0.4.10        
##  [73] reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [76] cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [79] generics_0.1.0       broom_0.7.5          ggridges_0.5.2      
##  [82] evaluate_0.14        fastmap_1.0.1        yaml_2.2.1          
##  [85] processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [88] nlme_3.1-152         mime_0.10            projpred_2.0.2      
##  [91] xml2_1.3.2           compiler_4.0.4       bayesplot_1.8.0     
##  [94] shinythemes_1.1.2    rstudioapi_0.13      png_0.1-7           
##  [97] gamm4_0.2-6          curl_4.3             reprex_0.3.0        
## [100] statmod_1.4.35       stringi_1.5.3        highr_0.8           
## [103] ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6   
## [106] lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
## [109] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
## [112] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0
## [115] estimability_1.3     data.table_1.14.0    httpuv_1.5.4        
## [118] latticeExtra_0.6-29  R6_2.5.0             bookdown_0.21       
## [121] promises_1.1.1       gridExtra_2.3        codetools_0.2-18    
## [124] boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [127] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1         
## [130] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33         
## [133] parallel_4.0.4       hms_0.5.3            rpart_4.1-15        
## [136] grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [139] rmarkdown_2.7        shiny_1.5.0          lubridate_1.7.9.2   
## [142] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Robust Linear Regression with Student’s $t$-Distribution</title>
      <link>/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/</link>
      <pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/</guid>
      <description>
&lt;script src=&#34;/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;[edited Nov 30, 2020]&lt;/p&gt;
&lt;p&gt;The purpose of this post is to demonstrate the advantages of the Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution for regression with outliers, particularly within a &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;Bayesian framework&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I make assumptions&lt;/h2&gt;
&lt;p&gt;I’m presuming you are familiar with linear regression, familiar with the basic differences between frequentist and Bayesian approaches to fitting regression models, and have a sense that the issue of outlier values is a pickle worth contending with. All code in is &lt;a href=&#34;https://www.r-bloggers.com/why-use-r-five-reasons/&#34;&gt;&lt;strong&gt;R&lt;/strong&gt;&lt;/a&gt;, with a heavy use of the &lt;a href=&#34;https://style.tidyverse.org/&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;, about which you might learn a lot more from &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-grolemundDataScience2017&#34; role=&#34;doc-biblioref&#34;&gt;Grolemund &amp;amp; Wickham&lt;/a&gt; (&lt;a href=&#34;#ref-grolemundDataScience2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, especially &lt;a href=&#34;https://r4ds.had.co.nz/transform.html&#34;&gt;chapter 5&lt;/a&gt;. The Bayesian models are fit with &lt;a href=&#34;https://twitter.com/paulbuerkner&#34;&gt;Paul Bürkner&lt;/a&gt;’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;Simple regression models typically use the Gaussian likelihood. Say you have some criterion variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, which you can reasonably describe with a mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Further, you’d like to describe &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; with a predictor &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Using the Gaussian likelihood, we can describe the model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i &amp;amp; \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 x_i.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With this formulation, we use &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to model the mean of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; parameter is the intercept of the regression model and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is its slope with respect to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. After accounting for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;’s relation with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, the leftover variability in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is described by &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, often called error or residual variance. The reason we describe the model in terms of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is because those are the two parameters by which we define the Normal distribution, the Gaussian likelihood.&lt;/p&gt;
&lt;p&gt;The Gaussian is a sensible default choice for many data types. You might say it works unreasonably well. Unfortunately, the normal (i.e., Gaussian) distribution is sensitive to outliers.&lt;/p&gt;
&lt;p&gt;The normal distribution is a special case of Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution with the &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; parameter (i.e., the degree of freedom) set to infinity. However, when &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; is small, Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution is more robust to multivariate outliers. See Gelman &amp;amp; Hill &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanDataAnalysisUsing2006&#34; role=&#34;doc-biblioref&#34;&gt;2006, Chapter 6&lt;/a&gt;)&lt;/span&gt;, Kruschke &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015, Chapter 16&lt;/a&gt;)&lt;/span&gt;, or McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020, Chapter 7&lt;/a&gt;)&lt;/span&gt; for textbook treatments on the topic.&lt;/p&gt;
&lt;p&gt;In this post, we demonstrate how vulnerable the Gaussian likelihood is to outliers and then compare it to different ways of using Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-likelihood for the same data.&lt;/p&gt;
&lt;p&gt;First, we’ll get a sense of the distributions with a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

tibble(x = seq(from = -6, to = 6, by = .01)) %&amp;gt;% 
  expand(x, nu = c(1, 2.5, 5, 10, Inf)) %&amp;gt;% 
  mutate(density = dt(x = x, df = nu),
         nu      = factor(nu, levels = c(&amp;quot;Inf&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;2.5&amp;quot;, &amp;quot;1&amp;quot;))) %&amp;gt;% 
  
  ggplot(aes(x = x, y = density, group = nu, color = nu)) +
  geom_line() +
  scale_color_viridis_d(expression(nu),
                        direction = 1, option = &amp;quot;C&amp;quot;, end = .85) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-5, 5)) +
  xlab(NULL) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the difference is that a Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution with a low &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; will have notably heavier tails than the conventional Gaussian distribution. It’s easiest to see the difference when &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; approaches 1. Even then, the difference can be subtle when looking at a plot. Another way is to compare how probable relatively extreme values are in a Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution relative to the Gaussian. For the sake of demonstration, here we’ll compare Gauss with Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; with a &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; of 5. In the plot above, they are clearly different, but not shockingly so. However, that difference is very notable in the tails.&lt;/p&gt;
&lt;p&gt;Let’s look more closely with a table. Below, we compare the probability of a given &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-score or lower within the Gaussian and a &lt;span class=&#34;math inline&#34;&gt;\(\nu = 5\)&lt;/span&gt; Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. In the rightmost column, we compare the probabilities in a ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# here we pic our nu
nu &amp;lt;- 5

tibble(z_score               = 0:-5,
       p_Gauss               = pnorm(z_score, mean = 0, sd = 1),
       p_Student_t           = pt(z_score, df = nu),
       `Student/Gauss ratio` = p_Student_t/p_Gauss) %&amp;gt;%
  mutate_if(is.double, round, digits = 5) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;z_score&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p_Gauss&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p_Student_t&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Student/Gauss ratio&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15866&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.18161&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.14468&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.02275&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05097&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.24042&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00135&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.01505&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.14871&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00003&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00516&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;162.97775&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00205&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7159.76534&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note how low &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scores are more probable in this Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; than in the Gaussian. This is most apparent in the &lt;code&gt;Student/Gauss ratio&lt;/code&gt; column on the right. A consequence of this is that extreme scores are less influential to your solutions when you use a small-&lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution in place of the Gaussian. That is, the small-&lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is more robust than the Gaussian to unusual and otherwise influential observations.&lt;/p&gt;
&lt;p&gt;In order to demonstrate, let’s simulate our own. We’ll start by creating multivariate normal data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-create-our-initial-tibble-of-well-behaved-data-d&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let’s create our initial &lt;a href=&#34;https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html&#34;&gt;tibble&lt;/a&gt; of well-behaved data, &lt;code&gt;d&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;First, we’ll need to define our variance/covariance matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- matrix(c(1, .6, 
              .6, 1), 
             nrow = 2, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By the two &lt;code&gt;.6&lt;/code&gt;s on the off-diagonal positions, we indicated we’d like our two variables to have a correlation of .6.&lt;/p&gt;
&lt;p&gt;Second, our variables also need means, which we’ll define with a mean vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- c(0, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With means of &lt;code&gt;0&lt;/code&gt; and variances of &lt;code&gt;1&lt;/code&gt;, our data are in a standardized metric.&lt;/p&gt;
&lt;p&gt;Third, we’ll use the &lt;code&gt;mvrnorm()&lt;/code&gt; function from the &lt;a href=&#34;https://CRAN.R-project.org/package=MASS&#34;&gt;&lt;strong&gt;MASS&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-MASS&#34; role=&#34;doc-biblioref&#34;&gt;Ripley, 2019&lt;/a&gt;)&lt;/span&gt; to simulate our data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(3)

d &amp;lt;- MASS::mvrnorm(n = 100, mu = m, Sigma = s) %&amp;gt;%
  as_tibble() %&amp;gt;%
  rename(y = V1, x = V2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first few rows look like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##         y      x
##     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 -1.14   -0.584
## 2 -0.0805 -0.443
## 3 -0.239   0.702
## 4 -1.30   -0.761
## 5 -0.280   0.630
## 6 -0.245   0.299&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an aside, check out &lt;a href=&#34;https://www.r-bloggers.com/creating-sample-datasets-exercises/&#34;&gt;this nice r-bloggers post&lt;/a&gt; for more information on simulating data with this method.&lt;/p&gt;
&lt;p&gt;Anyway, this line reorders our data by &lt;code&gt;x&lt;/code&gt;, placing the smallest values on top.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;-
  d %&amp;gt;%
  arrange(x)

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##        y     x
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 -2.21  -1.84
## 2 -1.27  -1.71
## 3 -0.168 -1.60
## 4 -0.292 -1.46
## 5 -0.785 -1.40
## 6 -0.157 -1.37&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-create-our-outlier-tibble-o&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let’s create our outlier tibble, &lt;code&gt;o&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Here we’ll make two outlying and unduly influential values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;o &amp;lt;- d
o[c(1:2), 1] &amp;lt;- c(5, 4.5)

head(o)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##        y     x
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1  5     -1.84
## 2  4.5   -1.71
## 3 -0.168 -1.60
## 4 -0.292 -1.46
## 5 -0.785 -1.40
## 6 -0.157 -1.37&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the code, above, we replaced the first two values of our first variable, &lt;code&gt;y&lt;/code&gt;. They both started out quite negative. Now they are positive values of a large magnitude within the standardized metric.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;frequentist-ols-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Frequentist &lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_least_squares&#34;&gt;OLS&lt;/a&gt; models&lt;/h2&gt;
&lt;p&gt;To get a quick sense of what we’ve done, we’ll first fit two models with OLS regression via the &lt;code&gt;lm()&lt;/code&gt; function. The first model, &lt;code&gt;ols0&lt;/code&gt;, is of the multivariate normal data, &lt;code&gt;d&lt;/code&gt;. The second model, &lt;code&gt;ols1&lt;/code&gt;, is on the otherwise identical data with the two odd and influential values, &lt;code&gt;o&lt;/code&gt;. Here is our model code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ols0 &amp;lt;- lm(data = d, y ~ 1 + x)
ols1 &amp;lt;- lm(data = o, y ~ 1 + x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll use the &lt;a href=&#34;https://cran.r-project.org/web/packages/broom/index.html&#34;&gt;&lt;strong&gt;broom&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-broom&#34; role=&#34;doc-biblioref&#34;&gt;Robinson &amp;amp; Hayes, 2020&lt;/a&gt;)&lt;/span&gt; to assist with model summaries and other things. Here are the parameter estimates for the first model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)

tidy(ols0) %&amp;gt;% mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   term        estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)    -0.01      0.09     -0.08    0.94
## 2 x               0.45      0.1       4.55    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now the parameters for the second model, the one based on the &lt;code&gt;o&lt;/code&gt; outlier data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(ols1) %&amp;gt;% mutate_if(is.double, round, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   term        estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)     0.13      0.11      1.15    0.25
## 2 x               0.14      0.13      1.1     0.27&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just two odd and influential values dramatically changed the model parameters, particularly the slope. Let’s plot the data and the models to get a visual sense of what happened.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the well-behaved data
p1 &amp;lt;-
  ggplot(data = d, aes(x = x, y = y)) +
  stat_smooth(method = &amp;quot;lm&amp;quot;, color = &amp;quot;grey92&amp;quot;, fill = &amp;quot;grey67&amp;quot;, alpha = 1, fullrange = T) +
  geom_point(size = 1, alpha = 3/4) +
  scale_x_continuous(limits = c(-4, 4)) +
  coord_cartesian(xlim = c(-3, 3), 
                  ylim = c(-3, 5)) +
  labs(title = &amp;quot;No Outliers&amp;quot;) +
  theme(panel.grid = element_blank())

# the data with two outliers
p2 &amp;lt;-
  ggplot(data = o, aes(x = x, y = y, color = y &amp;gt; 3)) +
  stat_smooth(method = &amp;quot;lm&amp;quot;, color = &amp;quot;grey92&amp;quot;, fill = &amp;quot;grey67&amp;quot;, alpha = 1, fullrange = T) +
  geom_point(size = 1, alpha = 3/4) +
  scale_color_viridis_d(option = &amp;quot;A&amp;quot;, end = 4/7) +
  scale_x_continuous(limits = c(-4, 4)) +
  coord_cartesian(xlim = c(-3, 3), 
                  ylim = c(-3, 5)) +
  labs(title = &amp;quot;Two Outliers&amp;quot;) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;)

# combine the ggplots with patchwork syntax
library(patchwork)

p1 + p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;648&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The two outliers were quite influential on the slope. It went from a nice clear diagonal to almost horizontal. You’ll also note how the 95% intervals (i.e., the bowtie shapes) were a bit wider when based on the &lt;code&gt;o&lt;/code&gt; data.&lt;/p&gt;
&lt;p&gt;One of the popular ways to quantify outlier status is with Mahalanobis’ distance. However, the Mahalanobis distance is primarily valid for multivariate normal data. Though the data in this example are indeed multivariate normal–or at least they were before we injected two outlying values into them–I am going to resist relying on Mahalanobis’ distance. There are other more general approaches that will be of greater use when you need to explore other variants of the generalized linear model. The &lt;code&gt;broom::augment()&lt;/code&gt; function will give us access to one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aug0 &amp;lt;- augment(ols0)
aug1 &amp;lt;- augment(ols1)

glimpse(aug1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 100
## Columns: 8
## $ y          &amp;lt;dbl&amp;gt; 5.00000000, 4.50000000, -0.16783167, -0.29164105, -0.784918…
## $ x          &amp;lt;dbl&amp;gt; -1.8439208, -1.7071418, -1.5996509, -1.4601550, -1.3954395,…
## $ .fitted    &amp;lt;dbl&amp;gt; -0.129991947, -0.110805943, -0.095728191, -0.076161086, -0.…
## $ .resid     &amp;lt;dbl&amp;gt; 5.12999195, 4.61080594, -0.07210348, -0.21547996, -0.717835…
## $ .hat       &amp;lt;dbl&amp;gt; 0.05521164, 0.04881414, 0.04412882, 0.03849763, 0.03605748,…
## $ .sigma     &amp;lt;dbl&amp;gt; 0.9887858, 1.0170749, 1.1246348, 1.1244384, 1.1222070, 1.12…
## $ .cooksd    &amp;lt;dbl&amp;gt; 6.500952e-01, 4.580898e-01, 1.002809e-04, 7.721988e-04, 7.9…
## $ .std.resid &amp;lt;dbl&amp;gt; 4.71688666, 4.22522826, -0.06591171, -0.19639831, -0.653439…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can compare the observations with Cook’s distance, &lt;span class=&#34;math inline&#34;&gt;\(D_i\)&lt;/span&gt; (i.e., &lt;code&gt;.cooksd&lt;/code&gt;). Cook’s &lt;span class=&#34;math inline&#34;&gt;\(D_i\)&lt;/span&gt; is a measure of the influence of a given observation on the model. To compute &lt;span class=&#34;math inline&#34;&gt;\(D_i\)&lt;/span&gt;, the model is fit once for each &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; case, after first dropping that case. Then the difference in the model with all observations and the model with all observations but the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th observation, as defined by the Euclidean distance between the estimators. Fahrmeir et al &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-fahrmeirRegressionModelsMethods2013&#34; role=&#34;doc-biblioref&#34;&gt;2013, p. 166&lt;/a&gt;)&lt;/span&gt; suggest that within the OLS framework “as a rule of thumb, observations with &lt;span class=&#34;math inline&#34;&gt;\(D_i &amp;gt; 0.5\)&lt;/span&gt; are worthy of attention, and observations with &lt;span class=&#34;math inline&#34;&gt;\(D_i &amp;gt; 1\)&lt;/span&gt; should always be examined.” Here we plot &lt;span class=&#34;math inline&#34;&gt;\(D_i\)&lt;/span&gt; against our observation index, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, for both models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  aug0 %&amp;gt;% mutate(i = 1:n()),  # the well-behaved data
  aug1 %&amp;gt;% mutate(i = 1:n())   # the data with two outliers
) %&amp;gt;%
  mutate(fit = rep(c(&amp;quot;fit b0&amp;quot;, &amp;quot;fit b1&amp;quot;), each = n()/2)) %&amp;gt;%
  ggplot(aes(x = i, y = .cooksd)) +
  geom_hline(yintercept = .5, color = &amp;quot;white&amp;quot;) +
  geom_point(alpha = .5) +
  geom_text(data = tibble(i = 46, 
                          .cooksd = .53,
                          fit = &amp;quot;fit b0&amp;quot;),
            label = &amp;quot;Fahrmeir et al said we might worry around here&amp;quot;,
            color = &amp;quot;grey50&amp;quot;) +
  coord_cartesian(ylim = c(0, .7)) +
  theme(panel.grid = element_blank(),
        axis.title.x = element_text(face = &amp;quot;italic&amp;quot;, family = &amp;quot;Times&amp;quot;)) +
    facet_wrap(~ fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For the model of the well-behaved data, &lt;code&gt;ols0&lt;/code&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(D_i\)&lt;/span&gt; values all hovering near zero. However, the plot for &lt;code&gt;ols1&lt;/code&gt; shows one &lt;span class=&#34;math inline&#34;&gt;\(D_i\)&lt;/span&gt; value well above the 0.5 level and another not quite that high but deviant relative to the rest. Our two outlier values look quite influential for the results of &lt;code&gt;ols1&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;switch-to-a-bayesian-framework&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Switch to a Bayesian framework&lt;/h2&gt;
&lt;p&gt;It’s time to fire up &lt;strong&gt;brms&lt;/strong&gt;, the package with which we’ll be fitting our Bayesian models. As with all Bayesian models, we’ll need to us use priors. To keep things simple, we’ll use weakly-regularizing priors of the sort &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;discussed by the Stan team&lt;/a&gt;. For more thoughts on how to set priors, check out Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text or either edition of McElreath’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;stick-with-gauss.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stick with Gauss.&lt;/h3&gt;
&lt;p&gt;For our first two Bayesian models, &lt;code&gt;b0&lt;/code&gt; and &lt;code&gt;b1&lt;/code&gt;, we’ll use the conventional Gaussian likelihood (i.e., &lt;code&gt;family = gaussian&lt;/code&gt; in the &lt;code&gt;brm()&lt;/code&gt; function). Like with &lt;code&gt;ols0&lt;/code&gt;, above, the first model is based on the nice &lt;code&gt;d&lt;/code&gt; data. The second, &lt;code&gt;b1&lt;/code&gt;, is based on the more-difficult &lt;code&gt;o&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b0 &amp;lt;- 
  brm(data = d, 
      family = gaussian,
      y ~ 1 + x,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 1),  class = sigma)),
      seed = 1)
b1 &amp;lt;- 
  update(b0, 
         newdata = o,
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the model summaries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_summary(b0)[1:3, ] %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Estimate Est.Error  Q2.5 Q97.5
## b_Intercept    -0.01      0.09 -0.18  0.16
## b_x             0.44      0.10  0.25  0.64
## sigma           0.86      0.06  0.75  0.99&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_summary(b1)[1:3, ] %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Estimate Est.Error  Q2.5 Q97.5
## b_Intercept     0.13      0.11 -0.09  0.35
## b_x             0.14      0.13 -0.11  0.40
## sigma           1.13      0.08  0.98  1.29&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We summarized our model parameters with &lt;code&gt;brms::posterior_summary()&lt;/code&gt; rather than &lt;code&gt;broom::tid()&lt;/code&gt;. Otherwise, these should look familiar. They’re very much like the results from the OLS models. Hopefully this isn’t surprising. Our priors were quite weak, so there’s no reason to suspect the results would differ much.&lt;/p&gt;
&lt;div id=&#34;the-loo-and-other-goodies-help-with-diagnostics.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The LOO and other goodies help with diagnostics.&lt;/h4&gt;
&lt;p&gt;With the &lt;code&gt;loo()&lt;/code&gt; function, we’ll extract loo objects, which contain some handy output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_b0 &amp;lt;- loo(b0)
loo_b1 &amp;lt;- loo(b1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll use &lt;code&gt;str()&lt;/code&gt; to get a sense of what’s all in there, using &lt;code&gt;loo_b1&lt;/code&gt; as an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(loo_b1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 10
##  $ estimates  : num [1:3, 1:2] -157.41 6.65 314.81 15.76 3.75 ...
##   ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. ..$ : chr [1:3] &amp;quot;elpd_loo&amp;quot; &amp;quot;p_loo&amp;quot; &amp;quot;looic&amp;quot;
##   .. ..$ : chr [1:2] &amp;quot;Estimate&amp;quot; &amp;quot;SE&amp;quot;
##  $ pointwise  : num [1:100, 1:5] -13.47 -10.79 -1.06 -1.08 -1.27 ...
##   ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. ..$ : NULL
##   .. ..$ : chr [1:5] &amp;quot;elpd_loo&amp;quot; &amp;quot;mcse_elpd_loo&amp;quot; &amp;quot;p_loo&amp;quot; &amp;quot;looic&amp;quot; ...
##  $ diagnostics:List of 2
##   ..$ pareto_k: num [1:100] 0.8171 0.6003 0.0139 -0.0705 -0.0817 ...
##   ..$ n_eff   : num [1:100] 71.1 186.8 2553.2 2795.7 3845.7 ...
##  $ psis_object: NULL
##  $ elpd_loo   : num -157
##  $ p_loo      : num 6.65
##  $ looic      : num 315
##  $ se_elpd_loo: num 15.8
##  $ se_p_loo   : num 3.75
##  $ se_looic   : num 31.5
##  - attr(*, &amp;quot;dims&amp;quot;)= int [1:2] 4000 100
##  - attr(*, &amp;quot;class&amp;quot;)= chr [1:3] &amp;quot;psis_loo&amp;quot; &amp;quot;importance_sampling_loo&amp;quot; &amp;quot;loo&amp;quot;
##  - attr(*, &amp;quot;yhash&amp;quot;)= chr &amp;quot;b52ef230de67f0bebc3480da360987ee2c0f4de8&amp;quot;
##  - attr(*, &amp;quot;model_name&amp;quot;)= chr &amp;quot;b1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For a detailed explanation of all those elements, see the &lt;a href=&#34;https://CRAN.R-project.org/package=loo/loo.pdf&#34;&gt;&lt;strong&gt;loo&lt;/strong&gt; reference manual&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-loo2020RM&#34; role=&#34;doc-biblioref&#34;&gt;Vehtari et al., 2020&lt;/a&gt;)&lt;/span&gt;. For our purposes, we’ll focus on the &lt;code&gt;pareto_k&lt;/code&gt;. Here’s a glimpse of what it contains for the &lt;code&gt;b1&lt;/code&gt; model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_b1$diagnostics$pareto_k %&amp;gt;% as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 100 x 1
##       value
##       &amp;lt;dbl&amp;gt;
##  1  0.817  
##  2  0.600  
##  3  0.0139 
##  4 -0.0705 
##  5 -0.0817 
##  6 -0.00611
##  7  0.0431 
##  8  0.00514
##  9  0.111  
## 10  0.0629 
## # … with 90 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve got us a numeric vector of as many values as our data had observations–100 in this case. The &lt;code&gt;pareto_k&lt;/code&gt; values can be used to examine overly-influential cases. See, for example &lt;a href=&#34;https://stackoverflow.com/questions/39578834/linear-model-diagnostics-for-bayesian-models-using-rstan/39595436&#34;&gt;this discussion on stackoverflow.com&lt;/a&gt; in which several members of the &lt;a href=&#34;http://mc-stan.org&#34;&gt;Stan team&lt;/a&gt; weighed in. The issue is also discussed in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-vehtariPracticalBayesianModel2017&#34; role=&#34;doc-biblioref&#34;&gt;Vehtari et al.&lt;/a&gt; (&lt;a href=&#34;#ref-vehtariPracticalBayesianModel2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, in the &lt;a href=&#34;https://CRAN.R-project.org/package=loo/loo.pdf&#34;&gt;&lt;strong&gt;loo&lt;/strong&gt; reference manual&lt;/a&gt;, and in &lt;a href=&#34;https://www.youtube.com/watch?v=FUROJM3u5HQ&amp;amp;feature=youtu.be&amp;amp;a=&#34;&gt;this presentation by Aki Vehtari&lt;/a&gt;, himself. If we explicitly open the &lt;a href=&#34;https://CRAN.R-project.org/package=loo&#34;&gt;&lt;strong&gt;loo&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-loo&#34; role=&#34;doc-biblioref&#34;&gt;Vehtari et al., 2019&lt;/a&gt;)&lt;/span&gt;, we can use a few convenience functions to leverage &lt;code&gt;pareto_k&lt;/code&gt; for diagnostic purposes. The &lt;code&gt;pareto_k_table()&lt;/code&gt; function will categorize the &lt;code&gt;pareto_k&lt;/code&gt; values and give us a sense of how many values are in problematic ranges.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(loo)

pareto_k_table(loo_b1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     98    98.0%   2439      
##  (0.5, 0.7]   (ok)        1     1.0%   187       
##    (0.7, 1]   (bad)       1     1.0%   71        
##    (1, Inf)   (very bad)  0     0.0%   &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Happily, most of our cases were in the “good” range. One pesky case was in the “bad” range [can you guess which one?] and another case was only “ok” [and can you guess that one, too?]. The &lt;code&gt;pareto_k_ids()&lt;/code&gt; function will tell exactly us which cases we’ll want to look at.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pareto_k_ids(loo_b1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those numbers correspond to the row numbers in the data, &lt;code&gt;o&lt;/code&gt;. These are exactly the cases that plagued our second OLS model, &lt;code&gt;fit1&lt;/code&gt;, and are also the ones we hand coded to be outliers. With the simple &lt;code&gt;plot()&lt;/code&gt; function, we can get a diagnostic plot for the &lt;code&gt;pareto_k&lt;/code&gt; values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(loo_b1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There they are, cases 1 and 2, lurking in the “bad” and “[just] ok” ranges. We can also make a similar plot with &lt;strong&gt;ggplot2&lt;/strong&gt;. Though it takes a little more work, &lt;strong&gt;ggplot2&lt;/strong&gt; makes it easy to compare &lt;code&gt;pareto_k&lt;/code&gt; plots across models with a little faceting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# for the annotation
text &amp;lt;-
  tibble(i     = 1, 
         k     = c(.45, .65, .95),
         label = c(&amp;quot;good&amp;quot;, &amp;quot;[just] ok&amp;quot;, &amp;quot;bad&amp;quot;),
         fit   = &amp;quot;fit b0&amp;quot;)

# extract the diagnostics
tibble(k   = c(loo_b0$diagnostics$pareto_k, loo_b1$diagnostics$pareto_k),
       i   = rep(1:100, times = 2),
       fit = rep(str_c(&amp;quot;fit b&amp;quot;, 0:1), each = 100)) %&amp;gt;%
  
  # plot!
  ggplot(aes(x = i, y = k)) +
  geom_hline(yintercept = c(.5, .7, 1), color = &amp;quot;white&amp;quot;) +
  geom_point(alpha = .5) +
  geom_text(data = text,
            aes(label = label),
            color = &amp;quot;grey50&amp;quot;, hjust = 0) +
  scale_y_continuous(expression(Pareto~italic(k)), breaks = c(0, .5, .7, 1)) +
  theme(panel.grid = element_blank(),
        axis.title.x = element_text(face = &amp;quot;italic&amp;quot;, family = &amp;quot;Times&amp;quot;)) +
  facet_wrap(~ fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So with &lt;code&gt;b0&lt;/code&gt;–the model based on the well-behaved multivariate normal data, &lt;code&gt;d&lt;/code&gt;–, all the &lt;code&gt;pareto_k&lt;/code&gt; values hovered around zero in the “good” range. Things got concerning with model &lt;code&gt;b1&lt;/code&gt;. But we know all that. Let’s move forward.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-do-we-do-with-those-overly-influential-outlying-values&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What do we do with those overly-influential outlying values?&lt;/h4&gt;
&lt;p&gt;A typical way to handle outlying values is to delete them based on some criterion, such as the Mahalanobis distance, Cook’s &lt;span class=&#34;math inline&#34;&gt;\(D_i\)&lt;/span&gt;, or our new friend the &lt;code&gt;pareto_k&lt;/code&gt;. In our next two models, we’ll do that. In our &lt;code&gt;data&lt;/code&gt; arguments, we can use the &lt;code&gt;slice()&lt;/code&gt; function to omit cases. In model &lt;code&gt;b1.1&lt;/code&gt;, we simply omit the first and most influential case. In model &lt;code&gt;b1.2&lt;/code&gt;, we omitted both unduly-influential cases, the values from rows 1 and 2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1.1 &amp;lt;- 
  update(b1, 
         newdata = o %&amp;gt;% slice(2:100),
         seed = 1)
b1.2 &amp;lt;- 
  update(b1, 
         newdata = o %&amp;gt;% slice(3:100),
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the summaries for our models based on the &lt;code&gt;slice[d]&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_summary(b1.1)[1:3, ] %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Estimate Est.Error  Q2.5 Q97.5
## b_Intercept     0.07      0.10 -0.12  0.27
## b_x             0.27      0.12  0.04  0.50
## sigma           1.00      0.07  0.87  1.15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_summary(b1.2)[1:3, ] %&amp;gt;% round(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Estimate Est.Error  Q2.5 Q97.5
## b_Intercept     0.01      0.09 -0.16  0.19
## b_x             0.39      0.10  0.19  0.59
## sigma           0.86      0.06  0.75  0.99&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They are closer to the true data generating model (i.e., the code we used to make &lt;code&gt;d&lt;/code&gt;), especially &lt;code&gt;b1.2&lt;/code&gt;. However, there are other ways to handle the influential cases without dropping them. Finally, we’re ready to switch to Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;time-to-leave-gauss-for-the-more-general-students-t&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Time to leave Gauss for the more general Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Recall that the normal distribution is equivalent to a Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; with the degrees of freedom parameter, &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;, set to infinity. That is, &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; is fixed. Here we’ll relax that assumption and estimate &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; from the data just like we estimate &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; with the linear model and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; as the residual spread. Since &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;’s now a parameter, we’ll have to give it a prior. For our first Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; model, we’ll estimate &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; with the &lt;strong&gt;brms&lt;/strong&gt; default &lt;code&gt;gamma(2, 0.1)&lt;/code&gt; prior.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b2 &amp;lt;- 
  brm(data = o, family = student,
      y ~ 1 + x,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(gamma(2, 0.1), class = nu),
                prior(cauchy(0, 1),  class = sigma)),
      seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the next model, we’ll switch out that weak &lt;code&gt;gamma(2, 0.1)&lt;/code&gt; for a stronger &lt;code&gt;gamma(4, 1)&lt;/code&gt;. In some disciplines, the gamma distribution is something of an exotic bird. So before fitting the model, it might be useful to take a peek at what these gamma priors looks like. In the plot, below, the orange density in the background is the default &lt;code&gt;gamma(2, 0.1)&lt;/code&gt; and the purple density in the foreground is the stronger &lt;code&gt;gamma(4, 1)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data
tibble(x = seq(from = 0, to = 60, by = .1)) %&amp;gt;% 
  expand(x, nesting(alpha = c(2, 4), 
                    beta  = c(0.1, 1))) %&amp;gt;% 
  mutate(density = dgamma(x, alpha, beta),
         group   = rep(letters[1:2], times = n() / 2)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = x, ymin = 0, ymax = density, 
             group = group, fill = group)) +
  geom_ribbon(size = 0, alpha = 3/4) +
  scale_fill_viridis_d(option = &amp;quot;B&amp;quot;, direction = -1, 
                       begin = 1/3, end = 2/3) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 50)) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/index_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the default prior is centered around values in the 2 to 30 range, but has a long gentle-sloping tail, allowing the model to yield much larger values for &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;, as needed. The prior we use below is almost entirely concentrated in the single-digit range. In this case, that will preference Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; likelihoods with very small &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; parameters and correspondingly thick tails–easily allowing for extreme values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b3 &amp;lt;- 
  update(b2,
         prior = c(prior(normal(0, 10), class = Intercept),
                   prior(normal(0, 10), class = b),
                   prior(gamma(4, 1),   class = nu),
                   prior(cauchy(0, 1),  class = sigma)),
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our final model, we’ll fix the &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; parameter in a &lt;code&gt;bf()&lt;/code&gt; statement.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b4 &amp;lt;-
  brm(data = o, family = student,
      bf(y ~ 1 + x, nu = 4),
      prior = c(prior(normal(0, 100), class = Intercept),
                prior(normal(0, 10),  class = b),
                prior(cauchy(0, 1),   class = sigma)),
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ve got all those models, we can gather their results into a single tibble.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b_estimates &amp;lt;-
  tibble(model = c(&amp;quot;b0&amp;quot;, &amp;quot;b1&amp;quot;, &amp;quot;b1.1&amp;quot;, &amp;quot;b1.2&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;, &amp;quot;b4&amp;quot;)) %&amp;gt;% 
  mutate(fit = map(model, get)) %&amp;gt;% 
  mutate(posterior_summary = map(fit, ~posterior_summary(.) %&amp;gt;% 
                                   data.frame() %&amp;gt;% 
                                   rownames_to_column(&amp;quot;term&amp;quot;))) %&amp;gt;% 
  unnest(posterior_summary) %&amp;gt;% 
  select(-fit) %&amp;gt;% 
  filter(term %in% c(&amp;quot;b_Intercept&amp;quot;, &amp;quot;b_x&amp;quot;)) %&amp;gt;%
  arrange(term)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get a sense of what we’ve done, let’s take a peek at our models tibble.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b_estimates %&amp;gt;%
  mutate_if(is.double, round, digits = 2)  # this is just to round the numbers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 14 x 6
##    model term        Estimate Est.Error  Q2.5 Q97.5
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 b0    b_Intercept    -0.01      0.09 -0.18 0.16 
##  2 b1    b_Intercept     0.13      0.11 -0.09 0.35 
##  3 b1.1  b_Intercept     0.07      0.1  -0.12 0.27 
##  4 b1.2  b_Intercept     0.01      0.09 -0.16 0.19 
##  5 b2    b_Intercept     0.04      0.09 -0.14 0.23 
##  6 b3    b_Intercept     0.04      0.09 -0.14 0.22 
##  7 b4    b_Intercept     0.04      0.09 -0.14 0.22 
##  8 b0    b_x             0.44      0.1   0.25 0.64 
##  9 b1    b_x             0.14      0.13 -0.11 0.4  
## 10 b1.1  b_x             0.27      0.12  0.04 0.5  
## 11 b1.2  b_x             0.39      0.1   0.19 0.59 
## 12 b2    b_x             0.36      0.11  0.15 0.56 
## 13 b3    b_x             0.36      0.1   0.16 0.56 
## 14 b4    b_x             0.37      0.11  0.16 0.580&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The models differ by their intercepts, slopes, sigmas, and &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;s. For the sake of this post, we’ll focus on the slopes. Here we compare the different Bayesian models’ slopes by their posterior means and 95% intervals in a coefficient plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b_estimates %&amp;gt;%
  filter(term == &amp;quot;b_x&amp;quot;) %&amp;gt;% # b_Intercept b_x
  
  ggplot(aes(x = model)) +
  geom_pointrange(aes(y    = Estimate,
                      ymin = Q2.5,
                      ymax = Q97.5),
                  shape = 20) +
  coord_flip(ylim = c(-.2, 1)) +
  labs(title    = &amp;quot;The x slope, varying by model&amp;quot;,
       subtitle = &amp;quot;The dots are the posterior means and the lines the percentile-based 95% intervals.&amp;quot;,
       x        = NULL,
       y        = NULL) +
  theme(panel.grid   = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/index_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You might think of the &lt;code&gt;b0&lt;/code&gt; slope as the “true” slope. That’s the one estimated from the well-behaved multivariate normal data, &lt;code&gt;d&lt;/code&gt;. That estimate’s just where we’d want it to be. The &lt;code&gt;b1&lt;/code&gt; slope is a disaster–way lower than the others. The slopes for &lt;code&gt;b1.1&lt;/code&gt; and &lt;code&gt;b1.2&lt;/code&gt; get better, but at the expense of deleting data. All three of our Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; models produced slopes that were pretty close to the &lt;code&gt;b0&lt;/code&gt; slope. They weren’t perfect, but, all in all, Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution did pretty okay.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-more-loo-and-more-pareto_k.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need more LOO and more &lt;code&gt;pareto_k&lt;/code&gt;.&lt;/h3&gt;
&lt;p&gt;We already have loo objects for our first two models, &lt;code&gt;b0&lt;/code&gt; and &lt;code&gt;b1&lt;/code&gt;. Let’s get some for models &lt;code&gt;b2&lt;/code&gt; through &lt;code&gt;b4&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_b2 &amp;lt;- loo(b2)
loo_b3 &amp;lt;- loo(b3)
loo_b4 &amp;lt;- loo(b4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a little data wrangling, we can compare our models by how they look in our custom &lt;code&gt;pareto_k&lt;/code&gt; diagnostic plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make a custom function to work with the loo objects in bulk
get_pareto_k &amp;lt;- function(l) {
  l$diagnostics$pareto_k %&amp;gt;% 
    as_tibble() %&amp;gt;%
    mutate(i = 1:n()) %&amp;gt;% 
    rename(pareto_k = value)
}

# wrangle
tibble(name = str_c(&amp;quot;loo_b&amp;quot;, 1:4)) %&amp;gt;% 
  mutate(loo_object = map(name, get)) %&amp;gt;% 
  mutate(pareto_k = map(loo_object, get_pareto_k)) %&amp;gt;% 
  unnest(pareto_k) %&amp;gt;% 
  mutate(fit = rep(c(&amp;quot;fit b1&amp;quot;, &amp;quot;fit b2&amp;quot;, &amp;quot;fit b3&amp;quot;, &amp;quot;fit b4&amp;quot;), each = n() / 4)) %&amp;gt;%
  
  # plot
  ggplot(aes(x = i, y = pareto_k)) +
  geom_hline(yintercept = c(.5, .7),
             color = &amp;quot;white&amp;quot;) +
  geom_point(alpha = .5) +
  scale_y_continuous(expression(Pareto~italic(k)), breaks = c(0, .5, .7)) +
  theme(panel.grid = element_blank(),
        axis.title.x = element_text(face = &amp;quot;italic&amp;quot;, family = &amp;quot;Times&amp;quot;)) +
    facet_wrap(~ fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/index_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Oh man, those Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; models worked sweet! In a succession from &lt;code&gt;b2&lt;/code&gt; through &lt;code&gt;b4&lt;/code&gt;, each model looked better by &lt;code&gt;pareto_k&lt;/code&gt;. All were way better than the typical Gaussian model, &lt;code&gt;b1&lt;/code&gt;. While we’re at it, we might compare those by their LOO values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo_b1, loo_b2, loo_b3, loo_b4) %&amp;gt;% print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic
## b4    0.0       0.0  -143.5     10.3         2.7    0.3    287.1   20.7  
## b3   -0.8       0.4  -144.4     10.7         3.6    0.8    288.8   21.4  
## b2   -1.9       1.8  -145.5     11.7         4.6    1.5    291.0   23.4  
## b1  -13.9       7.6  -157.4     15.8         6.7    3.7    314.8   31.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In terms of the LOO, &lt;code&gt;b2&lt;/code&gt; through &lt;code&gt;b4&lt;/code&gt; were about the same, but all looked better than &lt;code&gt;b1&lt;/code&gt;. In fairness, though, the standard errors for the difference scores were a bit on the wide side.
If you’re new to using information criteria to compare models, you might sit down and soak in &lt;a href=&#34;https://www.youtube.com/watch?v=t0pRuy1_190&amp;amp;list=PLDcUM9US4XdM9_N6XUUFrhghGJ4K25bFc&amp;amp;index=8&#34;&gt;one of McElreath’s lectures on the topic&lt;/a&gt; or the &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-vehtariUsingLooPackage2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; vignette by Vehtari and Gabry, &lt;a href=&#34;https://cran.r-project.org/web/packages/loo/vignettes/loo2-example.html#plotting-pareto-k-diagnostics&#34;&gt;&lt;em&gt;Using the loo package (version &amp;gt;= 2.0.0)&lt;/em&gt;&lt;/a&gt;. For a more technical introduction, you might check out the references in the &lt;strong&gt;loo&lt;/strong&gt; package’s &lt;a href=&#34;https://CRAN.R-project.org/package=loo&#34;&gt;reference manual&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For one final LOO-related comparison, we can use the &lt;code&gt;brms::model_weights()&lt;/code&gt; function to see how much relative weight we might put on each of those four models if we were to use a model averaging approach. Here we use the default method, which is model averaging via posterior predictive stacking.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_weights(b1, b2, b3, b4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           b1           b2           b3           b4 
## 3.310561e-07 8.617446e-07 1.808979e-06 9.999970e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re not a fan of scientific notation, just tack on &lt;code&gt;round(digits = 2)&lt;/code&gt;. The stacking method suggests that we should place virtually all the weight on &lt;code&gt;b4&lt;/code&gt;, the model in which we fixed our Student-&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; parameter at 4. To learn more about model stacking, check out Yao, Vehtari, Simpson, and Gelman’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-yaoUsingStackingAverage2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; paper, &lt;a href=&#34;https://projecteuclid.org/euclid.ba/1516093227&#34;&gt;&lt;em&gt;Using stacking to average Bayesian predictive distributions&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-compare-a-few-bayesian-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Let’s compare a few Bayesian models.&lt;/h3&gt;
&lt;p&gt;That’s enough with coefficients, &lt;code&gt;pareto_k&lt;/code&gt;, and the LOO. Let’s get a sense of the implications of the models by comparing a few in plots. Here we use convenience functions from &lt;a href=&#34;https://twitter.com/mjskay&#34;&gt;Matthew Kay&lt;/a&gt;’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;http://mjskay.github.io/tidybayes&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt; to streamline the data wrangling and plotting. The method came from a &lt;a href=&#34;https://twitter.com/mjskay/status/1091926564101599232&#34;&gt;kind twitter suggesion from Kay&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

# these are the values of x we&amp;#39;d like model-implied summaries for
nd &amp;lt;- tibble(x = seq(from = -4, to = 4, length.out = 50))

# here&amp;#39;s another way to arrange the models
list(b0 = b0, b1 = b1, b3 = b3) %&amp;gt;% 
  # with help from `tidybayes::add_fitted_draws()`, here we use `fitted()` in bulk
  map_dfr(add_fitted_draws, newdata = nd, .id = &amp;quot;model&amp;quot;) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = x)) +
  stat_lineribbon(aes(y = .value),
                  .width = .95,
                  color = &amp;quot;grey92&amp;quot;, fill = &amp;quot;grey67&amp;quot;) +
  geom_point(data = d %&amp;gt;%
               bind_rows(o, o) %&amp;gt;%
               mutate(model = rep(c(&amp;quot;b0&amp;quot;, &amp;quot;b1&amp;quot;, &amp;quot;b3&amp;quot;), each = 100)), 
             aes(y = y, color = y &amp;gt; 3),
             size = 1, alpha = 3/4) +
  scale_color_viridis_d(option = &amp;quot;A&amp;quot;, end = 4/7) +
  coord_cartesian(xlim = c(-3, 3), 
                  ylim = c(-3, 5)) +
  ylab(NULL) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;) +
  facet_wrap(~ model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/index_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For each subplot, the gray band is the 95% interval band and the overlapping light gray line is the posterior mean. Model &lt;code&gt;b0&lt;/code&gt;, recall, is our baseline comparison model. This is of the well-behaved no-outlier data, &lt;code&gt;d&lt;/code&gt;, using the good old Gaussian likelihood. Model &lt;code&gt;b1&lt;/code&gt; is of the outlier data, &lt;code&gt;o&lt;/code&gt;, but still using the non-robust Gaussian likelihood. Model &lt;code&gt;b3&lt;/code&gt; uses a robust Student’s &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; likelihood with &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; estimated with the fairly narrow &lt;code&gt;gamma(4, 1)&lt;/code&gt; prior. For my money, &lt;code&gt;b3&lt;/code&gt; did a pretty good job.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 loo_2.4.1       brms_2.15.0     Rcpp_1.0.6     
##  [5] patchwork_1.1.1 broom_0.7.5     forcats_0.5.1   stringr_1.4.0  
##  [9] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
## [13] tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.3         splines_4.0.4       
##   [7] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          checkmate_2.0.0     
##  [16] magrittr_2.0.1       modelr_0.1.8         RcppParallel_5.0.2  
##  [19] matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [22] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6         
##  [25] ggdist_2.4.0.9000    haven_2.3.1          xfun_0.22           
##  [28] callr_3.5.1          crayon_1.4.1         jsonlite_1.7.2      
##  [31] lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [34] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1     
##  [37] V8_3.4.0             distributional_0.2.2 pkgbuild_1.2.0      
##  [40] rstan_2.21.2         abind_1.4-5          scales_1.1.1        
##  [43] mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [46] viridisLite_0.3.0    xtable_1.8-4         stats4_4.0.4        
##  [49] StanHeaders_2.21.0-7 DT_0.16              htmlwidgets_1.5.2   
##  [52] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [55] ellipsis_0.3.1       pkgconfig_2.0.3      farver_2.0.3        
##  [58] dbplyr_2.0.0         utf8_1.1.4           tidyselect_1.1.0    
##  [61] labeling_0.4.2       rlang_0.4.10         reshape2_1.4.4      
##  [64] later_1.1.0.1        munsell_0.5.0        cellranger_1.1.0    
##  [67] tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [70] ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1       
##  [73] yaml_2.2.1           processx_3.4.5       knitr_1.31          
##  [76] fs_1.5.0             nlme_3.1-152         mime_0.10           
##  [79] projpred_2.0.2       xml2_1.3.2           compiler_4.0.4      
##  [82] bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [85] gamm4_0.2-6          curl_4.3             reprex_0.3.0        
##  [88] statmod_1.4.35       stringi_1.5.3        highr_0.8           
##  [91] ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6   
##  [94] lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [97] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6         
## [100] pillar_1.5.1         lifecycle_1.0.0      bridgesampling_1.0-0
## [103] estimability_1.3     httpuv_1.5.4         R6_2.5.0            
## [106] bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [109] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0  
## [112] MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1    
## [115] withr_2.4.1          shinystan_2.5.0      multcomp_1.4-16     
## [118] mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4         
## [124] rmarkdown_2.7        shiny_1.5.0          lubridate_1.7.9.2   
## [127] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-fahrmeirRegressionModelsMethods2013&#34; class=&#34;csl-entry&#34;&gt;
Fahrmeir, L., Kneib, T., Lang, S., &amp;amp; Marx, B. (2013). &lt;em&gt;Regression: &lt;span&gt;Models&lt;/span&gt;, methods and applications&lt;/em&gt;. &lt;span&gt;Springer-Verlag&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1007/978-3-642-34333-9&#34;&gt;https://doi.org/10.1007/978-3-642-34333-9&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanDataAnalysisUsing2006&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., &amp;amp; Hill, J. (2006). &lt;em&gt;Data analysis using regression and multilevel/hierarchical models&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9780511790942&#34;&gt;https://doi.org/10.1017/CBO9780511790942&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-grolemundDataScience2017&#34; class=&#34;csl-entry&#34;&gt;
Grolemund, G., &amp;amp; Wickham, H. (2017). &lt;em&gt;R for data science&lt;/em&gt;. &lt;span&gt;O’Reilly&lt;/span&gt;. &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;https://r4ds.had.co.nz&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-MASS&#34; class=&#34;csl-entry&#34;&gt;
Ripley, B. (2019). &lt;em&gt;&lt;span&gt;MASS&lt;/span&gt;: &lt;span&gt;Support&lt;/span&gt; functions and datasets for venables and ripley’s &lt;span&gt;MASS&lt;/span&gt;&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=MASS&#34;&gt;https://CRAN.R-project.org/package=MASS&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-broom&#34; class=&#34;csl-entry&#34;&gt;
Robinson, D., &amp;amp; Hayes, A. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;broom&lt;/span&gt;: &lt;span&gt;Convert&lt;/span&gt; statistical analysis objects into tidy tibbles&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=broom&#34;&gt;https://CRAN.R-project.org/package=broom&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vehtariUsingLooPackage2020&#34; class=&#34;csl-entry&#34;&gt;
Vehtari, A., &amp;amp; Gabry, J. (2020). &lt;em&gt;Using the loo package (version &lt;span&gt;&lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;\)&lt;/span&gt;&lt;/span&gt;= 2.0.0)&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html&#34;&gt;https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-loo2020RM&#34; class=&#34;csl-entry&#34;&gt;
Vehtari, A., Gabry, J., Magnusson, M., Yao, Y., Bürkner, P.-C., Paananen, T., &amp;amp; Gelman, A. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;loo&lt;/span&gt; reference manual, &lt;span&gt;Version&lt;/span&gt; 2.3.1&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=loo/loo.pdf&#34;&gt;https://CRAN.R-project.org/package=loo/loo.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-loo&#34; class=&#34;csl-entry&#34;&gt;
Vehtari, A., Gabry, J., Magnusson, M., Yao, Y., &amp;amp; Gelman, A. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;loo&lt;/span&gt;: &lt;span&gt;Efficient&lt;/span&gt; leave-one-out cross-validation and &lt;span&gt;WAIC&lt;/span&gt; for bayesian models&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=loo/&#34;&gt;https://CRAN.R-project.org/package=loo/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vehtariPracticalBayesianModel2017&#34; class=&#34;csl-entry&#34;&gt;
Vehtari, A., Gelman, A., &amp;amp; Gabry, J. (2017). Practical &lt;span&gt;Bayesian&lt;/span&gt; model evaluation using leave-one-out cross-validation and &lt;span&gt;WAIC&lt;/span&gt;. &lt;em&gt;Statistics and Computing&lt;/em&gt;, &lt;em&gt;27&lt;/em&gt;(5), 1413–1432. &lt;a href=&#34;https://doi.org/10.1007/s11222-016-9696-4&#34;&gt;https://doi.org/10.1007/s11222-016-9696-4&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-yaoUsingStackingAverage2018&#34; class=&#34;csl-entry&#34;&gt;
Yao, Y., Vehtari, A., Simpson, D., &amp;amp; Gelman, A. (2018). Using stacking to average &lt;span&gt;Bayesian&lt;/span&gt; predictive distributions (with discussion). &lt;em&gt;Bayesian Analysis&lt;/em&gt;, &lt;em&gt;13&lt;/em&gt;(3), 917–1007. &lt;a href=&#34;https://doi.org/10.1214/17-BA1091&#34;&gt;https://doi.org/10.1214/17-BA1091&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Make rotated Gaussians, Kruschke style</title>
      <link>/post/2018-12-20-make-rotated-gaussians-kruschke-style/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-12-20-make-rotated-gaussians-kruschke-style/</guid>
      <description>
&lt;script src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;[edited Apr 21, 2021]&lt;/p&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;You too can make sideways Gaussian density curves within the tidyverse. Here’s how.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;heres-the-deal-i-like-making-pictures.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Here’s the deal: I like making pictures.&lt;/h2&gt;
&lt;p&gt;Over the past several months, I’ve been slowly chipping away&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; at John Kruschke’s &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;&lt;em&gt;Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan&lt;/em&gt;&lt;/a&gt;. Kruschke has a unique plotting style. One of the quirks is once in a while he likes to express the results of his analyses in plots where he shows the data alongside density curves of the model-implied data-generating distributions. Here’s an example from chapter 19 (p. 563).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kruschke_sideways_Gaussians.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, he has lifespan data (i.e., &lt;code&gt;Longevity&lt;/code&gt;) for fruit flies from five experimental conditions (i.e., &lt;code&gt;CompanionNumber&lt;/code&gt;). Those are the black circles. In this section of the chapter, he used a Gaussian multilevel model in which the mean value for &lt;code&gt;Longevity&lt;/code&gt; had a grand mean in addition to random effects for the five experimental conditions. Those sideways-turned blue Gaussians are his attempt to express the model-implied data generating distributions for each group.&lt;/p&gt;
&lt;p&gt;If you haven’t gone through Kruschke’s text, you should know he relies on base R and all its &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/control-structures.html#for-loops&#34;&gt;loop&lt;/a&gt;y glory. If you carefully go through his code, you can reproduce his plots in that fashion. I’m a &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;tidyverse&lt;/a&gt; man and prefer to avoid writing a &lt;code&gt;for()&lt;/code&gt; loop at all costs. At first, I tried to work with convenience functions within ggplot2 and friends, but only had limited success. After staring long and hard at Kruschke’s base code, I came up with a robust solution, which I’d like to share here.&lt;/p&gt;
&lt;p&gt;In this post, we’ll practice making sideways Gaussians in the Kruschke style. We’ll do so with a simple intercept-only single-level model and then expand our approach to an intercept-only multilevel model like the one in the picture, above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My assumptions&lt;/h2&gt;
&lt;p&gt;For the sake of this post, I’m presuming you’re familiar with &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html&#34;&gt;R&lt;/a&gt;, aware of the &lt;a href=&#34;https://www.rstudio.com/resources/videos/data-science-in-the-tidyverse/&#34;&gt;tidyverse&lt;/a&gt;, and have fit a &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&#34;&gt;Bayesian model&lt;/a&gt; or two. Yes. I admit that’s a narrow crowd. Sometimes the target’s a small one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need data.&lt;/h2&gt;
&lt;p&gt;First, we need data. Here we’ll borrow code from Matthew Kay’s nice &lt;a href=&#34;https://mjskay.github.io/tidybayes/articles/tidy-brms.html&#34;&gt;tutorial&lt;/a&gt; on how to use his great &lt;a href=&#34;https://github.com/mjskay/tidybayes&#34;&gt;tidybayes package&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

set.seed(5)
n           &amp;lt;- 10
n_condition &amp;lt;- 5

abc &amp;lt;-
  tibble(condition = rep(letters[1:5], times = n),
         response  = rnorm(n * 5, mean = c(0, 1, 2, 1, -1), sd = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data structure looks like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(abc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [50 × 2] (S3: tbl_df/tbl/data.frame)
##  $ condition: chr [1:50] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; &amp;quot;d&amp;quot; ...
##  $ response : num [1:50] -0.42 1.692 1.372 1.035 -0.144 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With Kay’s code, we have &lt;code&gt;response&lt;/code&gt; values for five &lt;code&gt;condition&lt;/code&gt;s. All follow the normal distribution and share a common standard deviation. However, they differ in their group means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;% 
  group_by(condition) %&amp;gt;% 
  summarise(mean = mean(response) %&amp;gt;% round(digits = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   condition  mean
##   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 a          0.18
## 2 b          1.01
## 3 c          1.87
## 4 d          1.03
## 5 e         -0.94&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Altogether, the data look like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_grey() + 
            theme(panel.grid = element_blank()))

abc %&amp;gt;%
  ggplot(aes(y = condition, x = response)) +
  geom_point(shape = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s get ready to model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;just-one-intercept&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Just one intercept&lt;/h2&gt;
&lt;p&gt;If you’ve read this far, you know we’re going Bayesian. Let’s open up our favorite Bayesian modeling package, Bürkner’s &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our first model, we’ll ignore the groups and just estimate a grand mean and a standard deviation. Relative to the scale of the &lt;code&gt;abc&lt;/code&gt; data, our priors are modestly &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;regularizing&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- 
  brm(data = abc,
      response ~ 1,
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(student_t(3, 0, 1), class = sigma)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extract the posterior draws and save them as a data frame we’ll call &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(fit1)

glimpse(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 3
## $ b_Intercept &amp;lt;dbl&amp;gt; 0.5729167, 0.6185517, 0.4430281, 0.4383346, 0.8541620, 0.6280931, 0.5159498, 0…
## $ sigma       &amp;lt;dbl&amp;gt; 1.1595969, 1.0350395, 1.0101029, 0.9758173, 1.1676389, 0.9694168, 1.0725615, 1…
## $ lp__        &amp;lt;dbl&amp;gt; -77.17416, -76.99795, -77.92546, -78.35923, -78.25847, -77.55006, -77.13486, -…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If all you want is a quick and dirty way to plot a few of the model-implied Gaussians from the simple model, you can just nest &lt;code&gt;stat_function()&lt;/code&gt; within &lt;code&gt;mapply()&lt;/code&gt; and tack on the original data in a &lt;code&gt;geom_jitter()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How many Gaussians would you like?
n_iter &amp;lt;- 20

tibble(response = c(-4, 4)) %&amp;gt;%
  ggplot(aes(x = response)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 1/2, 
                  color = &amp;quot;steelblue&amp;quot;)
    }, 
    # Enter means and standard deviations here
    mean = post[1:n_iter, &amp;quot;b_Intercept&amp;quot;],
    sd   = post[1:n_iter, &amp;quot;sigma&amp;quot;]
    ) +
  geom_jitter(data = abc, aes(y = -0.02),
              height = .025, shape = 1, alpha = 2/3) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This works pretty okay. But notice the orientation is the usual horizontal. Kruschke’s Gaussians were on their sides. If we switch out our &lt;code&gt;scale_y_continuous()&lt;/code&gt; line with &lt;code&gt;scale_y_reverse()&lt;/code&gt; and add in &lt;code&gt;coord_flip()&lt;/code&gt;, we’ll have it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(response = c(-4, 4)) %&amp;gt;%
  ggplot(aes(x = response)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 1/2, 
                  color = &amp;quot;steelblue&amp;quot;)
    }, 
    mean = post[1:n_iter, &amp;quot;b_Intercept&amp;quot;],
    sd   = post[1:n_iter, &amp;quot;sigma&amp;quot;]
    ) +
  geom_jitter(data = abc, aes(y = -0.02),
              height = .025, shape = 1, alpha = 2/3) +
  scale_y_reverse(NULL, breaks = NULL) +
  coord_flip() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boom. It won’t always be this easy, though.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-intercepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple intercepts&lt;/h2&gt;
&lt;p&gt;Since the &lt;code&gt;response&lt;/code&gt; values are from a combination of five &lt;code&gt;condition&lt;/code&gt; groups, we can fit a multilevel model to compute both the grand mean and the group-level deviations from the grand mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- 
  brm(data = abc,
      response ~ 1 + (1 | condition),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(student_t(3, 0, 1), class = sigma),
                prior(student_t(3, 0, 1), class = sd)),
      cores = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;“Wait. Whoa. I’m so confused”—you say. “What’s a multilevel model, again?” Read this &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;book&lt;/a&gt;, or this &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;book&lt;/a&gt;; start &lt;a href=&#34;https://www.youtube.com/watch?v=2sTQ7TG_85Q&#34;&gt;here&lt;/a&gt; on this lecture series; or even check out &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;my ebook&lt;/a&gt;, starting with Chapter 12.&lt;/p&gt;
&lt;p&gt;Once again, extract the posterior draws and save them as a data frame, &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(fit2)

str(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    4000 obs. of  9 variables:
##  $ b_Intercept             : num  1.16 1.41 1.17 1.49 1.39 ...
##  $ sd_condition__Intercept : num  1.012 0.892 0.871 0.846 0.813 ...
##  $ sigma                   : num  0.581 0.537 0.491 0.537 0.541 ...
##  $ r_condition[a,Intercept]: num  -0.934 -1.313 -0.9 -1.363 -1.337 ...
##  $ r_condition[b,Intercept]: num  -0.336 -0.176 -0.523 -0.273 -0.241 ...
##  $ r_condition[c,Intercept]: num  0.969 0.413 0.8 0.483 0.394 ...
##  $ r_condition[d,Intercept]: num  -0.287 -0.198 -0.31 -0.105 -0.101 ...
##  $ r_condition[e,Intercept]: num  -2.12 -2.32 -2.12 -2.37 -2.25 ...
##  $ lp__                    : num  -52 -53 -54.6 -55.5 -54.1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is where our task becomes difficult. Now each level of &lt;code&gt;condition&lt;/code&gt; has its own mean estimate, which is a combination of the grand mean &lt;code&gt;b_Intercept&lt;/code&gt; and the group-specific deviation, &lt;code&gt;r_condition[a,Intercept]&lt;/code&gt; through &lt;code&gt;r_condition[e,Intercept]&lt;/code&gt;. If all we wanted to do was show the model-implied Gaussians for, say, &lt;code&gt;condition == a&lt;/code&gt;, that’d be a small extension of our last approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(response = c(-4, 4)) %&amp;gt;%
  ggplot(aes(x = response)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 1/2, 
                  color = &amp;quot;steelblue&amp;quot;)
    }, 
    # Here&amp;#39;s the small extension, part a
    mean = post[1:n_iter, &amp;quot;b_Intercept&amp;quot;] + post[1:n_iter, &amp;quot;r_condition[a,Intercept]&amp;quot;],
    sd   = post[1:n_iter, &amp;quot;sigma&amp;quot;]
    ) +
  # The small extension, part b:
  geom_jitter(data = abc %&amp;gt;% filter(condition == &amp;quot;a&amp;quot;), aes(y = 0),
              height = .025, shape = 1, alpha = 2/3) +
  scale_y_reverse(NULL, breaks = NULL) +
  coord_flip() +
  labs(subtitle = &amp;quot;This is just for condition a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The main thing we did was add to the definition of the &lt;code&gt;mean&lt;/code&gt; within &lt;code&gt;mapply()&lt;/code&gt;: &lt;code&gt;mean = post[1:n_iter, &#34;b_Intercept&#34;] + post[1:n_iter, &#34;r_condition[a,Intercept]&#34;]&lt;/code&gt;. Within &lt;code&gt;geom_jitter()&lt;/code&gt;, we also isolated the &lt;code&gt;condition == &#34;a&#34;&lt;/code&gt; cases with &lt;code&gt;filter()&lt;/code&gt;. Simple. However, it’s more of a pickle if we want multiple densities stacked atop/next to one another within the same plot.&lt;/p&gt;
&lt;p&gt;Unfortunately, we can’t extend our &lt;code&gt;mapply(stat_function())&lt;/code&gt; method to the group-level estimates–at least not that I’m aware. But there are other ways. We’ll need a little help from &lt;code&gt;tidybayes::spread_draws()&lt;/code&gt;, about which you can learn more &lt;a href=&#34;https://mjskay.github.io/tidybayes/articles/tidy-brms.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

sd &amp;lt;-
  fit2 %&amp;gt;% 
  spread_draws(b_Intercept, sigma, r_condition[condition,])
  
head(sd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
## # Groups:   condition [5]
##   .chain .iteration .draw b_Intercept sigma condition r_condition
##    &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
## 1      1          1     1        1.16 0.581 a              -0.934
## 2      1          1     1        1.16 0.581 b              -0.336
## 3      1          1     1        1.16 0.581 c               0.969
## 4      1          1     1        1.16 0.581 d              -0.287
## 5      1          1     1        1.16 0.581 e              -2.12 
## 6      1          2     2        1.41 0.537 a              -1.31&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our &lt;code&gt;sp&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html&#34;&gt;tibble&lt;/a&gt;, we have much of the same information we’d get from &lt;code&gt;brms::posterior_samples()&lt;/code&gt;, but in the long format with respect to the random effects for &lt;code&gt;condition&lt;/code&gt;. Also notice that each row is indexed by the chain, iteration, and draw number. Among those, &lt;code&gt;.draw&lt;/code&gt; is the column that corresponds to a unique row like what we’d get from &lt;code&gt;brms::posterior_samples()&lt;/code&gt;. This is the index that ranges from 1 to the number of chains multiplied by the number of post-warmup iterations (i.e., default 4000 in our case).&lt;/p&gt;
&lt;p&gt;But we need to wrangle a bit. Within the &lt;code&gt;expand()&lt;/code&gt; function, we’ll select the columns we’d like to keep within the &lt;code&gt;nesting()&lt;/code&gt; function and then expand the tibble by adding a sequence of &lt;code&gt;response&lt;/code&gt; values ranging from -4 to 4, for each. This sets us up to use the &lt;code&gt;dnorm()&lt;/code&gt; function in the next line to compute the density for each of those &lt;code&gt;response&lt;/code&gt; values based on 20 unique normal distributions for each of the five &lt;code&gt;condition&lt;/code&gt; groups. “Why 20?” Because we need some reasonably small number and 20’s the one Kruschke tended to use in his text and because, well, we set &lt;code&gt;filter(.draw &amp;lt; 21)&lt;/code&gt;. But choose whatever number you like.&lt;/p&gt;
&lt;p&gt;The difficulty, however, is that all of these densities will have a minimum value of around 0 and all will be on the same basic scale. So we need a way to serially shift the density values up the y-axis in such a way that they’ll be sensibly separated by group. As far as I can figure, this’ll take us a couple steps. For the first step, we’ll create an intermediary variable, &lt;code&gt;g&lt;/code&gt;, with which we’ll arbitrarily assign each of our five groups an integer index ranging from 0 to 4.&lt;/p&gt;
&lt;p&gt;The second step is tricky. There we use our &lt;code&gt;g&lt;/code&gt; integers to sequentially shift the density values up. Since our &lt;code&gt;g&lt;/code&gt; value for &lt;code&gt;a == 0&lt;/code&gt;, those we’ll keep 0 as their baseline. As our &lt;code&gt;g&lt;/code&gt; value for &lt;code&gt;b == 1&lt;/code&gt;, the baseline for those will now increase by 1. And so on for the other groups. But we still need to do a little more fiddling. What we want is for the maximum values of the density estimates to be a little lower than the baselines of the ones one grouping variable up. That is, we want the maximum values for the &lt;code&gt;a&lt;/code&gt; densities to fall a little bit below 1 on the y-axis. It’s with the &lt;code&gt;* .75 / max(density)&lt;/code&gt; part of the code that we accomplish that task. If you want to experiment with more or less room between the top and bottom of each density, play around with increasing/decreasing that .75 value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  filter(.draw &amp;lt; 21) %&amp;gt;% 
  expand(nesting(.draw, b_Intercept, sigma, condition, r_condition), 
         response = seq(from = -4, to = 4, length.out = 200)) %&amp;gt;%
  mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma),
         g       = recode(condition,
                          a = 0,
                          b = 1,
                          c = 2,
                          d = 3,
                          e = 4)) %&amp;gt;% 
  mutate(density = g + density * .75 / max(density))

glimpse(sd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 8
## Groups: condition [5]
## $ .draw       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
## $ b_Intercept &amp;lt;dbl&amp;gt; 1.164396, 1.164396, 1.164396, 1.164396, 1.164396, 1.164396, 1.164396, 1.164396…
## $ sigma       &amp;lt;dbl&amp;gt; 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0…
## $ condition   &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;…
## $ r_condition &amp;lt;dbl&amp;gt; -0.9342895, -0.9342895, -0.9342895, -0.9342895, -0.9342895, -0.9342895, -0.934…
## $ response    &amp;lt;dbl&amp;gt; -4.000000, -3.959799, -3.919598, -3.879397, -3.839196, -3.798995, -3.758794, -…
## $ density     &amp;lt;dbl&amp;gt; 1.874794e-12, 3.094499e-12, 5.083339e-12, 8.310546e-12, 1.352172e-11, 2.189555…
## $ g           &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we’ll now be using the same axis for both the densities and the five &lt;code&gt;condition&lt;/code&gt; groups, we’ll need to add a &lt;code&gt;density&lt;/code&gt; column to our &lt;code&gt;abc&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc &amp;lt;-
  abc %&amp;gt;% 
  mutate(density = recode(condition,
                          a = 0,
                          b = 1,
                          c = 2,
                          d = 3,
                          e = 4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time to plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  ggplot(aes(x = response, y = density)) +
  # here we make our density lines
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  # use the original data for the jittered points
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we’re rolling. Let’s make a cosmetic adjustment. Recall that the full range of the normal distribution spans from &lt;span class=&#34;math inline&#34;&gt;\(-\infty\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. At a certain point, it’s just not informative to show the left and right tails. If you look back up at our motivating example, you’ll note Kruschke’s densities stopped well before trailing off into the tails. If you look closely to the code from his text, you’ll see he’s just showing the inner 95-percentile range for each. To follow suit, we can compute those ranges with &lt;code&gt;qnorm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  mutate(ll = qnorm(.025, mean = b_Intercept + r_condition, sd = sigma),
         ul = qnorm(.975, mean = b_Intercept + r_condition, sd = sigma))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have our lower- and upper-level points for each iteration, we can limit the ranges of our Gaussians with &lt;code&gt;filter()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  
  ggplot(aes(x = response, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Oh man, just look how sweet that is. Although I prefer our current method, another difference between it and Kruschke’s example is all of his densities are the same relative height. In all our plots so far, though, the densities differ by their heights. We’ll need a slight adjustment in our &lt;code&gt;sd&lt;/code&gt; workflow for that. All we need to do is insert a &lt;code&gt;group_by()&lt;/code&gt; statement between the two &lt;code&gt;mutate()&lt;/code&gt; lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma),
         g       = recode(condition,
                          a = 0,
                          b = 1,
                          c = 2,
                          d = 3,
                          e = 4)) %&amp;gt;% 
  # here&amp;#39;s the new line
  group_by(.draw) %&amp;gt;% 
  mutate(density = g + density * .75 / max(density))

# now plot
sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  ggplot(aes(x = response, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nice. “But wait!”, you say. “We wanted our Gaussians to be on their sides.” We can do that in at least two ways. At this point, the quickest way is to use our &lt;code&gt;scale_y_reverse() + coord_flip()&lt;/code&gt; combo from before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  
  ggplot(aes(x = response, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_reverse(&amp;quot;condition&amp;quot;,
                  breaks = 0:4,
                  labels = letters[1:5]) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way to get those sideways Gaussians is to alter our &lt;code&gt;sd&lt;/code&gt; data workflow. The main difference is this time we change the original &lt;code&gt;mutate(density = g + density * .75 / max(density))&lt;/code&gt; line to &lt;code&gt;mutate(density = g - density * .75 / max(density))&lt;/code&gt;. In case you missed it, the only difference is we changed the &lt;code&gt;+&lt;/code&gt; to a &lt;code&gt;-&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  # step one: starting fresh
  mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma)) %&amp;gt;% 
  group_by(.draw) %&amp;gt;% 
  # step two: now SUBTRACTING density from g within the equation
  mutate(density = g - density * .75 / max(density))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now in our global &lt;code&gt;aes()&lt;/code&gt; statement in the plot, we put &lt;code&gt;density&lt;/code&gt; on the x and &lt;code&gt;response&lt;/code&gt; on the y. We need to take a few other subtle steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Switch out &lt;code&gt;geom_line()&lt;/code&gt; for &lt;code&gt;geom_path()&lt;/code&gt; (see &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/geom_path.html&#34;&gt;here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Drop the &lt;code&gt;height&lt;/code&gt; argument within &lt;code&gt;geom_jitter()&lt;/code&gt; for &lt;code&gt;width&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Switch out &lt;code&gt;scale_y_continuous()&lt;/code&gt; for &lt;code&gt;scale_x_continuous()&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Though totally not necessary, we’ll add a little something extra by coloring the Gaussians by their means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  
  ggplot(aes(x = density, y = response)) +
  geom_path(aes(group = interaction(.draw, g), 
                color = b_Intercept + r_condition),
            alpha = 1/2, size = 1/3, show.legend = F) +
  geom_jitter(data = abc,
              width = .05, shape = 1, alpha = 2/3) +
  scale_x_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5]) +
  scale_color_viridis_c(option = &amp;quot;A&amp;quot;, end = .92)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you have it–Kruschke-style sideways Gaussians for your model plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;afterward&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Afterward&lt;/h2&gt;
&lt;p&gt;After releasing the initial version of this post, some of us had a lively twitter discussion on how to improve the code.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Ah, hrm. Took some digging, but it looks like negative density + setting `min_height = NA` (otherwise negative values are cut off) might work &lt;a href=&#34;https://t.co/gmF9kpo2T7&#34;&gt;pic.twitter.com/gmF9kpo2T7&lt;/a&gt;&lt;/p&gt;&amp;mdash; Matthew Kay (@mjskay) &lt;a href=&#34;https://twitter.com/mjskay/status/1076395687020056576?ref_src=twsrc%5Etfw&#34;&gt;December 22, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Part of that discussion had to do with the possibility of using functions from &lt;a href=&#34;https://twitter.com/ClausWilke/&#34;&gt;Claus Wilke&lt;/a&gt;’s great &lt;a href=&#34;https://github.com/clauswilke/ggridges&#34;&gt;ggridges package&lt;/a&gt;. After some great efforts, especially from &lt;a href=&#34;https://twitter.com/mjskay/&#34;&gt;Matthew Kay&lt;/a&gt;, we came up with solutions. In this section, we’ll cover them in some detail.&lt;/p&gt;
&lt;p&gt;First, here’s a more compact way to prepare the data for the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  mutate(density  = pmap(list(response, mu, sigma), dnorm)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  group_by(.draw) %&amp;gt;% 
  mutate(density  = density * .75 / max(density)) %&amp;gt;% 
  
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required when using unnest().
## Please use `cols = c(response, density)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 12
## Groups: .draw [20]
## $ condition  &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;,…
## $ .row       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ .chain     &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .iteration &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .draw      &amp;lt;int&amp;gt; 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,…
## $ .value     &amp;lt;dbl&amp;gt; -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -…
## $ mu         &amp;lt;dbl&amp;gt; -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -…
## $ sigma      &amp;lt;dbl&amp;gt; 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.…
## $ lower      &amp;lt;dbl&amp;gt; -1.103438, -1.103438, -1.103438, -1.103438, -1.103438, -1.103438, -1.103438, -1…
## $ upper      &amp;lt;dbl&amp;gt; 1.011903, 1.011903, 1.011903, 1.011903, 1.011903, 1.011903, 1.011903, 1.011903,…
## $ response   &amp;lt;dbl&amp;gt; -1.1034383, -1.0928085, -1.0821786, -1.0715488, -1.0609189, -1.0502890, -1.0396…
## $ density    &amp;lt;dbl&amp;gt; 0.1098804, 0.1141834, 0.1186089, 0.1231581, 0.1278322, 0.1326322, 0.1375591, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This could use some walking out. With the first two lines, we made a &lt;span class=&#34;math inline&#34;&gt;\(5 \times 1\)&lt;/span&gt; tibble containing the five levels of &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; through &lt;code&gt;f&lt;/code&gt;. The &lt;code&gt;add_fitted_draws()&lt;/code&gt; function comes from tidybayes. The first argument took our brms model fit, &lt;code&gt;fit2&lt;/code&gt;. With the &lt;code&gt;n&lt;/code&gt; argument, we indicated we just wanted &lt;code&gt;20&lt;/code&gt; draws. With &lt;code&gt;dpar&lt;/code&gt;, we requested distributional regression parameters in the output. In our case, those were the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; values for each level of &lt;code&gt;condition&lt;/code&gt;. Here’s what that looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
## # Groups:   condition, .row [1]
##   condition  .row .chain .iteration .draw .value     mu sigma
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 a             1     NA         NA    57 0.562  0.562  0.645
## 2 a             1     NA         NA    97 0.163  0.163  0.593
## 3 a             1     NA         NA   277 0.0490 0.0490 0.682
## 4 a             1     NA         NA   305 0.386  0.386  0.607
## 5 a             1     NA         NA   360 0.162  0.162  0.563
## 6 a             1     NA         NA   496 0.0224 0.0224 0.650&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we established the lower- and upper-bounds bounds for the density lines, which were 95% intervals in this example. Within the second &lt;code&gt;mutate()&lt;/code&gt; function, we used the &lt;a href=&#34;https://purrr.tidyverse.org/reference/map2.html&#34;&gt;&lt;code&gt;purrr::map2()&lt;/code&gt;&lt;/a&gt; function to feed those two values into the first two arguments of the &lt;code&gt;seq()&lt;/code&gt; function. Those arguments, recall, are &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt;. We then hard coded &lt;code&gt;200&lt;/code&gt; into the &lt;code&gt;length.out&lt;/code&gt; argument. As a result, we turned our regular old tibble into a &lt;a href=&#34;https://tidyr.tidyverse.org/reference/nest.html&#34;&gt;nested tibble&lt;/a&gt;. In each row of our new &lt;code&gt;response&lt;/code&gt; column, we now have a &lt;span class=&#34;math inline&#34;&gt;\(200 \times 1\)&lt;/span&gt; data frame containing the &lt;code&gt;seq()&lt;/code&gt; output. If you’re new to nested data structures, I recommend checking out Hadley Wickham’s &lt;a href=&#34;https://www.youtube.com/watch?v=rz3_FDVt9eg&#34;&gt;&lt;em&gt;Managing many models with R&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 11
## # Groups:   condition, .row [1]
##   condition  .row .chain .iteration .draw .value    mu sigma  lower upper response   
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;     
## 1 a             1     NA         NA   130  0.105 0.105 0.553 -0.979  1.19 &amp;lt;dbl [200]&amp;gt;
## 2 a             1     NA         NA   647  0.121 0.121 0.602 -1.06   1.30 &amp;lt;dbl [200]&amp;gt;
## 3 a             1     NA         NA  1087  0.260 0.260 0.727 -1.17   1.68 &amp;lt;dbl [200]&amp;gt;
## 4 a             1     NA         NA  1343  0.396 0.396 0.587 -0.754  1.55 &amp;lt;dbl [200]&amp;gt;
## 5 a             1     NA         NA  1618  0.342 0.342 0.500 -0.638  1.32 &amp;lt;dbl [200]&amp;gt;
## 6 a             1     NA         NA  1701  0.238 0.238 0.569 -0.878  1.35 &amp;lt;dbl [200]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Much as the &lt;code&gt;purrr::map2()&lt;/code&gt; function allowed us to iterate over two arguments, the &lt;code&gt;purrr::pmap()&lt;/code&gt; function will allow us to iterate over an arbitrary number of arguments. In the case of our third &lt;code&gt;mutate()&lt;/code&gt; function, we’ll iterate over the first three arguments of the &lt;code&gt;dnorm()&lt;/code&gt; function. In case you forgot, those arguments are &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;mean&lt;/code&gt;, and &lt;code&gt;sd&lt;/code&gt;, respectively. Within our &lt;code&gt;list()&lt;/code&gt;, we indicated we wanted to insert into them the &lt;code&gt;response&lt;/code&gt;, &lt;code&gt;mu&lt;/code&gt;, and &lt;code&gt;sigma&lt;/code&gt; values. This returns the desired &lt;code&gt;density&lt;/code&gt; values. Since our &lt;code&gt;map2()&lt;/code&gt; and &lt;code&gt;pmap()&lt;/code&gt; operations returned a nested tibble, we then followed them up with the &lt;code&gt;unnest()&lt;/code&gt; function to make it easier to access the results.&lt;/p&gt;
&lt;p&gt;Before &lt;code&gt;unnest&lt;/code&gt;ing, our nested tibble had 100 observations. After &lt;code&gt;unnest()&lt;/code&gt;, we converted it to the long format, resulting in &lt;span class=&#34;math inline&#34;&gt;\(100 \times 200 = 20,000\)&lt;/span&gt; observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  mutate(density  = pmap(list(response, mu, sigma), dnorm)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required when using unnest().
## Please use `cols = c(response, density)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 12
## Groups: condition, .row [5]
## $ condition  &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;,…
## $ .row       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ .chain     &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .iteration &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .draw      &amp;lt;int&amp;gt; 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,…
## $ .value     &amp;lt;dbl&amp;gt; 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.…
## $ mu         &amp;lt;dbl&amp;gt; 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.…
## $ sigma      &amp;lt;dbl&amp;gt; 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.…
## $ lower      &amp;lt;dbl&amp;gt; -0.9057199, -0.9057199, -0.9057199, -0.9057199, -0.9057199, -0.9057199, -0.9057…
## $ upper      &amp;lt;dbl&amp;gt; 1.278939, 1.278939, 1.278939, 1.278939, 1.278939, 1.278939, 1.278939, 1.278939,…
## $ response   &amp;lt;dbl&amp;gt; -0.9057199, -0.8947417, -0.8837635, -0.8727853, -0.8618071, -0.8508289, -0.8398…
## $ density    &amp;lt;dbl&amp;gt; 0.1048678, 0.1089746, 0.1131982, 0.1175399, 0.1220008, 0.1265818, 0.1312839, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hopefully, our last two lines look familiar. We &lt;code&gt;group_by(.draw)&lt;/code&gt; just like in previous examples. However, our final &lt;code&gt;mutate()&lt;/code&gt; line is a little simpler than in previous versions. Before we had to make that intermediary variable, &lt;code&gt;g&lt;/code&gt;. Because we intend to plot these data with help from ggridges, we no longer have need for &lt;code&gt;g&lt;/code&gt;. You’ll see. But the upshot is the only reason we’re adding this last &lt;code&gt;mutate()&lt;/code&gt; line is to scale all the Gaussians to have the same maximum height the way Kruschke did.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afd &amp;lt;-
  abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  mutate(density  = pmap(list(response, mu, sigma), dnorm)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  group_by(.draw) %&amp;gt;% 
  mutate(density  = density * .75 / max(density))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required when using unnest().
## Please use `cols = c(response, density)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(afd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 12
## Groups: .draw [20]
## $ condition  &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;,…
## $ .row       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ .chain     &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .iteration &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .draw      &amp;lt;int&amp;gt; 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,…
## $ .value     &amp;lt;dbl&amp;gt; 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.…
## $ mu         &amp;lt;dbl&amp;gt; 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.…
## $ sigma      &amp;lt;dbl&amp;gt; 0.570355, 0.570355, 0.570355, 0.570355, 0.570355, 0.570355, 0.570355, 0.570355,…
## $ lower      &amp;lt;dbl&amp;gt; -0.857776, -0.857776, -0.857776, -0.857776, -0.857776, -0.857776, -0.857776, -0…
## $ upper      &amp;lt;dbl&amp;gt; 1.377974, 1.377974, 1.377974, 1.377974, 1.377974, 1.377974, 1.377974, 1.377974,…
## $ response   &amp;lt;dbl&amp;gt; -0.8577760, -0.8465410, -0.8353061, -0.8240712, -0.8128362, -0.8016013, -0.7903…
## $ density    &amp;lt;dbl&amp;gt; 0.1098804, 0.1141834, 0.1186089, 0.1231581, 0.1278322, 0.1326322, 0.1375591, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s open &lt;a href=&#34;https://github.com/clauswilke/ggridges&#34;&gt;ggridges&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggridges)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how contrary to before, we set the global y axis to our &lt;code&gt;condition&lt;/code&gt; grouping variable. It’s within the &lt;code&gt;geom_ridgeline()&lt;/code&gt; function that we now specify &lt;code&gt;height = density&lt;/code&gt;. Other than that, the main thing to point out is you might want to adjust the &lt;code&gt;ylim&lt;/code&gt; parameters. Otherwise the margins aren’t the best.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afd %&amp;gt;%
  
  ggplot(aes(x = response, y = condition)) +
  geom_ridgeline(aes(height = density, group = interaction(condition, .draw)),
                 fill = NA, size = 1/3, color = adjustcolor(&amp;quot;steelblue&amp;quot;, alpha.f = 1/2)) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  coord_cartesian(ylim = c(1.25, 5.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;“But I wanted my Gaussians tipped to the left!”, you say. Yep, we can do that, too. Three things: First, we’ll want to adjust the &lt;code&gt;height&lt;/code&gt; parameter to &lt;code&gt;-density&lt;/code&gt;. We want our Gaussians to extend under their baselines. Along with that, we need to include &lt;code&gt;min_height = NA&lt;/code&gt;. Finally, we’ll switch out &lt;code&gt;coord_cartesian()&lt;/code&gt; for good old &lt;code&gt;coord_flip()&lt;/code&gt;. And you can adjust your &lt;code&gt;ylim&lt;/code&gt; parameters as desired.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afd %&amp;gt;%
  
  ggplot(aes(x = response, y = condition)) +
  geom_ridgeline(aes(height = -density, group = interaction(condition, .draw)),
                 fill = NA, size = 1/3, color = adjustcolor(&amp;quot;steelblue&amp;quot;, alpha.f = 1/2),
                 min_height = NA) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  coord_flip(ylim = c(0.5, 4.75))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think it’s important to note that I’ve never met any of the people who helped me with this project. Academic twitter, man–it’s a good place to be.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggridges_0.5.2  tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0  
##  [7] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3  
## [13] tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] svUnit_1.0.3         splines_4.0.4        crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] viridisLite_0.3.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3       
##  [53] arrayhelpers_1.1-0   ellipsis_0.3.1       pkgconfig_2.0.3      loo_2.4.1           
##  [57] farver_2.0.3         dbplyr_2.0.0         utf8_1.1.4           tidyselect_1.1.0    
##  [61] labeling_0.4.2       rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1       
##  [65] munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [69] generics_0.1.0       broom_0.7.5          evaluate_0.14        fastmap_1.0.1       
##  [73] yaml_2.2.1           processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [77] nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [81] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [85] gamm4_0.2-6          curl_4.3             reprex_0.3.0         statmod_1.4.35      
##  [89] stringi_1.5.3        highr_0.8            ps_1.6.0             blogdown_1.3        
##  [93] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [97] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1        
## [101] lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [105] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [109] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [113] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [117] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [125] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I’ve made a lot of progress working through Kruschke’s material since the initial release of this blog post. You can find the results in an ebook, &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian meta-analysis in brms</title>
      <link>/post/bayesian-meta-analysis/</link>
      <pubDate>Sun, 14 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/post/bayesian-meta-analysis/</guid>
      <description>
&lt;script src=&#34;/post/bayesian-meta-analysis/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;[edited Apr 21, 2021]&lt;/p&gt;
&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble&lt;/h2&gt;
&lt;p&gt;I released the first &lt;a href=&#34;https://bookdown.org&#34;&gt;bookdown&lt;/a&gt; version of my &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt; with brms, ggplot2, and the tidyverse&lt;/a&gt; project a couple weeks ago. I consider it the 0.9.0 version&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. I wanted a little time to step back from the project before giving it a final edit for the first major edition. I also wanted to give others a little time to take a look and suggest edits, which some thankfully have.&lt;/p&gt;
&lt;p&gt;Now some time has passed, it’s become clear I’d like to add a bonus section on Bayesian meta-analysis. IMO, this is a natural extension of the hierarchical models McElreath introduced in chapter’s 12 and 13 of &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;his text&lt;/a&gt; and of the measurement-error models he introduced in chapter 14. So the purpose of this post is to present a rough draft of how I’d like to introduce fitting meta-analyses with Bürkner’s great brms package.&lt;/p&gt;
&lt;p&gt;I intend to tack this section onto the end of chapter 14. If you have any &lt;a href=&#34;https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse/issues&#34;&gt;constrictive criticisms, please pass them along&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here’s the rough draft (which I updated on 2018-11-12):&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rough-draft-meta-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rough draft: Meta-analysis&lt;/h2&gt;
&lt;p&gt;If your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use &lt;code&gt;brms::brm()&lt;/code&gt; to fit Bayesian meta-analyses, too.&lt;/p&gt;
&lt;p&gt;Before we proceed, I should acknowledge that this section is heavily influenced by Matti Vourre’s great blog post, &lt;a href=&#34;https://mvuorre.github.io/blog/posts/2016-09-29-bayesian-meta-analysis/&#34;&gt;&lt;em&gt;Meta-analysis is a special case of Bayesian multilevel modeling&lt;/em&gt;&lt;/a&gt;. And since McElreath’s text doesn’t directly address meta-analyses, we’ll take further inspiration from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;&lt;em&gt;Bayesian data analysis, Third edition&lt;/em&gt;&lt;/a&gt;. We’ll let Gelman and colleagues introduce the topic:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Discussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected &lt;em&gt;a priori&lt;/em&gt; to have predictable effects favoring one study over another.… This third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis…&lt;/p&gt;
&lt;p&gt;The first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125–126, &lt;em&gt;emphasis&lt;/em&gt; in the original)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The basic version of a Bayesian meta-analysis follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i \sim \text{Normal}(\theta_i, \sigma_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; = the point estimate for the effect size of a single study, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, which is presumed to have been a draw from a Normal distribution centered on &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;. The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. Which we do. The standard error from study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is specified &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt;, which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt;, here. Those values we take directly from the original studies.&lt;/p&gt;
&lt;p&gt;Building on the model, we further presume that study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is itself just one draw from a population of related studies, each of which have their own effect sizes. As such. we presume &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; itself has a distribution following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_i \sim \text{Normal} (\mu, \tau)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the meta-analytic effect (i.e., the population mean) and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; is the variation around that mean, what you might also think of as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\tau\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Since there’s no example of a meta-analysis in the text, we’ll have to look elsewhere. We’ll focus on Gershoff and Grogan-Kaylor’s (2016) paper, &lt;a href=&#34;https://pdfs.semanticscholar.org/0d03/a2e9f085f0a268b4c0a52f5ac31c17a3e5f3.pdf&#34;&gt;&lt;em&gt;Spanking and Child Outcomes: Old Controversies and New Meta-Analyses&lt;/em&gt;&lt;/a&gt;. From their introduction, we read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Around the world, most children (80%) are spanked or otherwise physically punished by their parents (&lt;a href=&#34;https://www.unicef.org/publications/index_74865.html&#34;&gt;UNICEF, 2014&lt;/a&gt;). The question of whether parents should spank their children to correct misbehaviors sits at a nexus of arguments from ethical, religious, and human rights perspectives both in the U.S. and around the world (&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/cdep.12038&#34;&gt;Gershoff, 2013&lt;/a&gt;). Several hundred studies have been conducted on the associations between parents’ use of spanking or physical punishment and children’s behavioral, emotional, cognitive, and physical outcomes, making spanking one of the most studied aspects of parenting. What has been learned from these hundreds of studies? (p. 453)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our goal will be to learn Bayesian meta-analysis by answering part of that question. I’ve transcribed the values directly from Gershoff and Grogan-Kaylor’s paper and saved them as a file called &lt;code&gt;spank.xlsx&lt;/code&gt;.
You can find the data in &lt;a href=&#34;https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse&#34;&gt;this project’s GitHub repository&lt;/a&gt;. Let’s load them and &lt;code&gt;glimpse()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spank &amp;lt;- readxl::read_excel(&amp;quot;spank.xlsx&amp;quot;)

library(tidyverse)
glimpse(spank)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 111
## Columns: 8
## $ study   &amp;lt;chr&amp;gt; &amp;quot;Bean and Roberts (1981)&amp;quot;, &amp;quot;Day and Roberts (1983)&amp;quot;, &amp;quot;Minton, …
## $ year    &amp;lt;dbl&amp;gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, 2005, 19…
## $ outcome &amp;lt;chr&amp;gt; &amp;quot;Immediate defiance&amp;quot;, &amp;quot;Immediate defiance&amp;quot;, &amp;quot;Immediate defianc…
## $ between &amp;lt;dbl&amp;gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,…
## $ within  &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,…
## $ d       &amp;lt;dbl&amp;gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14, -0.18,…
## $ ll      &amp;lt;dbl&amp;gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, -0.42, -…
## $ ul      &amp;lt;dbl&amp;gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, 0.13, 2.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this paper, the effect size of interest is a &lt;em&gt;Cohen’s d&lt;/em&gt;, derived from the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d = \frac{\mu_\text{treatment} - \mu_\text{comparison}}{\sigma_{pooled}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_{pooled} = \sqrt{\frac{((n_1 - 1) \sigma_1^2) + ((n_2 - 1) \sigma_2^2)}{n_1 + n_2 -2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To help make the equation for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; clearer for our example, we might re-express it as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d = \frac{\mu_\text{spanked} - \mu_\text{not spanked}}{\sigma_{pooled}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;McElreath didn’t really focus on effect sizes in his text. If you need a refresher, you might check out Kelley and Preacher’s &lt;a href=&#34;https://www.researchgate.net/profile/Ken_Kelley/publication/270757972_On_Effect_Size/links/0046351b0cd48217ce000000/On-Effect-Size.pdf&#34;&gt;&lt;em&gt;On effect size&lt;/em&gt;&lt;/a&gt;. But in words, &lt;em&gt;Cohen’s d&lt;/em&gt; is a standardized mean difference between two groups.&lt;/p&gt;
&lt;p&gt;So if you look back up at the results of &lt;code&gt;glimpse(spank)&lt;/code&gt;, you’ll notice the column &lt;code&gt;d&lt;/code&gt;, which is indeed a vector of &lt;em&gt;Cohen’s d&lt;/em&gt; effect sizes. The last two columns, &lt;code&gt;ll&lt;/code&gt; and &lt;code&gt;ul&lt;/code&gt; are the lower and upper limits of the associated 95% frequentist confidence intervals. But we don’t want confidence intervals for our &lt;code&gt;d&lt;/code&gt;-values; we want their standard errors. Fortunately, we can compute those with the following formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SE = \frac{\text{upper limit } – \text{lower limit}}{3.92}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here it is in code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spank &amp;lt;-
  spank %&amp;gt;% 
  mutate(se = (ul - ll) / 3.92)

glimpse(spank)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 111
## Columns: 9
## $ study   &amp;lt;chr&amp;gt; &amp;quot;Bean and Roberts (1981)&amp;quot;, &amp;quot;Day and Roberts (1983)&amp;quot;, &amp;quot;Minton, …
## $ year    &amp;lt;dbl&amp;gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, 2005, 19…
## $ outcome &amp;lt;chr&amp;gt; &amp;quot;Immediate defiance&amp;quot;, &amp;quot;Immediate defiance&amp;quot;, &amp;quot;Immediate defianc…
## $ between &amp;lt;dbl&amp;gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,…
## $ within  &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,…
## $ d       &amp;lt;dbl&amp;gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14, -0.18,…
## $ ll      &amp;lt;dbl&amp;gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, -0.42, -…
## $ ul      &amp;lt;dbl&amp;gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, 0.13, 2.…
## $ se      &amp;lt;dbl&amp;gt; 0.52040816, 0.71683673, 0.21683673, 0.47193878, 0.47193878, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now our data are ready, we can express our first Bayesian meta-analysis with the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\text{d}_i &amp;amp; \sim &amp;amp; \text{Normal}(\theta_i, \sigma_i = \text{se}_i) \\
\theta_i &amp;amp; \sim &amp;amp; \text{Normal} (\mu, \tau) \\
\mu &amp;amp; \sim &amp;amp; \text{Normal} (0, 1) \\
\tau &amp;amp; \sim &amp;amp; \text{HalfCauchy} (0, 1)
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The last two lines, of course, spell out our priors. In psychology, it’s pretty rare to see &lt;em&gt;Cohen’s d&lt;/em&gt;-values greater than the absolute value of &lt;span class=&#34;math inline&#34;&gt;\(\pm 1\)&lt;/span&gt;. So in the absence of more specific domain knowledge–which I don’t have–, it seems like &lt;span class=&#34;math inline&#34;&gt;\(\text{Normal} (0, 1)\)&lt;/span&gt; is a reasonable place to start. And just like McElreath used &lt;span class=&#34;math inline&#34;&gt;\(\text{HalfCauchy} (0, 1)\)&lt;/span&gt; as the default prior for the group-level standard deviations, &lt;a href=&#34;https://psyarxiv.com/7tbrm/&#34;&gt;it makes sense to use it here&lt;/a&gt; for our meta-analytic &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; parameter.&lt;/p&gt;
&lt;p&gt;Let’s load brms.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the code for the first model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b14.5 &amp;lt;- 
  brm(data = spank, family = gaussian,
      d | se(se) ~ 1 + (1 | study),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(cauchy(0, 1), class = sd)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing you might notice is our &lt;code&gt;se(se)&lt;/code&gt; function excluded the &lt;code&gt;sigma&lt;/code&gt; argument. If you recall from section 14.1, we specified &lt;code&gt;sigma = T&lt;/code&gt; in our measurement-error models. The brms default is that within &lt;code&gt;se()&lt;/code&gt;, &lt;code&gt;sigma = FALSE&lt;/code&gt;. As such, we have no estimate for sigma the way we would if we were doing this analysis with the raw data from the studies. Hopefully this makes sense. The uncertainty around the &lt;code&gt;d&lt;/code&gt;-value for each study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; has already been encoded in the data as &lt;code&gt;se&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This brings us to another point. We typically perform meta-analyses on data summaries. In my field and perhaps in yours, this is due to the historical accident that it has not been the norm among researchers to make their data publicly available. So effect size summaries were the best we typically had. However, times are changing (e.g., &lt;a href=&#34;https://www.apa.org/monitor/2017/11/trends-open-science.aspx&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://www.blog.google/products/search/making-it-easier-discover-datasets/&#34;&gt;here&lt;/a&gt;). If the raw data from all the studies for your meta-analysis are available, you can just fit a multilevel model in which the data are nested in the studies. Heck, you could even allow the studies to vary by &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; by taking the &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html#a-simple-distributional-model&#34;&gt;distributional modeling approach&lt;/a&gt; and specify something like &lt;code&gt;sigma ~ 0 + study&lt;/code&gt; or even &lt;code&gt;sigma ~ 1 + (1 | study)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But enough technical talk. Let’s look at the model results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(b14.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: d | se(se) ~ 1 + (1 | study) 
##    Data: spank (Number of observations: 111) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~study (Number of levels: 76) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.26      0.03     0.21     0.33 1.01      754     1582
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.38      0.04     0.31     0.45 1.00      605     1021
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, in our simple Bayesian meta-analysis, we have a population &lt;em&gt;Cohen’s d&lt;/em&gt; of about 0.38. Our estimate for &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, 0.26, suggests we have quite a bit of between-study variability. One question you might ask is: &lt;em&gt;What exactly are these&lt;/em&gt; Cohen’s d&lt;em&gt;s measuring, anyways?&lt;/em&gt; We’ve encoded that in the &lt;code&gt;outcome&lt;/code&gt; vector of the &lt;code&gt;spank&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spank %&amp;gt;% 
  distinct(outcome) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;outcome&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Immediate defiance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Low moral internalization&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child aggression&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child antisocial behavior&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child externalizing behavior problems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child internalizing behavior problems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child mental health problems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Child alcohol or substance abuse&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Negative parent–child relationship&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Impaired cognitive ability&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Low self-esteem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Low self-regulation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Victim of physical abuse&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Adult antisocial behavior&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Adult mental health problems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Adult alcohol or substance abuse&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Adult support for physical punishment&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are a few things to note. First, with the possible exception of &lt;code&gt;Adult support for physical punishment&lt;/code&gt;, all of the outcomes are negative. We prefer conditions associated with lower values for things like &lt;code&gt;Child aggression&lt;/code&gt; and &lt;code&gt;Adult mental health problems&lt;/code&gt;. Second, the way the data are coded, larger effect sizes are interpreted as more negative outcomes associated with children having been spanked. That is, our analysis suggests spanking children is associated with worse outcomes. What might not be immediately apparent is that even though there are 111 cases in the data, there are only 76 distinct studies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spank %&amp;gt;% 
  distinct(study) %&amp;gt;% 
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##       n
##   &amp;lt;int&amp;gt;
## 1    76&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, some studies have multiple outcomes. In order to better accommodate the &lt;code&gt;study&lt;/code&gt;- and &lt;code&gt;outcome&lt;/code&gt;-level variances, let’s fit a cross-classified Bayesian meta-analysis reminiscent of the cross-classified chimp model from Chapter 13.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b14.6 &amp;lt;- 
  brm(data = spank, family = gaussian,
      d | se(se) ~ 1 + (1 | study) + (1 | outcome),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(cauchy(0, 1), class = sd)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(b14.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: d | se(se) ~ 1 + (1 | study) + (1 | outcome) 
##    Data: spank (Number of observations: 111) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~outcome (Number of levels: 17) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.08      0.03     0.04     0.14 1.00     1018     1756
## 
## ~study (Number of levels: 76) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.25      0.03     0.20     0.31 1.00      827     1571
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.36      0.04     0.28     0.43 1.01      664     1562
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have two &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; parameters. We might plot them to get a sense of where the variance is at.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(b14.6) %&amp;gt;% 
  select(starts_with(&amp;quot;sd&amp;quot;)) %&amp;gt;% 
  gather(key, tau) %&amp;gt;% 
  mutate(key = str_remove(key, &amp;quot;sd_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;__Intercept&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = tau, fill = key)) +
  geom_density(color = &amp;quot;transparent&amp;quot;, alpha = 2/3) +
  scale_fill_viridis_d(NULL, end = .85) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(tau)) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-meta-analysis/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So at this point, the big story is there’s more variability between the studies than there is the outcomes. But I still want to get a sense of the individual outcomes. Here we’ll use &lt;code&gt;tidybayes::stat_halfeye()&lt;/code&gt; to help us make our version of a &lt;a href=&#34;https://cran.r-project.org/web/packages/forestplot/vignettes/forestplot.html&#34;&gt;forest plot&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load tidybayes
library(tidybayes)

b14.6 %&amp;gt;%
  spread_draws(b_Intercept, r_outcome[outcome,]) %&amp;gt;%
  # add the grand mean to the group-specific deviations
  mutate(mu = b_Intercept + r_outcome) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(outcome = str_replace_all(outcome, &amp;quot;[.]&amp;quot;, &amp;quot; &amp;quot;)) %&amp;gt;% 

  # plot
  ggplot(aes(x = mu, y = reorder(outcome, mu))) +
  geom_vline(xintercept = fixef(b14.6)[1, 1], color = &amp;quot;white&amp;quot;, size = 1) +
  geom_vline(xintercept = fixef(b14.6)[1, 3:4], color = &amp;quot;white&amp;quot;, linetype = 2) +
  stat_halfeye(.width = .95, size = 2/3) +
  labs(x = expression(italic(&amp;quot;Cohen&amp;#39;s d&amp;quot;)),
       y = NULL) +
  theme(panel.grid   = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-meta-analysis/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The solid and dashed vertical white lines in the background mark off the grand mean (i.e., the meta-analytic effect) and its 95% intervals. But anyway, there’s not a lot of variability across the outcomes. Let’s go one step further with the model. Doubling back to Gelman and colleagues, we read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When assuming exchangeability we assume there are no important covariates that might form the basis of a more complex model, and this assumption (perhaps misguidedly) is widely adopted in meta-analysis. What if other information (in addition to the data &lt;span class=&#34;math inline&#34;&gt;\((n, y)\)&lt;/span&gt;) is available to distinguish among the &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; studies in a meta-analysis, so that an exchangeable model is inappropriate? In this situation, we can expand the framework of the model to be exchangeable in the observed data and covariates, for example using a hierarchical regression model. (p. 126)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One important covariate Gershoff and Grogan-Kaylor addressed in their meta-analysis was the type of study. The 76 papers they based their meta-analysis on contained both between- and within-participants designs. In the &lt;code&gt;spank&lt;/code&gt; data, we’ve dummy coded that information with the &lt;code&gt;between&lt;/code&gt; and &lt;code&gt;within&lt;/code&gt; vectors. Both are dummy variables and &lt;code&gt;within&lt;/code&gt; = 1 - &lt;code&gt;between&lt;/code&gt;. Here are the counts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spank %&amp;gt;% 
  count(between)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   between     n
##     &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1       0    71
## 2       1    40&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I use dummies in my models, I prefer to have the majority group stand as the reference category. As such, I typically name those variables by the minority group. In this case, most occasions are based on within-participant designs. Thus, we’ll go ahead and add the &lt;code&gt;between&lt;/code&gt; variable to the model. While we’re at it, we’ll practice using the &lt;code&gt;0 + intercept&lt;/code&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b14.7 &amp;lt;- 
  brm(data = spank, family = gaussian,
      d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome),
      prior = c(prior(normal(0, 1), class = b),
                prior(cauchy(0, 1), class = sd)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(b14.7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome) 
##    Data: spank (Number of observations: 111) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~outcome (Number of levels: 17) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.08      0.03     0.04     0.14 1.00     1490     2304
## 
## ~study (Number of levels: 76) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.25      0.03     0.20     0.31 1.01      820     2203
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## intercept     0.38      0.05     0.29     0.48 1.01      925     1662
## between      -0.07      0.07    -0.21     0.07 1.00      891     1645
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a closer look at &lt;code&gt;b_between&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(b14.7) %&amp;gt;% 
  
  ggplot(aes(x = b_between, y = 0)) +
  stat_halfeye(point_interval = median_qi, .width = c(.5, .95)) +
  labs(x = &amp;quot;Overall difference for between- vs within-participant designs&amp;quot;,
       y = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid   = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-meta-analysis/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;432&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That difference isn’t as large I’d expect it to be. But then again, I’m no spanking researcher. So what do I know?&lt;/p&gt;
&lt;p&gt;There are other things you might do with these data. For example, you might check for trends by year or, as the authors did in their manuscript, distinguish among different severities of corporal punishment. But I think we’ve gone far enough to get you started.&lt;/p&gt;
&lt;p&gt;If you’d like to learn more about these methods, do check out Vourre’s &lt;a href=&#34;https://mvuorre.github.io/blog/posts/2016-09-29-bayesian-meta-analysis/&#34;&gt;&lt;em&gt;Meta-analysis is a special case of Bayesian multilevel modeling&lt;/em&gt;&lt;/a&gt;. From his blog, you’ll learn additional tricks, like making a more traditional-looking forest plot with the &lt;code&gt;brmstools::forest()&lt;/code&gt; function and how our Bayesian brms method compares with frequentist meta-analyses via the &lt;a href=&#34;https://CRAN.R-project.org/package=metafor&#34;&gt;metafor package&lt;/a&gt;. You might also check out Williams, Rast, and Bürkner’s manuscript, &lt;a href=&#34;https://psyarxiv.com/7tbrm/&#34;&gt;&lt;em&gt;Bayesian Meta-Analysis with Weakly Informative Prior Distributions&lt;/em&gt;&lt;/a&gt; to give you an empirical justification for using a half-Cauchy prior for your meta-analysis &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1  
##  [5] stringr_1.4.0   dplyr_1.0.5     purrr_0.3.4     readr_1.4.0    
##  [9] tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.3         splines_4.0.4       
##   [7] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1      
##  [16] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0  
##  [19] xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [22] colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1         
##  [28] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25         
##  [31] survival_3.2-10      zoo_1.8-8            glue_1.4.2          
##  [34] gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2        
##  [40] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       
##  [43] DBI_1.1.0            miniUI_0.1.1.1       viridisLite_0.3.0   
##  [46] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2          
##  [52] threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.1      
##  [55] farver_2.0.3         pkgconfig_2.0.3      loo_2.4.1           
##  [58] dbplyr_2.0.0         utf8_1.1.4           tidyselect_1.1.0    
##  [61] labeling_0.4.2       rlang_0.4.10         reshape2_1.4.4      
##  [64] later_1.1.0.1        munsell_0.5.0        cellranger_1.1.0    
##  [67] tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [70] broom_0.7.5          ggridges_0.5.2       evaluate_0.14       
##  [73] fastmap_1.0.1        yaml_2.2.1           processx_3.4.5      
##  [76] knitr_1.31           fs_1.5.0             nlme_3.1-152        
##  [79] mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [82] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2   
##  [85] rstudioapi_0.13      gamm4_0.2-6          curl_4.3            
##  [88] reprex_0.3.0         statmod_1.4.35       stringi_1.5.3       
##  [91] highr_0.8            ps_1.6.0             blogdown_1.3        
##  [94] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2        
##  [97] nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0       
## [100] vctrs_0.3.6          pillar_1.5.1         lifecycle_1.0.0     
## [103] bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [106] R6_2.5.0             bookdown_0.21        promises_1.1.1      
## [109] gridExtra_2.3        codetools_0.2-18     boot_1.3-26         
## [112] colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [115] assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [118] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4      
## [121] hms_0.5.3            grid_4.0.4           coda_0.19-4         
## [124] minqa_1.2.4          rmarkdown_2.7        shiny_1.5.0         
## [127] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;At the time of this revision (2021-04-21), this ebook is now in &lt;a href=&#34;https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse/releases/tag/1.2.0&#34;&gt;version 1.2.0&lt;/a&gt;. The revision of this post includes fixes to a couple code breaks and a few updated hyperlinks. If you’d like to see the current version of this meta-analysis material, you can find it &lt;a href=&#34;https://bookdown.org/content/3890/missing-data-and-other-opportunities.html#summary-bonus-meta-analysis&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>bookdown, My Process</title>
      <link>/post/how-bookdown/</link>
      <pubDate>Thu, 04 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/post/how-bookdown/</guid>
      <description>
&lt;script src=&#34;/post/how-bookdown/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;I just self-published a book-length version of my project &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt; with brms, ggplot2, and the tidyverse&lt;/a&gt;. By using Yihui Xie’s &lt;a href=&#34;https://bookdown.org&#34;&gt;bookdown package&lt;/a&gt;, I was able to do it for free. If you’ve never heard of it, bookdown enables &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html#what-is-r&#34;&gt;R&lt;/a&gt; users to write books and other long-form articles with &lt;a href=&#34;https://rmarkdown.rstudio.com&#34;&gt;R Markdown&lt;/a&gt;. You can save your bookdown products in a variety of formats (e.g., PDF, HTML) and publish them in several ways, too. The purpose of this post is to give readers a sense of how I used bookdown to make my project. I propose there are three fundamental skill sets you need basic fluency in before playing with bookdown. Those three are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R and R Studio,&lt;/li&gt;
&lt;li&gt;Scripts and R Markdown files, and&lt;/li&gt;
&lt;li&gt;Git and GitHub.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;start-with-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Start with &lt;a href=&#34;https://cran.r-project.org&#34;&gt;R&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;First things first. Since bookdown is a package for use in the R environment, you’re going to have to use R. If you’re unfamiliar with it, &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html&#34;&gt;R is a freely-available programming language particularly well-suited for data analysis&lt;/a&gt;. If you’ve not used R before, learning how to self-publish books is a great incentive to start learning. But unless you already have a background in programming, I think bookdown is poorly-suited for novices. R newbies should check out Roger Peng’s &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/&#34;&gt;&lt;em&gt;R Programming for Data Science&lt;/em&gt;&lt;/a&gt; or Grolemund and Wickham’s &lt;a href=&#34;http://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R for Data Science&lt;/em&gt;&lt;/a&gt;. Both are freely available online and, as it would turn out, made with bookdown. Also, new users should be aware that although you can interact with R directly, there are a variety of other ways to interface with R. I recommend using &lt;a href=&#34;https://www.rstudio.com&#34;&gt;R Studio&lt;/a&gt;. You can find some nice reasons, &lt;a href=&#34;https://www.theanalysisfactor.com/the-advantages-of-rstudio/&#34;&gt;here&lt;/a&gt;. For basic instructions on how to install R and R Studio, you might start &lt;a href=&#34;http://r4ds.had.co.nz/introduction.html#prerequisites&#34;&gt;here&lt;/a&gt;. And if you prefer video tutorials to help you with the installation, just do a simple search in your favorite video-sharing website and several should pop up.&lt;/p&gt;
&lt;p&gt;Personally, I started using R—via R Studio—during the 2015/2016 winter break before taking a spring semester statistics course based around an R package. [In case you’re curious, it was a structural equation modeling course based around a &lt;a href=&#34;https://blogs.baylor.edu/rlatentvariable/&#34;&gt;text by Beaujean&lt;/a&gt; which featured the &lt;a href=&#34;http://lavaan.ugent.be&#34;&gt;lavaan package&lt;/a&gt;]. At the time, I was already familiar with structural equation modeling, so the course was a nice opportunity to learn R. In addition, I was concurrently enrolled in a course on multilevel modeling based on &lt;a href=&#34;http://gseacademic.harvard.edu/alda/&#34;&gt;Singer and Willet’s classic text&lt;/a&gt;. The professor of that course primarily used SAS to teach the material, but he was flexible and allowed me to do the work with R, instead. So that was my introduction to R–a semester of immersion in &lt;a href=&#34;https://twitter.com/search?q=%23rstats&amp;amp;src=typd&#34;&gt;#rstats&lt;/a&gt;. Here are some other &lt;a href=&#34;https://www.r-bloggers.com/the-5-most-effective-ways-to-learn-r/&#34;&gt;tips on how to learn R&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bookdown-uses-markdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;bookdown uses &lt;a href=&#34;https://daringfireball.net/projects/markdown/syntax&#34;&gt;Markdown&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you work with R through R Studio, you can do a handful of things through dropdowns. But really, if you’re going to be using R, you’re going to be coding. As it turns out, there are a variety of ways to code in R. One of the most basic ways is via the &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/200404846-Working-in-the-Console&#34;&gt;console&lt;/a&gt;, which I’m not going to cover in any detail.&lt;/p&gt;
&lt;p&gt;The console is fine for quick operations, but you’re going to want to do most of your coding in some kind of a script. R Studio allows users to save and execute code in script files, which you can learn more about &lt;a href=&#34;http://r4ds.had.co.nz/workflow-scripts.html&#34;&gt;here&lt;/a&gt;. Basic script files are nice in that they allow you to both save and annotate your code.&lt;/p&gt;
&lt;p&gt;However, the annotation options in R Studio script files are limited. After using R Studio scripts for about a year, I learned about &lt;a href=&#34;https://rmarkdown.rstudio.com/r_notebooks&#34;&gt;R Notebooks&lt;/a&gt;. These are special files that allow you to intermingle your R code with prose and the results of the code. R Notebooks also allow users to transform the working documents into professional-looking reports in various formats (e.g., PDF, HTML). And unlike the primitive annotation options with simple script files, R Notebooks use Markdown to allow users to format their prose with things like headers, italicized font, insert hyperlinks, and even embed images. So &lt;a href=&#34;https://daringfireball.net/projects/markdown/&#34;&gt;Markdown&lt;/a&gt;, then, is a simple language that allows for many of those functions.&lt;/p&gt;
&lt;p&gt;Within the R Studio environment, you can use Markdown with two basic file types: &lt;a href=&#34;https://rmarkdown.rstudio.com/lesson-1.html&#34;&gt;R Markdown&lt;/a&gt; files and &lt;a href=&#34;https://rmarkdown.rstudio.com/r_notebooks&#34;&gt;R Notebook&lt;/a&gt; files. R Notebook files are just special kinds of R Markdown files that have, IMO, a better interface. That is, R Notebooks are the newer nicer version of R Markdown files. The main point here is that when I say “bookdown uses Markdown”, I’m pointing out that one of the important skills you’ll want to develop before making content with bookdown is how to use Markdown within R Studio. It’s not terribly complicated to learn, and you can get an overview of the basics &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;http://r4ds.had.co.nz/r-markdown.html&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://bookdown.org/yihui/bookdown/markdown-syntax.html&#34;&gt;here&lt;/a&gt;, or an exhaustive treatment &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you’re a novice, it’ll take you a few days, weeks, or months to get a firm grasp of R. Not so with R Markdown files. You’ll have the basics of those down in an afternoon. That said, I had been an R Notebook user for more than a year before trying my hand at bookdown.&lt;/p&gt;
&lt;p&gt;The first big edition of my &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt; with brms, ggplot2, and the tidyverse&lt;/a&gt; project came in the form of R Notebook files and their HTML counterparts stored in one of my projects on the &lt;a href=&#34;https://osf.io/?gclid=EAIaIQobChMI2ZDP9svj3QIVQ7nACh2rYQIHEAAYASAAEgL7avD_BwE&#34;&gt;Open Science Framework&lt;/a&gt;. I don’t update it very often, but you can still find it &lt;a href=&#34;https://osf.io/97t6w/&#34;&gt;here&lt;/a&gt;. If you’re not familiar with it, the OSF is a &lt;a href=&#34;https://osf.io/4znzp/wiki/home/&#34;&gt;“free, open source web application that connects and supports the research workflow, enabling scientists to increase the efficiency and effectiveness of their research.”&lt;/a&gt; In addition to their wiki, you might check out some of their &lt;a href=&#34;https://cos.io/our-services/training-services/cos-training-tutorials/&#34;&gt;video tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;youll-need-github-too&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;You’ll need &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;, too&lt;/h2&gt;
&lt;p&gt;I’m actually not sure whether you need to know how to use Git and GitHub to use bookdown. In his authoritative book, &lt;a href=&#34;https://bookdown.org/yihui/bookdown/&#34;&gt;&lt;em&gt;bookdown: Authoring Books and Technical Documents with R Markdown&lt;/em&gt;&lt;/a&gt;, Yihui Xie mentioned GitHub in every chapter. If you go to your favorite video-sharing website to look for instructional videos on bookdown, you’ll see the instructors take GitHub as a given, too. If you’re stubborn and have enough ingenuity, you might find a way to successfully use bookdown without GitHub, but you may as well go with the flow on this one.&lt;/p&gt;
&lt;p&gt;If you’ve never heard of it before, &lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&#34;&gt;Git is a system for version control&lt;/a&gt;. By version control, I mean a system by which you can keep track of changes to your code, over time. Even if you don’t have a background in programming, consider a scenario where you had to keep track of many versions of a writing project, perhaps saving your files as &lt;code&gt;first_draft.docx&lt;/code&gt;, &lt;code&gt;second_draft.docx&lt;/code&gt;, &lt;code&gt;final_draft.docx&lt;/code&gt;, &lt;code&gt;final_draft_2.docx&lt;/code&gt;… This was your own make-shift attempt at version control for writing. I’ve seen a lot of introductory material recommend Git and GitHub by leading with version control. And indeed, they do serve that purpose. But IMO, leading with version control is a rhetorical mistake when talking to non-programmers. I haven’t found Git and GitHub the most intuitive and if version control was the only benefit, they wouldn’t be worth the effort. But there are other good reasons to learn.&lt;/p&gt;
&lt;p&gt;IMO, the best reason to learn Git and GitHub is because they allow you to make your work publicly available. When you just use Git, the work stays on your computer. But GitHub allows you to save your files online, too. This makes it easy for others to review them and give you feedback. GitHub also allows you to save things like data files online. So if you’re a working scientist, Git and GitHub might allow you to make a site—a repository—to house the de-identified data and statistical code for one of your projects. It’s another way to do open science. In addition, you can repurpose GitHub to work as blog or an analytic portfolio. And if you’d like to use bookdown, Git and GitHub will be a part of how you manage the files for your projects and make your work more accessible to others.&lt;/p&gt;
&lt;p&gt;If you’re new to all this, you could probably blindly follow along with the steps in Yihui Xie’s bookdown &lt;a href=&#34;https://bookdown.org/yihui/bookdown/&#34;&gt;manual&lt;/a&gt; or any of the online video tutorials. But I suspect that’d be pretty confusing. Before attempting a bookdown project, spend some time getting comfortable with Git and GitHub, first. The best introduction to the topic I’ve seen is Jenny Bryan’s &lt;a href=&#34;http://happygitwithr.com&#34;&gt;&lt;em&gt;Happy Git and GitHub for the useR&lt;/em&gt;&lt;/a&gt;, which, you guessed it, is also freely available and powered by bookdown.&lt;/p&gt;
&lt;p&gt;As I hinted, I found Git and GitHub baffling, at first. I checked out a few online video tutorials, but found them of little help. It really was Bryan’s &lt;a href=&#34;http://happygitwithr.com&#34;&gt;book&lt;/a&gt; that finally got me going. And I’m glad I did. I’ve been slowly working with GitHub for about a year—here’s &lt;a href=&#34;https://github.com/ASKurz&#34;&gt;my profile&lt;/a&gt;—and my first major project was putting together the files for the individual chapters in the &lt;a href=&#34;https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse&#34;&gt;Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse&lt;/a&gt; project. They originally lived as R Notebook files, eventually rendered in a &lt;a href=&#34;https://rmarkdown.rstudio.com/github_document_format.html&#34;&gt;GitHub-friendly .md file format&lt;/a&gt;. After a while, I started playing around with &lt;code&gt;README&lt;/code&gt;-only projects, which are basically a poor man’s GitHub version of blog posts (e.g., check out &lt;a href=&#34;https://github.com/ASKurz/James-Stein-and-Bayesian-partial-pooling&#34;&gt;this one&lt;/a&gt;). For me, and probably for your future bookdown projects, the most important GitHub skills to learn are commits, pushes, and forkes.&lt;/p&gt;
&lt;p&gt;I’d fooled around with GitHub a tiny bit before launching my &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt; with brms, ggplot2, and the tidyverse&lt;/a&gt; project on the OSF. But it was confusing and after an hour or two of trying to make sense of it, I gave up and just figured the OSF would be good enough. After folks started noticing the project, I got a few comments that it’d be more accessible on GitHub. That was what finally influenced me to buckle down learn it in earnest. I’m still a little clunky with it, but I’m functional enough to do things like &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;make this blog&lt;/a&gt;. With a little patience and practice, you can get there, too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;let-yihui-xie-guide-you&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let Yihui Xie guide you&lt;/h2&gt;
&lt;p&gt;So far we’ve covered&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R and R Studio&lt;/li&gt;
&lt;li&gt;Scripts and R Markdown files&lt;/li&gt;
&lt;li&gt;Git and GitHub&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You don’t have become an expert, but you’ll need to become roughly fluent in all three to make good use of bookdown. Basically, if you are able to load data into R, document a rudimentary analysis in an R Notebook file, and then share the project in a non-embarrassing way in GitHub, you’re ready to use bookdown.&lt;/p&gt;
&lt;p&gt;I’ve already mentioned it, but the authoritative work on bookdown is Yihui Xie’s &lt;a href=&#34;https://bookdown.org/yihui/bookdown/&#34;&gt;&lt;em&gt;bookdown: Authoring Books and Technical Documents with R Markdown&lt;/em&gt;&lt;/a&gt;. Yihui Xie, of course, is the author of the package. It’s probably best to just start there, going bit by bit. He also gave an RStudio webinar, &lt;a href=&#34;https://www.youtube.com/watch?v=dVqVscgwSpw&amp;amp;t=12s&#34;&gt;&lt;em&gt;Authoring Books with R Markdown&lt;/em&gt;&lt;/a&gt;, which I found to be a helpful supplement.&lt;/p&gt;
&lt;p&gt;The complete version of my &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt; with brms, ggplot2, and the tidyverse&lt;/a&gt; project has 15 chapters and several preamble sections. Almost all the chapters files include a lot of computationally-intensive code, with the simulations for chapter 6 taking multiple hours to compute. I do not recommend starting off with a project like that, at least not all at once. If you follow along with Yihui Xie’s guide, you’ll practice stitching together simple files, first. After learning those basics, I then picked up other helpful tricks, like &lt;a href=&#34;https://bookdown.org/yihui/bookdown/preview-a-chapter.html#&#34;&gt;caching analyses&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although I didn’t use these resources while I was learning bookdown, you might also benefit from checking out&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sean Kross’s &lt;a href=&#34;http://seankross.com/2016/11/17/How-to-Start-a-Bookdown-Book.html&#34;&gt;&lt;em&gt;How to Start a Bookdown Book&lt;/em&gt;&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Karl Broman’s &lt;a href=&#34;https://kbroman.org/blog/2017/05/31/omg-bookdown/&#34;&gt;&lt;em&gt;omg, bookdown!&lt;/em&gt;&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Rachael Lappan’s &lt;a href=&#34;https://rachaellappan.github.io/bookdown/&#34;&gt;&lt;em&gt;Using Bookdown for tidy documentation&lt;/em&gt;&lt;/a&gt;, or&lt;/li&gt;
&lt;li&gt;Pablo Casas’s &lt;a href=&#34;https://blog.datascienceheroes.com/how-to-self-publish-a-book/&#34;&gt;&lt;em&gt;How to self-publish a book: A handy list of resources&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>logistic regression | Fahim Ahmad</title>
    <link>/tag/logistic-regression/</link>
      <atom:link href="/tag/logistic-regression/index.xml" rel="self" type="application/rss+xml" />
    <description>logistic regression</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Fahim Ahmad (2020)</copyright><lastBuildDate>Wed, 22 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>logistic regression</title>
      <link>/tag/logistic-regression/</link>
    </image>
    
    <item>
      <title>Sexy up your logistic regression model with logit dotplots</title>
      <link>/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/</link>
      <pubDate>Wed, 22 Sep 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/</guid>
      <description>
&lt;script src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;what&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What&lt;/h2&gt;
&lt;p&gt;When you fit a logistic regression model, there are a lot of ways to display the results. One of the least inspiring ways is to report a summary of the coefficients in prose or within a table. A more artistic approach is to show the fitted line in a plot, which often looks nice due to the curvy nature of logistic regression lines. The major shortcoming in typical logistic regression line plots is they usually don’t show the data due to overplottong across the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis. Happily, new developments with Matthew Kay’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-ggdist&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://mjskay.github.io/ggdist/&#34;&gt;&lt;strong&gt;ggdist&lt;/strong&gt; package&lt;/a&gt; make it easy to show your data when you plot your logistic regression curves. In this post I’ll show you how.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;For this post, I’m presuming some background knowledge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should be familiar with logistic regression. For introductions, I recommend Roback and Legler’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-roback2021beyond&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; online text or James, Witten, Hastie, and Tibshirani’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-james2021AnIntroduction&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; online text. Both texts are written from a frequentist perspective, which is also the framework we’ll be using in this blog post. For Bayesian introductions to logistic regression, I recommend either edition of McElreath’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text; or Gelman, Hill, and Vehtari’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; text.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;. Data wrangling and plotting were done with help from the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;broom&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-broom&#34; role=&#34;doc-biblioref&#34;&gt;Robinson et al., 2021&lt;/a&gt;)&lt;/span&gt;. The data are from the &lt;a href=&#34;https://github.com/debruine/faux&#34;&gt;&lt;strong&gt;fivethirtyeight&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-fivethirtyeight2018&#34; role=&#34;doc-biblioref&#34;&gt;Kim et al., 2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-fivethirtyeight&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we load our primary &lt;strong&gt;R&lt;/strong&gt; packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(fivethirtyeight)
library(broom)
library(ggdist)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;In this post, we’ll be working with the &lt;code&gt;bechdel&lt;/code&gt; data set. From the documentation, we read these are “the raw data behind the story ‘&lt;a href=&#34;https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/&#34;&gt;The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women&lt;/a&gt;.’”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(bechdel)

glimpse(bechdel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1,794
## Columns: 15
## $ year          &amp;lt;int&amp;gt; 2013, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 20…
## $ imdb          &amp;lt;chr&amp;gt; &amp;quot;tt1711425&amp;quot;, &amp;quot;tt1343727&amp;quot;, &amp;quot;tt2024544&amp;quot;, &amp;quot;tt1272878&amp;quot;, &amp;quot;tt0…
## $ title         &amp;lt;chr&amp;gt; &amp;quot;21 &amp;amp; Over&amp;quot;, &amp;quot;Dredd 3D&amp;quot;, &amp;quot;12 Years a Slave&amp;quot;, &amp;quot;2 Guns&amp;quot;, &amp;quot;…
## $ test          &amp;lt;chr&amp;gt; &amp;quot;notalk&amp;quot;, &amp;quot;ok-disagree&amp;quot;, &amp;quot;notalk-disagree&amp;quot;, &amp;quot;notalk&amp;quot;, &amp;quot;m…
## $ clean_test    &amp;lt;ord&amp;gt; notalk, ok, notalk, notalk, men, men, notalk, ok, ok, no…
## $ binary        &amp;lt;chr&amp;gt; &amp;quot;FAIL&amp;quot;, &amp;quot;PASS&amp;quot;, &amp;quot;FAIL&amp;quot;, &amp;quot;FAIL&amp;quot;, &amp;quot;FAIL&amp;quot;, &amp;quot;FAIL&amp;quot;, &amp;quot;FAIL&amp;quot;, …
## $ budget        &amp;lt;int&amp;gt; 13000000, 45000000, 20000000, 61000000, 40000000, 225000…
## $ domgross      &amp;lt;dbl&amp;gt; 25682380, 13414714, 53107035, 75612460, 95020213, 383624…
## $ intgross      &amp;lt;dbl&amp;gt; 42195766, 40868994, 158607035, 132493015, 95020213, 1458…
## $ code          &amp;lt;chr&amp;gt; &amp;quot;2013FAIL&amp;quot;, &amp;quot;2012PASS&amp;quot;, &amp;quot;2013FAIL&amp;quot;, &amp;quot;2013FAIL&amp;quot;, &amp;quot;2013FAI…
## $ budget_2013   &amp;lt;int&amp;gt; 13000000, 45658735, 20000000, 61000000, 40000000, 225000…
## $ domgross_2013 &amp;lt;dbl&amp;gt; 25682380, 13611086, 53107035, 75612460, 95020213, 383624…
## $ intgross_2013 &amp;lt;dbl&amp;gt; 42195766, 41467257, 158607035, 132493015, 95020213, 1458…
## $ period_code   &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ decade_code   &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data were collected on Hollywood movies made between 1970 and 2013.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bechdel %&amp;gt;% 
  pull(year) %&amp;gt;% 
  range()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1970 2013&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our focal variable will be &lt;code&gt;binary&lt;/code&gt;, which indicates whether a given movie passed the Bechdel test. Of the &lt;span class=&#34;math inline&#34;&gt;\(1{,}794\)&lt;/span&gt; movies in the data set, just under half of them passed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bechdel %&amp;gt;% 
  count(binary) %&amp;gt;% 
  mutate(percent = 100 * n / sum(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 3
##   binary     n percent
##   &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 FAIL     991    55.2
## 2 PASS     803    44.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our sole predictor variable will be &lt;code&gt;budget_2013&lt;/code&gt;, each movie’s budget as expressed in 2013 dollars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bechdel %&amp;gt;% 
  ggplot(aes(x = budget_2013)) +
  geom_histogram() +
  facet_wrap(~ binary, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To make our lives a little easier, we’ll convert the character variable &lt;code&gt;binary&lt;/code&gt; into a conventional &lt;span class=&#34;math inline&#34;&gt;\(0/1\)&lt;/span&gt; numeric variable called &lt;code&gt;pass&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute
bechdel &amp;lt;- bechdel  %&amp;gt;% 
  mutate(pass = ifelse(binary == &amp;quot;FAIL&amp;quot;, 0, 1)) 

# compare
bechdel %&amp;gt;% 
  select(binary, pass) %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 2
##   binary  pass
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 FAIL       0
## 2 PASS       1
## 3 FAIL       0
## 4 FAIL       0
## 5 FAIL       0
## 6 FAIL       0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;p&gt;We can express our statistical model in formal notation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{pass}_i &amp;amp; \sim \operatorname{Binomial}(n = 1, p_i) \\
\operatorname{logit}(p_i) &amp;amp; = \beta_0 + \beta_1 \text{budget_2013}_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we use the conventional logit link to ensure the binomial probabilities are restricted within the bounds of zero and one. We can fit such a model with the base &lt;strong&gt;R&lt;/strong&gt; &lt;code&gt;glm()&lt;/code&gt; function like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- glm(
  data = bechdel,
  family = binomial,
  pass ~ 1 + budget_2013)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A conventional way to present the results would in a coefficient table, the rudiments of which we can get from the &lt;code&gt;broom::tidy()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(fit) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1113148&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0689661&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.614051&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1065163&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;budget_2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-6.249724&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Because of the scale of the &lt;code&gt;budget_2013&lt;/code&gt; variable, its point estimate and standard errors are both very small. To give a little perspective, here is the expected decrease in log-odds for a budget increase in &lt;span class=&#34;math inline&#34;&gt;\(\$100{,}000{,}000\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(coef(fit)[2], confint(fit)[2, ]) * 1e8&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## budget_2013       2.5 %      97.5 % 
##  -0.5972374  -0.7875178  -0.4126709&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how we added in the 95% confidence intervals for good measure.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;line-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Line plots&lt;/h2&gt;
&lt;p&gt;Now we have interpreted the model in the dullest way possible, with a table and in prose, let’s practice plotting the results. First, we’ll use the widely-used method of displaying only the fitted line.&lt;/p&gt;
&lt;div id=&#34;fitted-line-wo-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fitted line w/o data.&lt;/h3&gt;
&lt;p&gt;We can use the &lt;code&gt;predict()&lt;/code&gt; function along with some post-processing strategies from &lt;a href=&#34;https://twitter.com/ucfagls&#34;&gt;Gavin Simpson&lt;/a&gt;’s fine blog post, &lt;a href=&#34;https://fromthebottomoftheheap.net/2018/12/10/confidence-intervals-for-glms/&#34;&gt;&lt;em&gt;Confidence intervals for GLMs&lt;/em&gt;&lt;/a&gt;, to prepare the data necessary for making our plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the new data
nd &amp;lt;- tibble(budget_2013 = seq(from = 0, to = 500000000, length.out = 100))

p &amp;lt;-
  # compute the fitted lines and SE&amp;#39;s
  predict(fit,
          newdata = nd,
          type = &amp;quot;link&amp;quot;,
          se.fit = TRUE) %&amp;gt;% 
  # wrangle
  data.frame() %&amp;gt;% 
  mutate(ll = fit - 1.96 * se.fit,
         ul = fit + 1.96 * se.fit) %&amp;gt;% 
  select(-residual.scale, -se.fit) %&amp;gt;% 
  mutate_all(plogis) %&amp;gt;%
  bind_cols(nd)

# what have we done?
glimpse(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 100
## Columns: 4
## $ fit         &amp;lt;dbl&amp;gt; 0.5278000, 0.5202767, 0.5127442, 0.5052059, 0.4976652, 0.4…
## $ ll          &amp;lt;dbl&amp;gt; 0.4940356, 0.4881515, 0.4821772, 0.4760998, 0.4699043, 0.4…
## $ ul          &amp;lt;dbl&amp;gt; 0.5613120, 0.5522351, 0.5432161, 0.5342767, 0.5254405, 0.5…
## $ budget_2013 &amp;lt;dbl&amp;gt; 0, 5050505, 10101010, 15151515, 20202020, 25252525, 303030…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a conventional line plot for our logistic regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013, y = fit)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line() +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;, 
                     expand = c(0, 0), limits = 0:1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The fitted line is in black and the semitransparent grey ribbon marks of the 95% confidence intervals. The plot does a nice job showing how movies with larger budgets tend to do a worse job passing the Bechdel test.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improve-the-visualization-by-adding-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Improve the visualization by adding data.&lt;/h3&gt;
&lt;p&gt;If you wanted to add the data to our plot, a naïve approach might be to use &lt;code&gt;geom_point()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013, y = fit)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line() +
  geom_point(data = bechdel,
             aes(y = pass),
             alpha = 1/2) +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;, 
                     expand = expansion(mult = 0.01))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even by making the dots semitransparent with the &lt;code&gt;alpha&lt;/code&gt; parameter, the overplotting issue makes it very difficult to make sense of the data. One of the approaches favored by Gelman and colleagues &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; is to add a little vertical jittering. We can do that with &lt;code&gt;geom_jitter()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013, y = fit)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line() +
  geom_jitter(data = bechdel,
              aes(y = pass),
              size = 1/4, alpha = 1/2, height = 0.05) +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;, 
                     expand = c(0, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though a big improvement, this approach still doesn’t do the best job depicting the distribution of the &lt;code&gt;budget_2013&lt;/code&gt; values. If possible, it would be better to explicitly depict the &lt;code&gt;budget_2013&lt;/code&gt; distributions for each level of &lt;code&gt;pass&lt;/code&gt; with something more like histograms. In his blogpost, &lt;a href=&#34;https://www.barelysignificant.com/post/glm/&#34;&gt;&lt;em&gt;Using R to make sense of the generalised linear model&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/lnalborczyk&#34;&gt;Ladislas Nalborczyk&lt;/a&gt; showed how you could do so with a custom function he named &lt;code&gt;logit_dotplot()&lt;/code&gt;, the source code for which you can find &lt;a href=&#34;https://github.com/lnalborczyk/lnalborczyk.github.io/blob/master/code/logit_dotplot.R&#34;&gt;here&lt;/a&gt; on his GitHub. Since Nalborczyk’s post, this kind of functionality has since been built into Kay’s &lt;strong&gt;ggdist&lt;/strong&gt; package. Here’s what it looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line(aes(y = fit)) +
  stat_dots(data = bechdel,
            aes(y = pass, side = ifelse(pass == 0, &amp;quot;top&amp;quot;, &amp;quot;bottom&amp;quot;)),
            scale = 1/3) +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;,
                     expand = c(0, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the &lt;code&gt;stat_dots()&lt;/code&gt; function, we added dotplots, which are nifty alternatives to histograms which display each data value as an individual dot. With the &lt;code&gt;side&lt;/code&gt; argument, we used a conditional statement to tell &lt;code&gt;stat_dots()&lt;/code&gt; we wanted some of the &lt;code&gt;budget_2013&lt;/code&gt; to be displayed on the bottom and other of those values to be displayed on the top. With the &lt;code&gt;scale&lt;/code&gt; argument, we indicated how much of the total space within the range of the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis we wanted the dot plot distributions to take up.&lt;/p&gt;
&lt;p&gt;For kicks and giggles, here’s a more polished version of what such a plot could look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line(aes(y = fit)) +
  stat_dots(data = bechdel %&amp;gt;% 
              mutate(binary = factor(binary, levels = c(&amp;quot;PASS&amp;quot;, &amp;quot;FAIL&amp;quot;))),
            aes(y = pass, 
                side = ifelse(pass == 0, &amp;quot;top&amp;quot;, &amp;quot;bottom&amp;quot;),
                color = binary),
            scale = 0.4, shape = 19) +
  scale_color_manual(&amp;quot;Bechdel test&amp;quot;, values = c(&amp;quot;#009E73&amp;quot;, &amp;quot;#D55E00&amp;quot;)) +
  scale_x_continuous(&amp;quot;budget (in 2013 dollars)&amp;quot;,
                     breaks = c(0, 1e8, 2e8, 3e8, 4e8),
                     labels = c(0, str_c(1:4 * 100, &amp;quot; mill&amp;quot;)),
                     expand = c(0, 0), limits = c(0, 48e7)) +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;,
                     expand = c(0, 0)) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Other distributional forms are possible, too. For example, here we set &lt;code&gt;slab_type = &#34;histogram&#34;&lt;/code&gt; within the &lt;code&gt;stat_slab()&lt;/code&gt; function to swap out the dotplots for histograms.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line(aes(y = fit)) +
  # the magic lives here
  stat_slab(data = bechdel %&amp;gt;% 
              mutate(binary = factor(binary, levels = c(&amp;quot;PASS&amp;quot;, &amp;quot;FAIL&amp;quot;))),
            aes(y = pass, 
                side = ifelse(pass == 0, &amp;quot;top&amp;quot;, &amp;quot;bottom&amp;quot;),
                fill = binary, color = binary),
            slab_type = &amp;quot;histogram&amp;quot;,
            scale = 0.4, breaks = 40, size = 1/2) +
  scale_fill_manual(&amp;quot;Bechdel test&amp;quot;, values = c(alpha(&amp;quot;#009E73&amp;quot;, .7), alpha(&amp;quot;#D55E00&amp;quot;, .7))) +
  scale_color_manual(&amp;quot;Bechdel test&amp;quot;, values = c(&amp;quot;#009E73&amp;quot;, &amp;quot;#D55E00&amp;quot;)) +
  scale_x_continuous(&amp;quot;budget (in 2013 dollars)&amp;quot;,
                     breaks = c(0, 1e8, 2e8, 3e8, 4e8),
                     labels = c(0, str_c(1:4 * 100, &amp;quot; mill&amp;quot;)),
                     expand = c(0, 0), limits = c(0, 48e7)) +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;,
                     expand = c(0, 0)) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s a wrap, friends. No more lonely logistic curves absent data. Flaunt those sexy data with &lt;strong&gt;ggdist&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.1 (2021-08-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggdist_3.0.0          broom_0.7.9           fivethirtyeight_0.6.1
##  [4] forcats_0.5.1         stringr_1.4.0         dplyr_1.0.7          
##  [7] purrr_0.3.4           readr_2.0.1           tidyr_1.1.3          
## [10] tibble_3.1.4          ggplot2_3.3.5         tidyverse_1.3.1      
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7           lubridate_1.7.10     assertthat_0.2.1    
##  [4] digest_0.6.27        utf8_1.2.2           R6_2.5.1            
##  [7] cellranger_1.1.0     backports_1.2.1      reprex_2.0.1        
## [10] evaluate_0.14        highr_0.9            httr_1.4.2          
## [13] blogdown_1.5         pillar_1.6.2         rlang_0.4.11        
## [16] readxl_1.3.1         rstudioapi_0.13      jquerylib_0.1.4     
## [19] rmarkdown_2.10       labeling_0.4.2       munsell_0.5.0       
## [22] compiler_4.1.1       modelr_0.1.8         xfun_0.25           
## [25] pkgconfig_2.0.3      htmltools_0.5.2      tidyselect_1.1.1    
## [28] bookdown_0.23        fansi_0.5.0          crayon_1.4.1        
## [31] tzdb_0.1.2           dbplyr_2.1.1         withr_2.4.2         
## [34] MASS_7.3-54          grid_4.1.1           distributional_0.2.2
## [37] jsonlite_1.7.2       gtable_0.3.0         lifecycle_1.0.0     
## [40] DBI_1.1.1            magrittr_2.0.1       scales_1.1.1        
## [43] cli_3.0.1            stringi_1.7.4        farver_2.1.0        
## [46] fs_1.5.0             xml2_1.3.2           bslib_0.3.0         
## [49] ellipsis_0.3.2       generics_0.1.0       vctrs_0.3.8         
## [52] tools_4.1.1          glue_1.4.2           hms_1.1.0           
## [55] fastmap_1.1.0        yaml_2.2.1           colorspace_2.0-2    
## [58] rvest_1.0.1          knitr_1.33           haven_2.4.3         
## [61] sass_0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-james2021AnIntroduction&#34; class=&#34;csl-entry&#34;&gt;
James, G., Witten, D., Hastie, T., &amp;amp; Tibshirani, R. (2021). &lt;em&gt;An introduction to statistical learning with applications in &lt;span&gt;R&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;Springer&lt;/span&gt;. &lt;a href=&#34;https://web.stanford.edu/~hastie/ISLRv2_website.pdf&#34;&gt;https://web.stanford.edu/~hastie/ISLRv2_website.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-ggdist&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;ggdist&lt;/span&gt;: &lt;span&gt;Visualizations&lt;/span&gt; of distributions and uncertainty&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=ggdist&#34;&gt;https://CRAN.R-project.org/package=ggdist&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-fivethirtyeight2018&#34; class=&#34;csl-entry&#34;&gt;
Kim, A. Y., Ismay, C., &amp;amp; Chunn, J. (2018). The fivethirtyeight &lt;span&gt;R&lt;/span&gt; package: ’Tame data’ principles for introductory statistics and data science courses. &lt;em&gt;Technology Innovations in Statistics Education&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(1). &lt;a href=&#34;https://escholarship.org/uc/item/0rx1231m&#34;&gt;https://escholarship.org/uc/item/0rx1231m&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-fivethirtyeight&#34; class=&#34;csl-entry&#34;&gt;
Kim, A. Y., Ismay, C., &amp;amp; Chunn, J. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;fivethirtyeight&lt;/span&gt;: &lt;span&gt;Data&lt;/span&gt; and code behind the stories and interactives at &lt;span&gt;FiveThirtyEight&lt;/span&gt;&lt;/em&gt; [Manual]. &lt;a href=&#34;https://github.com/rudeboybert/fivethirtyeight&#34;&gt;https://github.com/rudeboybert/fivethirtyeight&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-roback2021beyond&#34; class=&#34;csl-entry&#34;&gt;
Roback, P., &amp;amp; Legler, J. (2021). &lt;em&gt;Beyond multiple linear regression: &lt;span&gt;Applied&lt;/span&gt; generalized linear models and multilevel models in &lt;span&gt;R&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://bookdown.org/roback/bookdown-BeyondMLR/&#34;&gt;https://bookdown.org/roback/bookdown-BeyondMLR/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-broom&#34; class=&#34;csl-entry&#34;&gt;
Robinson, D., Hayes, A., &amp;amp; Couch, S. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;broom&lt;/span&gt;: &lt;span&gt;Convert&lt;/span&gt; statistical objects into tidy tibbles&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=broom&#34;&gt;https://CRAN.R-project.org/package=broom&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian power analysis: Part III.b. What about 0/1 data?</title>
      <link>/post/bayesian-power-analysis-part-iii-b/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/bayesian-power-analysis-part-iii-b/</guid>
      <description>
&lt;script src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;version-1.1.0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Version 1.1.0&lt;/h2&gt;
&lt;p&gt;Edited on April 21, 2021, to fix a few code breaks and add a Reference section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;orientation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Orientation&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&#34;https://solomonkurz.netlify.com/post/bayesian-power-analysis-part-iii-a/&#34;&gt;last post&lt;/a&gt;, we covered how the Poisson distribution is handy for modeling count data. Binary data are even weirder than counts. They typically only take on two values: 0 and 1. Sometimes 0 is a stand-in for “no” and 1 for “yes” (e.g., &lt;em&gt;Are you an expert in Bayesian power analysis?&lt;/em&gt; For me that would be &lt;code&gt;0&lt;/code&gt;). You can also have data of this kind if you asked people whether they’d like to choose option A or B. With those kinds of data, you might arbitrarily code A as 0 and B as 1. Binary data also often stand in for trials where 0 = “fail” and 1 = “success.” For example, if you answered “Yes” to the question &lt;em&gt;Are all data normally distributed?&lt;/em&gt; we’d mark your answer down as a &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Though 0’s and 1’s are popular, sometimes binary data appear in their aggregated form. Let’s say I gave you 10 algebra questions and you got 7 of them right. Here’s one way to encode those data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 10
z &amp;lt;- 7

rep(0:1, times = c(n - z, z))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0 0 0 1 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In that example, &lt;code&gt;n&lt;/code&gt; stood for the total number of trials and &lt;code&gt;z&lt;/code&gt; was the number you got correct (i.e., the number of times we encoded your response as a 1). A more compact way to encode that data is with two columns, one for &lt;code&gt;z&lt;/code&gt; and the other for &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

tibble(z = z,
       n = n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##       z     n
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     7    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So then if you gave those same 10 questions to four of your friends, we could encode the results like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(3)

tibble(id = letters[1:5],
       z  = rpois(n = 5, lambda = 5),
       n  = n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 3
##   id        z     n
##   &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 a         3    10
## 2 b         7    10
## 3 c         4    10
## 4 d         4    10
## 5 e         5    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you were &lt;code&gt;b&lt;/code&gt;, you’d be the smart one in the group.&lt;/p&gt;
&lt;p&gt;Anyway, whether working with binary or aggregated binary data, we’re interested in the probability a given trial will be 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression-with-unaggregated-binary-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic regression with unaggregated binary data&lt;/h2&gt;
&lt;p&gt;Taking unaggregated binary data as a starting point, given &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; data that includes a variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; where the value in the &lt;span class=&#34;math inline&#34;&gt;\(i^\text{th}\)&lt;/span&gt; row is a 0 or a 1, we’d like to know the probability a given trial would be 1, given &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; [i.e., &lt;span class=&#34;math inline&#34;&gt;\(p(y_i = 1 | d)\)&lt;/span&gt;]. The binomial distribution will help us get that estimate for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We’ll do so within the context of a logistic regression model following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i                        &amp;amp; \sim \text{Binomial} (n = 1, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \beta_0,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;were the logit function is defined as the log odds&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\operatorname{logit} (p_i) = \log \left (\frac{p_i}{1 - p_i} \right ),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which also means that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\log \left (\frac{p_i}{1 - p_i} \right ) = \beta_0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In those formulas, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the intercept. In a binomial model with no predictors&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is just the estimate for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, but in the log-odds metric. So yes, similar to the Poisson models from the last post, we typically use a link function with our binomial models. Instead of the log link, we use the logit because it constrains the posterior for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to values between 0 and 1. Just as the null value for a probability is .5, the null value for the parameters within a logistic regression model is typically 0.&lt;/p&gt;
&lt;p&gt;As with the Poisson, I’m not going to go into a full-blown tutorial on the binomial distribution or on logistic regression. For more thorough introductions, check out chapters 9 through 10 in McElreath’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;&lt;em&gt;Statistical rethinking&lt;/em&gt;&lt;/a&gt; or Agresti’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-agrestiFoundationsLinearGeneralized2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&#34;&gt;&lt;em&gt;Foundations of linear and generalized linear models&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;Time to simulate some data. Let’s say we’d like to estimate the probability someone will hit a ball in a baseball game. Nowadays, batting averages for professional baseball players tend around .25 (see &lt;a href=&#34;http://www.baseball-almanac.com/hitting/hibavg4.shtml&#34;&gt;here&lt;/a&gt;). So if we wanted to simulate 50 at-bats, we might do so like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(3)

d &amp;lt;- tibble(y = rbinom(n = 50, size = 1, prob = .25))

str(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [50 × 1] (S3: tbl_df/tbl/data.frame)
##  $ y: int [1:50] 0 1 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are what those data look like in a bar plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_gray() + theme(panel.grid = element_blank()))

d %&amp;gt;% 
  mutate(y = factor(y)) %&amp;gt;% 
  
  ggplot(aes(x = y)) +
  geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;time-to-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Time to model.&lt;/h3&gt;
&lt;p&gt;To practice modeling those data, we’ll want to fire up the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;get_prior()&lt;/code&gt; function to get the &lt;strong&gt;brms&lt;/strong&gt; default for our intercept-only logistic regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_prior(data = d, 
          family = binomial,
          y | trials(1) ~ 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Intercept ~ student_t(3, 0, 2.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it turns out, that’s a really liberal prior. We might step up a bit and put a more skeptical &lt;code&gt;normal(0, 2)&lt;/code&gt; prior on that intercept. With the context of our logit link, that still puts a 95% probability that the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is between .02 and .98, which is almost the entire parameter space. Here’s how to fit the model with the &lt;code&gt;brm()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d, 
      family = binomial,
      y | trials(1) ~ 1,
      prior(normal(0, 2), class = Intercept),
      seed = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;brm()&lt;/code&gt; formula syntax, including a &lt;code&gt;|&lt;/code&gt; bar on the left side of a formula indicates we have extra supplementary information about our criterion variable. In this case, that information is that each &lt;code&gt;y&lt;/code&gt; value corresponds to a single trial [i.e., &lt;code&gt;trials(1)&lt;/code&gt;], which itself corresponds to the &lt;span class=&#34;math inline&#34;&gt;\(n = 1\)&lt;/span&gt; portion of the statistical formula, above. Here are the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: y | trials(1) ~ 1 
##    Data: d (Number of observations: 50) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -1.39      0.36    -2.12    -0.71 1.00     1622     1434
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that that intercept is on the scale of the logit link, the log odds. We can transform it with the &lt;code&gt;brms::inv_logit_scaled()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(fit1)[&amp;quot;Intercept&amp;quot;, 1] %&amp;gt;% 
  inv_logit_scaled()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1995929&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we’d like to view the full posterior distribution, we’ll need to work with the posterior draws themselves. Then we’ll plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws
posterior_samples(fit1) %&amp;gt;% 
  # transform from the log-odds to a probability metric
  transmute(p = inv_logit_scaled(b_Intercept)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = p)) +
  geom_density(fill = &amp;quot;grey25&amp;quot;, size = 0) +
  scale_x_continuous(&amp;quot;probability of a hit&amp;quot;, limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Method &amp;#39;posterior_samples&amp;#39; is deprecated. Please see ?as_draws for
## recommended alternatives.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like the null hypothesis of &lt;span class=&#34;math inline&#34;&gt;\(p = .5\)&lt;/span&gt; is not credible for this simulation. If we’d like the posterior median and percentile-based 95% intervals, we might use the &lt;code&gt;median_qi()&lt;/code&gt; function from the handy &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

posterior_samples(fit1) %&amp;gt;% 
  transmute(p = inv_logit_scaled(b_Intercept)) %&amp;gt;% 
  median_qi()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 6
##       p .lower .upper .width .point .interval
##   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
## 1 0.201  0.108  0.330   0.95 median qi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yep, .5 was not within those intervals.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-what-about-power&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;But what about power?&lt;/h3&gt;
&lt;p&gt;That’s enough preliminary work. Let’s see what happens when we do a mini power analysis with 100 iterations. First we set up our simulation function using the same methods we introduced in earlier blog posts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_data_fit &amp;lt;- function(seed, n_player) {
  
  n_trials &amp;lt;- 1
  prob_hit &amp;lt;- .25
  
  set.seed(seed)
  
  d &amp;lt;- tibble(y = rbinom(n    = n_player, 
                         size = n_trials, 
                         prob = prob_hit))
  
  update(fit1,
         newdata = d,
         seed = seed) %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  transmute(p = inv_logit_scaled(b_Intercept)) %&amp;gt;% 
  median_qi() %&amp;gt;% 
    select(.lower:.upper)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n_player = 50)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might plot the intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 %&amp;gt;% 
  ggplot(aes(x = seed, ymin = .lower, ymax = .upper)) +
  geom_hline(yintercept = c(.25, .5), color = &amp;quot;white&amp;quot;) +
  geom_linerange() +
  xlab(&amp;quot;seed (i.e., simulation index)&amp;quot;) +
  scale_y_continuous(&amp;quot;probability of hitting the ball&amp;quot;, limits = c(0, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Like one of my old coworkers used to say: &lt;em&gt;Purtier ’n a hog!&lt;/em&gt; Here we’ll summarize the results both in terms of their conventional power, their mean width, and the proportion of widths more narrow than .25. &lt;em&gt;Why .25?&lt;/em&gt; I don’t know. Without a substantively-informed alternative, it’s as good a criterion as any.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim1 %&amp;gt;% 
  mutate(width = .upper - .lower) %&amp;gt;% 
  summarise(`conventional power` = mean(.upper &amp;lt; .5),
            `mean width`         = mean(width),
            `width below .25`    = mean(width &amp;lt; .25))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 3
##   `conventional power` `mean width` `width below .25`
##                  &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;
## 1                 0.95        0.231              0.78&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Depending on your study needs, you’d adjust your sample size accordingly, do a mini simulation or two first, and then follow up with a proper power simulation with 1000+ iterations.&lt;/p&gt;
&lt;p&gt;I should point out that whereas in the last post we evaluated the power of the Poisson model with the parameters on the scale of the link function, here we evaluated the power for our logistic regression model after transforming the intercept back into the probability metric. Both methods are fine. I recommend you run your power simulation based on how you want to interpret and report your results.&lt;/p&gt;
&lt;p&gt;We should also acknowledge that this was our first example of a power simulation that wasn’t based on some group comparison. Comparing groups is fine and normal and important. And it’s also the case that we can care about power and/or parameter precision for more than group-based analyses. Our simulation-based approach is fine for both.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregated-binomial-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Aggregated binomial regression&lt;/h2&gt;
&lt;p&gt;It’s no more difficult to simulate and work with aggregated binomial data. But since the mechanics for &lt;code&gt;brms::brm()&lt;/code&gt; and thus the down-the-road simulation setup are a little different, we should practice. With our new setup, we’ll consider a new example. Since .25 is the typical batting average, it might better sense to define the null hypothesis like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0 \text{: } p = .25.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Consider a case where we had some intervention where we expected a new batting average of .35. How many trials would we need, then, to either reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; or perhaps estimate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; with a satisfactory degree of precision? Here’s what the statistical formula for the implied aggregated binomial model might look like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i                        &amp;amp; \sim \text{Binomial} (n, p_i) \\
\operatorname{logit} (p_i) &amp;amp; = \beta_0.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The big change is we no longer defined &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; as 1. Let’s say we wanted our aggregated binomial data set to contain the summary statistics for &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt; trials. Here’s what that might look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_trials &amp;lt;- 100
prob_hit &amp;lt;- .35

set.seed(3)

d &amp;lt;- tibble(n_trials = n_trials,
            y = rbinom(n    = 1, 
                       size = n_trials, 
                       prob = prob_hit))

d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   n_trials     y
##      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1      100    32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have two columns. The first, &lt;code&gt;n_trials&lt;/code&gt;, indicates how many cases or trials we’re summarizing. The second, &lt;code&gt;y&lt;/code&gt;, indicates how many successes/1’s/hits we might expect given &lt;span class=&#34;math inline&#34;&gt;\(p = .35\)&lt;/span&gt;. This is the aggregated binomial equivalent of if we had a 100-row vector composed of 32 1s and 68 0s.&lt;/p&gt;
&lt;p&gt;Now, before we discuss fitting the model with &lt;strong&gt;brms&lt;/strong&gt;, let’s talk priors. Since we’ve updated our definition of &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;, it might make sense to update the prior for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;. As it turns out, setting that prior to &lt;code&gt;normal(-1, 0.5)&lt;/code&gt; puts the posterior mode at about .25 on the probability space, but with fairly wide 95% intervals ranging from about .12 to .5. Though centered on our updated null value, this prior is still quite permissive given our hypothesized &lt;span class=&#34;math inline&#34;&gt;\(p = .35\)&lt;/span&gt;. Let’s give it a whirl.&lt;/p&gt;
&lt;p&gt;To fit an aggregated binomial model with the &lt;code&gt;brm()&lt;/code&gt; function, we augment the &lt;code&gt;&amp;lt;criterion&amp;gt; | trials()&lt;/code&gt; syntax where the value that goes in &lt;code&gt;trials()&lt;/code&gt; is either a fixed number or variable in the data indexing &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Our approach will be the latter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;-
  brm(data = d, 
      family = binomial,
      y | trials(n_trials) ~ 1,
      prior(normal(-1, 0.5), class = Intercept),
      seed = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inspect the summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: binomial 
##   Links: mu = logit 
## Formula: y | trials(n_trials) ~ 1 
##    Data: d (Number of observations: 1) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -0.80      0.20    -1.19    -0.42 1.00     1524     1697
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After a transformation, here’s what that looks like in a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(fit2) %&amp;gt;% 
  transmute(p = inv_logit_scaled(b_Intercept)) %&amp;gt;% 
  
  ggplot(aes(x = p, y = 0)) +
  stat_halfeye(.width = c(.5, .95)) +
  scale_x_continuous(&amp;quot;probability of a hit&amp;quot;, limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Method &amp;#39;posterior_samples&amp;#39; is deprecated. Please see ?as_draws for
## recommended alternatives.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on a single simulation, it looks like &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt; won’t quite be enough to reject &lt;span class=&#34;math inline&#34;&gt;\(H_0 \text{: } p = .25\)&lt;/span&gt; with a conventional 2-sided 95% interval. But it does look like we’re in the ballpark and that our basic data + model setup will work for a larger-scale simulation. Here’s an example of how you might update our custom simulation function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_data_fit &amp;lt;- function(seed, n_trials) {
  
  prob_hit &amp;lt;- .35
  
  set.seed(seed)
  
  d &amp;lt;- tibble(y = rbinom(n    = 1, 
                         size = n_trials, 
                         prob = prob_hit),
              n_trials = n_trials)
  
  update(fit2,
         newdata = d,
         seed = seed) %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  transmute(p = inv_logit_scaled(b_Intercept)) %&amp;gt;% 
  median_qi() %&amp;gt;% 
    select(.lower:.upper)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate, this time trying out &lt;span class=&#34;math inline&#34;&gt;\(n = 120\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim2 &amp;lt;-
  tibble(seed = 1:100) %&amp;gt;% 
  mutate(ci = map(seed, sim_data_fit, n_trials = 120)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot the intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim2 %&amp;gt;% 
  ggplot(aes(x = seed, ymin = .lower, ymax = .upper)) +
  geom_hline(yintercept = c(.25, .35), color = &amp;quot;white&amp;quot;) +
  geom_linerange() +
  xlab(&amp;quot;seed (i.e., simulation index)&amp;quot;) +
  scale_y_continuous(&amp;quot;probability of hitting the ball&amp;quot;,
                     limits = c(0, 1), breaks = c(0, .25, .35, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian-power-analysis-part-iii-b/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Overall, those intervals look pretty good. They’re fairly narrow and are hovering around the data generating &lt;span class=&#34;math inline&#34;&gt;\(p = .35\)&lt;/span&gt;. But many are still crossing the .25 threshold. Let’s see the results of a formal summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim2 %&amp;gt;% 
  mutate(width = .upper - .lower) %&amp;gt;% 
  summarise(`conventional power` = mean(.lower &amp;gt; .25),
            `mean width`         = mean(width),
            `width below .2`     = mean(width &amp;lt; .2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 3
##   `conventional power` `mean width` `width below .2`
##                  &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1                 0.54        0.155                1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All widths were narrower than .2 and the mean width was about .16. In the abstract that might seem reasonably precise. But we’re still not precise enough to reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; with a conventional power level. Depending on your needs, adjust the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; accordingly and simulate again.&lt;/p&gt;
&lt;p&gt;Now you’ve got a sense of how to work with the binomial likelihood for (aggregated)binary data, next time we’ll play with Likert-type data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.1 (2021-08-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_3.0.1 brms_2.16.2     Rcpp_1.0.7      forcats_0.5.1  
##  [5] stringr_1.4.0   dplyr_1.0.7     purrr_0.3.4     readr_2.0.1    
##  [9] tidyr_1.1.3     tibble_3.1.4    ggplot2_3.3.5   tidyverse_1.3.1
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6          
##   [4] igraph_1.2.6         svUnit_1.0.6         splines_4.1.1       
##   [7] crosstalk_1.1.1      TH.data_1.0-10       rstantools_2.1.1    
##  [10] inline_0.3.19        digest_0.6.27        htmltools_0.5.2     
##  [13] rsconnect_0.8.24     fansi_0.5.0          magrittr_2.0.1      
##  [16] checkmate_2.0.0      tzdb_0.1.2           modelr_0.1.8        
##  [19] RcppParallel_5.1.4   matrixStats_0.60.1   sandwich_3.0-1      
##  [22] xts_0.12.1           prettyunits_1.1.1    colorspace_2.0-2    
##  [25] rvest_1.0.1          ggdist_3.0.0         haven_2.4.3         
##  [28] xfun_0.25            callr_3.7.0          crayon_1.4.1        
##  [31] jsonlite_1.7.2       lme4_1.1-27.1        survival_3.2-11     
##  [34] zoo_1.8-9            glue_1.4.2           gtable_0.3.0        
##  [37] emmeans_1.6.3        V8_3.4.2             distributional_0.2.2
##  [40] pkgbuild_1.2.0       rstan_2.26.3         abind_1.4-5         
##  [43] scales_1.1.1         mvtnorm_1.1-2        DBI_1.1.1           
##  [46] miniUI_0.1.1.1       xtable_1.8-4         stats4_4.1.1        
##  [49] StanHeaders_2.26.3   DT_0.19              htmlwidgets_1.5.3   
##  [52] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [55] posterior_1.0.1      ellipsis_0.3.2       pkgconfig_2.0.3     
##  [58] loo_2.4.1            farver_2.1.0         sass_0.4.0          
##  [61] dbplyr_2.1.1         utf8_1.2.2           tidyselect_1.1.1    
##  [64] labeling_0.4.2       rlang_0.4.11         reshape2_1.4.4      
##  [67] later_1.3.0          munsell_0.5.0        cellranger_1.1.0    
##  [70] tools_4.1.1          cli_3.0.1            generics_0.1.0      
##  [73] broom_0.7.9          ggridges_0.5.3       evaluate_0.14       
##  [76] fastmap_1.1.0        yaml_2.2.1           processx_3.5.2      
##  [79] knitr_1.33           fs_1.5.0             nlme_3.1-152        
##  [82] mime_0.11            projpred_2.0.2       xml2_1.3.2          
##  [85] compiler_4.1.1       bayesplot_1.8.1      shinythemes_1.2.0   
##  [88] rstudioapi_0.13      gamm4_0.2-6          curl_4.3.2          
##  [91] reprex_2.0.1         bslib_0.3.0          stringi_1.7.4       
##  [94] highr_0.9            ps_1.6.0             blogdown_1.5        
##  [97] Brobdingnag_1.2-6    lattice_0.20-44      Matrix_1.3-4        
## [100] nloptr_1.2.2.2       markdown_1.1         shinyjs_2.0.0       
## [103] tensorA_0.36.2       vctrs_0.3.8          pillar_1.6.2        
## [106] lifecycle_1.0.0      jquerylib_0.1.4      bridgesampling_1.1-2
## [109] estimability_1.3     httpuv_1.6.2         R6_2.5.1            
## [112] bookdown_0.23        promises_1.2.0.1     gridExtra_2.3       
## [115] codetools_0.2-18     boot_1.3-28          colourpicker_1.1.0  
## [118] MASS_7.3-54          gtools_3.9.2         assertthat_0.2.1    
## [121] withr_2.4.2          shinystan_2.5.0      multcomp_1.4-17     
## [124] mgcv_1.8-36          parallel_4.1.1       hms_1.1.0           
## [127] grid_4.1.1           coda_0.19-4          minqa_1.2.4         
## [130] rmarkdown_2.10       shiny_1.6.0          lubridate_1.7.10    
## [133] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-agrestiFoundationsLinearGeneralized2015&#34; class=&#34;csl-entry&#34;&gt;
Agresti, A. (2015). &lt;em&gt;Foundations of linear and generalized linear models&lt;/em&gt;. &lt;span&gt;John Wiley &amp;amp; Sons&lt;/span&gt;. &lt;a href=&#34;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&#34;&gt;https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In case this is all new to you and you and you had the question in your mind: Yes, you can add predictors to the logistic regression model. Say we had a model with two predictors, &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. Our statistical model would then follow the form &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit} (p_i) = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}\)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

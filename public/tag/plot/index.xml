<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plot | Fahim Ahmad</title>
    <link>/tag/plot/</link>
      <atom:link href="/tag/plot/index.xml" rel="self" type="application/rss+xml" />
    <description>plot</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Fahim Ahmad (2020)</copyright><lastBuildDate>Wed, 17 Nov 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>plot</title>
      <link>/tag/plot/</link>
    </image>
    
    <item>
      <title>Conditional logistic models with brms: Rough draft.</title>
      <link>/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/</link>
      <pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/</guid>
      <description>
&lt;script src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble&lt;/h2&gt;
&lt;p&gt;After tremendous help from &lt;a href=&#34;http://singmann.org/&#34;&gt;Henrik Singmann&lt;/a&gt; and &lt;a href=&#34;https://sites.google.com/view/mattansb&#34;&gt;Mattan Ben-Shachar&lt;/a&gt;, I finally have two (!) workflows for conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. These workflows are on track to make it into the next update of my ebook translation of Kruschke’s text (see &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;here&lt;/a&gt;). But these models are new to me and I’m not entirely confident I’ve walked them out properly.&lt;/p&gt;
&lt;p&gt;The goal of this blog post is to present a draft of my workflow, which will eventually make it’s way into &lt;a href=&#34;https://bookdown.org/content/3686/nominal-predicted-variable.html&#34;&gt;Chapter 22&lt;/a&gt; of the ebook. If you have any constrictive criticisms, please pass them along on&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/22&#34;&gt;GitHub&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433&#34;&gt;this thread&lt;/a&gt; in the Stan forums,&lt;/li&gt;
&lt;li&gt;or on &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1461016859420024842&#34;&gt;twitter&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To streamline this post a little, I have removed the content on the softmax model. For that material, just go to the ebook proper.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# in the ebook, this code will have already been executed
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)

theme_set(
  theme_gray() +
    theme(panel.grid = element_blank())
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nominal-predicted-variable&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nominal Predicted Variable&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;This chapter considers data structures that have a nominal predicted variable. When the nominal predicted variable has only two possible values, this reduces to the case of the dichotomous predicted variable considered in the previous chapter. In the present chapter, we generalize to cases in which the predicted variable has three or more categorical values…&lt;/p&gt;
&lt;p&gt;The traditional treatment of this sort of data structure is called multinomial logistic regression or conditional logistic regression. We will consider Bayesian approaches to these methods. As usual, in Bayesian software it is easy to generalize the traditional models so they are robust to outliers, allow different variances within levels of a nominal predictor, and have hierarchical structure to share information across levels or factors as appropriate. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke, 2015, p. 649&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;softmax-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Softmax regression&lt;/h2&gt;
&lt;div id=&#34;softmax-reduces-to-logistic-for-two-outcomes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Softmax reduces to logistic for two outcomes.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;independence-from-irrelevant-attributes.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Independence from irrelevant attributes.&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional logistic regression&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Softmax regression conceives of each outcome as an independent change in log odds from the reference outcome, and a special case of that is dichotomous logistic regression. But we can generalize logistic regression another way, which may better capture some patterns of data. The idea of this generalization is that we divide the set of outcomes into a hierarchy of two-set divisions, and use a logistic to describe the probability of each branch of the two-set divisions. (p. 655)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The model follows the generic equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\phi_{S^* | S} = \operatorname{logistic}(\lambda_{S^* | S}) \\
\lambda_{S^* | S} = \beta_{0, S^* | S} + \beta_{1, {S^* | S}} x,
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the conditional response probability (i.e., the goal of the analysis) is &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(S^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; denote the subset of outcomes and larger set of outcomes, respectively, and &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{S^* | S}\)&lt;/span&gt; is the propensity based on some linear model. The overall point is these “regression coefficients refer to the conditional probability of outcomes for the designated subsets, not necessarily to a single outcome among the full set of outcomes” (p. 655).&lt;/p&gt;
&lt;p&gt;In Figure 22.2 (p. 656), Kruschke depicted the two hierarchies of binary divisions of the models he fit to the data in his &lt;code&gt;CondLogistRegData1.csv&lt;/code&gt; and &lt;code&gt;CondLogistRegData2.csv&lt;/code&gt; files. Here we load those data, save them as &lt;code&gt;d3&lt;/code&gt; and &lt;code&gt;d4&lt;/code&gt;, and take a look at their structures.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d3 &amp;lt;- read_csv(&amp;quot;data.R/CondLogistRegData1.csv&amp;quot;)
d4 &amp;lt;- read_csv(&amp;quot;data.R/CondLogistRegData2.csv&amp;quot;)

glimpse(d3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 475
## Columns: 3
## $ X1 &amp;lt;dbl&amp;gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.5…
## $ X2 &amp;lt;dbl&amp;gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.774…
## $ Y  &amp;lt;dbl&amp;gt; 2, 1, 3, 1, 3, 3, 2, 3, 2, 4, 1, 2, 2, 3, 4, 2, 2, 4, 2, 3, 4, 2, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(d4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 475
## Columns: 3
## $ X1 &amp;lt;dbl&amp;gt; -0.08714736, -0.72256565, 0.17918961, -1.15975176, -0.72711762, 0.5…
## $ X2 &amp;lt;dbl&amp;gt; -1.08134218, -1.58386308, 0.97179045, 0.50262438, 1.37570446, 1.774…
## $ Y  &amp;lt;dbl&amp;gt; 4, 4, 3, 4, 2, 3, 4, 3, 4, 4, 2, 4, 4, 3, 3, 4, 4, 4, 4, 3, 4, 4, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In both data sets, the nominal criterion is &lt;code&gt;Y&lt;/code&gt; and the two predictors are &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt;. Though the data seem simple, the conditional logistic models are complex enough that it seems like we’ll be better served by focusing on them one at a time, which means I’m going to break up Figure 22.2. Here’s how to make the diagram in the left panel.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the big numbers
numbers &amp;lt;- tibble(
  x = c(3, 5, 2, 4, 1, 3, 2),
  y = c(0, 0, 1, 1, 2, 2, 3),
  label = c(&amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3,4&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2,3,4&amp;quot;, &amp;quot;1,2,3,4&amp;quot;)
)

# the smaller Greek numbers
greek &amp;lt;- tibble(
  x = c(3.4, 4.6, 2.4, 3.6, 1.4, 2.6),
  y = c(0.5, 0.5, 1.5, 1.5, 2.5, 2.5),
  hjust = c(1, 0, 1, 0, 1, 0),
  label = c(&amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;)
)

# arrows
tibble(
  x = c(4, 4, 3, 3, 2, 2),
  y = c(0.85, 0.85, 1.85, 1.85, 2.85, 2.85),
  xend = c(3, 5, 2, 4, 1, 3),
  yend = c(0.15, 0.15, 1.15, 1.15, 2.15, 2.15)
) %&amp;gt;%  
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_segment(aes(xend = xend, yend = yend),
               size = 1/4,
               arrow = arrow(length = unit(0.08, &amp;quot;in&amp;quot;), type = &amp;quot;closed&amp;quot;)) +
  geom_text(data = numbers,
            aes(label = label),
            size = 5, family = &amp;quot;Times&amp;quot;)+
  geom_text(data = greek,
            aes(label = label, hjust = hjust),
            size = 4.25, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-1, 7) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The large numbers are the four levels in the criterion &lt;code&gt;Y&lt;/code&gt; and the smaller numbers in the curly braces are various sets of those numbers. The diagram shows three levels of outcome-set divisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 versus 2, 3, or 4;&lt;/li&gt;
&lt;li&gt;2 versus 3 or 4; and&lt;/li&gt;
&lt;li&gt;3 versus 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The divisions in each of these levels can be expressed as linear models which we’ll denote &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Given our data with two predictors &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt;, we can express the three linear models as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\lambda_{\{ 1 \} | \{ 1,2,3,4 \}} &amp;amp; = \beta_{0,\{ 1 \} | \{ 1,2,3,4 \}} + \beta_{1,\{ 1 \} | \{ 1,2,3,4 \}} \text{X1} + \beta_{2,\{ 1 \} | \{ 1,2,3,4 \}} \text{X2} \\
\lambda_{\{ 2 \} | \{ 2,3,4 \}}   &amp;amp; = \beta_{0,\{ 2 \} | \{ 2,3,4 \}} + \beta_{1,\{ 2 \} | \{ 2,3,4 \}} \text{X1} + \beta_{2,\{ 2 \} | \{ 2,3,4 \}} \text{X2} \\
\lambda_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \beta_{0,\{ 3 \} | \{ 3,4 \}} + \beta_{1,\{ 3 \} | \{ 3,4 \}} \text{X1} + \beta_{2,\{ 3 \} | \{ 3,4 \}} \text{X2},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, for convenience, we’re omitting the typical &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; subscripts. As these linear models are all defined within the context of the logit link, we can express the conditional probabilities of the outcome sets as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_{\{ 1 \} | \{ 1,2,3,4 \}} &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_{\{ 2 \} | \{ 2,3,4 \}}   &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 2 \} | \{ 2,3,4 \}} \right) \\
\phi_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 3 \} | \{ 3,4 \}} \right),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 1 \} | \{ 1,2,3,4 \}}\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 3 \} | \{ 3,4 \}}\)&lt;/span&gt; are the conditional probabilities for the outcome sets. If, however, we want the conditional probabilities for the actual levels of the criterion &lt;code&gt;Y&lt;/code&gt;, we define those with a series of (in this case) four equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \phi_{\{ 2 \} | \{ 2,3,4 \}} \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the sum of the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\phi_1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(\phi_4\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. To get a sense of what this all means in practice, let’s visualize the data and the data-generating equations for our version of Figure 22.3. As with the previous figure, I’m going to break this figure up to focus on one model at a time. Thus, here’s the left panel of Figure 22.3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the various population parameters

# lambda 1
b01 &amp;lt;- -4
b11 &amp;lt;- -5
b21 &amp;lt;- 0.01  # rounding up to avoid dividing by zero
# lambda 2
b02 &amp;lt;- -2
b12 &amp;lt;- 1
b22 &amp;lt;- -5
# lambda 3
b03 &amp;lt;- -1
b13 &amp;lt;- 3
b23 &amp;lt;- 3

# use the parameters to define the lines 
lines &amp;lt;- tibble(
  intercept = c(-b01 / b21, -b02 / b22, -b03 / b23),
  slope = c(-b11 / b21, -b12 / b22, -b13 / b23),
  label = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;)
)

# wrangle
d3 %&amp;gt;% 
  mutate(Y = factor(Y)) %&amp;gt;% 
  
  # plot!
  ggplot() +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_text(aes(x = X1, y = X2, label = Y, color = Y),
            size = 3, show.legend = F) +
  geom_abline(data = lines,
              aes(intercept = intercept,
                  slope = slope,
                  linetype = label)) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]==-4+-5*x[1]+0*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]==-2+1*x[1]+-5*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]==-1+3*x[1]+3*x[2]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;240&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Recall back on page 629, Kruschke showed the equation for the 50% threshold of a logistic regression model given two continuous predictors was&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_2 = (-\beta_0 / \beta_2) + (-\beta_1 / \beta_2) x_1.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It was that equation that gave us the values for the &lt;code&gt;intercept&lt;/code&gt; and &lt;code&gt;slope&lt;/code&gt; arguments (&lt;span class=&#34;math inline&#34;&gt;\(-\beta_0 / \beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-\beta_1 / \beta_2\)&lt;/span&gt;, respectively) for the &lt;code&gt;geom_abline()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;It still might not be clear how the various &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt; values connect to the data. Though not in the text, here’s an alternative way of expressing the relations in Figure 22.3. This time the plot is faceted by the three levels of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt; and the background fill is based on those conditional probabilities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define a grid of X1 and X2 values
crossing(X1 = seq(from = -2, to = 2, length.out = 50),
         X2 = seq(from = -2, to = 2, length.out = 50)) %&amp;gt;% 
  # compute the lambda&amp;#39;s
  mutate(`lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]` = b01 + b11 * X1 + b21 * X2,
         `lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]`   = b02 + b12 * X1 + b22 * X2,
         `lambda[&amp;#39;{3}|{3,4}&amp;#39;]`     = b03 + b13 * X1 + b23 * X2) %&amp;gt;% 
  # compute the phi&amp;#39;s
  mutate(`phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]` = plogis(`lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]`),
         `phi[&amp;#39;{2}|{2,3,4}&amp;#39;]`   = plogis(`lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]`),
         `phi[&amp;#39;{3}|{3,4}&amp;#39;]`     = plogis(`lambda[&amp;#39;{3}|{3,4}&amp;#39;]`)) %&amp;gt;% 
  # wrangle
  pivot_longer(contains(&amp;quot;phi&amp;quot;), values_to = &amp;quot;phi&amp;quot;) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = X1, y = X2)) +
  geom_raster(aes(fill = phi),
              interpolate = T) +
  # note how we&amp;#39;re subsetting the d3 data by facet
  geom_text(data = bind_rows(
    d3 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;),
    d3 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;gt; 1),
    d3 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;gt; 2)),
            aes(label = Y),
            size = 2.5, color = &amp;quot;grey20&amp;quot;) +
  scale_fill_viridis_c(expression(phi[italic(S)*&amp;quot;*|&amp;quot;*italic(S)]),
                        option = &amp;quot;F&amp;quot;, breaks = 0:2 / 2, limits = c(0, 1)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_equal() +
  theme(legend.position = c(0.8, 0.2)) +
  facet_wrap(~ name, labeller = label_parsed, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how because each of the levels of &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is defined by a different subset of the data, each of the facets contains a different subset of the &lt;code&gt;d3&lt;/code&gt; data, too. For example, since &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 1 \} | \{ 1,2,3,4 \}}\)&lt;/span&gt; is defined by the full subset of the possible values of &lt;code&gt;Y&lt;/code&gt;, you see all the &lt;code&gt;Y&lt;/code&gt; data displayed by &lt;code&gt;geom_text()&lt;/code&gt; for that facet. In contrast, since &lt;span class=&#34;math inline&#34;&gt;\(\phi_{\{ 3 \} | \{ 3,4 \}}\)&lt;/span&gt; is defined by a subset of the data for which &lt;code&gt;Y&lt;/code&gt; is only &lt;code&gt;3&lt;/code&gt; or &lt;code&gt;4&lt;/code&gt;, those are the only values you see displayed within that facet of the plot.&lt;/p&gt;
&lt;p&gt;Now we’ll consider an alternative way to set up the binary-choices hierarchy, as seen in the right panel of Figure 22.2. First, here’s that half of the figure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the big numbers
numbers &amp;lt;- tibble(
  x = c(0, 2, 6, 8, 1, 7, 4),
  y = c(0, 0, 0, 0, 1, 1, 2),
  label = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;1,2&amp;quot;, &amp;quot;3,4&amp;quot;, &amp;quot;1,2,3,4&amp;quot;)
)

# the smaller Greek numbers
greek &amp;lt;- tibble(
  x = c(0.4, 1.6, 6.4, 7.6, 2.1, 5.8),
  y = c(0.5, 0.5, 0.5, 0.5, 1.5, 1.5),
  hjust = c(1, 0, 1, 0, 1, 0),
  label = c(&amp;quot;phi[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;,
            &amp;quot;phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;, &amp;quot;1-phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;)
)

# arrows
tibble(
  x = c(1, 1, 7, 7, 4, 4),
  y = c(0.85, 0.85, 0.85, 0.85, 1.85, 1.85),
  xend = c(0, 2, 6, 8, 1, 7),
  yend = c(0.15, 0.15, 0.15, 0.15, 1.15, 1.15)
) %&amp;gt;%  
  
  # plot!
  ggplot(aes(x = x, y = y)) +
  geom_segment(aes(xend = xend, yend = yend),
               size = 1/4,
               arrow = arrow(length = unit(0.08, &amp;quot;in&amp;quot;), type = &amp;quot;closed&amp;quot;)) +
  geom_text(data = numbers,
            aes(label = label),
            size = 5, family = &amp;quot;Times&amp;quot;)+
  geom_text(data = greek,
            aes(label = label, hjust = hjust),
            size = 4.25, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-1, 10) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This diagram shows three levels of outcome-set divisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 or 2 versus 3 or 4;&lt;/li&gt;
&lt;li&gt;1 versus 2; and&lt;/li&gt;
&lt;li&gt;3 versus 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given our data with two predictors &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt;, we can express the three linear models as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\lambda_{\{ 1,2 \} | \{ 1,2,3,4 \}} &amp;amp; = \beta_{0,\{ 1,2 \} | \{ 1,2,3,4 \}} + \beta_{1,\{ 1,2 \} | \{ 1,2,3,4 \}} \text{X1} + \beta_{2,\{ 1,2 \} | \{ 1,2,3,4 \}} \text{X2} \\
\lambda_{\{ 1 \} | \{ 1,2 \}}   &amp;amp; = \beta_{0,\{ 1 \} | \{ 1,2 \}} + \beta_{1,\{ 1 \} | \{ 1,2 \}} \text{X1} + \beta_{2,\{ 1 \} | \{ 1,2 \}} \text{X2} \\
\lambda_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \beta_{0,\{ 3 \} | \{ 3,4 \}} + \beta_{1,\{ 3 \} | \{ 3,4 \}} \text{X1} + \beta_{2,\{ 3 \} | \{ 3,4 \}} \text{X2}.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can then express the conditional probabilities of the outcome sets as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_{\{ 1 \} | \{ 1,2 \}}   &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 1 \} | \{ 1,2 \}} \right) \\
\phi_{\{ 3 \} | \{ 3,4 \}}     &amp;amp; = \operatorname{logistic} \left (\lambda_{\{ 3 \} | \{ 3,4 \}} \right).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the conditional probabilities of the actual levels of the criterion &lt;code&gt;Y&lt;/code&gt;, we define those with a series of (in this case) four equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2 \}} \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \left ( 1 - \phi_{\{ 1 \} | \{ 1,2 \}} \right) \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the sum of the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\phi_1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(\phi_4\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. To get a sense of what this all means, let’s visualize the data and the data-generating equations in our version of the right panel of Figure 22.3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d4 %&amp;gt;% 
  mutate(Y = factor(Y)) %&amp;gt;% 
  
  ggplot() +
  geom_hline(yintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_text(aes(x = X1, y = X2, label = Y, color = Y),
            size = 3, show.legend = F) +
  geom_abline(data = lines,
              aes(intercept = intercept,
                  slope = slope,
                  linetype = label)) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]==-4+-5*x[1]+0*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{1}|{1,2}&amp;#39;]==-2+1*x[1]+-5*x[2]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]==-1+3*x[1]+3*x[2]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;240&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s an alternative way of expression the relations in the right panel of Figure 22.3. This time the plot is faceted by the three levels of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{S^* | S}\)&lt;/span&gt; and the background fill is based on those conditional probabilities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define a grid of X1 and X2 values
crossing(X1 = seq(from = -2, to = 2, length.out = 50),
         X2 = seq(from = -2, to = 2, length.out = 50)) %&amp;gt;% 
  # compute the lambda&amp;#39;s
  mutate(`lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]` = b01 + b11 * X1 + b21 * X2,
         `lambda[&amp;#39;{1}|{1,2}&amp;#39;]`       = b02 + b12 * X1 + b22 * X2,
         `lambda[&amp;#39;{3}|{3,4}&amp;#39;]`       = b03 + b13 * X1 + b23 * X2) %&amp;gt;% 
  # compute the phi&amp;#39;s
  mutate(`phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]` = plogis(`lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]`),
         `phi[&amp;#39;{1}|{1,2}&amp;#39;]`       = plogis(`lambda[&amp;#39;{1}|{1,2}&amp;#39;]`),
         `phi[&amp;#39;{3}|{3,4}&amp;#39;]`       = plogis(`lambda[&amp;#39;{3}|{3,4}&amp;#39;]`)) %&amp;gt;% 
  # wrangle
  pivot_longer(contains(&amp;quot;phi&amp;quot;), values_to = &amp;quot;phi&amp;quot;) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = X1, y = X2)) +
  geom_raster(aes(fill = phi),
              interpolate = T) +
  # note how we&amp;#39;re subsetting the d3 data by facet
  geom_text(data = bind_rows(
    d4 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;),
    d4 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;lt; 3),
    d4 %&amp;gt;% mutate(name = &amp;quot;phi[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;) %&amp;gt;% filter(Y &amp;gt; 2)),
            aes(label = Y),
            size = 2.5, color = &amp;quot;grey20&amp;quot;) +
  scale_fill_viridis_c(expression(phi[italic(S)*&amp;quot;*|&amp;quot;*italic(S)]),
                       option = &amp;quot;F&amp;quot;, breaks = 0:2 / 2, limits = c(0, 1)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_equal() +
  theme(legend.position = c(0.8, 0.2)) +
  facet_wrap(~ name, labeller = label_parsed, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It could be easy to miss due to the way we broke up our workflow, but if you look closely at the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; equations at the top of both panels of Figure 22.3, you’ll see the right-hand side of the equations are the same. But because of the differences in the two data hierarchies, those &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; equations had different consequences for how the &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; values generated the &lt;code&gt;Y&lt;/code&gt; data. Also,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In general, conditional logistic regression requires that there is a linear division between two subsets of the outcomes, and then within each of those subsets there is a linear division of smaller subsets, and so on. This sort of linear division is not required of the softmax regression model… Real data can be extremely noisy, and there can be multiple predictors, so it can be challenging or impossible to visually ascertain which sort of model is most appropriate. The choice of model is driven primarily by theoretical meaningfulness. (p. 659)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-in-jags-brms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implementation in &lt;del&gt;JAGS&lt;/del&gt; brms&lt;/h2&gt;
&lt;div id=&#34;softmax-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Softmax model.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-model.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conditional logistic model.&lt;/h3&gt;
&lt;p&gt;The conditional logistic regression models are not natively supported in &lt;strong&gt;brms&lt;/strong&gt; at this time. Based on &lt;a href=&#34;https://github.com/paul-buerkner/brms/issues/560&#34;&gt;issue #560&lt;/a&gt; in the &lt;strong&gt;brms&lt;/strong&gt; GitHub, there are ways to fit them using the nonlinear syntax. If you compare the syntax Bürkner used in that thread on January 30&lt;sup&gt;th&lt;/sup&gt; to the JAGS syntax Kruschke showed on pages 661 and 662, you’ll see they appear to follow contrasting parameterizations.&lt;/p&gt;
&lt;p&gt;However, there are at least two other ways to fit conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. Based on insights from &lt;a href=&#34;http://singmann.org/&#34;&gt;Henrik Singmann&lt;/a&gt;, we can define conditional logistic models using the custom family approach. In contrast, &lt;a href=&#34;https://sites.google.com/view/mattansb&#34;&gt;Mattan Ben-Shachar&lt;/a&gt; has shown we can also fit conditional logistic models using a tricky application of sequential ordinal regression. Rather than present them in the abstract, here, we will showcase both of these approaches in the sections below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results-interpreting-the-regression-coefficients.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Results: Interpreting the regression coefficients.&lt;/h3&gt;
&lt;div id=&#34;softmax-model.-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Softmax model.&lt;/h4&gt;
&lt;div id=&#34;bonus-consider-the-interceps-only-softmax-model.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Bonus: Consider the interceps-only softmax model.&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-model.-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Conditional logistic model.&lt;/h4&gt;
&lt;p&gt;Since we will be fitting the conditional logistic model with two different strategies, I’m going to deviate from how Kruschke organized this part of the text and break this section up into two subsections:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First we’ll walk through the custom family approach.&lt;/li&gt;
&lt;li&gt;Second we’ll explore the sequential ordinal approach.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;conditional-logistic-models-with-custom-likelihoods.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Conditional logistic models with custom likelihoods.&lt;/h5&gt;
&lt;p&gt;As we briefly learned in [Section 8.6.1][Defining new likelihood functions.], &lt;strong&gt;brms&lt;/strong&gt; users can define their own custom likelihood functions, which Bürkner outlined in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Define&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; vignette, &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html&#34;&gt;&lt;em&gt;Define custom response distributions with brms&lt;/em&gt;&lt;/a&gt;. As part of the &lt;a href=&#34;https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433&#34;&gt;&lt;em&gt;Nominal data and Kruschke’s “conditional logistic” approach&lt;/em&gt;&lt;/a&gt; thread on the Stan forums, Henrik Singmann showed how you can use this functionality to fit conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. We will practice how to do this for the models of both the &lt;code&gt;d3&lt;/code&gt; and &lt;code&gt;d4&lt;/code&gt; data sets, which were showcased in the left and right panels of Figure 22.3 in &lt;a href=&#34;#conditional-logistic-regression&#34;&gt;Section 22.2&lt;/a&gt;. Going in order, we’ll focus first on how to model the data in &lt;code&gt;d3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the first step, we use the &lt;code&gt;custom_family()&lt;/code&gt; function to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;name the new family with the &lt;code&gt;name&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;name the family’s parameters with the &lt;code&gt;dpars&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;name the link function(s) with the &lt;code&gt;links&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;define whether the distribution is discrete or continuous with the &lt;code&gt;type&lt;/code&gt; argument,&lt;/li&gt;
&lt;li&gt;provide the names of any variables that are part of the internal workings of the family but are not among the distributional parameters with the &lt;code&gt;vars&lt;/code&gt; argument, and&lt;/li&gt;
&lt;li&gt;provide supporting information with the &lt;code&gt;specials&lt;/code&gt; argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cond_log_1 &amp;lt;- custom_family(
  name     = &amp;quot;cond_log_1&amp;quot;, 
  dpars    = c(&amp;quot;mu&amp;quot;, &amp;quot;mub&amp;quot;, &amp;quot;muc&amp;quot;), 
  links    = &amp;quot;identity&amp;quot;, 
  type     = &amp;quot;int&amp;quot;,
  vars     = c(&amp;quot;n_cat&amp;quot;),
  specials = &amp;quot;categorical&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the second step, we use the &lt;code&gt;stanvar()&lt;/code&gt; function to define our custom probability mass function and the corresponding function that will allow us to return predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_lpmf_1 &amp;lt;- stanvar(block = &amp;quot;functions&amp;quot;, 
                       scode = &amp;quot;
real cond_log_1_lpmf(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mu;
  prob[2] = p_mub * (1 - p_mu);
  prob[3] = p_muc * (1 - p_mub) * (1 - p_mu);
  prob[4] = (1 - p_mu) * (1 - p_mub) * (1 - p_muc);
  return(categorical_lpmf(y | prob));
}

vector cond_log_1_pred(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mu;
  prob[2] = p_mub * (1 - p_mu);
  prob[3] = p_muc * (1 - p_mub) * (1 - p_mu);
  prob[4] = (1 - p_mu) * (1 - p_mub) * (1 - p_muc);
  return(prob);
}
&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how we have defined the four &lt;code&gt;prob[i]&lt;/code&gt; values based on the four equations from above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \phi_{\{ 2 \} | \{ 2,3,4 \}} \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 2 \} | \{ 2,3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1 \} | \{ 1,2,3,4 \}} \right).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Third, we save another &lt;code&gt;stanvar()&lt;/code&gt; object with additional information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stanvars &amp;lt;- stanvar(x = 4, name = &amp;quot;n_cat&amp;quot;, scode = &amp;quot;  int n_cat;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to fit the model with &lt;code&gt;brm()&lt;/code&gt;. Notice how our use of the &lt;code&gt;family&lt;/code&gt; and &lt;code&gt;stanvars&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.3 &amp;lt;-
  brm(data = d3, 
      family = cond_log_1,
      Y ~ 1 + X1 + X2,
      prior = c(prior(normal(0, 20), class = Intercept, dpar = mu2),
                prior(normal(0, 20), class = Intercept, dpar = mu3),
                prior(normal(0, 20), class = Intercept, dpar = mu4),
                prior(normal(0, 20), class = b, dpar = mu2),
                prior(normal(0, 20), class = b, dpar = mu3),
                prior(normal(0, 20), class = b, dpar = mu4)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      stanvars = stan_lpmf_1 + stanvars,
      file = &amp;quot;fits/fit22.03&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cond_log_1 
##   Links: mu2 = identity; mu3 = identity; mu4 = identity 
## Formula: Y ~ 1 + X1 + X2 
##    Data: d3 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## mu2_Intercept    -4.02      0.47    -5.01    -3.17 1.00     3108     2656
## mu3_Intercept    -2.13      0.36    -2.88    -1.50 1.00     2490     2211
## mu4_Intercept    -0.96      0.32    -1.62    -0.38 1.00     3014     2674
## mu2_X1           -4.92      0.54    -6.03    -3.94 1.00     3029     2832
## mu2_X2            0.01      0.20    -0.37     0.40 1.00     4950     2436
## mu3_X1            0.74      0.30     0.16     1.35 1.00     3343     2736
## mu3_X2           -5.21      0.63    -6.54    -4.06 1.00     2748     2794
## mu4_X1            3.00      0.49     2.10     4.05 1.00     3002     2280
## mu4_X2            3.10      0.53     2.16     4.25 1.00     2646     2412
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As they aren’t the most intuitive, here’s how to understand our three prefixes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mu2_&lt;/code&gt; has to do with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\{ 1 \} | \{ 1,2,3,4 \}}\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu3_&lt;/code&gt; has to do with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\{ 2 \} | \{ 2,3,4 \}}\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu4_&lt;/code&gt; has to do with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\{ 3 \} | \{ 3,4 \}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you compare those posterior means of each of those parameters from the data-generating equations at the top of Figure 22.3, you’ll see they are spot on (within simulation variance). Here’s how we might visualize those posteriors in our version of the histograms in the top right panel(s) of Figure 22.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws
post &amp;lt;- posterior_samples(fit22.3)

# wrangle
p1 &amp;lt;-
  post %&amp;gt;% 
  pivot_longer(-lp__) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(number = str_extract(name, &amp;quot;[2-4]+&amp;quot;)) %&amp;gt;% 
  mutate(lambda    = case_when(number == &amp;quot;2&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;,
                               number == &amp;quot;3&amp;quot; ~ &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;,
                               number == &amp;quot;4&amp;quot; ~ &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;),
         parameter = case_when(str_detect(name, &amp;quot;Intercept&amp;quot;) ~ &amp;quot;beta[0]&amp;quot;,
                               str_detect(name, &amp;quot;X1&amp;quot;)        ~ &amp;quot;beta[1]&amp;quot;,
                               str_detect(name, &amp;quot;X2&amp;quot;)        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95, size = 1,
                    normalize = &amp;quot;panels&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we use the threshold formula from above,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_2 = (-\beta_0 / \beta_2) + (-\beta_1 / \beta_2)x_1,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to the posterior draws, we can make our version of the upper left panel of Figure 22.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22)

p2 &amp;lt;-
  post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_mu&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;lambda&amp;quot;, &amp;quot;parameter&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  mutate(intercept = -Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now combine the two ggplots, add a little formatting, and show the full upper half of Figure 22.6, based on the &lt;code&gt;custom_family()&lt;/code&gt; approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p2 + p1) &amp;amp; 
  plot_layout(widths = c(1, 2)) &amp;amp;
  plot_annotation(title = &amp;quot;Figure 22.6, upper half&amp;quot;,
                  subtitle = &amp;quot;Results from the conditional logistic model fit to the d3 data via the custom-family approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though it isn’t necessary to reproduce any of the plots in this section of Kruschke’s text, we’ll want to use the &lt;code&gt;expose_functions()&lt;/code&gt; function if we wanted to use any of the &lt;strong&gt;brms&lt;/strong&gt; post-processing functions for our model fit with the custom likelihood.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expose_functions(fit22.3, vectorize = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what we’d need to do before computing information criteria estimates, such as with the WAIC.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_lik_cond_log_1 &amp;lt;- function(i, prep) {
  mu  &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mub &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  muc &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  cond_log_1_lpmf(y, mu, mub, muc, n_cat)
}

fit22.3 &amp;lt;- add_criterion(fit22.3, criterion = &amp;quot;waic&amp;quot;)

waic(fit22.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 4000 by 475 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -230.8 16.8
## p_waic         9.3  1.1
## waic         461.6 33.6
## 
## 2 (0.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to use one of the functions that relies on conditional expectations, such as &lt;code&gt;conditional_effects()&lt;/code&gt;, we’d execute something like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_epred_cond_log_1 &amp;lt;- function(prep) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y
  prob &amp;lt;- cond_log_1_pred(y = y, mu = mu, mu_b = mu_b, mu_c = mu_c, n_cat = n_cat)
  dim(prob) &amp;lt;- c(dim(prob)[1], dim(mu))
  prob &amp;lt;- aperm(prob, c(2,3,1))
  dimnames(prob) &amp;lt;- list(
    as.character(seq_len(dim(prob)[1])), 
    NULL, 
    as.character(seq_len(dim(prob)[3]))
  )
  prob
}

ce &amp;lt;- conditional_effects(
  fit22.3, 
  categorical = T,
  effects = &amp;quot;X1&amp;quot;)

plot(ce, plot = FALSE)[[1]] + 
  scale_fill_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we wanted to do a posterior predictive check with the &lt;code&gt;pp_check()&lt;/code&gt; function, we’d need to do something like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_predict_cond_log_1 &amp;lt;- function(i, prep, ...) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  prob &amp;lt;- cond_log_1_pred(y, mu, mu_b, mu_c, n_cat)
  # make sure you have the extraDistr package
  extraDistr::rcat(length(mu), t(prob))
}

pp_check(fit22.3, 
         type = &amp;quot;bars&amp;quot;, 
         ndraws = 100, 
         size = 1/2, 
         fatten = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So far all of this has been with the conditional logistic model based on the first hierarchy of two-set divisions, which Kruschke used to simulate the &lt;code&gt;d3&lt;/code&gt; data. Now we’ll switch to consider the second hierarchy of two-set divisions, with which Kruschke simulated the &lt;code&gt;d4&lt;/code&gt; data. That second hierarchy, recall, resulted in the following definition for the conditional probabilities for the four levels of &lt;code&gt;Y&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\phi_1 &amp;amp; = \phi_{\{ 1 \} | \{ 1,2 \}} \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_2 &amp;amp; = \left ( 1 - \phi_{\{ 1 \} | \{ 1,2 \}} \right) \cdot \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \\
\phi_3 &amp;amp; = \phi_{\{ 3 \} | \{ 3,4 \}} \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right) \\
\phi_4 &amp;amp; = \left ( 1 - \phi_{\{ 3 \} | \{ 3,4 \}} \right) \cdot \left ( 1 - \phi_{\{ 1,2 \} | \{ 1,2,3,4 \}} \right).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This will require us to define a new custom family, which we’ll call &lt;code&gt;cond_log_2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cond_log_2 &amp;lt;- custom_family(
  name     = &amp;quot;cond_log_2&amp;quot;, 
  dpars    = c(&amp;quot;mu&amp;quot;, &amp;quot;mub&amp;quot;, &amp;quot;muc&amp;quot;), 
  links    = &amp;quot;identity&amp;quot;, 
  type     = &amp;quot;int&amp;quot;,
  vars     = c(&amp;quot;n_cat&amp;quot;),
  specials = &amp;quot;categorical&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we use the &lt;code&gt;stanvar()&lt;/code&gt; function to define our custom probability mass function and the corresponding function that will allow us to return predictions, which we’ll just save as &lt;code&gt;stan_lpmf_2&lt;/code&gt;. Other than the names, notice that the major change is how we have defined the &lt;code&gt;prob[i]&lt;/code&gt; parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_lpmf_2 &amp;lt;- stanvar(block = &amp;quot;functions&amp;quot;, 
                       scode = &amp;quot;
real cond_log_2_lpmf(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mub * p_mu;
  prob[2] = (1 - p_mub) * p_mu;
  prob[3] = p_muc * (1 - p_mu);
  prob[4] = (1 - p_muc) * (1 - p_mu);
  return(categorical_lpmf(y | prob));
}

vector cond_log_2_pred(int y, real mu, real mu_b, real mu_c, int n_cat) {
  real p_mu  = inv_logit(mu);
  real p_mub = inv_logit(mu_b);
  real p_muc = inv_logit(mu_c);
  vector[n_cat] prob;
  prob[1] = p_mub * p_mu;
  prob[2] = (1 - p_mub) * p_mu;
  prob[3] = p_muc * (1 - p_mu);
  prob[4] = (1 - p_muc) * (1 - p_mu);
  return(prob);
}
&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to fit the model with &lt;code&gt;brm()&lt;/code&gt;. Again, notice how our use of the &lt;code&gt;family&lt;/code&gt; and &lt;code&gt;stanvars&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.4 &amp;lt;-
  brm(data = d4, 
      family = cond_log_2,
      Y ~ 1 + X1 + X2,
      prior = c(prior(normal(0, 20), class = Intercept, dpar = mu2),
                prior(normal(0, 20), class = Intercept, dpar = mu3),
                prior(normal(0, 20), class = Intercept, dpar = mu4),
                prior(normal(0, 20), class = b, dpar = mu2),
                prior(normal(0, 20), class = b, dpar = mu3),
                prior(normal(0, 20), class = b, dpar = mu4)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      stanvars = stan_lpmf_2 + stanvars,
      file = &amp;quot;fits/fit22.04&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: cond_log_2 
##   Links: mu2 = identity; mu3 = identity; mu4 = identity 
## Formula: Y ~ 1 + X1 + X2 
##    Data: d4 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## mu2_Intercept    -4.05      0.46    -5.02    -3.23 1.00     2757     2184
## mu3_Intercept    -1.39      1.19    -3.80     0.87 1.00     2632     2185
## mu4_Intercept    -1.02      0.23    -1.50    -0.60 1.00     2824     2668
## mu2_X1           -4.79      0.52    -5.90    -3.84 1.00     2769     2286
## mu2_X2            0.35      0.20    -0.04     0.74 1.00     4245     2441
## mu3_X1            1.54      0.88    -0.13     3.28 1.00     2668     2139
## mu3_X2           -5.36      1.16    -7.86    -3.39 1.00     3306     2241
## mu4_X1            3.03      0.38     2.34     3.81 1.00     1872     2428
## mu4_X2            3.13      0.36     2.49     3.86 1.00     2219     2511
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the same basic workflow as before to make our version of the upper half of Figure 22.7.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws
post &amp;lt;- posterior_samples(fit22.4)

# 2D thresholds on the left
set.seed(22)

p1 &amp;lt;-
  post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_mu&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;lambda&amp;quot;, &amp;quot;parameter&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  mutate(intercept = -Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d4,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)

# marginal posteriors on the right
p2 &amp;lt;-
  post %&amp;gt;% 
  pivot_longer(-lp__) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;)) %&amp;gt;% 
  mutate(number = str_extract(name, &amp;quot;[2-4]+&amp;quot;)) %&amp;gt;% 
  mutate(lambda    = case_when(number == &amp;quot;2&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1,2}|{1,2,3,4}&amp;#39;]&amp;quot;,
                               number == &amp;quot;3&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1}|{1,2}&amp;#39;]&amp;quot;,
                               number == &amp;quot;4&amp;quot; ~ &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;),
         parameter = case_when(str_detect(name, &amp;quot;Intercept&amp;quot;) ~ &amp;quot;beta[0]&amp;quot;,
                               str_detect(name, &amp;quot;X1&amp;quot;)        ~ &amp;quot;beta[1]&amp;quot;,
                               str_detect(name, &amp;quot;X2&amp;quot;)        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95, size = 1,
                    normalize = &amp;quot;panels&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)

# combine, entitle, and display the results
(p1 + p2) &amp;amp; 
  plot_layout(widths = c(1, 2)) &amp;amp;
  plot_annotation(title = &amp;quot;Figure 22.7, upper half&amp;quot;,
                  subtitle = &amp;quot;Results from the conditional logistic model fit to the d4 data via the custom-family approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As Kruschke pointed out in the text,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;notice that the estimates for &lt;span class=&#34;math inline&#34;&gt;\(\lambda_2\)&lt;/span&gt; are more uncertain, with wider HDI’s, than the other coefficients. This uncertainty is also shown in the threshold lines on the data: The lines separating the 1’s from the 2’s have a much wider spread than the other boundaries. Inspection of the scatter plot explains why: There is only a small zone of data that informs the separation of 1’s from 2’s, and therefore the estimate must be relatively ambiguous. (p. 665)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’m not going to go through a full demonstration like before, but if you want to use more &lt;strong&gt;brms&lt;/strong&gt; post processing functions for &lt;code&gt;fit22.4&lt;/code&gt; or any other model fit with our custom &lt;code&gt;cond_log_2&lt;/code&gt; function, you’d need to execute this block of code first. Then post process to your heart’s desire.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expose_functions(fit22.4, vectorize = TRUE)

# for information criteria
log_lik_cond_log_2 &amp;lt;- function(i, prep) {
  mu  &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mub &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  muc &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  cond_log_2_lpmf(y, mu, mub, muc, n_cat)
}

# for conditional expectations
posterior_epred_cond_log_2 &amp;lt;- function(prep) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y
  prob &amp;lt;- cond_log_2_pred(y = y, mu = mu, mu_b = mu_b, mu_c = mu_c, n_cat = n_cat)
  dim(prob) &amp;lt;- c(dim(prob)[1], dim(mu))
  prob &amp;lt;- aperm(prob, c(2,3,1))
  dimnames(prob) &amp;lt;- list(
    as.character(seq_len(dim(prob)[1])), 
    NULL, 
    as.character(seq_len(dim(prob)[3]))
  )
  prob
}

# for posterior predictions
posterior_predict_cond_log_2 &amp;lt;- function(i, prep, ...) {
  mu   &amp;lt;- brms::get_dpar(prep, &amp;quot;mu2&amp;quot;, i = i)
  mu_b &amp;lt;- brms::get_dpar(prep, &amp;quot;mu3&amp;quot;, i = i)
  mu_c &amp;lt;- brms::get_dpar(prep, &amp;quot;mu4&amp;quot;, i = i)
  n_cat &amp;lt;- prep$data$n_cat
  y &amp;lt;- prep$data$Y[i]
  prob &amp;lt;- cond_log_2_pred(y, mu, mu_b, mu_c, n_cat)
  # make sure you have the extraDistr package
  extraDistr::rcat(length(mu), t(prob))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this section of the text, Kruschke also showed the results of when he analyzed the two data sets with the non-data-generating likelihoods. In the lower half of Figure 22.6, he showed the results of his second version of the conditional logistic model applied to the &lt;code&gt;d3&lt;/code&gt; data. In the lower half of Figure 22.7, he showed the results of his first version of the conditional logistic model applied to the &lt;code&gt;d4&lt;/code&gt; data. Since this section is already complicated enough, we’re not going to do that. But if you’d like to see what happens, consider it a personal homework assignment.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In principle, the different conditional logistic models could be put into an overarching hierarchical model comparison. If you have only a few specific candidate models to compare, this could be a feasible approach. But it is not an easily pursued approach to selecting a partition of outcomes from all possible partitions of outcomes when there are many outcomes… Therefore, it is typical to consider a single model, or small set of models, that are motivated by being meaningful in the context of the application, and interpreting the parameter estimates in that meaningful context. (p. 667)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kruschke finished this section with:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Finally, when you run the models in JAGS, you may find that there is high autocorrelation in the MCMC chains (even with standardized data), which requires a very long chain for adequate ESS. This suggests that Stan might be a more efficient approach.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since we fit our models with Stan via &lt;strong&gt;brms&lt;/strong&gt;, high autocorrelations and low effective sample sizes weren’t a problem. For example, here are the bulk and tail effective sample sizes for both of our two models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(posterior)

bind_rows(
  posterior_samples(fit22.3) %&amp;gt;% summarise_draws(),
  posterior_samples(fit22.4) %&amp;gt;% summarise_draws()
  ) %&amp;gt;% 
  mutate(fit = rep(c(&amp;quot;fit22.3&amp;quot;, &amp;quot;fit22.4&amp;quot;), each = n() / 2)) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;ess&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 250) +
  xlim(0, NA) +
  facet_grid(fit ~ name)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The values look pretty good. We may as well look at the autocorrelations. To keep things simple, this time we’ll restrict our analysis to &lt;code&gt;fit22.4&lt;/code&gt;. [The results are largely the same for &lt;code&gt;fit22.3&lt;/code&gt;.]&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

ac &amp;lt;-
  posterior_samples(fit22.4, add_chain = T) %&amp;gt;% 
  select(b_mu2_Intercept:b_mu4_X2, chain) %&amp;gt;% 
  mcmc_acf(lags = 5)

ac$data %&amp;gt;% 
  filter(Lag &amp;gt; 0) %&amp;gt;% 
  
  ggplot(aes(x = AC)) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_histogram(binwidth = 0.05) +
  scale_x_continuous(&amp;quot;autocorrelation&amp;quot;, limits = c(-1, 1),
                     labels = c(&amp;quot;-1&amp;quot;, &amp;quot;-.5&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;.5&amp;quot;, &amp;quot;1&amp;quot;)) +
  facet_grid(Lag ~ Chain, labeller = label_both)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the whole, the autocorrelations are reasonably low across all parameters, chains, and lags.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-models-by-sequential-ordinal-regression.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Conditional logistic models by sequential ordinal regression.&lt;/h5&gt;
&lt;p&gt;In their &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerOrdinalRegressionModels2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; paper, &lt;a href=&#34;https://psyarxiv.com/x8swp/&#34;&gt;&lt;em&gt;Ordinal regression models in psychology: A tutorial&lt;/em&gt;&lt;/a&gt;, Bürkner and Vourre outlined a framework for fitting a variety of orginal models with &lt;strong&gt;brms&lt;/strong&gt;. We’ll learn more about ordinal models in [Chapter 23][Ordinal Predicted Variable]. In this section, we’ll use Mattan Ben-Shachar’s strategy and purpose one of the ordinal models to fit a conditional logistic model to our nominal data.&lt;/p&gt;
&lt;p&gt;As outlined in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-burknerOrdinalRegressionModels2019&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner &amp;amp; Vuorre&lt;/a&gt; (&lt;a href=&#34;#ref-burknerOrdinalRegressionModels2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, and as we will learn in greater detain in the next chapter, many ordinal regression models presume an underlying continuous process. However, you can use a sequential model in cases where one level of the criterion is only possible after the lower levels of the criterion have been achieved. Although this is not technically correct for the nominal variable &lt;code&gt;Y&lt;/code&gt; in the &lt;code&gt;d3&lt;/code&gt; data set, the simple hierarchical sequence Kruschke used to model those data does follow that same pattern. Ben-Shachar’s insight was that if we treat our nominal variable &lt;code&gt;Y&lt;/code&gt; as ordinal, the sequential model will mimic the sequential-ness of Kruschke’s binary-choices hierarchy. To get this to work, we first have to save an ordinal version of &lt;code&gt;Y&lt;/code&gt;, which we’ll call &lt;code&gt;Y_ord&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d3 &amp;lt;-
  d3 %&amp;gt;% 
  mutate(Y_ord = ordered(Y))

# what are the new attributes?
attributes(d3$Y_ord)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $levels
## [1] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot;
## 
## $class
## [1] &amp;quot;ordered&amp;quot; &amp;quot;factor&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within &lt;code&gt;brm()&lt;/code&gt; we fit sequential models using &lt;code&gt;family = sratio&lt;/code&gt;, which defaults to the logit link. If you want to use predictors in a model of this kind and you would like those coefficients to vary across the different levels of the criterion, you need to insert the predictor terms within the &lt;code&gt;cs()&lt;/code&gt; function. Here’s how to fit the model with &lt;code&gt;brm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.5 &amp;lt;-
  brm(data = d3, 
      family = sratio,
      Y_ord ~ 1 + cs(X1) + cs(X2),
      prior = c(prior(normal(0, 20), class = Intercept),
                prior(normal(0, 20), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      file = &amp;quot;fits/fit22.05&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: sratio 
##   Links: mu = logit; disc = identity 
## Formula: Y_ord ~ 1 + cs(X1) + cs(X2) 
##    Data: d3 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -4.01      0.47    -4.97    -3.14 1.00     2850     2689
## Intercept[2]    -2.12      0.35    -2.84    -1.48 1.00     2864     2875
## Intercept[3]    -0.97      0.32    -1.61    -0.37 1.00     3162     3039
## X1[1]            4.92      0.54     3.93     6.04 1.00     2820     2570
## X1[2]           -0.74      0.29    -1.30    -0.19 1.00     3511     3244
## X1[3]           -3.00      0.50    -4.02    -2.10 1.00     3307     2966
## X2[1]           -0.01      0.20    -0.38     0.38 1.00     4316     2519
## X2[2]            5.22      0.63     4.07     6.52 1.00     2837     2616
## X2[3]           -3.11      0.53    -4.23    -2.14 1.00     2902     2640
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing that might not be apparent at first glance is that although this model is essentially equivalent to the &lt;code&gt;family = cond_log_1&lt;/code&gt; version of the model we fit with &lt;code&gt;fit22.3&lt;/code&gt;, above, the parameters are a little different. The intercepts are largely the same. However, the coefficients for the &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; predictors have switched signs. This will be easier to see with a coefficient plot comparing &lt;code&gt;fit22.3&lt;/code&gt; and &lt;code&gt;fit22.5&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws for fit22.5
post &amp;lt;- posterior_samples(fit22.5)

# wrangle and combine the draws from the two models
bind_rows(
  # family = &amp;quot;cond_log_1&amp;quot;
  posterior_samples(fit22.3) %&amp;gt;% 
    pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
    mutate(name = str_remove(name, &amp;quot;b_mu&amp;quot;)) %&amp;gt;% 
    separate(name, into = c(&amp;quot;lambda&amp;quot;, &amp;quot;parameter&amp;quot;)) %&amp;gt;% 
    mutate(lambda = as.double(lambda) - 1,
           family = &amp;quot;cond_log_1&amp;quot;) %&amp;gt;% 
    select(family, lambda, parameter, value),
  # family = &amp;quot;sratio&amp;quot;
  post %&amp;gt;% 
    pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
    mutate(name = str_remove(name, &amp;quot;b_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
    separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;%
    mutate(lambda = as.double(lambda),
           family = &amp;quot;sratio&amp;quot;) %&amp;gt;% 
    select(family, lambda, parameter, value)
) %&amp;gt;% 
  mutate(lambda    = str_c(&amp;quot;lambda==&amp;quot;, lambda),
         parameter = case_when(str_detect(parameter, &amp;quot;Intercept&amp;quot;) ~ &amp;quot;beta[0]&amp;quot;,
                               str_detect(parameter, &amp;quot;X1&amp;quot;)        ~ &amp;quot;beta[1]&amp;quot;,
                               str_detect(parameter, &amp;quot;X2&amp;quot;)        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = value, y = family)) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  stat_pointinterval(.width = .95, point_size = 1.5, size = 1) +
  labs(x = &amp;quot;marginal posterior&amp;quot;,
       y = NULL) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even though the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; parameters switched signs, their magnitudes are about the same. Thus, if we want to use our &lt;code&gt;fit22.5&lt;/code&gt; to plot the thresholds as in Figure 22.6, we’ll have to update our threshold formula to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_2 = (\color{red}{+} \color{black}{\beta_0 / \beta_2) + (\beta_1 / \beta_2)x_1.}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With that adjustment in line, here’s our updated version of the left panel of Figure 22.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22)

post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  # this line is different
  mutate(intercept = Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  coord_equal() +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though we used a different likelihood and a different formula for the thresholds, we got same basic model results. They’re just parameterized in a slightly different way. The nice thing with the &lt;code&gt;family = sratio&lt;/code&gt; approach is all of the typical &lt;strong&gt;brms&lt;/strong&gt; post processing functions will work out of the box. For example, here’s the posterior predictive check via &lt;code&gt;pp_check()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp_check(fit22.5, 
         type = &amp;quot;bars&amp;quot;, 
         ndraws = 100, 
         size = 1/2, 
         fatten = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Also note how the information criteria estimates for the two approaches are essentially the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit22.5 &amp;lt;- add_criterion(fit22.5, criterion = &amp;quot;waic&amp;quot;)

loo_compare(fit22.3, fit22.5, criterion = &amp;quot;waic&amp;quot;) %&amp;gt;% 
  print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  
## fit22.3    0.0       0.0  -230.8      16.8          9.3    1.1     461.6
## fit22.5   -0.1       0.1  -230.9      16.8          9.4    1.1     461.8
##         se_waic
## fit22.3   33.6 
## fit22.5   33.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A limitation of the &lt;code&gt;family = sratio&lt;/code&gt; method for conditional logistic models is it requires a simple binary-divisions hierarchy that resembles the one we just used, the one in the left panel of Figure 22.2. It is not well suited for the more complicated hierarchy displayed in the right panel of Figure 22.2, nor will it help you make sense of data generated by that kind of mechanism. For example, consider what happens when we try to use &lt;code&gt;family = sratio&lt;/code&gt; with the &lt;code&gt;d4&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make an ordinal version of Y
d4 &amp;lt;-
  d4 %&amp;gt;% 
  mutate(Y_ord = ordered(Y))

# fit the model
fit22.6 &amp;lt;-
  brm(data = d4, 
      family = sratio,
      Y_ord ~ 1 + cs(X1) + cs(X2),
      prior = c(prior(normal(0, 20), class = Intercept),
                prior(normal(0, 20), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 22,
      file = &amp;quot;fits/fit22.06&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit22.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: sratio 
##   Links: mu = logit; disc = identity 
## Formula: Y_ord ~ 1 + cs(X1) + cs(X2) 
##    Data: d4 (Number of observations: 475) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept[1]    -6.03      0.76    -7.66    -4.67 1.00     2040     2169
## Intercept[2]    -5.40      0.68    -6.85    -4.18 1.00     1757     2104
## Intercept[3]    -1.02      0.24    -1.50    -0.59 1.00     2666     2522
## X1[1]            2.71      0.47     1.85     3.70 1.00     2251     2266
## X1[2]            5.56      0.70     4.30     7.04 1.00     1900     2424
## X1[3]           -3.03      0.39    -3.84    -2.32 1.00     2565     2447
## X2[1]            2.38      0.43     1.60     3.26 1.00     2525     2449
## X2[2]           -1.16      0.29    -1.75    -0.63 1.00     3524     2736
## X2[3]           -3.13      0.37    -3.89    -2.46 1.00     2718     2738
## 
## Family Specific Parameters: 
##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## disc     1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look at the parameter summary, nothing obviously bad happened. The computer didn’t crash or anything. To get a better sense of the damage, we plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the posterior draws for fit22.6
post &amp;lt;- posterior_samples(fit22.6)

# 2D thresholds on the left
set.seed(22)

p1 &amp;lt;- 
  post %&amp;gt;% 
  mutate(draw = 1:n()) %&amp;gt;% 
  slice_sample(n = 30) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;b&amp;quot;)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;) %&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  # still using the adjusted formula for the thresholds
  mutate(intercept = Intercept / X2,
         slope     = -X1 / X2) %&amp;gt;% 
  
  ggplot() +
  geom_text(data = d3,
            aes(x = X1, y = X2, label = Y, color = factor(Y)),
            size = 3, show.legend = F) +
  geom_abline(aes(intercept = intercept,
                  slope = slope,
                  group = interaction(draw, lambda),
                  linetype = lambda),
              size = 1/4, alpha = 1/2) +
  scale_color_viridis_d(option = &amp;quot;F&amp;quot;, begin = .15, end = .85) +
  scale_linetype(NULL,
                 labels = parse(text = c(
                   &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;, 
                   &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;)),
                 guide = guide_legend(
                   direction = &amp;quot;vertical&amp;quot;,
                   label.hjust = 0.5,
                   label.theme = element_text(size = 10))) +
  labs(x = expression(x[1]),
       y = expression(x[2])) +
  theme(legend.justification = 0.5,
        legend.position = &amp;quot;top&amp;quot;)

# marginal posteriors on the right
p2 &amp;lt;-
post %&amp;gt;% 
  pivot_longer(cols = c(-disc, -lp__)) %&amp;gt;% 
  mutate(name = str_remove(name, &amp;quot;b_&amp;quot;)%&amp;gt;% str_remove(., &amp;quot;bcs_&amp;quot;)) %&amp;gt;% 
  separate(name, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;lambda&amp;quot;)) %&amp;gt;% 
  mutate(lambda    = case_when(lambda == &amp;quot;1&amp;quot; ~ &amp;quot;lambda[&amp;#39;{1}|{1,2,3,4}&amp;#39;]&amp;quot;,
                               lambda == &amp;quot;2&amp;quot; ~ &amp;quot;lambda[&amp;#39;{2}|{2,3,4}&amp;#39;]&amp;quot;,
                               lambda == &amp;quot;3&amp;quot; ~ &amp;quot;lambda[&amp;#39;{3}|{3,4}&amp;#39;]&amp;quot;),
         parameter = case_when(parameter == &amp;quot;Intercept&amp;quot; ~ &amp;quot;beta[0]&amp;quot;,
                               parameter == &amp;quot;X1&amp;quot;        ~ &amp;quot;beta[1]&amp;quot;,
                               parameter == &amp;quot;X2&amp;quot;        ~ &amp;quot;beta[2]&amp;quot;)) %&amp;gt;% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95, size = 1,
                    normalize = &amp;quot;panels&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;marginal posterior&amp;quot;) +
  facet_grid(lambda ~ parameter, labeller = label_parsed, scales = &amp;quot;free_x&amp;quot;)

# combine, entitle, and display the results
(p1 + p2) &amp;amp; 
  plot_layout(widths = c(1, 2)) &amp;amp;
  plot_annotation(title = &amp;quot;Figure 22.7, lower half&amp;quot;,
                  subtitle = &amp;quot;Results from the conditional logistic model fit to the d4 data via the sequential-ordinal approach&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-11-17-conditional-logistic-models-with-brms-rough-draft/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We ended up with our version of the lower half of Figure 22.7. As with the previous model, the sequential-ordinal approach reverses the signs for the &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; parameters, which isn’t a big deal as long as you keep that in mind. The larger issue is that the thresholds displayed in the left panel do a poor job differentiating among the various &lt;code&gt;Y&lt;/code&gt; categories. The model underlying those thresholds is a bad match for the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-logistic-wrap-up.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Conditional logistic wrap-up.&lt;/h5&gt;
&lt;p&gt;To wrap this section up, we walked through approaches for fitting conditional logistic models with &lt;strong&gt;brms&lt;/strong&gt;. First we considered Singmann’s method for using the &lt;strong&gt;brms&lt;/strong&gt; custom family functionality to define bespoke likelihood functions. Though it requires a lot of custom coding and an above-average knowledge of the inner workings of &lt;strong&gt;brms&lt;/strong&gt; and Stan, the custom family approach is very general and will possibly work for all your conditional-logistic needs. Then we considered Ben-Shachar sequential-ordinal approach. Ben-Shachar’s insight was that if we are willing to augment the nominal data with the &lt;code&gt;ordered()&lt;/code&gt; function, modeling them with a sequential-ordinal model via &lt;code&gt;family = sratio&lt;/code&gt; will return near equivalent results to the conditional logistic method. Though this method is attractive in that it uses a built-in likelihood and thus avoids a lot of custom coding, it is limited in that it will only handle nominal data which are well described by the simple binary-divisions hierarchy displayed in the left panel of Figure 22.2.&lt;/p&gt;
&lt;p&gt;In closing, I would like to thank Singmann and Ben-Shachar for their time and insights. 🍻 I could not have finished this section without them. If you would like more examples of both of their methods applied to different data sets, check out the Stan forum thread called &lt;a href=&#34;https://discourse.mc-stan.org/t/nominal-data-and-kruschkes-conditional-logistic-approach/21433&#34;&gt;Nominal data and Kruschke’s “conditional logistic” approach&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.1 (2021-08-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bayesplot_1.8.1      posterior_1.1.0.9000 patchwork_1.1.1     
##  [4] tidybayes_3.0.1      brms_2.16.2          Rcpp_1.0.7          
##  [7] forcats_0.5.1        stringr_1.4.0        dplyr_1.0.7         
## [10] purrr_0.3.4          readr_2.0.1          tidyr_1.1.3         
## [13] tibble_3.1.6         ggplot2_3.3.5        tidyverse_1.3.1     
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.3.0      RcppEigen_0.3.3.9.1 
##   [4] plyr_1.8.6           igraph_1.2.6         svUnit_1.0.6        
##   [7] splines_4.1.1        crosstalk_1.1.1      TH.data_1.0-10      
##  [10] rstantools_2.1.1     inline_0.3.19        digest_0.6.28       
##  [13] htmltools_0.5.2      rsconnect_0.8.24     fansi_0.5.0         
##  [16] BH_1.75.0-0          magrittr_2.0.1       checkmate_2.0.0     
##  [19] tzdb_0.1.2           modelr_0.1.8         RcppParallel_5.1.4  
##  [22] matrixStats_0.61.0   vroom_1.5.4          xts_0.12.1          
##  [25] sandwich_3.0-1       prettyunits_1.1.1    colorspace_2.0-2    
##  [28] rvest_1.0.1          ggdist_3.0.0         haven_2.4.3         
##  [31] xfun_0.25            callr_3.7.0          crayon_1.4.2        
##  [34] jsonlite_1.7.2       lme4_1.1-27.1        survival_3.2-11     
##  [37] zoo_1.8-9            glue_1.5.0           gtable_0.3.0        
##  [40] emmeans_1.6.3        V8_3.4.2             distributional_0.2.2
##  [43] pkgbuild_1.2.0       rstan_2.26.3         abind_1.4-5         
##  [46] scales_1.1.1         mvtnorm_1.1-2        emo_0.0.0.9000      
##  [49] DBI_1.1.1            miniUI_0.1.1.1       viridisLite_0.4.0   
##  [52] xtable_1.8-4         HDInterval_0.2.2     diffobj_0.3.4       
##  [55] bit_4.0.4            stats4_4.1.1         StanHeaders_2.26.3  
##  [58] DT_0.19              htmlwidgets_1.5.3    httr_1.4.2          
##  [61] threejs_0.3.3        arrayhelpers_1.1-0   ellipsis_0.3.2      
##  [64] pkgconfig_2.0.3      loo_2.4.1            farver_2.1.0        
##  [67] sass_0.4.0           dbplyr_2.1.1         utf8_1.2.2          
##  [70] labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.12        
##  [73] reshape2_1.4.4       later_1.3.0          munsell_0.5.0       
##  [76] cellranger_1.1.0     tools_4.1.1          cli_3.1.0           
##  [79] generics_0.1.1       broom_0.7.9          ggridges_0.5.3      
##  [82] evaluate_0.14        fastmap_1.1.0        yaml_2.2.1          
##  [85] bit64_4.0.5          processx_3.5.2       knitr_1.33          
##  [88] fs_1.5.0             nlme_3.1-152         mime_0.11           
##  [91] projpred_2.0.2       xml2_1.3.2           compiler_4.1.1      
##  [94] shinythemes_1.2.0    rstudioapi_0.13      gamm4_0.2-6         
##  [97] curl_4.3.2           reprex_2.0.1         bslib_0.3.0         
## [100] stringi_1.7.4        highr_0.9            ps_1.6.0            
## [103] blogdown_1.5         Brobdingnag_1.2-6    lattice_0.20-44     
## [106] Matrix_1.3-4         nloptr_1.2.2.2       markdown_1.1        
## [109] shinyjs_2.0.0        tensorA_0.36.2       vctrs_0.3.8         
## [112] pillar_1.6.4         lifecycle_1.0.1      jquerylib_0.1.4     
## [115] bridgesampling_1.1-2 estimability_1.3     httpuv_1.6.2        
## [118] extraDistr_1.9.1     R6_2.5.1             bookdown_0.23       
## [121] promises_1.2.0.1     gridExtra_2.3        codetools_0.2-18    
## [124] boot_1.3-28          colourpicker_1.1.0   MASS_7.3-54         
## [127] gtools_3.9.2         assertthat_0.2.1     withr_2.4.2         
## [130] shinystan_2.5.0      multcomp_1.4-17      mgcv_1.8-36         
## [133] parallel_4.1.1       hms_1.1.0            grid_4.1.1          
## [136] coda_0.19-4          minqa_1.2.4          rmarkdown_2.10      
## [139] shiny_1.6.0          lubridate_1.7.10     base64enc_0.1-3     
## [142] dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-Bürkner2021Define&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Define custom response distributions with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerOrdinalRegressionModels2019&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C., &amp;amp; Vuorre, M. (2019). Ordinal regression models in psychology: &lt;span&gt;A&lt;/span&gt; tutorial. &lt;em&gt;Advances in Methods and Practices in Psychological Science&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;(1), 77–101. &lt;a href=&#34;https://doi.org/10.1177/2515245918823199&#34;&gt;https://doi.org/10.1177/2515245918823199&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>If you fit a model with multiply imputed data, you can still plot the line.</title>
      <link>/post/2021-10-21-if-you-fit-a-model-with-multiply-imputed-data-you-can-still-plot-the-line/</link>
      <pubDate>Thu, 21 Oct 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-10-21-if-you-fit-a-model-with-multiply-imputed-data-you-can-still-plot-the-line/</guid>
      <description>
&lt;script src=&#34;/post/2021-10-21-if-you-fit-a-model-with-multiply-imputed-data-you-can-still-plot-the-line/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;what&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What?&lt;/h2&gt;
&lt;p&gt;If you’re in the know, you know there are three major ways to handle missing data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;full-information maximum likelihood,&lt;/li&gt;
&lt;li&gt;multiple imputation, and&lt;/li&gt;
&lt;li&gt;one-step full-luxury&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Bayesian imputation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you’re a frequentist, you only have the first two options. If you’re an &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt; user and like multiple imputation, you probably know all about the &lt;strong&gt;mice&lt;/strong&gt; package &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mice2011&#34; role=&#34;doc-biblioref&#34;&gt;van Buuren &amp;amp; Groothuis-Oudshoorn, 2011&lt;/a&gt;, &lt;a href=&#34;#ref-R-mice&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;, which generally works great. The bummer is there are no built-in ways to plot the fitted lines from models fit from multiply-imputed data using van Buuren’s &lt;strong&gt;mice&lt;/strong&gt;-oriented workflow (see &lt;a href=&#34;https://github.com/amices/mice/issues/82&#34;&gt;GitHub issue #82&lt;/a&gt;). However, there is a way to plot your fitted lines by hand and in this blog post I’ll show you how.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;For this post, I’m presuming some background knowledge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should be familiar with regression. For frequentist introductions, I recommend Roback and Legler’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-roback2021beyond&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; online text or James, Witten, Hastie, and Tibshirani’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-james2021AnIntroduction&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; online text. For Bayesian introductions, I recommend either edition of McElreath’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text; or Gelman, Hill, and Vehtari’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; text.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You should be familiar with contemporary missing data theory. You can find brief overviews in the texts by McElreath and Gelman et al, above. For a deeper dive, I recommend &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-enders2010applied&#34; role=&#34;doc-biblioref&#34;&gt;Enders&lt;/a&gt; (&lt;a href=&#34;#ref-enders2010applied&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-little2019statistical&#34; role=&#34;doc-biblioref&#34;&gt;Little &amp;amp; Rubin&lt;/a&gt; (&lt;a href=&#34;#ref-little2019statistical&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, or &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-vanbuurenFlexibleImputationMissing2018&#34; role=&#34;doc-biblioref&#34;&gt;van Buuren&lt;/a&gt; (&lt;a href=&#34;#ref-vanbuurenFlexibleImputationMissing2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. Also, heads up: &lt;a href=&#34;https://twitter.com/AmandaKMontoya/status/1341936335301406722&#34;&gt;word on the street&lt;/a&gt; is Enders is working on the second edition of his book.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt;. Data wrangling and plotting were done with help from the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;GGally&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-GGally&#34; role=&#34;doc-biblioref&#34;&gt;Schloerke et al., 2021&lt;/a&gt;)&lt;/span&gt;. The data and multiple-imputation workflow are from the &lt;a href=&#34;https://CRAN.R-project.org/package=mice&#34;&gt;&lt;strong&gt;mice&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we load our primary &lt;strong&gt;R&lt;/strong&gt; packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(GGally)
library(mice)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;In this post we’ll focus on a subset of the &lt;code&gt;brandsma&lt;/code&gt; data set &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brandsma1989effects&#34; role=&#34;doc-biblioref&#34;&gt;Brandsma &amp;amp; Knuver, 1989&lt;/a&gt;)&lt;/span&gt;. The goal, here, is to take a small enough subset that there will be noticeable differences across the imputed data sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(201)

b_small &amp;lt;-
  brandsma %&amp;gt;% 
  filter(!complete.cases(.)) %&amp;gt;% 
  slice_sample(n = 50) %&amp;gt;% 
  select(ses, iqv, iqp)

glimpse(b_small)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 50
## Columns: 3
## $ ses &amp;lt;dbl&amp;gt; -12.6666667, NA, -4.6666667, 19.3333333, NA, NA, 0.3333333, -4.666…
## $ iqv &amp;lt;dbl&amp;gt; NA, -0.8535094, -0.3535094, 1.1464906, 1.1464906, -0.3535094, -0.3…
## $ iqp &amp;lt;dbl&amp;gt; -1.72274979, -4.05608313, 2.61058354, 3.94391687, 1.61058354, -1.3…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are our three variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpairs(b_small)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-10-21-if-you-fit-a-model-with-multiply-imputed-data-you-can-still-plot-the-line/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’ll be focusing on the relation between socioeconomic status (&lt;code&gt;ses&lt;/code&gt;) and verbal IQ (&lt;code&gt;iqv&lt;/code&gt;) and performance IQ (&lt;code&gt;iqp&lt;/code&gt;) will be a missing data covariate.&lt;/p&gt;
&lt;p&gt;Here’s what the missing data patterns look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b_small %&amp;gt;% 
  mutate_all(.funs = ~ ifelse(is.na(.), 0, 1)) %&amp;gt;% 
  count(ses, iqv, iqp, sort = TRUE) %&amp;gt;% 
  mutate(percent = 100 * n / sum(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   ses iqv iqp  n percent
## 1   1   1   1 36      72
## 2   0   1   1 11      22
## 3   1   0   1  2       4
## 4   1   0   0  1       2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;1&lt;/code&gt; means the value was observed and &lt;code&gt;0&lt;/code&gt; means the value was missing. Twenty-eight percent of the cases have missingness on one of the two focal variables. The bulk of the missingness is in &lt;code&gt;ses&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;impute&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Impute&lt;/h2&gt;
&lt;p&gt;We’ll use the &lt;code&gt;mice()&lt;/code&gt; function to impute. By setting &lt;code&gt;m = 10&lt;/code&gt;, we’ll get back 10 multiply-imputed data sets. By setting &lt;code&gt;method = &#34;norm&#34;&lt;/code&gt;, we will be using Bayesian linear regression with the Gaussian likelihood to compute the imputed values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp &amp;lt;- mice(b_small, seed = 540, m = 10, method = &amp;quot;norm&amp;quot;, print = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;p&gt;Our statistical model will be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{iqv}_i &amp;amp; \sim \mathcal N(\mu_i, \sigma) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 \text{ses}_i.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With the &lt;code&gt;mice::with()&lt;/code&gt; function, we fit that model once to each of the 10 imputed data sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- with(imp, lm(iqv ~ 1 + ses))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There’s a lot of information packed into our &lt;code&gt;fit&lt;/code&gt; object. Within the &lt;code&gt;analyses&lt;/code&gt; section we can find the results of all 10 models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit$analyses %&amp;gt;% str(max.level = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 10
##  $ :List of 12
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;lm&amp;quot;
##  $ :List of 12
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;lm&amp;quot;
##  $ :List of 12
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;lm&amp;quot;
##  $ :List of 12
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;lm&amp;quot;
##  $ :List of 12
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;lm&amp;quot;
##  $ :List of 12
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;lm&amp;quot;
##  $ :List of 12
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;lm&amp;quot;
##  $ :List of 12
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;lm&amp;quot;
##  $ :List of 12
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;lm&amp;quot;
##  $ :List of 12
##   ..- attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;lm&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This insight will come in handy in just a bit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-want-lines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We want lines!&lt;/h2&gt;
&lt;div id=&#34;start-naïve.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Start naïve.&lt;/h3&gt;
&lt;p&gt;If you wanted to plot the fitted line for a simple linear model, you’d probably use the &lt;code&gt;fitted()&lt;/code&gt; or &lt;code&gt;predict()&lt;/code&gt; function. But when you have fit that model to your multiply-imputed data sets, that just won’t work. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you try executing that line, you’ll get a nasty error message reading:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Error in UseMethod(“predict”) : no applicable method for ‘predict’ applied to an object of class “c(‘mira,’ ‘matrix’)”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our &lt;code&gt;fit&lt;/code&gt; object is not a regular fit object. It’s an object of class &lt;code&gt;&#34;mira&#34;&lt;/code&gt; and &lt;code&gt;&#34;matrix&#34;&lt;/code&gt;, which means it’s fancy and temperamental.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;mira&amp;quot;   &amp;quot;matrix&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the time of this writing, the &lt;strong&gt;mice&lt;/strong&gt; package does not have a built-in solution to this problem. If you’re willing to put in a little work, you can do the job yourself.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;off-label.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Off label.&lt;/h3&gt;
&lt;p&gt;Remember how we showed how our &lt;code&gt;fit$analyses&lt;/code&gt; is a list of all 10 of our individual model fits? Turns out we can leverage that. For example, here’s the model summary for the model fit to the seventh imputed data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit$analyses[[7]] %&amp;gt;% 
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = iqv ~ 1 + ses)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.1947 -1.0600  0.1209  0.9678  5.5680 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.26981    0.29403   0.918 0.363405    
## ses          0.11479    0.02732   4.201 0.000115 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.024 on 48 degrees of freedom
## Multiple R-squared:  0.2689, Adjusted R-squared:  0.2536 
## F-statistic: 17.65 on 1 and 48 DF,  p-value: 0.0001146&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All we needed to do was use the double-bracket indexing. If you’re not up on how to do that, Hadley Wickham has a &lt;a href=&#34;https://twitter.com/hadleywickham/status/643381054758363136&#34;&gt;famous tweet&lt;/a&gt; on the subject and Jenny Bryan has a &lt;a href=&#34;https://www.youtube.com/watch?v=4MfUCX_KpdE&amp;amp;t=615s&amp;amp;frags=pl%2Cwn&#34;&gt;great talk&lt;/a&gt; discussing the role of lists within data wrangling. With the double-bracket indexing trick, you can use &lt;code&gt;fitted()&lt;/code&gt; or &lt;code&gt;predict()&lt;/code&gt; one model iteration at a time. E.g.,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit$analyses[[1]] %&amp;gt;% 
  fitted() %&amp;gt;% 
  str()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Named num [1:50] -1.341 0.135 -0.512 1.975 -0.794 ...
##  - attr(*, &amp;quot;names&amp;quot;)= chr [1:50] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Building, here’s what that can look like if we use &lt;code&gt;predict()&lt;/code&gt; for all 10 of our models, bind the individual results, and plot them all at once.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the sequence of predictor values
ses_min &amp;lt;- min(b_small$ses, na.rm = T)
ses_max &amp;lt;- max(b_small$ses, na.rm = T)

ses_length &amp;lt;- 30

nd &amp;lt;- tibble(ses = seq(from = ses_min, to = ses_max, length.out = ses_length))

# use `predict()` for each separate model
rbind(
  predict(fit$analyses[[1]], newdata = nd, interval = &amp;quot;confidence&amp;quot;),
  predict(fit$analyses[[2]], newdata = nd, interval = &amp;quot;confidence&amp;quot;),
  predict(fit$analyses[[3]], newdata = nd, interval = &amp;quot;confidence&amp;quot;),
  predict(fit$analyses[[4]], newdata = nd, interval = &amp;quot;confidence&amp;quot;),
  predict(fit$analyses[[5]], newdata = nd, interval = &amp;quot;confidence&amp;quot;),
  predict(fit$analyses[[6]], newdata = nd, interval = &amp;quot;confidence&amp;quot;),
  predict(fit$analyses[[7]], newdata = nd, interval = &amp;quot;confidence&amp;quot;),
  predict(fit$analyses[[8]], newdata = nd, interval = &amp;quot;confidence&amp;quot;),
  predict(fit$analyses[[9]], newdata = nd, interval = &amp;quot;confidence&amp;quot;),
  predict(fit$analyses[[10]], newdata = nd, interval = &amp;quot;confidence&amp;quot;)
) %&amp;gt;%
  # wrangle a bit
  data.frame() %&amp;gt;% 
  bind_cols(
    bind_rows(replicate(10, nd, simplify = FALSE))
    ) %&amp;gt;% 
  mutate(.imp = rep(1:10, each = ses_length)) %&amp;gt;% 
  
  # plot!
  ggplot(aes(x = ses)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr, group = .imp),
              alpha = 1/10) +
  geom_line(aes(y = fit, group = .imp), 
            size = 1/4) +
  # add the observed data for good measure
  geom_point(data = b_small,
             aes(y = iqv)) +
  ylab(&amp;quot;iqv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-10-21-if-you-fit-a-model-with-multiply-imputed-data-you-can-still-plot-the-line/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I kinda like this visualization approach. It has a certain Bayesian flair and it does an okay job displaying the stochasticity built in to the multiple imputation framework. However, this approach is totally off label and will probably get shot down by any self-respecting Reviewer #2.&lt;/p&gt;
&lt;p&gt;Fortunately for us, we have a principled and peer-reviewed solution, instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;level-up-with-miles.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Level up with Miles.&lt;/h3&gt;
&lt;p&gt;In his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-miles2016obtaining&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; paper, &lt;em&gt;Obtaining predictions from models fit to multiply imputed data&lt;/em&gt;, &lt;a href=&#34;https://www.andrewamiles.com/&#34;&gt;Andrew Miles&lt;/a&gt; presented two methods for, well, doing what his title said he’d do. Miles called these two methods &lt;em&gt;Predict Then Combine&lt;/em&gt; (PC) and &lt;em&gt;Combine Then Predict&lt;/em&gt; (CP). The CP approach invokes first derivatives in a way I’m not prepared to implement on my own. Fortunately for us, the PC approach just requires a little iteration, a few lines within a grouped &lt;code&gt;summarise()&lt;/code&gt;, and a tiny bit of wrangling. In my world, that’s cake. 🍰&lt;/p&gt;
&lt;div id=&#34;first-iteration.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;First: iteration.&lt;/h4&gt;
&lt;p&gt;For our first step, we’ll use &lt;code&gt;predict()&lt;/code&gt; again for each of our individual versions of the model. This time, however, we’ll use thriftier code and iterate with help from &lt;code&gt;purrr::map()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted_lines &amp;lt;-
  tibble(.imp = 1:10) %&amp;gt;% 
  mutate(p = map(.imp, ~ predict(fit$analyses[[.]], 
                                 newdata = nd, 
                                 se.fit = TRUE) %&amp;gt;% 
                   data.frame())
         )

# what have we done?
fitted_lines&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 2
##     .imp p            
##    &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;       
##  1     1 &amp;lt;df [30 × 4]&amp;gt;
##  2     2 &amp;lt;df [30 × 4]&amp;gt;
##  3     3 &amp;lt;df [30 × 4]&amp;gt;
##  4     4 &amp;lt;df [30 × 4]&amp;gt;
##  5     5 &amp;lt;df [30 × 4]&amp;gt;
##  6     6 &amp;lt;df [30 × 4]&amp;gt;
##  7     7 &amp;lt;df [30 × 4]&amp;gt;
##  8     8 &amp;lt;df [30 × 4]&amp;gt;
##  9     9 &amp;lt;df [30 × 4]&amp;gt;
## 10    10 &amp;lt;df [30 × 4]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a nested tibble where the results of all 10 &lt;code&gt;predict()&lt;/code&gt; operations are waiting for us in the &lt;code&gt;p&lt;/code&gt; column and each is conveniently indexed by &lt;code&gt;.imp&lt;/code&gt;. Note also how we did not request confidence intervals in the output, but we did set &lt;code&gt;se.fit = TRUE&lt;/code&gt;. We’ll be all about those standard errors in just a bit.&lt;/p&gt;
&lt;p&gt;Here’s how we unnest the results and then augment a little.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted_lines &amp;lt;- fitted_lines %&amp;gt;% 
  unnest(p) %&amp;gt;% 
  # add in the nd predictor data
  bind_cols(
    bind_rows(replicate(10, nd, simplify = FALSE))
    ) %&amp;gt;% 
  # drop two unneeded columns
  select(-df, -residual.scale)

# now what did we do?
glimpse(fitted_lines)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 300
## Columns: 4
## $ .imp   &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
## $ fit    &amp;lt;dbl&amp;gt; -1.75590067, -1.62725529, -1.49860992, -1.36996455, -1.24131917…
## $ se.fit &amp;lt;dbl&amp;gt; 0.5285145, 0.4991145, 0.4705449, 0.4429666, 0.4165764, 0.391614…
## $ ses    &amp;lt;dbl&amp;gt; -16.6666667, -15.4252874, -14.1839080, -12.9425287, -11.7011494…&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;second-equations-and-the-implied-code.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Second: equations and the implied code.&lt;/h4&gt;
&lt;p&gt;In his paper (p. 176), Miles’s used &lt;em&gt;predictions&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;as a blanket term for any value &lt;span class=&#34;math inline&#34;&gt;\(\hat p\)&lt;/span&gt; that can
be calculated by applying some type of transformation &lt;span class=&#34;math inline&#34;&gt;\(t()\)&lt;/span&gt; to the vector of coefficients from a fitted model &lt;span class=&#34;math inline&#34;&gt;\((\hat \beta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat p = t(\hat \beta)\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our case, &lt;span class=&#34;math inline&#34;&gt;\(\hat p\)&lt;/span&gt; covers the values in our &lt;code&gt;fit&lt;/code&gt; column and the &lt;span class=&#34;math inline&#34;&gt;\(t(\hat \beta)\)&lt;/span&gt; part is what we did with &lt;code&gt;predict()&lt;/code&gt;. Well, technically we should refer to those &lt;code&gt;fit&lt;/code&gt; values as &lt;span class=&#34;math inline&#34;&gt;\(\hat p_j\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is the index for a given imputed data set, &lt;span class=&#34;math inline&#34;&gt;\(j = 1, \dots, m\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is the total number of imputations. In our &lt;code&gt;fitted_lines&lt;/code&gt; tibble, we have called Miles’s &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; index &lt;code&gt;.imp&lt;/code&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Anyway, Miles showed we can compute the conditional pooled point estimate &lt;span class=&#34;math inline&#34;&gt;\(\bar p\)&lt;/span&gt; by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\bar p = \frac{1}{m} \sum_{j=1}^m \hat p_j,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is a formal way of saying we simply average across the &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; imputed solutions. Here’s that in code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted_lines %&amp;gt;% 
  group_by(ses) %&amp;gt;% 
  summarise(fit_bar = mean(fit))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30 × 2
##       ses fit_bar
##     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 -16.7   -2.07 
##  2 -15.4   -1.91 
##  3 -14.2   -1.76 
##  4 -12.9   -1.60 
##  5 -11.7   -1.45 
##  6 -10.5   -1.30 
##  7  -9.22  -1.14 
##  8  -7.98  -0.989
##  9  -6.74  -0.835
## 10  -5.49  -0.681
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though the expected values are pretty easy to compute, it’ll take a little more effort to express the uncertainty around those expectations because we have to account for both within- and between-imputation variance. We can define the within-imputation variance &lt;span class=&#34;math inline&#34;&gt;\(V_W\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_W = \frac{1}{m} \sum_{j=1}^m \widehat{SE}_j^2,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is a formal way of saying we simply average the squared standard errors across the &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; imputed solutions, for each fitted value. Here’s that in code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted_lines %&amp;gt;% 
  group_by(ses) %&amp;gt;% 
  summarise(fit_bar = mean(fit),
            v_w     = mean(se.fit^2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30 × 3
##       ses fit_bar    v_w
##     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 -16.7   -2.07  0.260 
##  2 -15.4   -1.91  0.232 
##  3 -14.2   -1.76  0.206 
##  4 -12.9   -1.60  0.182 
##  5 -11.7   -1.45  0.161 
##  6 -10.5   -1.30  0.142 
##  7  -9.22  -1.14  0.126 
##  8  -7.98  -0.989 0.112 
##  9  -6.74  -0.835 0.100 
## 10  -5.49  -0.681 0.0908
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can define the between imputation variance &lt;span class=&#34;math inline&#34;&gt;\(V_B\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_B = \frac{1}{m - 1} \sum_{j=1}^m (\hat p_j - \bar p_j)^2,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we’re no longer quite averaging across the &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; imputations because our denominator is now the corrected value &lt;span class=&#34;math inline&#34;&gt;\((m - 1)\)&lt;/span&gt;. What can I say? Variances are tricky. Here’s the code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the total number of imputations
m &amp;lt;- 10

fitted_lines %&amp;gt;% 
  group_by(ses) %&amp;gt;% 
  summarise(fit_bar = mean(fit),
            v_w     = mean(se.fit^2),
            v_b     = sum(fit - fit_bar)^2 / (m - 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30 × 4
##       ses fit_bar    v_w      v_b
##     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 -16.7   -2.07  0.260  2.68e-31
##  2 -15.4   -1.91  0.232  8.77e-32
##  3 -14.2   -1.76  0.206  4.93e-32
##  4 -12.9   -1.60  0.182  8.77e-32
##  5 -11.7   -1.45  0.161  4.93e-32
##  6 -10.5   -1.30  0.142  1.37e-33
##  7  -9.22  -1.14  0.126  0       
##  8  -7.98  -0.989 0.112  1.23e-32
##  9  -6.74  -0.835 0.100  3.42e-32
## 10  -5.49  -0.681 0.0908 5.48e-33
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can define the total variance of the prediction &lt;span class=&#34;math inline&#34;&gt;\(V_{\bar p}\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{\bar p} = V_W + V_B \left ( 1 + \frac{1}{m} \right ),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the pooled standard error is just &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{V_{\bar p}}\)&lt;/span&gt;. Here are those in code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted_lines %&amp;gt;% 
  group_by(ses) %&amp;gt;% 
  summarise(fit_bar = mean(fit),
            v_w     = mean(se.fit^2),
            v_b     = sum(fit - fit_bar)^2 / (m - 1),
            v_p     = v_w + v_b * (1 + (1 / m)),
            se_p    = sqrt(v_p))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30 × 6
##       ses fit_bar    v_w      v_b    v_p  se_p
##     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 -16.7   -2.07  0.260  2.68e-31 0.260  0.510
##  2 -15.4   -1.91  0.232  8.77e-32 0.232  0.481
##  3 -14.2   -1.76  0.206  4.93e-32 0.206  0.453
##  4 -12.9   -1.60  0.182  8.77e-32 0.182  0.427
##  5 -11.7   -1.45  0.161  4.93e-32 0.161  0.401
##  6 -10.5   -1.30  0.142  1.37e-33 0.142  0.377
##  7  -9.22  -1.14  0.126  0        0.126  0.354
##  8  -7.98  -0.989 0.112  1.23e-32 0.112  0.334
##  9  -6.74  -0.835 0.100  3.42e-32 0.100  0.316
## 10  -5.49  -0.681 0.0908 5.48e-33 0.0908 0.301
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we finally have both &lt;span class=&#34;math inline&#34;&gt;\(\bar p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_{\bar p}\)&lt;/span&gt; for each desired level of &lt;code&gt;ses&lt;/code&gt;, we can use the conventional normal-theory approach to compute the pooled 95% confidence intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this time we&amp;#39;ll save the results
fitted_lines &amp;lt;- fitted_lines %&amp;gt;% 
  group_by(ses) %&amp;gt;% 
  summarise(fit_bar = mean(fit),
            v_w     = mean(se.fit^2),
            v_b     = sum(fit - fit_bar)^2 / (m - 1),
            v_p     = v_w + v_b * (1 + (1 / m)),
            se_p    = sqrt(v_p)) %&amp;gt;% 
  # use the _p suffix to indicate these are pooled
  mutate(lwr_p = fit_bar - se_p * 1.96,
         upr_p = fit_bar + se_p * 1.96) 

# what do we have?
glimpse(fitted_lines)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 30
## Columns: 8
## $ ses     &amp;lt;dbl&amp;gt; -16.6666667, -15.4252874, -14.1839080, -12.9425287, -11.701149…
## $ fit_bar &amp;lt;dbl&amp;gt; -2.06663732, -1.91265660, -1.75867587, -1.60469515, -1.4507144…
## $ v_w     &amp;lt;dbl&amp;gt; 0.25989881, 0.23153126, 0.20555857, 0.18198075, 0.16079779, 0.…
## $ v_b     &amp;lt;dbl&amp;gt; 2.684318e-31, 8.765121e-32, 4.930381e-32, 8.765121e-32, 4.9303…
## $ v_p     &amp;lt;dbl&amp;gt; 0.25989881, 0.23153126, 0.20555857, 0.18198075, 0.16079779, 0.…
## $ se_p    &amp;lt;dbl&amp;gt; 0.5098027, 0.4811770, 0.4533857, 0.4265920, 0.4009960, 0.37684…
## $ lwr_p   &amp;lt;dbl&amp;gt; -3.06585065, -2.85576342, -2.64731180, -2.44081550, -2.2366665…
## $ upr_p   &amp;lt;dbl&amp;gt; -1.06742400, -0.96954978, -0.87003995, -0.76857480, -0.6647622…&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;third-plot.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Third: plot.&lt;/h4&gt;
&lt;p&gt;Now the hard part is over, we’re finally ready to plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted_lines %&amp;gt;% 
  ggplot(aes(x = ses)) +
  geom_ribbon(aes(ymin = lwr_p, ymax = upr_p),
              alpha = 1/2) +
  geom_line(aes(y = fit_bar), 
            size = 1/2) +
  # add the observed data for good measure
  geom_point(data = b_small,
             aes(y = iqv)) +
  labs(title = &amp;quot;Pooled fitted line from a model fit to multiply-imputed data&amp;quot;,
       subtitle = expression(&amp;quot;The pooled conditional mean line &amp;quot;*(bar(italic(p)))*&amp;quot; sits atop the pooled confidence interval &amp;quot;*(95*&amp;#39;%&amp;#39;*~CI[bar(italic(p))])*&amp;#39; band.&amp;#39;),
       y = &amp;quot;iqv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-10-21-if-you-fit-a-model-with-multiply-imputed-data-you-can-still-plot-the-line/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There it is, friends. We have the pooled fitted line and its pooled 95% confidence interval band from our model fit to multiply-imputed data. Until the day that Stef van Buuren and friends get around to building this functionality into &lt;strong&gt;mice&lt;/strong&gt;, our realization in &lt;strong&gt;R&lt;/strong&gt; code of Andrew Miles’s &lt;em&gt;Predict Then Combine&lt;/em&gt; (PC) approach has you covered.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.1 (2021-08-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] mice_3.13.0     GGally_2.1.2    forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.7     purrr_0.3.4     readr_2.0.1     tidyr_1.1.3    
##  [9] tibble_3.1.5    ggplot2_3.3.5   tidyverse_1.3.1
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         lattice_0.20-44    lubridate_1.7.10   assertthat_0.2.1  
##  [5] digest_0.6.28      utf8_1.2.2         R6_2.5.1           cellranger_1.1.0  
##  [9] plyr_1.8.6         backports_1.2.1    reprex_2.0.1       evaluate_0.14     
## [13] highr_0.9          httr_1.4.2         blogdown_1.5       pillar_1.6.3      
## [17] rlang_0.4.11       readxl_1.3.1       rstudioapi_0.13    jquerylib_0.1.4   
## [21] rmarkdown_2.10     labeling_0.4.2     munsell_0.5.0      broom_0.7.9       
## [25] compiler_4.1.1     modelr_0.1.8       xfun_0.25          pkgconfig_2.0.3   
## [29] htmltools_0.5.2    tidyselect_1.1.1   bookdown_0.23      emo_0.0.0.9000    
## [33] reshape_0.8.8      fansi_0.5.0        crayon_1.4.1       tzdb_0.1.2        
## [37] dbplyr_2.1.1       withr_2.4.2        grid_4.1.1         jsonlite_1.7.2    
## [41] gtable_0.3.0       lifecycle_1.0.1    DBI_1.1.1          magrittr_2.0.1    
## [45] scales_1.1.1       cli_3.0.1          stringi_1.7.4      farver_2.1.0      
## [49] fs_1.5.0           xml2_1.3.2         bslib_0.3.0        ellipsis_0.3.2    
## [53] generics_0.1.0     vctrs_0.3.8        RColorBrewer_1.1-2 tools_4.1.1       
## [57] glue_1.4.2         hms_1.1.0          fastmap_1.1.0      yaml_2.2.1        
## [61] colorspace_2.0-2   rvest_1.0.1        knitr_1.33         haven_2.4.3       
## [65] sass_0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-brandsma1989effects&#34; class=&#34;csl-entry&#34;&gt;
Brandsma, H., &amp;amp; Knuver, J. (1989). Effects of school and classroom characteristics on pupil progress in language and arithmetic. &lt;em&gt;International Journal of Educational Research&lt;/em&gt;, &lt;em&gt;13&lt;/em&gt;(7), 777–788. &lt;a href=&#34;https://doi.org/10.1016/0883-0355(89)90028-1&#34;&gt;https://doi.org/10.1016/0883-0355(89)90028-1&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-enders2010applied&#34; class=&#34;csl-entry&#34;&gt;
Enders, C. K. (2010). &lt;em&gt;Applied missing data analysis&lt;/em&gt;. &lt;span&gt;Guilford press&lt;/span&gt;. &lt;a href=&#34;http://www.appliedmissingdata.com/&#34;&gt;http://www.appliedmissingdata.com/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-james2021AnIntroduction&#34; class=&#34;csl-entry&#34;&gt;
James, G., Witten, D., Hastie, T., &amp;amp; Tibshirani, R. (2021). &lt;em&gt;An introduction to statistical learning with applications in &lt;span&gt;R&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;Springer&lt;/span&gt;. &lt;a href=&#34;https://web.stanford.edu/~hastie/ISLRv2_website.pdf&#34;&gt;https://web.stanford.edu/~hastie/ISLRv2_website.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34; class=&#34;csl-entry&#34;&gt;
Little, R. J., &amp;amp; Rubin, D. B. (2019). &lt;em&gt;Statistical analysis with missing data&lt;/em&gt; (third, Vol. 793). &lt;span&gt;John Wiley &amp;amp; Sons&lt;/span&gt;. &lt;a href=&#34;https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798&#34;&gt;https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-miles2016obtaining&#34; class=&#34;csl-entry&#34;&gt;
Miles, A. (2016). Obtaining predictions from models fit to multiply imputed data. &lt;em&gt;Sociological Methods &amp;amp; Research&lt;/em&gt;, &lt;em&gt;45&lt;/em&gt;(1), 175–185. &lt;a href=&#34;https://doi.org/10.1177/0049124115610345&#34;&gt;https://doi.org/10.1177/0049124115610345&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-roback2021beyond&#34; class=&#34;csl-entry&#34;&gt;
Roback, P., &amp;amp; Legler, J. (2021). &lt;em&gt;Beyond multiple linear regression: &lt;span&gt;Applied&lt;/span&gt; generalized linear models and multilevel models in &lt;span&gt;R&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://bookdown.org/roback/bookdown-BeyondMLR/&#34;&gt;https://bookdown.org/roback/bookdown-BeyondMLR/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-GGally&#34; class=&#34;csl-entry&#34;&gt;
Schloerke, B., Crowley, J., Di Cook, Briatte, F., Marbach, M., Thoen, E., Elberg, A., &amp;amp; Larmarange, J. (2021). &lt;em&gt;&lt;span&gt;GGally&lt;/span&gt;: &lt;span&gt;Extension&lt;/span&gt; to &lt;span class=&#34;nocase&#34;&gt;’ggplot2’&lt;/span&gt;&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=GGally&#34;&gt;https://CRAN.R-project.org/package=GGally&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vanbuurenFlexibleImputationMissing2018&#34; class=&#34;csl-entry&#34;&gt;
van Buuren, S. (2018). &lt;em&gt;Flexible imputation of missing data&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://stefvanbuuren.name/fimd/&#34;&gt;https://stefvanbuuren.name/fimd/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mice2011&#34; class=&#34;csl-entry&#34;&gt;
van Buuren, S., &amp;amp; Groothuis-Oudshoorn, K. (2011). &lt;span class=&#34;nocase&#34;&gt;mice&lt;/span&gt;: &lt;span&gt;Multivariate&lt;/span&gt; imputation by chained equations in &lt;span&gt;R&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;45&lt;/em&gt;(3), 1–67. &lt;a href=&#34;https://www.jstatsoft.org/v45/i03/&#34;&gt;https://www.jstatsoft.org/v45/i03/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-mice&#34; class=&#34;csl-entry&#34;&gt;
van Buuren, S., &amp;amp; Groothuis-Oudshoorn, K. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;mice&lt;/span&gt;: &lt;span&gt;Multivariate&lt;/span&gt; imputation by chained equations&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=mice&#34;&gt;https://CRAN.R-project.org/package=mice&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Be warned that “full-luxury Bayesian …” isn’t a real term. Rather, it’s a playful descriptor coined by the great Richard McElreath. To hear him use it in action, check out his &lt;a href=&#34;https://www.youtube.com/watch?v=KNPYUVmY3NM&#34;&gt;nifty talk&lt;/a&gt; on causal inference. One-step Bayesian imputation is a real thing, though. McElreath covered it in both editions of his text and I’ve even blogged about it &lt;a href=&#34;https://solomonkurz.netlify.app/post/2021-07-27-one-step-bayesian-imputation-when-you-have-dropout-in-your-rct/&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;When you do this on your own, you might instead name the &lt;code&gt;.imp&lt;/code&gt; column as &lt;code&gt;m&lt;/code&gt;, which goes nicely with Miles’s notation. In this post and in some of my personal work, I used &lt;code&gt;.imp&lt;/code&gt; because it lines up nicely with the output from some of the &lt;strong&gt;mice&lt;/strong&gt; functions.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sexy up your logistic regression model with logit dotplots</title>
      <link>/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/</link>
      <pubDate>Wed, 22 Sep 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/</guid>
      <description>
&lt;script src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;what&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What&lt;/h2&gt;
&lt;p&gt;When you fit a logistic regression model, there are a lot of ways to display the results. One of the least inspiring ways is to report a summary of the coefficients in prose or within a table. A more artistic approach is to show the fitted line in a plot, which often looks nice due to the curvy nature of logistic regression lines. The major shortcoming in typical logistic regression line plots is they usually don’t show the data due to overplottong across the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis. Happily, new developments with Matthew Kay’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-ggdist&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;https://mjskay.github.io/ggdist/&#34;&gt;&lt;strong&gt;ggdist&lt;/strong&gt; package&lt;/a&gt; make it easy to show your data when you plot your logistic regression curves. In this post I’ll show you how.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;For this post, I’m presuming some background knowledge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should be familiar with logistic regression. For introductions, I recommend Roback and Legler’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-roback2021beyond&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; online text or James, Witten, Hastie, and Tibshirani’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-james2021AnIntroduction&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; online text. Both texts are written from a frequentist perspective, which is also the framework we’ll be using in this blog post. For Bayesian introductions to logistic regression, I recommend either edition of McElreath’s text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;; Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; text; or Gelman, Hill, and Vehtari’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; text.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;. Data wrangling and plotting were done with help from the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt; and &lt;strong&gt;broom&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-broom&#34; role=&#34;doc-biblioref&#34;&gt;Robinson et al., 2021&lt;/a&gt;)&lt;/span&gt;. The data are from the &lt;a href=&#34;https://github.com/debruine/faux&#34;&gt;&lt;strong&gt;fivethirtyeight&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-fivethirtyeight2018&#34; role=&#34;doc-biblioref&#34;&gt;Kim et al., 2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-fivethirtyeight&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we load our primary &lt;strong&gt;R&lt;/strong&gt; packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(fivethirtyeight)
library(broom)
library(ggdist)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We need data.&lt;/h3&gt;
&lt;p&gt;In this post, we’ll be working with the &lt;code&gt;bechdel&lt;/code&gt; data set. From the documentation, we read these are “the raw data behind the story ‘&lt;a href=&#34;https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/&#34;&gt;The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women&lt;/a&gt;.’”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(bechdel)

glimpse(bechdel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1,794
## Columns: 15
## $ year          &amp;lt;int&amp;gt; 2013, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 20…
## $ imdb          &amp;lt;chr&amp;gt; &amp;quot;tt1711425&amp;quot;, &amp;quot;tt1343727&amp;quot;, &amp;quot;tt2024544&amp;quot;, &amp;quot;tt1272878&amp;quot;, &amp;quot;tt0…
## $ title         &amp;lt;chr&amp;gt; &amp;quot;21 &amp;amp; Over&amp;quot;, &amp;quot;Dredd 3D&amp;quot;, &amp;quot;12 Years a Slave&amp;quot;, &amp;quot;2 Guns&amp;quot;, &amp;quot;…
## $ test          &amp;lt;chr&amp;gt; &amp;quot;notalk&amp;quot;, &amp;quot;ok-disagree&amp;quot;, &amp;quot;notalk-disagree&amp;quot;, &amp;quot;notalk&amp;quot;, &amp;quot;m…
## $ clean_test    &amp;lt;ord&amp;gt; notalk, ok, notalk, notalk, men, men, notalk, ok, ok, no…
## $ binary        &amp;lt;chr&amp;gt; &amp;quot;FAIL&amp;quot;, &amp;quot;PASS&amp;quot;, &amp;quot;FAIL&amp;quot;, &amp;quot;FAIL&amp;quot;, &amp;quot;FAIL&amp;quot;, &amp;quot;FAIL&amp;quot;, &amp;quot;FAIL&amp;quot;, …
## $ budget        &amp;lt;int&amp;gt; 13000000, 45000000, 20000000, 61000000, 40000000, 225000…
## $ domgross      &amp;lt;dbl&amp;gt; 25682380, 13414714, 53107035, 75612460, 95020213, 383624…
## $ intgross      &amp;lt;dbl&amp;gt; 42195766, 40868994, 158607035, 132493015, 95020213, 1458…
## $ code          &amp;lt;chr&amp;gt; &amp;quot;2013FAIL&amp;quot;, &amp;quot;2012PASS&amp;quot;, &amp;quot;2013FAIL&amp;quot;, &amp;quot;2013FAIL&amp;quot;, &amp;quot;2013FAI…
## $ budget_2013   &amp;lt;int&amp;gt; 13000000, 45658735, 20000000, 61000000, 40000000, 225000…
## $ domgross_2013 &amp;lt;dbl&amp;gt; 25682380, 13611086, 53107035, 75612460, 95020213, 383624…
## $ intgross_2013 &amp;lt;dbl&amp;gt; 42195766, 41467257, 158607035, 132493015, 95020213, 1458…
## $ period_code   &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ decade_code   &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data were collected on Hollywood movies made between 1970 and 2013.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bechdel %&amp;gt;% 
  pull(year) %&amp;gt;% 
  range()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1970 2013&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our focal variable will be &lt;code&gt;binary&lt;/code&gt;, which indicates whether a given movie passed the Bechdel test. Of the &lt;span class=&#34;math inline&#34;&gt;\(1{,}794\)&lt;/span&gt; movies in the data set, just under half of them passed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bechdel %&amp;gt;% 
  count(binary) %&amp;gt;% 
  mutate(percent = 100 * n / sum(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 3
##   binary     n percent
##   &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 FAIL     991    55.2
## 2 PASS     803    44.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our sole predictor variable will be &lt;code&gt;budget_2013&lt;/code&gt;, each movie’s budget as expressed in 2013 dollars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bechdel %&amp;gt;% 
  ggplot(aes(x = budget_2013)) +
  geom_histogram() +
  facet_wrap(~ binary, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To make our lives a little easier, we’ll convert the character variable &lt;code&gt;binary&lt;/code&gt; into a conventional &lt;span class=&#34;math inline&#34;&gt;\(0/1\)&lt;/span&gt; numeric variable called &lt;code&gt;pass&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute
bechdel &amp;lt;- bechdel  %&amp;gt;% 
  mutate(pass = ifelse(binary == &amp;quot;FAIL&amp;quot;, 0, 1)) 

# compare
bechdel %&amp;gt;% 
  select(binary, pass) %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 2
##   binary  pass
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 FAIL       0
## 2 PASS       1
## 3 FAIL       0
## 4 FAIL       0
## 5 FAIL       0
## 6 FAIL       0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;p&gt;We can express our statistical model in formal notation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{pass}_i &amp;amp; \sim \operatorname{Binomial}(n = 1, p_i) \\
\operatorname{logit}(p_i) &amp;amp; = \beta_0 + \beta_1 \text{budget_2013}_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we use the conventional logit link to ensure the binomial probabilities are restricted within the bounds of zero and one. We can fit such a model with the base &lt;strong&gt;R&lt;/strong&gt; &lt;code&gt;glm()&lt;/code&gt; function like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- glm(
  data = bechdel,
  family = binomial,
  pass ~ 1 + budget_2013)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A conventional way to present the results would in a coefficient table, the rudiments of which we can get from the &lt;code&gt;broom::tidy()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(fit) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1113148&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0689661&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.614051&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1065163&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;budget_2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-6.249724&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Because of the scale of the &lt;code&gt;budget_2013&lt;/code&gt; variable, its point estimate and standard errors are both very small. To give a little perspective, here is the expected decrease in log-odds for a budget increase in &lt;span class=&#34;math inline&#34;&gt;\(\$100{,}000{,}000\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(coef(fit)[2], confint(fit)[2, ]) * 1e8&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## budget_2013       2.5 %      97.5 % 
##  -0.5972374  -0.7875178  -0.4126709&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how we added in the 95% confidence intervals for good measure.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;line-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Line plots&lt;/h2&gt;
&lt;p&gt;Now we have interpreted the model in the dullest way possible, with a table and in prose, let’s practice plotting the results. First, we’ll use the widely-used method of displaying only the fitted line.&lt;/p&gt;
&lt;div id=&#34;fitted-line-wo-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fitted line w/o data.&lt;/h3&gt;
&lt;p&gt;We can use the &lt;code&gt;predict()&lt;/code&gt; function along with some post-processing strategies from &lt;a href=&#34;https://twitter.com/ucfagls&#34;&gt;Gavin Simpson&lt;/a&gt;’s fine blog post, &lt;a href=&#34;https://fromthebottomoftheheap.net/2018/12/10/confidence-intervals-for-glms/&#34;&gt;&lt;em&gt;Confidence intervals for GLMs&lt;/em&gt;&lt;/a&gt;, to prepare the data necessary for making our plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the new data
nd &amp;lt;- tibble(budget_2013 = seq(from = 0, to = 500000000, length.out = 100))

p &amp;lt;-
  # compute the fitted lines and SE&amp;#39;s
  predict(fit,
          newdata = nd,
          type = &amp;quot;link&amp;quot;,
          se.fit = TRUE) %&amp;gt;% 
  # wrangle
  data.frame() %&amp;gt;% 
  mutate(ll = fit - 1.96 * se.fit,
         ul = fit + 1.96 * se.fit) %&amp;gt;% 
  select(-residual.scale, -se.fit) %&amp;gt;% 
  mutate_all(plogis) %&amp;gt;%
  bind_cols(nd)

# what have we done?
glimpse(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 100
## Columns: 4
## $ fit         &amp;lt;dbl&amp;gt; 0.5278000, 0.5202767, 0.5127442, 0.5052059, 0.4976652, 0.4…
## $ ll          &amp;lt;dbl&amp;gt; 0.4940356, 0.4881515, 0.4821772, 0.4760998, 0.4699043, 0.4…
## $ ul          &amp;lt;dbl&amp;gt; 0.5613120, 0.5522351, 0.5432161, 0.5342767, 0.5254405, 0.5…
## $ budget_2013 &amp;lt;dbl&amp;gt; 0, 5050505, 10101010, 15151515, 20202020, 25252525, 303030…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a conventional line plot for our logistic regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013, y = fit)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line() +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;, 
                     expand = c(0, 0), limits = 0:1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The fitted line is in black and the semitransparent grey ribbon marks of the 95% confidence intervals. The plot does a nice job showing how movies with larger budgets tend to do a worse job passing the Bechdel test.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improve-the-visualization-by-adding-data.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Improve the visualization by adding data.&lt;/h3&gt;
&lt;p&gt;If you wanted to add the data to our plot, a naïve approach might be to use &lt;code&gt;geom_point()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013, y = fit)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line() +
  geom_point(data = bechdel,
             aes(y = pass),
             alpha = 1/2) +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;, 
                     expand = expansion(mult = 0.01))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even by making the dots semitransparent with the &lt;code&gt;alpha&lt;/code&gt; parameter, the overplotting issue makes it very difficult to make sense of the data. One of the approaches favored by Gelman and colleagues &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelmanRegressionOtherStories2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; is to add a little vertical jittering. We can do that with &lt;code&gt;geom_jitter()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013, y = fit)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line() +
  geom_jitter(data = bechdel,
              aes(y = pass),
              size = 1/4, alpha = 1/2, height = 0.05) +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;, 
                     expand = c(0, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Though a big improvement, this approach still doesn’t do the best job depicting the distribution of the &lt;code&gt;budget_2013&lt;/code&gt; values. If possible, it would be better to explicitly depict the &lt;code&gt;budget_2013&lt;/code&gt; distributions for each level of &lt;code&gt;pass&lt;/code&gt; with something more like histograms. In his blogpost, &lt;a href=&#34;https://www.barelysignificant.com/post/glm/&#34;&gt;&lt;em&gt;Using R to make sense of the generalised linear model&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/lnalborczyk&#34;&gt;Ladislas Nalborczyk&lt;/a&gt; showed how you could do so with a custom function he named &lt;code&gt;logit_dotplot()&lt;/code&gt;, the source code for which you can find &lt;a href=&#34;https://github.com/lnalborczyk/lnalborczyk.github.io/blob/master/code/logit_dotplot.R&#34;&gt;here&lt;/a&gt; on his GitHub. Since Nalborczyk’s post, this kind of functionality has since been built into Kay’s &lt;strong&gt;ggdist&lt;/strong&gt; package. Here’s what it looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line(aes(y = fit)) +
  stat_dots(data = bechdel,
            aes(y = pass, side = ifelse(pass == 0, &amp;quot;top&amp;quot;, &amp;quot;bottom&amp;quot;)),
            scale = 1/3) +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;,
                     expand = c(0, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the &lt;code&gt;stat_dots()&lt;/code&gt; function, we added dotplots, which are nifty alternatives to histograms which display each data value as an individual dot. With the &lt;code&gt;side&lt;/code&gt; argument, we used a conditional statement to tell &lt;code&gt;stat_dots()&lt;/code&gt; we wanted some of the &lt;code&gt;budget_2013&lt;/code&gt; to be displayed on the bottom and other of those values to be displayed on the top. With the &lt;code&gt;scale&lt;/code&gt; argument, we indicated how much of the total space within the range of the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis we wanted the dot plot distributions to take up.&lt;/p&gt;
&lt;p&gt;For kicks and giggles, here’s a more polished version of what such a plot could look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line(aes(y = fit)) +
  stat_dots(data = bechdel %&amp;gt;% 
              mutate(binary = factor(binary, levels = c(&amp;quot;PASS&amp;quot;, &amp;quot;FAIL&amp;quot;))),
            aes(y = pass, 
                side = ifelse(pass == 0, &amp;quot;top&amp;quot;, &amp;quot;bottom&amp;quot;),
                color = binary),
            scale = 0.4, shape = 19) +
  scale_color_manual(&amp;quot;Bechdel test&amp;quot;, values = c(&amp;quot;#009E73&amp;quot;, &amp;quot;#D55E00&amp;quot;)) +
  scale_x_continuous(&amp;quot;budget (in 2013 dollars)&amp;quot;,
                     breaks = c(0, 1e8, 2e8, 3e8, 4e8),
                     labels = c(0, str_c(1:4 * 100, &amp;quot; mill&amp;quot;)),
                     expand = c(0, 0), limits = c(0, 48e7)) +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;,
                     expand = c(0, 0)) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Other distributional forms are possible, too. For example, here we set &lt;code&gt;slab_type = &#34;histogram&#34;&lt;/code&gt; within the &lt;code&gt;stat_slab()&lt;/code&gt; function to swap out the dotplots for histograms.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p %&amp;gt;% 
  ggplot(aes(x = budget_2013)) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line(aes(y = fit)) +
  # the magic lives here
  stat_slab(data = bechdel %&amp;gt;% 
              mutate(binary = factor(binary, levels = c(&amp;quot;PASS&amp;quot;, &amp;quot;FAIL&amp;quot;))),
            aes(y = pass, 
                side = ifelse(pass == 0, &amp;quot;top&amp;quot;, &amp;quot;bottom&amp;quot;),
                fill = binary, color = binary),
            slab_type = &amp;quot;histogram&amp;quot;,
            scale = 0.4, breaks = 40, size = 1/2) +
  scale_fill_manual(&amp;quot;Bechdel test&amp;quot;, values = c(alpha(&amp;quot;#009E73&amp;quot;, .7), alpha(&amp;quot;#D55E00&amp;quot;, .7))) +
  scale_color_manual(&amp;quot;Bechdel test&amp;quot;, values = c(&amp;quot;#009E73&amp;quot;, &amp;quot;#D55E00&amp;quot;)) +
  scale_x_continuous(&amp;quot;budget (in 2013 dollars)&amp;quot;,
                     breaks = c(0, 1e8, 2e8, 3e8, 4e8),
                     labels = c(0, str_c(1:4 * 100, &amp;quot; mill&amp;quot;)),
                     expand = c(0, 0), limits = c(0, 48e7)) +
  scale_y_continuous(&amp;quot;probability of passing&amp;quot;,
                     expand = c(0, 0)) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-09-22-sexy-up-your-logistic-regression-model-with-logit-dotplots/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s a wrap, friends. No more lonely logistic curves absent data. Flaunt those sexy data with &lt;strong&gt;ggdist&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.1 (2021-08-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggdist_3.0.0          broom_0.7.9           fivethirtyeight_0.6.1
##  [4] forcats_0.5.1         stringr_1.4.0         dplyr_1.0.7          
##  [7] purrr_0.3.4           readr_2.0.1           tidyr_1.1.3          
## [10] tibble_3.1.4          ggplot2_3.3.5         tidyverse_1.3.1      
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7           lubridate_1.7.10     assertthat_0.2.1    
##  [4] digest_0.6.27        utf8_1.2.2           R6_2.5.1            
##  [7] cellranger_1.1.0     backports_1.2.1      reprex_2.0.1        
## [10] evaluate_0.14        highr_0.9            httr_1.4.2          
## [13] blogdown_1.5         pillar_1.6.2         rlang_0.4.11        
## [16] readxl_1.3.1         rstudioapi_0.13      jquerylib_0.1.4     
## [19] rmarkdown_2.10       labeling_0.4.2       munsell_0.5.0       
## [22] compiler_4.1.1       modelr_0.1.8         xfun_0.25           
## [25] pkgconfig_2.0.3      htmltools_0.5.2      tidyselect_1.1.1    
## [28] bookdown_0.23        fansi_0.5.0          crayon_1.4.1        
## [31] tzdb_0.1.2           dbplyr_2.1.1         withr_2.4.2         
## [34] MASS_7.3-54          grid_4.1.1           distributional_0.2.2
## [37] jsonlite_1.7.2       gtable_0.3.0         lifecycle_1.0.0     
## [40] DBI_1.1.1            magrittr_2.0.1       scales_1.1.1        
## [43] cli_3.0.1            stringi_1.7.4        farver_2.1.0        
## [46] fs_1.5.0             xml2_1.3.2           bslib_0.3.0         
## [49] ellipsis_0.3.2       generics_0.1.0       vctrs_0.3.8         
## [52] tools_4.1.1          glue_1.4.2           hms_1.1.0           
## [55] fastmap_1.1.0        yaml_2.2.1           colorspace_2.0-2    
## [58] rvest_1.0.1          knitr_1.33           haven_2.4.3         
## [61] sass_0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-gelmanRegressionOtherStories2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A., Hill, J., &amp;amp; Vehtari, A. (2020). &lt;em&gt;Regression and other stories&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/9781139161879&#34;&gt;https://doi.org/10.1017/9781139161879&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-james2021AnIntroduction&#34; class=&#34;csl-entry&#34;&gt;
James, G., Witten, D., Hastie, T., &amp;amp; Tibshirani, R. (2021). &lt;em&gt;An introduction to statistical learning with applications in &lt;span&gt;R&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;Springer&lt;/span&gt;. &lt;a href=&#34;https://web.stanford.edu/~hastie/ISLRv2_website.pdf&#34;&gt;https://web.stanford.edu/~hastie/ISLRv2_website.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-ggdist&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;ggdist&lt;/span&gt;: &lt;span&gt;Visualizations&lt;/span&gt; of distributions and uncertainty&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=ggdist&#34;&gt;https://CRAN.R-project.org/package=ggdist&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-fivethirtyeight2018&#34; class=&#34;csl-entry&#34;&gt;
Kim, A. Y., Ismay, C., &amp;amp; Chunn, J. (2018). The fivethirtyeight &lt;span&gt;R&lt;/span&gt; package: ’Tame data’ principles for introductory statistics and data science courses. &lt;em&gt;Technology Innovations in Statistics Education&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(1). &lt;a href=&#34;https://escholarship.org/uc/item/0rx1231m&#34;&gt;https://escholarship.org/uc/item/0rx1231m&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-fivethirtyeight&#34; class=&#34;csl-entry&#34;&gt;
Kim, A. Y., Ismay, C., &amp;amp; Chunn, J. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;fivethirtyeight&lt;/span&gt;: &lt;span&gt;Data&lt;/span&gt; and code behind the stories and interactives at &lt;span&gt;FiveThirtyEight&lt;/span&gt;&lt;/em&gt; [Manual]. &lt;a href=&#34;https://github.com/rudeboybert/fivethirtyeight&#34;&gt;https://github.com/rudeboybert/fivethirtyeight&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-roback2021beyond&#34; class=&#34;csl-entry&#34;&gt;
Roback, P., &amp;amp; Legler, J. (2021). &lt;em&gt;Beyond multiple linear regression: &lt;span&gt;Applied&lt;/span&gt; generalized linear models and multilevel models in &lt;span&gt;R&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://bookdown.org/roback/bookdown-BeyondMLR/&#34;&gt;https://bookdown.org/roback/bookdown-BeyondMLR/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-broom&#34; class=&#34;csl-entry&#34;&gt;
Robinson, D., Hayes, A., &amp;amp; Couch, S. (2021). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;broom&lt;/span&gt;: &lt;span&gt;Convert&lt;/span&gt; statistical objects into tidy tibbles&lt;/em&gt; [Manual]. &lt;a href=&#34;https://CRAN.R-project.org/package=broom&#34;&gt;https://CRAN.R-project.org/package=broom&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Make ICC plots for your brms IRT models</title>
      <link>/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/</guid>
      <description>
&lt;script src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;context&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Context&lt;/h2&gt;
&lt;p&gt;Someone recently posted a &lt;a href=&#34;https://discourse.mc-stan.org/t/item-characteristic-curves-and-item-information-curves-from-item-response-models/22964&#34;&gt;thread on the Stan forums&lt;/a&gt; asking how one might make item-characteristic curve (ICC) and item-information curve (IIC) plots for an item-response theory (IRT) model fit with &lt;strong&gt;brms&lt;/strong&gt;. People were slow to provide answers and I came up disappointingly empty handed after a quick web search. The purpose of this blog post is to show how one might make ICC and IIC plots for &lt;strong&gt;brms&lt;/strong&gt; IRT models using general-purpose data wrangling steps.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;I make assumptions.&lt;/h3&gt;
&lt;p&gt;This tutorial is for those with a passing familiarity with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You’ll want to be familiar with the &lt;strong&gt;brms&lt;/strong&gt; package &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBrmsPackageBayesian2017&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2017&lt;/a&gt;, &lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;#ref-R-brms&#34; role=&#34;doc-biblioref&#34;&gt;2020b&lt;/a&gt;)&lt;/span&gt;. In addition to the references I just cited, you can find several helpful vignettes at &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;https://github.com/paul-buerkner/brms&lt;/a&gt;. I’ve also written a few ebooks highlighting &lt;strong&gt;brms&lt;/strong&gt;, which you can find at &lt;a href=&#34;https://solomonkurz.netlify.app/bookdown/&#34;&gt;https://solomonkurz.netlify.app/bookdown/&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll want to be familiar with Bayesian multilevel regression. In addition to the resources, above, I recommend either edition of McElreath’s introductory text &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; or Kruschke’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; introductory text.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll want to be familiar with IRT. The framework in this blog comes most directly from Bürkner’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; preprint. Though I’m not in a position to vouch for them myself, I’ve had people recommend the texts by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-crockerIntroductionToClassical2006&#34; role=&#34;doc-biblioref&#34;&gt;Crocker &amp;amp; Algina&lt;/a&gt; (&lt;a href=&#34;#ref-crockerIntroductionToClassical2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-deayalaTheoryAndPractice2008&#34; role=&#34;doc-biblioref&#34;&gt;De Ayala&lt;/a&gt; (&lt;a href=&#34;#ref-deayalaTheoryAndPractice2008&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-reckaseMultidimensionalIRT2009&#34; role=&#34;doc-biblioref&#34;&gt;Reckase&lt;/a&gt; (&lt;a href=&#34;#ref-reckaseMultidimensionalIRT2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-bonifayMultidimensionalIRT2019&#34; role=&#34;doc-biblioref&#34;&gt;Bonifay&lt;/a&gt; (&lt;a href=&#34;#ref-bonifayMultidimensionalIRT2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-albanoIntroductionToEducational2020&#34; role=&#34;doc-biblioref&#34;&gt;Albano&lt;/a&gt; (&lt;a href=&#34;#ref-albanoIntroductionToEducational2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All code is in &lt;strong&gt;R&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-base&#34; role=&#34;doc-biblioref&#34;&gt;R Core Team, 2020&lt;/a&gt;)&lt;/span&gt;, with healthy doses of the &lt;strong&gt;tidyverse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, 2019&lt;/a&gt;; &lt;a href=&#34;#ref-wickhamWelcomeTidyverse2019&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al., 2019&lt;/a&gt;)&lt;/span&gt;. Probably the best place to learn about the &lt;strong&gt;tidyverse&lt;/strong&gt;-style of coding, as well as an introduction to &lt;strong&gt;R&lt;/strong&gt;, is Grolemund and Wickham’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-grolemundDataScience2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; freely-available online text, &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R for data science&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Load the primary &lt;strong&gt;R&lt;/strong&gt; packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for this post come from the preprint by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;Loram et al.&lt;/a&gt; (&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, who generously shared their data and code on &lt;a href=&#34;https://github.com/Lingtax/2018_measures_study&#34;&gt;GitHub&lt;/a&gt; and the &lt;a href=&#34;https://osf.io/t9w2x/&#34;&gt;Open Science Framework&lt;/a&gt;. In their paper, they used IRT to make a self-report measure of climate change denial. After pruning their initial item set, Loram and colleagues settled on eight binary items for their measure. Here we load the data for those items&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;data/ccdrefined02.rda&amp;quot;)

ccdrefined02 %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 206
## Columns: 8
## $ ccd05 &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0…
## $ ccd18 &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0…
## $ ccd11 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0…
## $ ccd13 &amp;lt;dbl&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0…
## $ ccd08 &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0…
## $ ccd06 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0…
## $ ccd09 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0…
## $ ccd16 &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you walk through the code in Loram and colleagues’ &lt;a href=&#34;https://github.com/Lingtax/2018_measures_study/blob/master/Rcode/2018_Loram_CC_IRT.R&#34;&gt;&lt;code&gt;2018_Loram_CC_IRT.R&lt;/code&gt;&lt;/a&gt; file, you’ll see where this version of the data comes from. For our purposes, we’ll want to make an explicit participant number column and then convert the data to the long format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_long &amp;lt;- ccdrefined02 %&amp;gt;% 
  mutate(id = 1:n()) %&amp;gt;% 
  pivot_longer(-id, names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;y&amp;quot;) %&amp;gt;% 
  mutate(item = str_remove(item, &amp;quot;ccd&amp;quot;))

# what did we do?
head(dat_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##      id item      y
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 05        0
## 2     1 18        0
## 3     1 11        1
## 4     1 13        1
## 5     1 08        0
## 6     1 06        1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now responses (&lt;code&gt;y&lt;/code&gt;) are nested within participants (&lt;code&gt;id&lt;/code&gt;) and items (&lt;code&gt;item&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;irt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IRT&lt;/h2&gt;
&lt;p&gt;In his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; preprint, Bürkner outlined the framework for the multilevel Bayesian approach to IRT, as implemented in &lt;strong&gt;brms&lt;/strong&gt;. In short, IRT allows one to decompose the information from assessment measures into person parameters &lt;span class=&#34;math inline&#34;&gt;\((\theta)\)&lt;/span&gt; and item parameters &lt;span class=&#34;math inline&#34;&gt;\((\xi)\)&lt;/span&gt;. The IRT framework offers a large variety of model types. In this post, we’ll focus on the widely-used 1PL and 2PL models. First, we’ll briefly introduce them within the context of Bürkner’s multilevel Bayesian approach. Then we’ll fit those models to the &lt;code&gt;dat_long&lt;/code&gt; data. Finally, we’ll show how to explore those models using ICC and IIC plots.&lt;/p&gt;
&lt;div id=&#34;what-is-the-1pl&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is the 1PL?&lt;/h3&gt;
&lt;p&gt;With a set of binary data &lt;span class=&#34;math inline&#34;&gt;\(y_{pi}\)&lt;/span&gt;, which vary across &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; persons and &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; items, we can express the simple one-parameter logistic (1PL) model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \theta_p + \xi_i,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(p_{pi}\)&lt;/span&gt; parameter from the Bernoulli distribution indicates the probability of &lt;code&gt;1&lt;/code&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(p\text{th}\)&lt;/span&gt; person on the &lt;span class=&#34;math inline&#34;&gt;\(i\text{th}\)&lt;/span&gt; item. To constrain the model predictions to within the &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt; probability space, we use the logit link. Note that with this parameterization, the linear model itself is just the additive sum of the person parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and item parameter &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Within our multilevel Bayesian framework, we will expand this a bit to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \beta_0 + \theta_p + \xi_i \\
\theta_p &amp;amp; \sim \operatorname{Normal}(0, \sigma_\theta) \\
\xi_i    &amp;amp; \sim \operatorname{Normal}(0, \sigma_\xi),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the new parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the grand mean. Now our &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; parameters are expressed as deviations around the grand mean &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;. As is typical within the multilevel framework, we model these deviations as normally distributed with means set to zero and standard deviations (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\xi\)&lt;/span&gt;) estimated from the data&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To finish off our multilevel Bayesian version the 1PL, we just need to add in our priors. In this blog post, we’ll follow the weakly-regularizing approach and set&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\beta_0 &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\sigma_\theta &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\sigma_\xi    &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; superscripts indicate the Student-&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; priors for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters are restricted to non-negative values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-about-the-2pl&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How about the 2PL?&lt;/h3&gt;
&lt;p&gt;We can express the two-parameter logistic (2PL) model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \alpha_i \theta_p + \alpha_i \xi_i \\
                             &amp;amp; = \alpha_i(\theta_p + \xi_i),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; parameters are now both multiplied by the discrimination parameter &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; subscript indicates the discrimination parameter varies across the items, but not across persons. We should note that because we are now multiplying parameters, this makes the 2PL a non-liner model. Within our multilevel Bayesian framework, we might express the 2PL as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \alpha (\beta_0 + \theta_p + \xi_i) \\
\alpha &amp;amp; = \beta_1 + \alpha_i \\
\theta_p &amp;amp; \sim \operatorname{Normal}(0, \sigma_\theta) \\
\begin{bmatrix} \alpha_i \\ \xi_i \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal}(\mathbf 0, \mathbf \Sigma) \\
\Sigma    &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_\alpha &amp;amp; 0 \\ 0 &amp;amp; \sigma_\xi \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix} ,
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; term is multiplied by &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, in addition to the &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; parameters. But note that &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is itself a composite of its own grand mean &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and the item-level deviations around it, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt;. Since both &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; vary across items, they are modeled as multivariate normal, with a mean vector of zeros and variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt;. As is typical with &lt;strong&gt;brms&lt;/strong&gt;, we will decompose &lt;span class=&#34;math inline&#34;&gt;\(\mathbf \Sigma\)&lt;/span&gt; into a diagonal matrix of standard deviations &lt;span class=&#34;math inline&#34;&gt;\((\mathbf S)\)&lt;/span&gt; and a correlation matrix &lt;span class=&#34;math inline&#34;&gt;\((\mathbf R)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As Bürkner &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; discussed in Section 5, this particular model might have identification problems without strong priors. The issue is “a switch in the sign of &lt;span class=&#34;math inline&#34;&gt;\([\alpha]\)&lt;/span&gt; can be corrected for by a switch in the sign of &lt;span class=&#34;math inline&#34;&gt;\([(\beta_0 + \theta_p + \xi_i)]\)&lt;/span&gt; without a change in the overall likelihood.” One solution, then, would be to constrain &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; to be positive. We can do that with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_{pi} &amp;amp; \sim \operatorname{Bernoulli}(p_{pi}) \\
\operatorname{logit}(p_{pi}) &amp;amp; = \color{#8b0000}{ \exp(\log \alpha) } \color{#000000}{\times (\beta_0 + \theta_p + \xi_i)} \\
\color{#8b0000}{\log \alpha} &amp;amp; = \beta_1 + \alpha_i \\
\theta_p &amp;amp; \sim \operatorname{Normal}(0, \sigma_\theta) \\
\begin{bmatrix} \alpha_i \\ \xi_i \end{bmatrix} &amp;amp; \sim \operatorname{MVNormal}(\mathbf 0, \mathbf \Sigma) \\
\Sigma    &amp;amp; = \mathbf{SRS} \\
\mathbf S &amp;amp; = \begin{bmatrix} \sigma_\alpha &amp;amp; 0 \\ 0 &amp;amp; \sigma_\xi \end{bmatrix} \\
\mathbf R &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix},
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;wherein we are now modeling &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; on the log scale and then exponentiating &lt;span class=&#34;math inline&#34;&gt;\(\log \alpha\)&lt;/span&gt; within the linear formula for &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit}(p_{pi})\)&lt;/span&gt;. Continuing on with our weakly-regularizing approach, we will express our priors for this model as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\beta_0 &amp;amp; \sim \operatorname{Normal}(0, 1.5) \\
\beta_1 &amp;amp; \sim \operatorname{Normal}(0, 1) \\
\sigma_\theta &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\sigma_\alpha &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\sigma_\xi    &amp;amp; \sim \operatorname{Student-t}^+(10, 0, 1) \\
\mathbf R &amp;amp; \sim \operatorname{LKJ}(2),
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where LKJ is the Lewandowski, Kurowicka, and Joe prior for correlation matrices &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lewandowski2009generating&#34; role=&#34;doc-biblioref&#34;&gt;Lewandowski et al., 2009&lt;/a&gt;)&lt;/span&gt;. With &lt;span class=&#34;math inline&#34;&gt;\(\eta = 2\)&lt;/span&gt;, the LKJ weakly regularizes the correlations away from extreme values&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fire-up-brms.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fire up &lt;strong&gt;brms&lt;/strong&gt;.&lt;/h3&gt;
&lt;p&gt;With &lt;code&gt;brms::brm()&lt;/code&gt;, we can fit our 1PL model with conventional multilevel syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;irt1 &amp;lt;- brm(
  data = dat_long,
  family = brmsfamily(&amp;quot;bernoulli&amp;quot;, &amp;quot;logit&amp;quot;),
  y ~ 1 + (1 | item) + (1 | id),
  prior = c(prior(normal(0, 1.5), class = Intercept),
            prior(student_t(10, 0, 1), class = sd)),
  cores = 4, seed = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our non-linear 2PL model, however, will require the &lt;strong&gt;brms&lt;/strong&gt; non-linear syntax &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Bürkner2021Non_linear&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2021&lt;/a&gt;)&lt;/span&gt;. Here we’ll follow the same basic configuration Bürkner used in his &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerBayesianItemResponse2020&#34; role=&#34;doc-biblioref&#34;&gt;2020a&lt;/a&gt;)&lt;/span&gt; IRT preprint.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;irt2 &amp;lt;- brm(
  data = dat_long,
  family = brmsfamily(&amp;quot;bernoulli&amp;quot;, &amp;quot;logit&amp;quot;),
  bf(
    y ~ exp(logalpha) * eta,
    eta ~ 1 + (1 |i| item) + (1 | id),
    logalpha ~ 1 + (1 |i| item),
    nl = TRUE
  ),
  prior = c(prior(normal(0, 1.5), class = b, nlpar = eta),
            prior(normal(0, 1), class = b, nlpar = logalpha),
            prior(student_t(10, 0, 1), class = sd, nlpar = eta),
            prior(student_t(10, 0, 1), class = sd, nlpar = logalpha),
            prior(lkj(2), class = cor)),
  cores = 4, seed = 1,
  control = list(adapt_delta = .99)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that for &lt;code&gt;irt2&lt;/code&gt;, we had to adjust the &lt;code&gt;adapt_delta&lt;/code&gt; settings to stave off a few divergent transitions. Anyway, here are the parameter summaries for the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(irt1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: bernoulli 
##   Links: mu = logit 
## Formula: y ~ 1 + (1 | item) + (1 | id) 
##    Data: dat_long (Number of observations: 1648) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 206) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     3.81      0.38     3.12     4.61 1.01     1129     2107
## 
## ~item (Number of levels: 8) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.96      0.29     0.55     1.67 1.00     1815     2835
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    -2.88      0.47    -3.82    -1.94 1.00     1308     2040
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(irt2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: bernoulli 
##   Links: mu = logit 
## Formula: y ~ exp(logalpha) * eta 
##          eta ~ 1 + (1 | i | item) + (1 | id)
##          logalpha ~ 1 + (1 | i | item)
##    Data: dat_long (Number of observations: 1648) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 206) 
##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(eta_Intercept)     1.78      0.69     0.68     3.39 1.00     1933     2321
## 
## ~item (Number of levels: 8) 
##                                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(eta_Intercept)                         0.50      0.25     0.16     1.16 1.00     1521     2181
## sd(logalpha_Intercept)                    0.36      0.16     0.13     0.74 1.00     1544     2075
## cor(eta_Intercept,logalpha_Intercept)     0.44      0.31    -0.26     0.90 1.00     2847     2922
## 
## Population-Level Effects: 
##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## eta_Intercept         -1.41      0.57    -2.71    -0.51 1.00     1700     2140
## logalpha_Intercept     0.91      0.42     0.16     1.82 1.00     1944     2282
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m not going to bother interpreting these results because, well, this isn’t a full-blown IRT tutorial. For our purposes, we’ll just note that the &lt;span class=&#34;math inline&#34;&gt;\(\widehat R\)&lt;/span&gt; and effective sample size values all look good and nothing seems off with the parameter summaries. They’re not shown here, but the trace plots look good, too. We’re on good footing to explore the models with our ICC and IIC plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iccs.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ICCs.&lt;/h3&gt;
&lt;p&gt;For IRT models of binary items, item-characteristic curves (ICCs) show the expected relation between one’s underlying “ability” and the probability of scoring 1 on a given item. In our models, above, each participant in the data had a their underlying ability estimated by way of the &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters. However, what we want, here, is is to specify the relevant part of the parameter space for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; without reference to any given participant. Since the the 1PL and 2PL models are fit with the logit link, this will mean entertaining &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values ranging within an interval like &lt;span class=&#34;math inline&#34;&gt;\([-4, 4]\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\([-6, 6]\)&lt;/span&gt;. This range will define our &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; axis. Since our &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis has to do with probabilities, it will range from 0 to 1. The trick is knowing how to work with the posterior draws to compute the relevant probability values for their corresponding &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values.&lt;/p&gt;
&lt;p&gt;We’ll start with our 1PL model, &lt;code&gt;irt1&lt;/code&gt;. First, we extract the posterior draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(irt1)

# what is this?
glimpse(post)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m not showing the output for &lt;code&gt;glimpse(post)&lt;/code&gt; because &lt;code&gt;post&lt;/code&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(4{,}000 \times 218\)&lt;/span&gt; data frame and all that output is just too much for a blog post. Here’s a more focused look at the primary columns of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 9
## $ b_Intercept            &amp;lt;dbl&amp;gt; -2.347027, -2.710615, -2.613853, -3.166867, -2.676470, -2.794820, -3.079290, …
## $ `r_item[05,Intercept]` &amp;lt;dbl&amp;gt; -1.6323003, -1.8058115, -1.8621189, -1.6090339, -2.5323344, -1.5353327, -1.43…
## $ `r_item[06,Intercept]` &amp;lt;dbl&amp;gt; 0.1559534455, -0.2575624215, 0.0149043141, 0.4396739597, -0.2644399951, 0.115…
## $ `r_item[08,Intercept]` &amp;lt;dbl&amp;gt; -0.4202201485, -0.3789631590, -0.5076833653, -0.1256742164, -0.9035688171, -0…
## $ `r_item[09,Intercept]` &amp;lt;dbl&amp;gt; 0.16016701, 0.04171247, 0.19869473, 0.92882887, 0.07287032, 0.41171578, 0.935…
## $ `r_item[11,Intercept]` &amp;lt;dbl&amp;gt; -0.80410817, -1.31470954, -0.98800140, -0.58681735, -1.75515227, -1.20933771,…
## $ `r_item[13,Intercept]` &amp;lt;dbl&amp;gt; -0.78569824, -0.68159922, -0.69046836, -0.33288882, -0.96657408, -0.01837113,…
## $ `r_item[16,Intercept]` &amp;lt;dbl&amp;gt; 0.2638089, 0.4226998, 0.1968664, 0.9465201, 0.1394158, 0.3445774, 1.0695805, …
## $ `r_item[18,Intercept]` &amp;lt;dbl&amp;gt; -1.1746881, -1.2400993, -1.4093647, -1.1124993, -1.8502372, -0.9874308, -1.10…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each of our 8 questionnaire items, we compute their conditional probability with the equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y = 1) = \operatorname{logit}^{-1}(\beta_0 + \xi_i + \theta),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit}^{-1}\)&lt;/span&gt; is the inverse logit function&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\exp(x)}{1 + \exp(x)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;brms&lt;/strong&gt;, we have access to the &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{logit}^{-1}\)&lt;/span&gt; function by way of the convenience function called &lt;code&gt;inv_logit_scaled()&lt;/code&gt;. Before we put the &lt;code&gt;inv_logit_scaled()&lt;/code&gt; function to use, we’ll want to rearrange our &lt;code&gt;post&lt;/code&gt; samples into the long format so that all the &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; draws for each of the eight items are nested within a single column, which we’ll call &lt;code&gt;xi&lt;/code&gt;. We’ll index which draw corresponds to which of the eight items with a nominal &lt;code&gt;item&lt;/code&gt; column. And to make this all work within the context of 4,000 posterior draws, we’ll also need to make an iteration index, which we’ll call &lt;code&gt;iter&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;), names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;xi&amp;quot;) %&amp;gt;% 
  mutate(item = str_extract(item, &amp;quot;\\d+&amp;quot;)) 

# what is this?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   b_Intercept  iter item      xi
##         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
## 1       -2.35     1 05    -1.63 
## 2       -2.35     1 06     0.156
## 3       -2.35     1 08    -0.420
## 4       -2.35     1 09     0.160
## 5       -2.35     1 11    -0.804
## 6       -2.35     1 13    -0.786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to compute our probabilities, conditional in different ability &lt;span class=&#34;math inline&#34;&gt;\((\theta)\)&lt;/span&gt; levels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  expand(nesting(iter, b_Intercept, item, xi),
         theta = seq(from = -6, to = 6, length.out = 100)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(b_Intercept + xi + theta)) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(p = mean(p))

# what have we done?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
## # Groups:   theta [1]
##   theta item          p
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1    -6 05    0.0000364
## 2    -6 06    0.000241 
## 3    -6 08    0.000162 
## 4    -6 09    0.000300 
## 5    -6 11    0.0000797
## 6    -6 13    0.000121&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With those summaries in hand, it’s trivial to make the ICC plot with good old &lt;strong&gt;ggplot2&lt;/strong&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  ggplot(aes(x = theta, y = p, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;ICCs for the 1PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior mean.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = expression(italic(p)(y==1))) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since each item had a relatively low response probability, you have to go pretty far into the right-hand side of the &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; range before the curves start to approach the top of the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis.&lt;/p&gt;
&lt;p&gt;To make the ICCs for the 2PL model, the data wrangling will require a couple more steps. First, we extract the posterior draws and take a quick look at the columns of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(irt2) 

# what do we care about?
post %&amp;gt;% 
  select(b_eta_Intercept, b_logalpha_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 18
## $ b_eta_Intercept                  &amp;lt;dbl&amp;gt; -1.3855075, -1.4968294, -1.6918246, -1.6252046, -1.7095222, -1.4090…
## $ b_logalpha_Intercept             &amp;lt;dbl&amp;gt; 0.7814705, 0.6435611, 0.3971659, 0.5084503, 0.7547493, 1.2154113, 1…
## $ `r_item__eta[05,Intercept]`      &amp;lt;dbl&amp;gt; -0.7922848, -0.7423704, -1.1893399, -1.0718978, -0.2636685, -0.3965…
## $ `r_item__eta[06,Intercept]`      &amp;lt;dbl&amp;gt; 0.26895443, 0.21153136, 0.24488884, 0.20332673, 0.52650711, 0.51766…
## $ `r_item__eta[08,Intercept]`      &amp;lt;dbl&amp;gt; 0.1730248531, -0.0251183662, 0.0966877581, -0.0626794892, 0.3235805…
## $ `r_item__eta[09,Intercept]`      &amp;lt;dbl&amp;gt; 0.50089389, 0.39236895, 0.35678623, 0.41680088, 0.66285740, 0.53730…
## $ `r_item__eta[11,Intercept]`      &amp;lt;dbl&amp;gt; -0.17619531, -1.00068058, -0.41426442, -0.71686852, -0.15409689, -0…
## $ `r_item__eta[13,Intercept]`      &amp;lt;dbl&amp;gt; 0.02890276, -0.19689487, 0.12965889, -0.13639774, 0.35999998, 0.250…
## $ `r_item__eta[16,Intercept]`      &amp;lt;dbl&amp;gt; 0.5650797, 0.3484697, 0.5692639, 0.4691862, 0.5286982, 0.7387770, 0…
## $ `r_item__eta[18,Intercept]`      &amp;lt;dbl&amp;gt; -0.03100911, -0.94785287, -0.40602161, -1.06680784, -0.29232278, 0.…
## $ `r_item__logalpha[05,Intercept]` &amp;lt;dbl&amp;gt; -0.445733584, -0.168897075, -0.175487957, -0.023652177, -0.38447241…
## $ `r_item__logalpha[06,Intercept]` &amp;lt;dbl&amp;gt; 0.106865933, 0.183709540, 0.352641061, 0.372592174, 0.606879092, 0.…
## $ `r_item__logalpha[08,Intercept]` &amp;lt;dbl&amp;gt; 0.279715442, 0.224570579, 0.613387942, 0.344415036, 0.249641920, -0…
## $ `r_item__logalpha[09,Intercept]` &amp;lt;dbl&amp;gt; 0.46984590, 0.59974910, 0.61973239, 0.57372330, 0.67782554, 0.38758…
## $ `r_item__logalpha[11,Intercept]` &amp;lt;dbl&amp;gt; -0.23306444, -0.85055396, -0.31303715, -0.60098567, -0.40559846, -0…
## $ `r_item__logalpha[13,Intercept]` &amp;lt;dbl&amp;gt; -0.008225223, 0.116253481, 0.122579334, 0.291166675, 0.689798894, 0…
## $ `r_item__logalpha[16,Intercept]` &amp;lt;dbl&amp;gt; 0.092113896, 0.104240677, 0.092359783, 0.097844755, 0.494488271, 0.…
## $ `r_item__logalpha[18,Intercept]` &amp;lt;dbl&amp;gt; 0.09498269, -0.15278053, 0.01792382, -0.01208098, -0.31325790, -0.0…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now there are 16 &lt;code&gt;r_item__&lt;/code&gt; columns, half of which correspond to the &lt;span class=&#34;math inline&#34;&gt;\(\xi_i\)&lt;/span&gt; deviations and the other half of which correspond to the &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; deviations. In addition, we also have the &lt;code&gt;b_logalpha_Intercept&lt;/code&gt; columns to content with. So this time, we’ll follow up our &lt;code&gt;pivot_longer()&lt;/code&gt; code with subsequent &lt;code&gt;mutate()&lt;/code&gt; and &lt;code&gt;select()&lt;/code&gt; steps, and complete the task with &lt;code&gt;pivot_wider()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  select(b_eta_Intercept, b_logalpha_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(item      = str_extract(name, &amp;quot;\\d+&amp;quot;),
         parameter = ifelse(str_detect(name, &amp;quot;eta&amp;quot;), &amp;quot;xi&amp;quot;, &amp;quot;logalpha&amp;quot;)) %&amp;gt;% 
  select(-name) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value)

# what does this look like, now?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   b_eta_Intercept b_logalpha_Intercept  iter item       xi logalpha
##             &amp;lt;dbl&amp;gt;                &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1           -1.39                0.781     1 05    -0.792  -0.446  
## 2           -1.39                0.781     1 06     0.269   0.107  
## 3           -1.39                0.781     1 08     0.173   0.280  
## 4           -1.39                0.781     1 09     0.501   0.470  
## 5           -1.39                0.781     1 11    -0.176  -0.233  
## 6           -1.39                0.781     1 13     0.0289 -0.00823&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this configuration, it’s only a little more complicated to compute the probability summaries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- post %&amp;gt;% 
  expand(nesting(iter, b_eta_Intercept, b_logalpha_Intercept, item, xi, logalpha),
         theta = seq(from = -6, to = 6, length.out = 100)) %&amp;gt;% 
  # note the difference in the equation
  mutate(p = inv_logit_scaled(exp(b_logalpha_Intercept + logalpha) * (b_eta_Intercept + theta + xi))) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(p = mean(p))

# what have we done?
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
## # Groups:   theta [1]
##   theta item           p
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;
## 1    -6 05    0.0000152 
## 2    -6 06    0.00000543
## 3    -6 08    0.00000967
## 4    -6 09    0.00000105
## 5    -6 11    0.000127  
## 6    -6 13    0.00000111&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
  ggplot(aes(x = theta, y = p, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;ICCs for the 2PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior mean.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = expression(italic(p)(y==1))) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like those &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; parameters made a big difference for the ICCs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iics.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;IICs.&lt;/h3&gt;
&lt;p&gt;From a computational standpoint, item information curves (IICs) are a transformation of the ICCs. Recall that the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis for the ICC is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, the probability &lt;span class=&#34;math inline&#34;&gt;\(y = 1\)&lt;/span&gt; for a given item. For the IIC plots, the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis shows information, which is a simple transformation of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, following the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{information} = p(1 - p).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So here’s how to use that equation and make the IIC plot for our 1PL model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these wrangling steps are all the same as before
posterior_samples(irt1) %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;), names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;xi&amp;quot;) %&amp;gt;% 
  mutate(item = str_extract(item, &amp;quot;\\d+&amp;quot;)) %&amp;gt;% 
  expand(nesting(iter, b_Intercept, item, xi),
         theta = seq(from = -6, to = 6, length.out = 200)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(b_Intercept + xi + theta)) %&amp;gt;% 
  
  # this part, right here, is what&amp;#39;s new
  mutate(i = p * (1 - p)) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(i = median(i)) %&amp;gt;%
  
  # now plot!
  ggplot(aes(x = theta, y = i, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;IICs for the 1PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior median.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = &amp;quot;information&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For kicks and giggles, we used the posterior medians, rather than the means. It’s similarly easy to compute the item-level information for the 2PL.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these wrangling steps are all the same as before
posterior_samples(irt2) %&amp;gt;% 
  select(b_eta_Intercept, b_logalpha_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(item      = str_extract(name, &amp;quot;\\d+&amp;quot;),
         parameter = ifelse(str_detect(name, &amp;quot;eta&amp;quot;), &amp;quot;xi&amp;quot;, &amp;quot;logalpha&amp;quot;)) %&amp;gt;% 
  select(-name) %&amp;gt;% 
  pivot_wider(names_from = parameter, values_from = value) %&amp;gt;% 
  expand(nesting(iter, b_eta_Intercept, b_logalpha_Intercept, item, xi, logalpha),
         theta = seq(from = -6, to = 6, length.out = 200)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(exp(b_logalpha_Intercept + logalpha) * (b_eta_Intercept + theta + xi))) %&amp;gt;% 

  # again, here&amp;#39;s the new part
  mutate(i = p * (1 - p)) %&amp;gt;% 
  group_by(theta, item) %&amp;gt;% 
  summarise(i = median(i)) %&amp;gt;%
  
  # now plot!
  ggplot(aes(x = theta, y = i, color = item)) +
  geom_line() +
  scale_color_viridis_d(option = &amp;quot;H&amp;quot;) +
  labs(title = &amp;quot;IICs for the 2PL&amp;quot;,
       subtitle = &amp;quot;Each curve is based on the posterior median.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = &amp;quot;information&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;tic.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;TIC.&lt;/h4&gt;
&lt;p&gt;Sometimes researchers want to get a overall sense of the information in a group of items. For simplicity, here, we’ll just call groups of items a &lt;em&gt;test&lt;/em&gt;. The test information curve (TIC) is a special case of the IIC, but applied to the whole test. In short, you compute the TIC by summing up the information for the individual items at each level of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Using the 1PL as an example, here’s how we might do that by hand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(irt1) %&amp;gt;% 
  select(b_Intercept, starts_with(&amp;quot;r_item&amp;quot;)) %&amp;gt;% 
  mutate(iter = 1:n()) %&amp;gt;% 
  pivot_longer(starts_with(&amp;quot;r_item&amp;quot;), names_to = &amp;quot;item&amp;quot;, values_to = &amp;quot;xi&amp;quot;) %&amp;gt;% 
  mutate(item = str_extract(item, &amp;quot;\\d+&amp;quot;)) %&amp;gt;% 
  expand(nesting(iter, b_Intercept, item, xi),
         theta = seq(from = -6, to = 6, length.out = 200)) %&amp;gt;% 
  mutate(p = inv_logit_scaled(b_Intercept + xi + theta)) %&amp;gt;% 
  mutate(i = p * (1 - p)) %&amp;gt;% 
  
  # this is where the TIC magic happens
  group_by(theta, iter) %&amp;gt;% 
  summarise(sum_i = sum(i)) %&amp;gt;% 
  group_by(theta) %&amp;gt;% 
  summarise(i = median(sum_i)) %&amp;gt;%
  
  # we plot
  ggplot(aes(x = theta, y = i)) +
  geom_line() +
  labs(title = &amp;quot;The test information curve for the 1PL&amp;quot;,
       subtitle = &amp;quot;The curve is based on the posterior median.&amp;quot;, 
       x = expression(theta~(&amp;#39;ability on the logit scale&amp;#39;)),
       y = &amp;quot;information&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Taken as a whole, the combination of the eight items &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;Loram et al.&lt;/a&gt; (&lt;a href=&#34;#ref-loramValidationOfANovel2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; settled on does a reasonable job differentiating among those with high &lt;span class=&#34;math inline&#34;&gt;\(\theta_p\)&lt;/span&gt; values. But this combination of items isn’t going to be the best at differentiating among those on the lower end of the &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; scale. You might say these eight items make for a difficult test.&lt;/p&gt;
&lt;p&gt;Our method of extending the 1PL IIC to the TIC should work the same for the 2PL. I’ll leave it as an exercise for the interested reader.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;We might outlines the steps in this post as:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fit your &lt;strong&gt;brms&lt;/strong&gt; IRT model.&lt;/li&gt;
&lt;li&gt;Inspect the model with all your standard quality checks (e.g., &lt;span class=&#34;math inline&#34;&gt;\(\widehat R\)&lt;/span&gt; values, trace plots).&lt;/li&gt;
&lt;li&gt;Extract your posterior draws with the &lt;code&gt;posterior_samples()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;Isolate the item-related columns. Within the multilevel IRT context, this will typically involve an overall intercept (e.g., &lt;code&gt;b_Intercept&lt;/code&gt; for our 1PL &lt;code&gt;irt1&lt;/code&gt;) and item-specific deviations (e.g., the columns starting with &lt;code&gt;r_item&lt;/code&gt; in our 1PL &lt;code&gt;irt1&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Arrange the data into a format that makes it easy to add the overall intercept in question to each of the item-level deviations in question. For me, this seemed easiest with the long format via the &lt;code&gt;pivot_longer()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;Expand the data over a range of ability &lt;span class=&#34;math inline&#34;&gt;\((\theta)\)&lt;/span&gt; values. For me, this worked well with the &lt;code&gt;expand()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;Use the model-implied formula to compute the &lt;span class=&#34;math inline&#34;&gt;\(p(y = 1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Group the results by item and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and summarize the &lt;span class=&#34;math inline&#34;&gt;\(p(y = 1)\)&lt;/span&gt; distributions with something like the mean or median.&lt;/li&gt;
&lt;li&gt;Plot the results with &lt;code&gt;ggplot2::geom_line()&lt;/code&gt; and friends.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;You should be able to generalize this workflow to IRT models for data with more than two categories. You’ll just have to be careful about juggling your thresholds. You might find some inspiration along these lines &lt;a href=&#34;https://bookdown.org/content/4857/monsters-and-mixtures.html#ordered-categorical-outcomes&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://bookdown.org/content/3686/ordinal-predicted-variable.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You could totally switch up this workflow to use some of the data wrangling helpers from the &lt;a href=&#34;https://CRAN.R-project.org/package=tidybayes&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-R-tidybayes&#34; role=&#34;doc-biblioref&#34;&gt;Kay, 2020&lt;/a&gt;)&lt;/span&gt;. That could be a nifty little blog post in and of itself.&lt;/p&gt;
&lt;p&gt;One thing that’s super lame about conventional ICC/IIC plots is there’s no expression of uncertainty. To overcome that, you could compute the 95% intervals (or 50% or whatever) in the same &lt;code&gt;summarise()&lt;/code&gt; line where you computed the mean and then express those interval bounds with something like &lt;code&gt;geom_ribbon()&lt;/code&gt; in your plot. The difficulty I foresee is it will result in overplotting for any models with more than like five items. Perhaps faceting would be the solution, there.&lt;/p&gt;
&lt;p&gt;I’m no IRT jock and may have goofed some of the steps or equations. To report mistakes or provide any other constructive criticism, just chime in on this Twitter thread:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New blog up!&lt;a href=&#34;https://t.co/ZOy9Cxrqat&#34;&gt;https://t.co/ZOy9Cxrqat&lt;/a&gt;&lt;br&gt;&lt;br&gt;This time we practice making ICC plots for &lt;a href=&#34;https://twitter.com/hashtag/brms?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#brms&lt;/a&gt;-based multilevel IRT models.&lt;/p&gt;&amp;mdash; Solomon Kurz (@SolomonKurz) &lt;a href=&#34;https://twitter.com/SolomonKurz/status/1409951540228628482?ref_src=twsrc%5Etfw&#34;&gt;June 29, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.6     purrr_0.3.4    
##  [7] readr_1.4.0     tidyr_1.1.3     tibble_3.1.2    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6         splines_4.0.4       
##   [6] crosstalk_1.1.0.1    TH.data_1.0-10       rstantools_2.1.1     inline_0.3.17        digest_0.6.27       
##  [11] htmltools_0.5.1.1    rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [16] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0       prettyunits_1.1.1   
##  [21] colorspace_2.0-0     rvest_0.3.6          haven_2.3.1          xfun_0.23            callr_3.7.0         
##  [26] crayon_1.4.1         jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [31] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0             pkgbuild_1.2.0      
##  [36] rstan_2.21.2         abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0           
##  [41] miniUI_0.1.1.1       viridisLite_0.4.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [46] DT_0.16              htmlwidgets_1.5.3    httr_1.4.2           threejs_0.3.3        ellipsis_0.3.2      
##  [51] farver_2.1.0         pkgconfig_2.0.3      loo_2.4.1            sass_0.3.1           dbplyr_2.0.0        
##  [56] utf8_1.2.1           labeling_0.4.2       tidyselect_1.1.1     rlang_0.4.11         reshape2_1.4.4      
##  [61] later_1.2.0          munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.5.0           
##  [66] generics_0.1.0       broom_0.7.6          ggridges_0.5.3       evaluate_0.14        fastmap_1.1.0       
##  [71] yaml_2.2.1           processx_3.5.2       knitr_1.33           fs_1.5.0             nlme_3.1-152        
##  [76] mime_0.10            projpred_2.0.2       xml2_1.3.2           rstudioapi_0.13      compiler_4.0.4      
##  [81] bayesplot_1.8.0      shinythemes_1.1.2    curl_4.3             gamm4_0.2-6          reprex_0.3.0        
##  [86] statmod_1.4.35       bslib_0.2.4          stringi_1.6.2        highr_0.9            ps_1.6.0            
##  [91] blogdown_1.3         Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [96] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.8          pillar_1.6.1         lifecycle_1.0.0     
## [101] jquerylib_0.1.4      bridgesampling_1.0-0 estimability_1.3     httpuv_1.6.0         R6_2.5.0            
## [106] bookdown_0.22        promises_1.2.0.1     gridExtra_2.3        codetools_0.2-18     boot_1.3-26         
## [111] colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2         assertthat_0.2.1     withr_2.4.2         
## [116] shinystan_2.5.0      multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.8        shiny_1.6.0         
## [126] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-albanoIntroductionToEducational2020&#34; class=&#34;csl-entry&#34;&gt;
Albano, T. (2020). &lt;em&gt;Introduction to educational and psychological measurement using &lt;span&gt;R&lt;/span&gt;&lt;/em&gt;. &lt;a href=&#34;https://www.thetaminusb.com/intro-measurement-r/&#34;&gt;https://www.thetaminusb.com/intro-measurement-r/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bonifayMultidimensionalIRT2019&#34; class=&#34;csl-entry&#34;&gt;
Bonifay, W. (2019). &lt;em&gt;Multidimensional item response theory&lt;/em&gt;. &lt;span&gt;SAGE Publications&lt;/span&gt;. &lt;a href=&#34;https://us.sagepub.com/en-us/nam/multidimensional-item-response-theory/book257740&#34;&gt;https://us.sagepub.com/en-us/nam/multidimensional-item-response-theory/book257740&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBayesianItemResponse2020&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020a). Bayesian item response modeling in &lt;span&gt;R&lt;/span&gt; with brms and &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;arXiv:1905.09501 [Stat]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/1905.09501&#34;&gt;http://arxiv.org/abs/1905.09501&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Bürkner2021Non_linear&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2021). &lt;em&gt;Estimating non-linear models with brms&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&#34;&gt;https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerBrmsPackageBayesian2017&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2017). &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;An R&lt;/span&gt; package for &lt;span&gt;Bayesian&lt;/span&gt; multilevel models using &lt;span&gt;Stan&lt;/span&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;80&lt;/em&gt;(1), 1–28. &lt;a href=&#34;https://doi.org/10.18637/jss.v080.i01&#34;&gt;https://doi.org/10.18637/jss.v080.i01&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package brms. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://doi.org/10.32614/RJ-2018-017&#34;&gt;https://doi.org/10.32614/RJ-2018-017&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2020b). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;: &lt;span&gt;Bayesian&lt;/span&gt; regression models using ’&lt;span&gt;Stan&lt;/span&gt;’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=brms&#34;&gt;https://CRAN.R-project.org/package=brms&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-crockerIntroductionToClassical2006&#34; class=&#34;csl-entry&#34;&gt;
Crocker, L., &amp;amp; Algina, J. (2006). &lt;em&gt;Introduction to classical and modern test theory&lt;/em&gt;. &lt;span&gt;Cengage Learning&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-deayalaTheoryAndPractice2008&#34; class=&#34;csl-entry&#34;&gt;
De Ayala, R. J. (2008). &lt;em&gt;The theory and practice of item response theory&lt;/em&gt;. &lt;span&gt;Guilford Publications&lt;/span&gt;. &lt;a href=&#34;https://www.guilford.com/books/The-Theory-and-Practice-of-Item-Response-Theory/R-de-Ayala/9781593858698&#34;&gt;https://www.guilford.com/books/The-Theory-and-Practice-of-Item-Response-Theory/R-de-Ayala/9781593858698&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-grolemundDataScience2017&#34; class=&#34;csl-entry&#34;&gt;
Grolemund, G., &amp;amp; Wickham, H. (2017). &lt;em&gt;R for data science&lt;/em&gt;. &lt;span&gt;O’Reilly&lt;/span&gt;. &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;https://r4ds.had.co.nz&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidybayes&#34; class=&#34;csl-entry&#34;&gt;
Kay, M. (2020). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidybayes&lt;/span&gt;: &lt;span&gt;Tidy&lt;/span&gt; data and ’geoms’ for &lt;span&gt;Bayesian&lt;/span&gt; models&lt;/em&gt;. &lt;a href=&#34;https://mjskay.github.io/tidybayes/&#34;&gt;https://mjskay.github.io/tidybayes/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeDoingBayesianData2015&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K. (2015). &lt;em&gt;Doing &lt;span&gt;Bayesian&lt;/span&gt; data analysis: &lt;span&gt;A&lt;/span&gt; tutorial with &lt;span&gt;R&lt;/span&gt;, &lt;span&gt;JAGS&lt;/span&gt;, and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;Academic Press&lt;/span&gt;. &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;https://sites.google.com/site/doingbayesiandataanalysis/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lewandowski2009generating&#34; class=&#34;csl-entry&#34;&gt;
Lewandowski, D., Kurowicka, D., &amp;amp; Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt;, &lt;em&gt;100&lt;/em&gt;(9), 1989–2001. &lt;a href=&#34;https://doi.org/10.1016/j.jmva.2009.04.008&#34;&gt;https://doi.org/10.1016/j.jmva.2009.04.008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-loramValidationOfANovel2019&#34; class=&#34;csl-entry&#34;&gt;
Loram, G., Ling, M., Head, A., &amp;amp; Clarke, E. J. R. (2019). &lt;em&gt;Validation of a novel climate change denial measure using item response theory&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.31234/osf.io/57nbk&#34;&gt;https://doi.org/10.31234/osf.io/57nbk&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2020&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2020). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt; (Second Edition). &lt;span&gt;CRC Press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcelreathStatisticalRethinkingBayesian2015&#34; class=&#34;csl-entry&#34;&gt;
McElreath, R. (2015). &lt;em&gt;Statistical rethinking: &lt;span&gt;A Bayesian&lt;/span&gt; course with examples in &lt;span&gt;R&lt;/span&gt; and &lt;span&gt;Stan&lt;/span&gt;&lt;/em&gt;. &lt;span&gt;CRC press&lt;/span&gt;. &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;https://xcelab.net/rm/statistical-rethinking/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. (2020). &lt;em&gt;R: &lt;span&gt;A&lt;/span&gt; language and environment for statistical computing&lt;/em&gt;. &lt;span&gt;R Foundation for Statistical Computing&lt;/span&gt;. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-reckaseMultidimensionalIRT2009&#34; class=&#34;csl-entry&#34;&gt;
Reckase, M. D. (2009). &lt;em&gt;Multidimensional item response theory models&lt;/em&gt;. &lt;span&gt;Springer&lt;/span&gt;. &lt;a href=&#34;https://www.springer.com/gp/book/9780387899756&#34;&gt;https://www.springer.com/gp/book/9780387899756&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. (2019). &lt;em&gt;&lt;span class=&#34;nocase&#34;&gt;tidyverse&lt;/span&gt;: &lt;span&gt;Easily&lt;/span&gt; install and load the ’tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickhamWelcomeTidyverse2019&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I should disclose that although I have not read through Bonifay’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bonifayMultidimensionalIRT2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; text, he offered to send me a copy around the time I uploaded this post.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;You can find a copy of these data on my GitHub &lt;a href=&#34;https://github.com/ASKurz/blogdown/tree/main/content/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/data&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Adopting the three-term multilevel structure–&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \theta_p + \xi_i\)&lt;/span&gt;, where the latter two terms are &lt;span class=&#34;math inline&#34;&gt;\(\operatorname{Normal}(0, \sigma_x)\)&lt;/span&gt;–places this form of the 1PL model squarely within the generalized linear multilevel model (GLMM). McElreath &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcelreathStatisticalRethinkingBayesian2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;, Chapter 12)&lt;/span&gt; referred to this particular model type as a cross-classified model. Coming from another perspective, Kruschke &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kruschkeDoingBayesianData2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;, Chapters 19 and 20)&lt;/span&gt; described this as a kind of multilevel analysis of variance (ANOVA).&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;For a nice blog post on the LKJ, check out Stephen Martin’s &lt;a href=&#34;http://srmart.in/is-the-lkj1-prior-uniform-yes/&#34;&gt;&lt;em&gt;Is the LKJ(1) prior uniform? “Yes”&lt;/em&gt;&lt;/a&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Make model diagrams, Kruschke style</title>
      <link>/post/2020-03-09-make-model-diagrams-kruschke-style/</link>
      <pubDate>Mon, 09 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-03-09-make-model-diagrams-kruschke-style/</guid>
      <description>
&lt;script src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;You too can make model diagrams with the &lt;strong&gt;tidyverse&lt;/strong&gt; and &lt;strong&gt;patchwork&lt;/strong&gt; packages. Here’s how.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diagrams-can-help-us-understand-statistical-models.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagrams can help us understand statistical models.&lt;/h2&gt;
&lt;p&gt;I’ve been working through John Kruschke’s &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;&lt;em&gt;Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan&lt;/em&gt;&lt;/a&gt; and translating it into &lt;strong&gt;brms&lt;/strong&gt; and &lt;strong&gt;tidyverse&lt;/strong&gt;-style workflow. At this point, the bulk of the work is done and you can check it out at &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;https://bookdown.org/content/3686/&lt;/a&gt;. One of Kruschke’s unique contributions was the way he used diagrams to depict his statistical models. Here’s an example from the text (Figure 8.2 on page 196):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kruschke_figure8.2.png&#34; style=&#34;width:33.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the figure’s caption, we read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Diagram of model with Bernoulli likelihood and beta prior. The pictures of the distributions are intended as stereotypical icons, and are not meant to indicate the exact forms of the distributions. Diagrams like this should be scanned from the bottom up, starting with the data &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; and working upward through the likelihood function and prior distribution. Every arrow in the diagram has a corresponding line of code in a JAGS model specification.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Making diagrams like this is a bit of a challenge because even Kruchke, who is no &lt;strong&gt;R&lt;/strong&gt; slouch, used other software to make his diagrams. In the comments section from his blog post, &lt;a href=&#34;http://doingbayesiandataanalysis.blogspot.com/2012/05/graphical-model-diagrams-in-doing.html&#34;&gt;&lt;em&gt;Graphical model diagrams in Doing Bayesian Data Analysis versus traditional convention&lt;/em&gt;&lt;/a&gt;, Kruschke remarked he made these “‘by hand’ in OpenOffice.” If you look over to the &lt;a href=&#34;https://tex.stackexchange.com/questions/55869/how-to-produce-john-kruschkes-bayesian-model-diagrams-using-tikz-or-similar-too&#34;&gt;&lt;em&gt;How to produce John Kruschke’s Bayesian model diagrams using TikZ or similar tools?&lt;/em&gt;&lt;/a&gt; thread in StackExchange, you’ll find a workflow to make plots like this with TikZ. In a related &lt;a href=&#34;https://github.com/rasmusab/distribution_diagrams/blob/master/readme.md&#34;&gt;GitHub repo&lt;/a&gt;, the great Rasmus Bååth showed how to make diagrams like this with a combination of base &lt;strong&gt;R&lt;/strong&gt; and &lt;a href=&#34;https://www.libreoffice.org/discover/draw/&#34;&gt;Libre Office Draw&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It’d be nice, however, if one could make plots like this entirely within &lt;strong&gt;R&lt;/strong&gt;, preferably with a &lt;strong&gt;tidyverse&lt;/strong&gt;-style workflow. With help from the handy new &lt;a href=&#34;https://CRAN.R-project.org/package=patchwork&#34;&gt;&lt;strong&gt;patchwork&lt;/strong&gt; package&lt;/a&gt;, I believe we can make it work. In this post, I’ll walk through a few attempts.&lt;/p&gt;
&lt;div id=&#34;my-assumptions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;My assumptions.&lt;/h3&gt;
&lt;p&gt;For the sake of this post, I’m presuming you’re familiar with &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html&#34;&gt;&lt;strong&gt;R&lt;/strong&gt;&lt;/a&gt;, aware of the &lt;a href=&#34;https://www.rstudio.com/resources/videos/data-science-in-the-tidyverse/&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt;, and have fit a &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&#34;&gt;Bayesian model&lt;/a&gt; or two.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;figure-8.2-keep-it-simple.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Figure 8.2: Keep it simple.&lt;/h3&gt;
&lt;p&gt;One way to conceptualize Figure 8.2, above, is to break it down into discrete parts. To my mind, there are five. Starting from the top and going down, we have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a plot of a beta density,&lt;/li&gt;
&lt;li&gt;an annotated arrow,&lt;/li&gt;
&lt;li&gt;a bar plot of Bernoulli data,&lt;/li&gt;
&lt;li&gt;another annotated arrow, and&lt;/li&gt;
&lt;li&gt;some text.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we make and save each component separately with &lt;strong&gt;ggplot2&lt;/strong&gt;, we can then combine them with &lt;strong&gt;patchwork&lt;/strong&gt; syntax. First we’ll load the necessary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork)
library(ggforce)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We won’t need &lt;strong&gt;ggforce&lt;/strong&gt; for this first diagram, but it’ll come in handy in the next section. Before we start making our subplots, we can use the &lt;code&gt;ggplot2::theme_set()&lt;/code&gt; function to adjust the global theme.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_grey() +
            theme_void() +
            theme(plot.margin = margin(0, 5.5, 0, 5.5)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we’ll make the 5 subplots, saving them as &lt;code&gt;p1&lt;/code&gt;, &lt;code&gt;p2&lt;/code&gt;, and so on. Since I’m presuming a working fluency with &lt;strong&gt;ggplot2&lt;/strong&gt; and &lt;strong&gt;tidyverse&lt;/strong&gt; basics, I’m not going to explain the plot code in detail. If you’re new to plotting like this, execute the code for a given plot line by line to see how each layer builds on the last.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot of a beta density
p1 &amp;lt;-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;beta&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .6,
           label = &amp;quot;italic(A)*&amp;#39;, &amp;#39;*italic(B)&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))

## an annotated arrow
# save our custom arrow settings
my_arrow &amp;lt;- arrow(angle = 20, length = unit(0.35, &amp;quot;cm&amp;quot;), type = &amp;quot;closed&amp;quot;)
p2 &amp;lt;-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = 0) %&amp;gt;%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .375, y = 1/3,
           label = &amp;quot;&amp;#39;~&amp;#39;&amp;quot;,
           size = 10, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(0, 1)

# bar plot of Bernoulli data
p3 &amp;lt;-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = &amp;quot;skyblue&amp;quot;, width = .4) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;Bernoulli&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .94,
           label = &amp;quot;theta&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-.75, 1.75) +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p4 &amp;lt;-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c(&amp;quot;&amp;#39;~&amp;#39;&amp;quot;, &amp;quot;italic(i)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = &amp;quot;Times&amp;quot;) +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1)

# some text
p5 &amp;lt;-
  tibble(x     = 1,
         y     = .5,
         label = &amp;quot;italic(y[i])&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = &amp;quot;Times&amp;quot;) +
  xlim(0, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ve saved each of the components as subplots, we can combine them with a little &lt;strong&gt;patchwork&lt;/strong&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout &amp;lt;- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 3, b = 3, l = 1, r = 1),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 7, l = 1, r = 1)
)

(p1 + p2 + p3 + p4 + p5) + 
  plot_layout(design = layout) &amp;amp;
  ylim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;192&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For that plot, the settings in the R Markdown code chunk were &lt;code&gt;fig.width = 2, fig.height = 3.5&lt;/code&gt;. An obvious difference between our plot and Kruschke’s is whereas he depicted the beta density with a line, we used &lt;code&gt;geom_area()&lt;/code&gt; to make the shape a solid blue. If you prefer Kruschke’s approach, just use something like &lt;code&gt;geom_line()&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;Within some of the &lt;code&gt;annotate()&lt;/code&gt; and &lt;code&gt;geom_text()&lt;/code&gt; functions, above, you may have noticed we set &lt;code&gt;parse = T&lt;/code&gt;. Though it wasn’t always necessary, it helps streamline the workflow. I found this particularly helpful when setting the coordinates for the tildes (i.e., the &lt;span class=&#34;math inline&#34;&gt;\(\sim\)&lt;/span&gt; signs).&lt;/p&gt;
&lt;p&gt;The main thing to focus on is the &lt;strong&gt;patchwork&lt;/strong&gt; syntax from that last code block. We combined the five subplots with the &lt;code&gt;(p1 + p2 + p3 + p4 + p5)&lt;/code&gt; code. It was the &lt;code&gt;plot_layout(design = layout)&lt;/code&gt; part and the associated code defining &lt;code&gt;layout&lt;/code&gt; that helped us arrange the subplots in the right order and according to the desired size ratios. For each subplot, we used the &lt;code&gt;t&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;l&lt;/code&gt;, and &lt;code&gt;r&lt;/code&gt; parameters to define the four bounds (top, bottom, left, and right) in overall plot grid. You can learn more about how this works from Thomas Lin Pedersen’s &lt;a href=&#34;https://patchwork.data-imaginist.com/articles/guides/layout.html&#34;&gt;&lt;em&gt;Controlling Layouts&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://patchwork.data-imaginist.com/reference/area.html&#34;&gt;&lt;em&gt;Specify a plotting area in a layout&lt;/em&gt;&lt;/a&gt; vignettes.&lt;/p&gt;
&lt;p&gt;Now we’ve covered the basics, it’s time to build.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;figure-9.1-add-an-offset-formula-and-some-curvy-lines.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Figure 9.1: Add an offset formula and some curvy lines.&lt;/h3&gt;
&lt;p&gt;For our next challenge, we’ll tackle Kruschke’s Figure 9.1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kruschke_figure9.1.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From a statistical perspective, this model is interesting in that it uses a hierarchical prior specification wherein the lower-level beta density is parameterized in terms &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; (mode) and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; (concentration). From a plotting perspective, adding more density and arrow subplots isn’t a big deal. But see how the &lt;span class=&#34;math inline&#34;&gt;\(\omega(K-2)+1, (1-\omega)(K-2)+1\)&lt;/span&gt; formula extends way out past the right bound of that second beta density? Also, check those wavy arrows right above. These require an amended workflow. Let’s go step by step. The top subplot is fairly simple.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;-
  tibble(x = seq(from = .01, to = .99, by = .01),
       d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&amp;gt;% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;beta&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .6,
           label = &amp;quot;italic(A[omega])*&amp;#39;, &amp;#39;*italic(B[omega])&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;192&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now things get wacky.&lt;/p&gt;
&lt;p&gt;We are going to make the formula and the wavy lies in one subplot. We can define the basic coordinates for the wavy lines with the &lt;code&gt;ggforce::geom_bspline()&lt;/code&gt; function (learn more &lt;a href=&#34;https://ggforce.data-imaginist.com/reference/geom_bspline.html&#34;&gt;here&lt;/a&gt;). For each line segment, we just need about 5 pairs of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; coordinates. There’s no magic solution to these coordinates. I came to them by trial and error. As far as the formula goes, it isn’t much more complicated from what we’ve been doing. It’s all just a bunch of &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/grDevices/html/plotmath.html&#34;&gt;plotmath syntax&lt;/a&gt;. The main deal is to notice how we set the &lt;code&gt;limits&lt;/code&gt; in &lt;code&gt;the scale_x_continuous()&lt;/code&gt; function to &lt;code&gt;(0, 2)&lt;/code&gt;. In the other plots, those are restricted to &lt;code&gt;0, 1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2 &amp;lt;-
  tibble(x = c(.5, .475, .26, .08, .06,
               .5, .55, .85, 1.15, 1.2),
         y = c(1, .7, .6, .5, .2,
               1, .7, .6, .5, .2),
         line = rep(letters[2:1], each = 5)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y)) +
  geom_bspline(aes(color = line),
               size = 2/3, show.legend = F) + 
  annotate(geom = &amp;quot;text&amp;quot;,
           x = 0, y = .125,
           label = &amp;quot;omega(italic(K)-2)+1*&amp;#39;, &amp;#39;*(1-omega)(italic(K)-2)+1&amp;quot;,
           size = 7, parse = T, family = &amp;quot;Times&amp;quot;, hjust = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = 1/3, y = .7,
           label = &amp;quot;&amp;#39;~&amp;#39;&amp;quot;,
           size = 10, parse = T, family = &amp;quot;Times&amp;quot;) +
  scale_color_manual(values = c(&amp;quot;grey75&amp;quot;, &amp;quot;black&amp;quot;)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1) +
  theme_void()

p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You’ll see how this will works when we combine all the subplots, below. The rest of the subplots are similar or identical to the ones from the first section. Here we’ll make them in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# another beta density
p3 &amp;lt;-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&amp;gt;% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;beta&amp;quot;,
           size = 7) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))

# an annotated arrow
p4 &amp;lt;-
  tibble(x    = .5,
         y    = 1,
         xend = .5,
         yend = 0) %&amp;gt;%
  
  ggplot(aes(x = x, xend = xend,
             y = y, yend = yend)) +
  geom_segment(arrow = my_arrow) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .375, y = 1/3,
           label = &amp;quot;&amp;#39;~&amp;#39;&amp;quot;,
           size = 10, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(0, 1)

# bar plot of Bernoulli data
p5 &amp;lt;-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = &amp;quot;skyblue&amp;quot;, width = .4) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;Bernoulli&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .94,
           label = &amp;quot;theta&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-.75, 1.75) +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p6 &amp;lt;-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c(&amp;quot;&amp;#39;~&amp;#39;&amp;quot;, &amp;quot;italic(i)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = &amp;quot;Times&amp;quot;) +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1)

# some text
p7 &amp;lt;-
  tibble(x     = .5,
         y     = .5,
         label = &amp;quot;italic(y[i])&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = &amp;quot;Times&amp;quot;) +
  xlim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now combine the subplots with &lt;strong&gt;patchwork&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout &amp;lt;- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 3, b = 4, l = 1, r = 2),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 8, l = 1, r = 1),
  area(t = 9, b = 9, l = 1, r = 1),
  area(t = 10, b = 10, l = 1, r = 1)
)

(p1 + p3 + p2 + p4 + p5 + p6 + p7) + 
  plot_layout(design = layout) &amp;amp;
  ylim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;369.6&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a few reasons why this worked. First, we superimposed the subplot with the formula and wavy lines (&lt;code&gt;p2&lt;/code&gt;) atop of the second density (&lt;code&gt;p3&lt;/code&gt;) by ordering the plots as &lt;code&gt;(p1 + p3 + p2 + p4 + p5 + p6 + p7)&lt;/code&gt;. Placing one plot atop another was made easy by our use of &lt;code&gt;theme_void()&lt;/code&gt;, which made the backgrounds for all the subplots transparent. But also look at how we set the &lt;code&gt;r&lt;/code&gt; argument to 2 within the &lt;code&gt;area()&lt;/code&gt; function for our &lt;code&gt;p2&lt;/code&gt;. That’s what bought us that extra space for the formula.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;figure-9.7-add-more-curvy-lines-and-a-second-density-to-the-top-row.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Figure 9.7: Add more curvy lines and a second density to the top row.&lt;/h3&gt;
&lt;p&gt;For our next challenge, we’ll tackle Kruschke’s Figure 9.7:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kruschke_figure9.7.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a mild extension of the previous one. From a plotting perspective, the noteworthy new features are we have two density plots on the top row and now we have to juggle two pairs of wiggly lines in the subplot with the formula. The two subplots in the top row are no big deal. To make the gamma density, just use the &lt;code&gt;dgamma()&lt;/code&gt; function in place of &lt;code&gt;dbeta()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a beta density
p1 &amp;lt;-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&amp;gt;% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;beta&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .6,
           label = &amp;quot;italic(A[omega])*&amp;#39;, &amp;#39;*italic(B[omega])&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))

# a gamma density
p2 &amp;lt;-
  tibble(x = seq(from = 0, to = 5, by = .01),
         d = (dgamma(x, 1.75, .85) / max(dgamma(x, 1.75, .85)))) %&amp;gt;% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = 2.5, y = .2,
           label = &amp;quot;gamma&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = 2.5, y = .6,
           label = &amp;quot;list(italic(S)[kappa], italic(R)[kappa])&amp;quot;,
           size = 7, family = &amp;quot;Times&amp;quot;, parse = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The third subplot contains our offset formula and two sets of wiggly lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;-
  tibble(x = c(.5, .475, .26, .08, .06,
               .5, .55, .85, 1.15, 1.175,
               1.5, 1.4, 1, .25, .2,
               1.5, 1.49, 1.445, 1.4, 1.39),
         y = c(1, .7, .6, .5, .2,
               1, .7, .6, .5, .2,
               1, .7, .6, .5, .2,
               1, .75, .6, .45, .2),
         line = rep(letters[2:1], each = 5) %&amp;gt;% rep(., times = 2),
         plot = rep(1:2, each = 10)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, group = interaction(plot, line))) +
  geom_bspline(aes(color = line),
               size = 2/3, show.legend = F) + 
  annotate(geom = &amp;quot;text&amp;quot;,
           x = 0, y = .1,
           label = &amp;quot;omega(kappa-2)+1*&amp;#39;, &amp;#39;*(1-omega)(kappa-2)+1&amp;quot;,
           size = 7, parse = T, family = &amp;quot;Times&amp;quot;, hjust = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = c(1/3, 1.15), y = .7,
           label = &amp;quot;&amp;#39;~&amp;#39;&amp;quot;,
           size = 10, parse = T, family = &amp;quot;Times&amp;quot;) +
  scale_color_manual(values = c(&amp;quot;grey75&amp;quot;, &amp;quot;black&amp;quot;)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) +
  ylim(0, 1)

p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The rest of the subplots are similar or identical to the ones from the last section. Here we’ll make them in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# another beta density
p4 &amp;lt;-
  tibble(x = seq(from = .01, to = .99, by = .01),
         d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %&amp;gt;% 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = &amp;quot;skyblue&amp;quot;, size = 0) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;beta&amp;quot;,
           size = 7) +
  scale_x_continuous(expand = c(0, 0)) +
  theme(axis.line.x = element_line(size = 0.5))

# an annotated arrow
p5 &amp;lt;-
  tibble(x     = c(.375, .625),
         y     = c(1/3, 1/3),
         label = c(&amp;quot;&amp;#39;~&amp;#39;&amp;quot;, &amp;quot;italic(s)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = &amp;quot;Times&amp;quot;) +
  geom_segment(x = 0.5, xend = 0.5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1)

# bar plot of Bernoulli data
p6 &amp;lt;-
  tibble(x = 0:1,
         d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %&amp;gt;% 
  
  ggplot(aes(x = x, y = d)) +
  geom_col(fill = &amp;quot;skyblue&amp;quot;, width = .4) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .2,
           label = &amp;quot;Bernoulli&amp;quot;,
           size = 7) +
  annotate(geom = &amp;quot;text&amp;quot;,
           x = .5, y = .94,
           label = &amp;quot;theta&amp;quot;, 
           size = 7, family = &amp;quot;Times&amp;quot;, parse = T) +
  xlim(-.75, 1.75) +
  theme(axis.line.x = element_line(size = 0.5))

# another annotated arrow
p7 &amp;lt;-
  tibble(x     = c(.35, .65),
         y     = c(1/3, 1/3),
         label = c(&amp;quot;&amp;#39;~&amp;#39;&amp;quot;, &amp;quot;italic(i)*&amp;#39;|&amp;#39;*italic(s)&amp;quot;)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = c(10, 7), parse = T, family = &amp;quot;Times&amp;quot;) +
  geom_segment(x = .5, xend = .5,
               y = 1, yend = 0,
               arrow = my_arrow) +
  xlim(0, 1)

# some text
p8 &amp;lt;-
  tibble(x     = .5,
         y     = .5,
         label = &amp;quot;italic(y[i])[&amp;#39;|&amp;#39;][italic(s)]&amp;quot;) %&amp;gt;% 
  
  ggplot(aes(x = x, y = y, label = label)) +
  geom_text(size = 7, parse = T, family = &amp;quot;Times&amp;quot;) +
  xlim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now combine the subplots with &lt;strong&gt;patchwork&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout &amp;lt;- c(
  area(t = 1, b = 2, l = 1, r = 1),
  area(t = 1, b = 2, l = 2, r = 2),
  area(t = 4, b = 5, l = 1, r = 1),
  area(t = 3, b = 4, l = 1, r = 2),
  area(t = 6, b = 6, l = 1, r = 1),
  area(t = 7, b = 8, l = 1, r = 1),
  area(t = 9, b = 9, l = 1, r = 1),
  area(t = 10, b = 10, l = 1, r = 1)
)

(p1 + p2 + p4 + p3 + p5 + p6 + p7 + p8) + 
  plot_layout(design = layout) &amp;amp;
  ylim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-09-make-model-diagrams-kruschke-style/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;374.4&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boom; we did it!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;Though I’m overall pleased with this workflow, it’s not without limitations. To keep the values in our &lt;code&gt;area()&lt;/code&gt; functions simple, we scaled the density plots to be twice the size of the arrow plots. With simple ratios like 1/2, this works well but it can be a bit of a pain with more exotic ratios. The size and proportions of the fonts are quite sensitive to the overall height and width values for the final plot. You’ll find similar issues with the coordinates for the wiggly &lt;code&gt;geom_bspline()&lt;/code&gt; lines. Getting these right will likely take a few iterations. Speaking of &lt;code&gt;geom_bspline()&lt;/code&gt;, I’m also not happy that there doesn’t appear to be an easy way to have them end with arrow heads. Perhaps you could hack some in with another layer of &lt;code&gt;geom_segment()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Limitations aside, I hope this helps makes it one step easier for applied researchers to create their own Kruschke-stype model diagrams. Happy plotting!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggforce_0.3.2   patchwork_1.1.1 forcats_0.5.1   stringr_1.4.0  
##  [5] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    
##  [9] tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.1.0  xfun_0.22         haven_2.3.1       colorspace_2.0-0 
##  [5] vctrs_0.3.6       generics_0.1.0    htmltools_0.5.1.1 yaml_2.2.1       
##  [9] utf8_1.1.4        rlang_0.4.10      pillar_1.5.1      withr_2.4.1      
## [13] glue_1.4.2        DBI_1.1.0         tweenr_1.0.1      dbplyr_2.0.0     
## [17] modelr_0.1.8      readxl_1.3.1      lifecycle_1.0.0   munsell_0.5.0    
## [21] blogdown_1.3      gtable_0.3.0      cellranger_1.1.0  rvest_0.3.6      
## [25] evaluate_0.14     labeling_0.4.2    knitr_1.31        fansi_0.4.2      
## [29] highr_0.8         broom_0.7.5       Rcpp_1.0.6        scales_1.1.1     
## [33] backports_1.2.1   jsonlite_1.7.2    farver_2.0.3      fs_1.5.0         
## [37] hms_0.5.3         digest_0.6.27     stringi_1.5.3     bookdown_0.21    
## [41] polyclip_1.10-0   grid_4.0.4        cli_2.3.1         tools_4.0.4      
## [45] magrittr_2.0.1    crayon_1.4.1      pkgconfig_2.0.3   MASS_7.3-53      
## [49] ellipsis_0.3.1    xml2_1.3.2        reprex_0.3.0      lubridate_1.7.9.2
## [53] assertthat_0.2.1  rmarkdown_2.7     httr_1.4.2        rstudioapi_0.13  
## [57] R6_2.5.0          compiler_4.0.4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Would you like all your posteriors in one plot?</title>
      <link>/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/</guid>
      <description>
&lt;script src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A colleague reached out to me earlier this week with a plotting question. They had fit a series of Bayesian models, all containing a common parameter of interest. They knew how to plot their focal parameter one model at a time, but were stumped on how to combine the plots across models into a seamless whole. It reminded me a bit of this gif&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/Bqn8Z7xdPCFy0/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;which I originally got from &lt;a href=&#34;https://twitter.com/JennyBryan&#34;&gt;Jenny Bryan&lt;/a&gt;’s great talk, &lt;a href=&#34;https://www.youtube.com/watch?v=4MfUCX_KpdE&#34;&gt;&lt;em&gt;Behind every great plot there’s a great deal of wrangling&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The goal of this post is to provide solutions. We’ll practice a few different ways you can combine the posterior samples from your Bayesian models into a single plot. As usual, we’ll be fitting our models with &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;&lt;strong&gt;brms&lt;/strong&gt;&lt;/a&gt;, wrangling with packages from the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;&lt;/a&gt;, and getting a little help from the &lt;a href=&#34;https://mjskay.github.io/tidybayes/index.html&#34;&gt;&lt;strong&gt;tidybayes&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;i-make-assumptions.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I make assumptions.&lt;/h2&gt;
&lt;p&gt;For this post, I’m presuming you are familiar Bayesian regression using &lt;strong&gt;brms.&lt;/strong&gt; I’m also assuming you’ve coded using some of the foundational functions from the &lt;strong&gt;tidyverse.&lt;/strong&gt; If you’d like to firm up your foundations a bit, check out these resources.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To learn about Bayesian regression, I recommend the introductory text books by either McElreath (&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;here&lt;/a&gt;) or Kruschke (&lt;a href=&#34;http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/&#34;&gt;here&lt;/a&gt;). Both authors host blogs (&lt;a href=&#34;http://doingbayesiandataanalysis.blogspot.com&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://elevanth.org/blog/&#34;&gt;here&lt;/a&gt;, respectively). If you go with McElreath, do check out his &lt;a href=&#34;https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists&#34;&gt;online lectures&lt;/a&gt; and my ebooks where I translated his text to &lt;strong&gt;brms&lt;/strong&gt; and &lt;strong&gt;tidyverse&lt;/strong&gt; code (&lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://bookdown.org/content/4857/&#34;&gt;here&lt;/a&gt;). I have a similar ebook translation for Kruschke’s text (&lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For even more &lt;strong&gt;brms&lt;/strong&gt;-related resources, you can find vignettes and documentation &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/index.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;tidyverse&lt;/strong&gt; introductions, your best bets are &lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;&lt;em&gt;R4DS&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://style.tidyverse.org&#34;&gt;&lt;em&gt;The tidyverse style guide&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;same-parameter-different-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Same parameter, different models&lt;/h2&gt;
&lt;p&gt;Let’s load our primary statistical packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)
library(tidybayes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simulate &lt;span class=&#34;math inline&#34;&gt;\(n = 150\)&lt;/span&gt; draws from the standard normal distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 150

set.seed(1)
d &amp;lt;-
  tibble(y = rnorm(n, mean = 0, sd = 1))

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##        y
##    &amp;lt;dbl&amp;gt;
## 1 -0.626
## 2  0.184
## 3 -0.836
## 4  1.60 
## 5  0.330
## 6 -0.820&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we’ll fit three intercept-only models for &lt;code&gt;y&lt;/code&gt;. Each will follow the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i     &amp;amp; \sim \text{Normal} (\mu, \sigma) \\
\mu     &amp;amp; = \beta_0 \\
\beta_0 &amp;amp; \sim \text{Normal} (0, x) \\
\sigma  &amp;amp; \sim \text{Student-t}(3, 0, 10)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the unconditional intercept (i.e., an intercept not conditioned on any predictors). We will be fitting three alternative models. All will have the same prior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{Student-t}(3, 0, 10)\)&lt;/span&gt;, which is the &lt;strong&gt;brms&lt;/strong&gt; default in this case. [If you’d like to check, use the &lt;code&gt;get_prior()&lt;/code&gt; function.] The only way the models will differ is by their prior on the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;. By model, those priors will be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fit1&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, 10)\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit2&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, 1)\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit3&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, 0.1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So if you were wondering, the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in the &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal} (0, x)\)&lt;/span&gt; line, above, was a stand-in for the varying &lt;a href=&#34;https://en.wikipedia.org/wiki/Hyperparameter&#34;&gt;hyperparameter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here we fit the models in bulk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 1,
      prior(normal(0, 10), class = Intercept),
      seed = 1)

fit2 &amp;lt;-
  update(fit1,
         prior = prior(normal(0, 1), class = Intercept),
         seed = 1)

fit3 &amp;lt;-
  update(fit1,
         prior = prior(normal(0, 0.1), class = Intercept),
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Normally we’d use &lt;code&gt;plot()&lt;/code&gt; to make sure the chains look good and then use something like &lt;code&gt;print()&lt;/code&gt; or &lt;code&gt;posterior_summary()&lt;/code&gt; to summarize the models’ results. I’ve checked and they’re all fine. For the sake of space, let’s press forward.&lt;/p&gt;
&lt;p&gt;If you were going to plot the results of an individual fit using something like the &lt;code&gt;tidybayes::stat_halfeye()&lt;/code&gt; function, the next step would be extracting the posterior draws. Here we’ll do so with the &lt;code&gt;brms::posterior_samples()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post1 &amp;lt;- posterior_samples(fit1)
post2 &amp;lt;- posterior_samples(fit2)
post3 &amp;lt;- posterior_samples(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Focusing on &lt;code&gt;fit1&lt;/code&gt;, here’s how we’d plot the results for the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this part is unnecessary; it just adjusts some theme defaults to my liking
theme_set(theme_gray() +
            theme(axis.text.y  = element_text(hjust = 0),
                  axis.ticks.y = element_blank(),
                  panel.grid   = element_blank()))

# plot!
post1 %&amp;gt;% 
  ggplot(aes(x = b_Intercept, y = 0)) +
  stat_halfeye() +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But how might we get the posterior draws from all three fits into one plot?&lt;/em&gt; The answer is by somehow combining the posterior draws from each into one data frame. There are many ways to do this. Perhaps the simplest is with the &lt;code&gt;bind_rows()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  bind_rows(
    post1,
    post2,
    post3
  ) %&amp;gt;% 
  mutate(prior = str_c(&amp;quot;normal(0, &amp;quot;, c(10, 1, 0.1), &amp;quot;)&amp;quot;) %&amp;gt;% rep(., each = 4000))

head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_Intercept     sigma      lp__         prior
## 1  0.06440413 0.9408454 -202.2537 normal(0, 10)
## 2  0.02603356 0.9416735 -202.1114 normal(0, 10)
## 3 -0.02122717 0.8967501 -202.0446 normal(0, 10)
## 4  0.02620046 0.9521795 -202.2594 normal(0, 10)
## 5  0.02620046 0.9521795 -202.2594 normal(0, 10)
## 6  0.08025366 0.9101939 -202.1808 normal(0, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;bind_rows()&lt;/code&gt; function worked well, here, because all three post objects had the same number of columns of the same names. So we just stacked them three high. That is, we went from three data objects of 4,000 rows and 3 columns to one data object with 12,000 rows and 3 columns. But with the &lt;code&gt;mutate()&lt;/code&gt; function we did add a fourth column, &lt;code&gt;prior&lt;/code&gt;, that indexed which model each row came from. Now our data are ready, we can plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  ggplot(aes(x = b_Intercept, y = prior)) +
  stat_halfeye()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our plot arrangement made it easy to compare the results of tightening the prior on &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;; the narrower the prior, the narrower the posterior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-if-my-posterior_samples-arent-of-the-same-dimensions-across-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What if my &lt;code&gt;posterior_samples()&lt;/code&gt; aren’t of the same dimensions across models?&lt;/h2&gt;
&lt;p&gt;For the next examples, we need new data. Here we’ll simulate three predictors–&lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, and &lt;code&gt;x3&lt;/code&gt;. We then simulate our criterion &lt;code&gt;y&lt;/code&gt; as a linear additive function of those predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
d &amp;lt;-
  tibble(x1 = rnorm(n, mean = 0, sd = 1),
         x2 = rnorm(n, mean = 0, sd = 1),
         x3 = rnorm(n, mean = 0, sd = 1)) %&amp;gt;% 
  mutate(y  = rnorm(n, mean = 0 + x1 * 0 + x2 * 0.2 + x3 * -0.4))

head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##       x1      x2     x3      y
##    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 -0.626  0.450   0.894  0.694
## 2  0.184 -0.0186 -1.05  -0.189
## 3 -0.836 -0.318   1.97  -1.61 
## 4  1.60  -0.929  -0.384 -1.59 
## 5  0.330 -1.49    1.65  -2.41 
## 6 -0.820 -1.08    1.51  -0.764&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to work with these data in two ways. For the first example, we’ll fit a series of univariable models following the same basic form, but each with a different predictor. For the second example, we’ll fit a series of multivariable models with various combinations of the predictors. Each requires its own approach.&lt;/p&gt;
&lt;div id=&#34;same-form-different-predictors.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Same form, different predictors.&lt;/h3&gt;
&lt;p&gt;This time we’re just using the &lt;strong&gt;brms&lt;/strong&gt; default priors. As such, the models all follow the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
y_i     &amp;amp; \sim \text{Normal} (\mu_i, \sigma) \\
\mu_i   &amp;amp; = \beta_0 + \beta_n x_n\\
\beta_0 &amp;amp; \sim \text{Student-t}(3, 0, 10) \\
\sigma  &amp;amp; \sim \text{Student-t}(3, 0, 10)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You may be wondering &lt;em&gt;What about the prior for&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\beta_n\)&lt;/span&gt;&lt;em&gt;?&lt;/em&gt; The &lt;strong&gt;brms&lt;/strong&gt; defaults for those are improper flat priors. We define &lt;span class=&#34;math inline&#34;&gt;\(\beta_n x_n\)&lt;/span&gt; for the next three models as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fit4&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 x_1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit5&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_2 x_2\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit5&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\beta_3 x_3\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s fit the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit4 &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 1 + x1,
      seed = 1)

fit5 &amp;lt;-
  update(fit4,
         newdata = d,
         y ~ 1 + x2,
         seed = 1)

fit6 &amp;lt;-
  update(fit4,
         newdata = d,
         y ~ 1 + x3,
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like before, save the posterior draws for each as separate data frames.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post4 &amp;lt;- posterior_samples(fit4)
post5 &amp;lt;- posterior_samples(fit5)
post6 &amp;lt;- posterior_samples(fit6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, our simple &lt;code&gt;bind_rows()&lt;/code&gt; trick won’t work well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  post4,
  post5,
  post6
) %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_Intercept        b_x1    sigma      lp__ b_x2 b_x3
## 1 -0.26609646 -0.07795464 1.249694 -242.9716   NA   NA
## 2 -0.11933443 -0.03143494 1.251379 -240.4618   NA   NA
## 3 -0.10952301  0.02739295 1.278072 -241.2102   NA   NA
## 4 -0.08785528 -0.01065453 1.443157 -245.2715   NA   NA
## 5 -0.22020421 -0.16635358 1.185220 -241.7569   NA   NA
## 6  0.02973246 -0.13106488 1.123438 -239.2940   NA   NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We don’t want separate columns for &lt;code&gt;b_x1&lt;/code&gt;, &lt;code&gt;b_x2&lt;/code&gt;, and &lt;code&gt;b_x3&lt;/code&gt;. We want them all stacked atop one another. One simple solution is a two-step wherein we (1) select the relevant columns from each and bind them together with &lt;code&gt;bind_cols()&lt;/code&gt; and then (2) stack them atop one another with the &lt;code&gt;gather()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  bind_cols(
    post4 %&amp;gt;% select(b_x1),
    post5 %&amp;gt;% select(b_x2),
    post6 %&amp;gt;% select(b_x3)
  ) %&amp;gt;% 
  gather() %&amp;gt;% 
  mutate(predictor = str_remove(key, &amp;quot;b_&amp;quot;))

head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    key       value predictor
## 1 b_x1 -0.07795464        x1
## 2 b_x1 -0.03143494        x1
## 3 b_x1  0.02739295        x1
## 4 b_x1 -0.01065453        x1
## 5 b_x1 -0.16635358        x1
## 6 b_x1 -0.13106488        x1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That &lt;code&gt;mutate()&lt;/code&gt; line at the end wasn’t necessary, but it will make the plot more attractive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  ggplot(aes(x = value, y = predictor)) +
  stat_halfeye()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;different-combinations-of-predictors-in-different-forms.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Different combinations of predictors in different forms.&lt;/h3&gt;
&lt;p&gt;Now we fit a series of multivariable models. The first three will have combinations of two of the predictors. The final model will have all three. For simplicity, we continue to use the &lt;strong&gt;brms&lt;/strong&gt; default priors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit7 &amp;lt;-
  brm(data = d,
      family = gaussian,
      y ~ 1 + x1 + x2,
      seed = 1)

fit8 &amp;lt;-
  update(fit7,
         newdata = d,
         y ~ 1 + x1 + x3,
         seed = 1)

fit9 &amp;lt;-
  update(fit7,
         newdata = d,
         y ~ 1 + x2 + x3,
         seed = 1)

fit10 &amp;lt;-
  update(fit7,
         newdata = d,
         y ~ 1 + x1 + x2 + x3,
         seed = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Individually extract the posterior draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post7  &amp;lt;- posterior_samples(fit7)
post8  &amp;lt;- posterior_samples(fit8)
post9  &amp;lt;- posterior_samples(fit9)
post10 &amp;lt;- posterior_samples(fit10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a look at what happens this time when we use the &lt;code&gt;bind_rows()&lt;/code&gt; approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  bind_rows(
    post7,
    post8,
    post9,
    post10
  ) 

glimpse(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 16,000
## Columns: 6
## $ b_Intercept &amp;lt;dbl&amp;gt; -0.034398871, 0.008116322, 0.109134954, -0.134114504, -0.148230448, 0.04629622…
## $ b_x1        &amp;lt;dbl&amp;gt; -0.018887709, -0.156024614, -0.248414749, 0.057442787, 0.241874229, -0.3504998…
## $ b_x2        &amp;lt;dbl&amp;gt; 0.23847261, 0.27500306, 0.37294396, 0.20640317, 0.15437136, 0.28201317, 0.1538…
## $ sigma       &amp;lt;dbl&amp;gt; 1.250134, 1.065501, 1.029253, 1.220301, 1.206074, 1.114755, 1.180636, 1.266597…
## $ lp__        &amp;lt;dbl&amp;gt; -236.9970, -236.7477, -241.3055, -237.9540, -242.0909, -239.3407, -237.2902, -…
## $ b_x3        &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still have the various data frames stacked atop another, with the data from &lt;code&gt;post7&lt;/code&gt; in the first 4,000 rows. See how the values in the &lt;code&gt;b_x3&lt;/code&gt; column are all missing (i.e., filled with &lt;code&gt;NA&lt;/code&gt; values)? That’s because &lt;code&gt;fit7&lt;/code&gt; didn’t contain &lt;code&gt;x3&lt;/code&gt; as a predictor. Similarly, if we were to look at rows 4,001 through 8,000, we’d see column &lt;code&gt;b_x2&lt;/code&gt; would be the one filled with &lt;code&gt;NA&lt;/code&gt;s. This behavior is a good thing, here. After a little more wrangling, we’ll plot and it should be become clear why. Here’s the wrangling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  posts %&amp;gt;% 
  select(starts_with(&amp;quot;b_x&amp;quot;)) %&amp;gt;% 
  mutate(contains = rep(c(&amp;quot;&amp;lt;1, 1, 0&amp;gt;&amp;quot;, &amp;quot;&amp;lt;1, 0, 1&amp;gt;&amp;quot;, &amp;quot;&amp;lt;0, 1, 1&amp;gt;&amp;quot;, &amp;quot;&amp;lt;1, 1, 1&amp;gt;&amp;quot;), each = 4000)) %&amp;gt;% 
  gather(key, value, -contains) %&amp;gt;% 
  mutate(coefficient = str_remove(key, &amp;quot;b_x&amp;quot;) %&amp;gt;% str_c(&amp;quot;beta[&amp;quot;, ., &amp;quot;]&amp;quot;))

head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    contains  key       value coefficient
## 1 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.01888771     beta[1]
## 2 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.15602461     beta[1]
## 3 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.24841475     beta[1]
## 4 &amp;lt;1, 1, 0&amp;gt; b_x1  0.05744279     beta[1]
## 5 &amp;lt;1, 1, 0&amp;gt; b_x1  0.24187423     beta[1]
## 6 &amp;lt;1, 1, 0&amp;gt; b_x1 -0.35049990     beta[1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;code&gt;contains&lt;/code&gt; variable, we indexed which fit the draws came from. The 1’s and 0’s within the angle brackets indicate which of the three predictors were present within the model with the 1’s indicating they were and the 0’s indicating they were not. For example, &lt;code&gt;&amp;lt;1, 1, 0&amp;gt;&lt;/code&gt; in the first row indicated this was the model including &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt;. Importantly, we also added a &lt;code&gt;coefficient&lt;/code&gt; index. This is just a variant of &lt;code&gt;key&lt;/code&gt; that’ll make the strip labels in our plot more attractive. Behold:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  drop_na(value) %&amp;gt;% 
  ggplot(aes(x = value, y = contains)) +
  stat_halfeye() +
  ylab(NULL) +
  facet_wrap(~coefficient, ncol = 1, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hopefully now it’s clear why it was good to save those cells with the &lt;code&gt;NA&lt;/code&gt;s.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bonus-you-can-streamline-your-workflow.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bonus: You can streamline your workflow.&lt;/h2&gt;
&lt;p&gt;The workflows above are generally fine. But they’re a little inefficient. If you’d like to reduce the amount of code you’re writing and the number of objects you have floating around in your environment, you might consider a more streamlined workflow where you work with your fit objects in bulk. Here we’ll demonstrate a nested tibble approach with the first three fits.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts &amp;lt;-
  tibble(name  = str_c(&amp;quot;fit&amp;quot;, 1:3),
         prior = str_c(&amp;quot;normal(0, &amp;quot;, c(10, 1, 0.1), &amp;quot;)&amp;quot;)) %&amp;gt;% 
  mutate(fit = map(name, get)) %&amp;gt;% 
  mutate(post = map(fit, posterior_samples))
  
head(posts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 4
##   name  prior          fit       post                
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;              
## 1 fit1  normal(0, 10)  &amp;lt;brmsfit&amp;gt; &amp;lt;df[,3] [4,000 × 3]&amp;gt;
## 2 fit2  normal(0, 1)   &amp;lt;brmsfit&amp;gt; &amp;lt;df[,3] [4,000 × 3]&amp;gt;
## 3 fit3  normal(0, 0.1) &amp;lt;brmsfit&amp;gt; &amp;lt;df[,3] [4,000 × 3]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a 3-row nested tibble. The first column, &lt;code&gt;name&lt;/code&gt; is just a character vector with the names of the fits. The next column isn’t necessary, but it nicely explicates the main difference in the models: the prior we used on the intercept. It’s in the &lt;code&gt;map()&lt;/code&gt; functions within the two &lt;code&gt;mutate()&lt;/code&gt;lines where all the magic happens. With the first, we used the &lt;code&gt;get()&lt;/code&gt; function to snatch up the &lt;strong&gt;brms&lt;/strong&gt; fit objects matching the names in the &lt;code&gt;name&lt;/code&gt; column. In the second, we used the &lt;code&gt;posterior_samples()&lt;/code&gt; function to extract the posterior draws from each of the fits saved in &lt;code&gt;fit&lt;/code&gt;. Do you see how each for in the &lt;code&gt;post&lt;/code&gt; column contains an entire &lt;span class=&#34;math inline&#34;&gt;\(4,000 \times 3\)&lt;/span&gt; data frame? That’s why we refer to this as a nested tibble. We have data frames compressed within data frames. If you’d like to access the data within the &lt;code&gt;post&lt;/code&gt; column, just &lt;code&gt;unnest()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  select(-fit) %&amp;gt;% 
  unnest(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12,000 x 5
##    name  prior         b_Intercept sigma  lp__
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 fit1  normal(0, 10)     0.0644  0.941 -202.
##  2 fit1  normal(0, 10)     0.0260  0.942 -202.
##  3 fit1  normal(0, 10)    -0.0212  0.897 -202.
##  4 fit1  normal(0, 10)     0.0262  0.952 -202.
##  5 fit1  normal(0, 10)     0.0262  0.952 -202.
##  6 fit1  normal(0, 10)     0.0803  0.910 -202.
##  7 fit1  normal(0, 10)    -0.00142 0.886 -202.
##  8 fit1  normal(0, 10)     0.0696  0.939 -202.
##  9 fit1  normal(0, 10)    -0.172   0.943 -205.
## 10 fit1  normal(0, 10)     0.0259  0.839 -203.
## # … with 11,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After un-nesting, we can remake the plot from above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts %&amp;gt;% 
  select(-fit) %&amp;gt;% 
  unnest(post) %&amp;gt;% 

  ggplot(aes(x = b_Intercept, y = prior)) +
  stat_halfeye()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-13-would-you-like-all-your-posteriors-in-one-plot/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To learn more about using the &lt;strong&gt;tidyverse&lt;/strong&gt; for iterating and saving the results in nested tibbles, check out &lt;a href=&#34;https://twitter.com/hadleywickham&#34;&gt;Hadley Wickham&lt;/a&gt;’s great talk, &lt;a href=&#34;https://www.youtube.com/watch?v=rz3_FDVt9eg&#34;&gt;&lt;em&gt;Managing many models&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session information&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0   dplyr_1.0.5    
##  [7] purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] splines_4.0.4        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [49] htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [53] ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3      loo_2.4.1           
##  [57] dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2       tidyselect_1.1.0    
##  [61] rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [65] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [69] broom_0.7.5          ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1       
##  [73] yaml_2.2.1           processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [77] nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [81] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [85] gamm4_0.2-6          curl_4.3             reprex_0.3.0         statmod_1.4.35      
##  [89] stringi_1.5.3        highr_0.8            ps_1.6.0             blogdown_1.3        
##  [93] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [97] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1        
## [101] lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [105] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [109] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [113] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [117] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [125] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Make rotated Gaussians, Kruschke style</title>
      <link>/post/2018-12-20-make-rotated-gaussians-kruschke-style/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-12-20-make-rotated-gaussians-kruschke-style/</guid>
      <description>
&lt;script src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;[edited Apr 21, 2021]&lt;/p&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;You too can make sideways Gaussian density curves within the tidyverse. Here’s how.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;heres-the-deal-i-like-making-pictures.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Here’s the deal: I like making pictures.&lt;/h2&gt;
&lt;p&gt;Over the past several months, I’ve been slowly chipping away&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; at John Kruschke’s &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;&lt;em&gt;Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan&lt;/em&gt;&lt;/a&gt;. Kruschke has a unique plotting style. One of the quirks is once in a while he likes to express the results of his analyses in plots where he shows the data alongside density curves of the model-implied data-generating distributions. Here’s an example from chapter 19 (p. 563).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kruschke_sideways_Gaussians.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, he has lifespan data (i.e., &lt;code&gt;Longevity&lt;/code&gt;) for fruit flies from five experimental conditions (i.e., &lt;code&gt;CompanionNumber&lt;/code&gt;). Those are the black circles. In this section of the chapter, he used a Gaussian multilevel model in which the mean value for &lt;code&gt;Longevity&lt;/code&gt; had a grand mean in addition to random effects for the five experimental conditions. Those sideways-turned blue Gaussians are his attempt to express the model-implied data generating distributions for each group.&lt;/p&gt;
&lt;p&gt;If you haven’t gone through Kruschke’s text, you should know he relies on base R and all its &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/control-structures.html#for-loops&#34;&gt;loop&lt;/a&gt;y glory. If you carefully go through his code, you can reproduce his plots in that fashion. I’m a &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;tidyverse&lt;/a&gt; man and prefer to avoid writing a &lt;code&gt;for()&lt;/code&gt; loop at all costs. At first, I tried to work with convenience functions within ggplot2 and friends, but only had limited success. After staring long and hard at Kruschke’s base code, I came up with a robust solution, which I’d like to share here.&lt;/p&gt;
&lt;p&gt;In this post, we’ll practice making sideways Gaussians in the Kruschke style. We’ll do so with a simple intercept-only single-level model and then expand our approach to an intercept-only multilevel model like the one in the picture, above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My assumptions&lt;/h2&gt;
&lt;p&gt;For the sake of this post, I’m presuming you’re familiar with &lt;a href=&#34;https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html&#34;&gt;R&lt;/a&gt;, aware of the &lt;a href=&#34;https://www.rstudio.com/resources/videos/data-science-in-the-tidyverse/&#34;&gt;tidyverse&lt;/a&gt;, and have fit a &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&#34;&gt;Bayesian model&lt;/a&gt; or two. Yes. I admit that’s a narrow crowd. Sometimes the target’s a small one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-data.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need data.&lt;/h2&gt;
&lt;p&gt;First, we need data. Here we’ll borrow code from Matthew Kay’s nice &lt;a href=&#34;https://mjskay.github.io/tidybayes/articles/tidy-brms.html&#34;&gt;tutorial&lt;/a&gt; on how to use his great &lt;a href=&#34;https://github.com/mjskay/tidybayes&#34;&gt;tidybayes package&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

set.seed(5)
n           &amp;lt;- 10
n_condition &amp;lt;- 5

abc &amp;lt;-
  tibble(condition = rep(letters[1:5], times = n),
         response  = rnorm(n * 5, mean = c(0, 1, 2, 1, -1), sd = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data structure looks like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(abc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [50 × 2] (S3: tbl_df/tbl/data.frame)
##  $ condition: chr [1:50] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; &amp;quot;d&amp;quot; ...
##  $ response : num [1:50] -0.42 1.692 1.372 1.035 -0.144 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With Kay’s code, we have &lt;code&gt;response&lt;/code&gt; values for five &lt;code&gt;condition&lt;/code&gt;s. All follow the normal distribution and share a common standard deviation. However, they differ in their group means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;% 
  group_by(condition) %&amp;gt;% 
  summarise(mean = mean(response) %&amp;gt;% round(digits = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   condition  mean
##   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 a          0.18
## 2 b          1.01
## 3 c          1.87
## 4 d          1.03
## 5 e         -0.94&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Altogether, the data look like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_grey() + 
            theme(panel.grid = element_blank()))

abc %&amp;gt;%
  ggplot(aes(y = condition, x = response)) +
  geom_point(shape = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s get ready to model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;just-one-intercept&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Just one intercept&lt;/h2&gt;
&lt;p&gt;If you’ve read this far, you know we’re going Bayesian. Let’s open up our favorite Bayesian modeling package, Bürkner’s &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our first model, we’ll ignore the groups and just estimate a grand mean and a standard deviation. Relative to the scale of the &lt;code&gt;abc&lt;/code&gt; data, our priors are modestly &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;regularizing&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- 
  brm(data = abc,
      response ~ 1,
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(student_t(3, 0, 1), class = sigma)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extract the posterior draws and save them as a data frame we’ll call &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(fit1)

glimpse(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 4,000
## Columns: 3
## $ b_Intercept &amp;lt;dbl&amp;gt; 0.5729167, 0.6185517, 0.4430281, 0.4383346, 0.8541620, 0.6280931, 0.5159498, 0…
## $ sigma       &amp;lt;dbl&amp;gt; 1.1595969, 1.0350395, 1.0101029, 0.9758173, 1.1676389, 0.9694168, 1.0725615, 1…
## $ lp__        &amp;lt;dbl&amp;gt; -77.17416, -76.99795, -77.92546, -78.35923, -78.25847, -77.55006, -77.13486, -…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If all you want is a quick and dirty way to plot a few of the model-implied Gaussians from the simple model, you can just nest &lt;code&gt;stat_function()&lt;/code&gt; within &lt;code&gt;mapply()&lt;/code&gt; and tack on the original data in a &lt;code&gt;geom_jitter()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How many Gaussians would you like?
n_iter &amp;lt;- 20

tibble(response = c(-4, 4)) %&amp;gt;%
  ggplot(aes(x = response)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 1/2, 
                  color = &amp;quot;steelblue&amp;quot;)
    }, 
    # Enter means and standard deviations here
    mean = post[1:n_iter, &amp;quot;b_Intercept&amp;quot;],
    sd   = post[1:n_iter, &amp;quot;sigma&amp;quot;]
    ) +
  geom_jitter(data = abc, aes(y = -0.02),
              height = .025, shape = 1, alpha = 2/3) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This works pretty okay. But notice the orientation is the usual horizontal. Kruschke’s Gaussians were on their sides. If we switch out our &lt;code&gt;scale_y_continuous()&lt;/code&gt; line with &lt;code&gt;scale_y_reverse()&lt;/code&gt; and add in &lt;code&gt;coord_flip()&lt;/code&gt;, we’ll have it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(response = c(-4, 4)) %&amp;gt;%
  ggplot(aes(x = response)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 1/2, 
                  color = &amp;quot;steelblue&amp;quot;)
    }, 
    mean = post[1:n_iter, &amp;quot;b_Intercept&amp;quot;],
    sd   = post[1:n_iter, &amp;quot;sigma&amp;quot;]
    ) +
  geom_jitter(data = abc, aes(y = -0.02),
              height = .025, shape = 1, alpha = 2/3) +
  scale_y_reverse(NULL, breaks = NULL) +
  coord_flip() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boom. It won’t always be this easy, though.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-intercepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple intercepts&lt;/h2&gt;
&lt;p&gt;Since the &lt;code&gt;response&lt;/code&gt; values are from a combination of five &lt;code&gt;condition&lt;/code&gt; groups, we can fit a multilevel model to compute both the grand mean and the group-level deviations from the grand mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- 
  brm(data = abc,
      response ~ 1 + (1 | condition),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(student_t(3, 0, 1), class = sigma),
                prior(student_t(3, 0, 1), class = sd)),
      cores = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;“Wait. Whoa. I’m so confused”—you say. “What’s a multilevel model, again?” Read this &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;book&lt;/a&gt;, or this &lt;a href=&#34;https://sites.google.com/site/doingbayesiandataanalysis/&#34;&gt;book&lt;/a&gt;; start &lt;a href=&#34;https://www.youtube.com/watch?v=2sTQ7TG_85Q&#34;&gt;here&lt;/a&gt; on this lecture series; or even check out &lt;a href=&#34;https://bookdown.org/content/3890/&#34;&gt;my ebook&lt;/a&gt;, starting with Chapter 12.&lt;/p&gt;
&lt;p&gt;Once again, extract the posterior draws and save them as a data frame, &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- posterior_samples(fit2)

str(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    4000 obs. of  9 variables:
##  $ b_Intercept             : num  1.16 1.41 1.17 1.49 1.39 ...
##  $ sd_condition__Intercept : num  1.012 0.892 0.871 0.846 0.813 ...
##  $ sigma                   : num  0.581 0.537 0.491 0.537 0.541 ...
##  $ r_condition[a,Intercept]: num  -0.934 -1.313 -0.9 -1.363 -1.337 ...
##  $ r_condition[b,Intercept]: num  -0.336 -0.176 -0.523 -0.273 -0.241 ...
##  $ r_condition[c,Intercept]: num  0.969 0.413 0.8 0.483 0.394 ...
##  $ r_condition[d,Intercept]: num  -0.287 -0.198 -0.31 -0.105 -0.101 ...
##  $ r_condition[e,Intercept]: num  -2.12 -2.32 -2.12 -2.37 -2.25 ...
##  $ lp__                    : num  -52 -53 -54.6 -55.5 -54.1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is where our task becomes difficult. Now each level of &lt;code&gt;condition&lt;/code&gt; has its own mean estimate, which is a combination of the grand mean &lt;code&gt;b_Intercept&lt;/code&gt; and the group-specific deviation, &lt;code&gt;r_condition[a,Intercept]&lt;/code&gt; through &lt;code&gt;r_condition[e,Intercept]&lt;/code&gt;. If all we wanted to do was show the model-implied Gaussians for, say, &lt;code&gt;condition == a&lt;/code&gt;, that’d be a small extension of our last approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(response = c(-4, 4)) %&amp;gt;%
  ggplot(aes(x = response)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 1/2, 
                  color = &amp;quot;steelblue&amp;quot;)
    }, 
    # Here&amp;#39;s the small extension, part a
    mean = post[1:n_iter, &amp;quot;b_Intercept&amp;quot;] + post[1:n_iter, &amp;quot;r_condition[a,Intercept]&amp;quot;],
    sd   = post[1:n_iter, &amp;quot;sigma&amp;quot;]
    ) +
  # The small extension, part b:
  geom_jitter(data = abc %&amp;gt;% filter(condition == &amp;quot;a&amp;quot;), aes(y = 0),
              height = .025, shape = 1, alpha = 2/3) +
  scale_y_reverse(NULL, breaks = NULL) +
  coord_flip() +
  labs(subtitle = &amp;quot;This is just for condition a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The main thing we did was add to the definition of the &lt;code&gt;mean&lt;/code&gt; within &lt;code&gt;mapply()&lt;/code&gt;: &lt;code&gt;mean = post[1:n_iter, &#34;b_Intercept&#34;] + post[1:n_iter, &#34;r_condition[a,Intercept]&#34;]&lt;/code&gt;. Within &lt;code&gt;geom_jitter()&lt;/code&gt;, we also isolated the &lt;code&gt;condition == &#34;a&#34;&lt;/code&gt; cases with &lt;code&gt;filter()&lt;/code&gt;. Simple. However, it’s more of a pickle if we want multiple densities stacked atop/next to one another within the same plot.&lt;/p&gt;
&lt;p&gt;Unfortunately, we can’t extend our &lt;code&gt;mapply(stat_function())&lt;/code&gt; method to the group-level estimates–at least not that I’m aware. But there are other ways. We’ll need a little help from &lt;code&gt;tidybayes::spread_draws()&lt;/code&gt;, about which you can learn more &lt;a href=&#34;https://mjskay.github.io/tidybayes/articles/tidy-brms.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)

sd &amp;lt;-
  fit2 %&amp;gt;% 
  spread_draws(b_Intercept, sigma, r_condition[condition,])
  
head(sd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
## # Groups:   condition [5]
##   .chain .iteration .draw b_Intercept sigma condition r_condition
##    &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
## 1      1          1     1        1.16 0.581 a              -0.934
## 2      1          1     1        1.16 0.581 b              -0.336
## 3      1          1     1        1.16 0.581 c               0.969
## 4      1          1     1        1.16 0.581 d              -0.287
## 5      1          1     1        1.16 0.581 e              -2.12 
## 6      1          2     2        1.41 0.537 a              -1.31&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our &lt;code&gt;sp&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html&#34;&gt;tibble&lt;/a&gt;, we have much of the same information we’d get from &lt;code&gt;brms::posterior_samples()&lt;/code&gt;, but in the long format with respect to the random effects for &lt;code&gt;condition&lt;/code&gt;. Also notice that each row is indexed by the chain, iteration, and draw number. Among those, &lt;code&gt;.draw&lt;/code&gt; is the column that corresponds to a unique row like what we’d get from &lt;code&gt;brms::posterior_samples()&lt;/code&gt;. This is the index that ranges from 1 to the number of chains multiplied by the number of post-warmup iterations (i.e., default 4000 in our case).&lt;/p&gt;
&lt;p&gt;But we need to wrangle a bit. Within the &lt;code&gt;expand()&lt;/code&gt; function, we’ll select the columns we’d like to keep within the &lt;code&gt;nesting()&lt;/code&gt; function and then expand the tibble by adding a sequence of &lt;code&gt;response&lt;/code&gt; values ranging from -4 to 4, for each. This sets us up to use the &lt;code&gt;dnorm()&lt;/code&gt; function in the next line to compute the density for each of those &lt;code&gt;response&lt;/code&gt; values based on 20 unique normal distributions for each of the five &lt;code&gt;condition&lt;/code&gt; groups. “Why 20?” Because we need some reasonably small number and 20’s the one Kruschke tended to use in his text and because, well, we set &lt;code&gt;filter(.draw &amp;lt; 21)&lt;/code&gt;. But choose whatever number you like.&lt;/p&gt;
&lt;p&gt;The difficulty, however, is that all of these densities will have a minimum value of around 0 and all will be on the same basic scale. So we need a way to serially shift the density values up the y-axis in such a way that they’ll be sensibly separated by group. As far as I can figure, this’ll take us a couple steps. For the first step, we’ll create an intermediary variable, &lt;code&gt;g&lt;/code&gt;, with which we’ll arbitrarily assign each of our five groups an integer index ranging from 0 to 4.&lt;/p&gt;
&lt;p&gt;The second step is tricky. There we use our &lt;code&gt;g&lt;/code&gt; integers to sequentially shift the density values up. Since our &lt;code&gt;g&lt;/code&gt; value for &lt;code&gt;a == 0&lt;/code&gt;, those we’ll keep 0 as their baseline. As our &lt;code&gt;g&lt;/code&gt; value for &lt;code&gt;b == 1&lt;/code&gt;, the baseline for those will now increase by 1. And so on for the other groups. But we still need to do a little more fiddling. What we want is for the maximum values of the density estimates to be a little lower than the baselines of the ones one grouping variable up. That is, we want the maximum values for the &lt;code&gt;a&lt;/code&gt; densities to fall a little bit below 1 on the y-axis. It’s with the &lt;code&gt;* .75 / max(density)&lt;/code&gt; part of the code that we accomplish that task. If you want to experiment with more or less room between the top and bottom of each density, play around with increasing/decreasing that .75 value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  filter(.draw &amp;lt; 21) %&amp;gt;% 
  expand(nesting(.draw, b_Intercept, sigma, condition, r_condition), 
         response = seq(from = -4, to = 4, length.out = 200)) %&amp;gt;%
  mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma),
         g       = recode(condition,
                          a = 0,
                          b = 1,
                          c = 2,
                          d = 3,
                          e = 4)) %&amp;gt;% 
  mutate(density = g + density * .75 / max(density))

glimpse(sd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 8
## Groups: condition [5]
## $ .draw       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
## $ b_Intercept &amp;lt;dbl&amp;gt; 1.164396, 1.164396, 1.164396, 1.164396, 1.164396, 1.164396, 1.164396, 1.164396…
## $ sigma       &amp;lt;dbl&amp;gt; 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0.5811467, 0…
## $ condition   &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;…
## $ r_condition &amp;lt;dbl&amp;gt; -0.9342895, -0.9342895, -0.9342895, -0.9342895, -0.9342895, -0.9342895, -0.934…
## $ response    &amp;lt;dbl&amp;gt; -4.000000, -3.959799, -3.919598, -3.879397, -3.839196, -3.798995, -3.758794, -…
## $ density     &amp;lt;dbl&amp;gt; 1.874794e-12, 3.094499e-12, 5.083339e-12, 8.310546e-12, 1.352172e-11, 2.189555…
## $ g           &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we’ll now be using the same axis for both the densities and the five &lt;code&gt;condition&lt;/code&gt; groups, we’ll need to add a &lt;code&gt;density&lt;/code&gt; column to our &lt;code&gt;abc&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc &amp;lt;-
  abc %&amp;gt;% 
  mutate(density = recode(condition,
                          a = 0,
                          b = 1,
                          c = 2,
                          d = 3,
                          e = 4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time to plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  ggplot(aes(x = response, y = density)) +
  # here we make our density lines
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  # use the original data for the jittered points
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we’re rolling. Let’s make a cosmetic adjustment. Recall that the full range of the normal distribution spans from &lt;span class=&#34;math inline&#34;&gt;\(-\infty\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. At a certain point, it’s just not informative to show the left and right tails. If you look back up at our motivating example, you’ll note Kruschke’s densities stopped well before trailing off into the tails. If you look closely to the code from his text, you’ll see he’s just showing the inner 95-percentile range for each. To follow suit, we can compute those ranges with &lt;code&gt;qnorm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  mutate(ll = qnorm(.025, mean = b_Intercept + r_condition, sd = sigma),
         ul = qnorm(.975, mean = b_Intercept + r_condition, sd = sigma))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have our lower- and upper-level points for each iteration, we can limit the ranges of our Gaussians with &lt;code&gt;filter()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  
  ggplot(aes(x = response, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Oh man, just look how sweet that is. Although I prefer our current method, another difference between it and Kruschke’s example is all of his densities are the same relative height. In all our plots so far, though, the densities differ by their heights. We’ll need a slight adjustment in our &lt;code&gt;sd&lt;/code&gt; workflow for that. All we need to do is insert a &lt;code&gt;group_by()&lt;/code&gt; statement between the two &lt;code&gt;mutate()&lt;/code&gt; lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma),
         g       = recode(condition,
                          a = 0,
                          b = 1,
                          c = 2,
                          d = 3,
                          e = 4)) %&amp;gt;% 
  # here&amp;#39;s the new line
  group_by(.draw) %&amp;gt;% 
  mutate(density = g + density * .75 / max(density))

# now plot
sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  ggplot(aes(x = response, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nice. “But wait!”, you say. “We wanted our Gaussians to be on their sides.” We can do that in at least two ways. At this point, the quickest way is to use our &lt;code&gt;scale_y_reverse() + coord_flip()&lt;/code&gt; combo from before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  
  ggplot(aes(x = response, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 1/2, size = 1/3, color = &amp;quot;steelblue&amp;quot;) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  scale_y_reverse(&amp;quot;condition&amp;quot;,
                  breaks = 0:4,
                  labels = letters[1:5]) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way to get those sideways Gaussians is to alter our &lt;code&gt;sd&lt;/code&gt; data workflow. The main difference is this time we change the original &lt;code&gt;mutate(density = g + density * .75 / max(density))&lt;/code&gt; line to &lt;code&gt;mutate(density = g - density * .75 / max(density))&lt;/code&gt;. In case you missed it, the only difference is we changed the &lt;code&gt;+&lt;/code&gt; to a &lt;code&gt;-&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd &amp;lt;-
  sd %&amp;gt;% 
  # step one: starting fresh
  mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma)) %&amp;gt;% 
  group_by(.draw) %&amp;gt;% 
  # step two: now SUBTRACTING density from g within the equation
  mutate(density = g - density * .75 / max(density))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now in our global &lt;code&gt;aes()&lt;/code&gt; statement in the plot, we put &lt;code&gt;density&lt;/code&gt; on the x and &lt;code&gt;response&lt;/code&gt; on the y. We need to take a few other subtle steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Switch out &lt;code&gt;geom_line()&lt;/code&gt; for &lt;code&gt;geom_path()&lt;/code&gt; (see &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/geom_path.html&#34;&gt;here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Drop the &lt;code&gt;height&lt;/code&gt; argument within &lt;code&gt;geom_jitter()&lt;/code&gt; for &lt;code&gt;width&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Switch out &lt;code&gt;scale_y_continuous()&lt;/code&gt; for &lt;code&gt;scale_x_continuous()&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Though totally not necessary, we’ll add a little something extra by coloring the Gaussians by their means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd %&amp;gt;% 
  filter(response &amp;gt; ll,
         response &amp;lt; ul) %&amp;gt;% 
  
  ggplot(aes(x = density, y = response)) +
  geom_path(aes(group = interaction(.draw, g), 
                color = b_Intercept + r_condition),
            alpha = 1/2, size = 1/3, show.legend = F) +
  geom_jitter(data = abc,
              width = .05, shape = 1, alpha = 2/3) +
  scale_x_continuous(&amp;quot;condition&amp;quot;,
                     breaks = 0:4,
                     labels = letters[1:5]) +
  scale_color_viridis_c(option = &amp;quot;A&amp;quot;, end = .92)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you have it–Kruschke-style sideways Gaussians for your model plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;afterward&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Afterward&lt;/h2&gt;
&lt;p&gt;After releasing the initial version of this post, some of us had a lively twitter discussion on how to improve the code.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Ah, hrm. Took some digging, but it looks like negative density + setting `min_height = NA` (otherwise negative values are cut off) might work &lt;a href=&#34;https://t.co/gmF9kpo2T7&#34;&gt;pic.twitter.com/gmF9kpo2T7&lt;/a&gt;&lt;/p&gt;&amp;mdash; Matthew Kay (@mjskay) &lt;a href=&#34;https://twitter.com/mjskay/status/1076395687020056576?ref_src=twsrc%5Etfw&#34;&gt;December 22, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Part of that discussion had to do with the possibility of using functions from &lt;a href=&#34;https://twitter.com/ClausWilke/&#34;&gt;Claus Wilke&lt;/a&gt;’s great &lt;a href=&#34;https://github.com/clauswilke/ggridges&#34;&gt;ggridges package&lt;/a&gt;. After some great efforts, especially from &lt;a href=&#34;https://twitter.com/mjskay/&#34;&gt;Matthew Kay&lt;/a&gt;, we came up with solutions. In this section, we’ll cover them in some detail.&lt;/p&gt;
&lt;p&gt;First, here’s a more compact way to prepare the data for the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  mutate(density  = pmap(list(response, mu, sigma), dnorm)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  group_by(.draw) %&amp;gt;% 
  mutate(density  = density * .75 / max(density)) %&amp;gt;% 
  
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required when using unnest().
## Please use `cols = c(response, density)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 12
## Groups: .draw [20]
## $ condition  &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;,…
## $ .row       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ .chain     &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .iteration &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .draw      &amp;lt;int&amp;gt; 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,…
## $ .value     &amp;lt;dbl&amp;gt; -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -…
## $ mu         &amp;lt;dbl&amp;gt; -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -0.04576743, -…
## $ sigma      &amp;lt;dbl&amp;gt; 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.5396379, 0.…
## $ lower      &amp;lt;dbl&amp;gt; -1.103438, -1.103438, -1.103438, -1.103438, -1.103438, -1.103438, -1.103438, -1…
## $ upper      &amp;lt;dbl&amp;gt; 1.011903, 1.011903, 1.011903, 1.011903, 1.011903, 1.011903, 1.011903, 1.011903,…
## $ response   &amp;lt;dbl&amp;gt; -1.1034383, -1.0928085, -1.0821786, -1.0715488, -1.0609189, -1.0502890, -1.0396…
## $ density    &amp;lt;dbl&amp;gt; 0.1098804, 0.1141834, 0.1186089, 0.1231581, 0.1278322, 0.1326322, 0.1375591, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This could use some walking out. With the first two lines, we made a &lt;span class=&#34;math inline&#34;&gt;\(5 \times 1\)&lt;/span&gt; tibble containing the five levels of &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; through &lt;code&gt;f&lt;/code&gt;. The &lt;code&gt;add_fitted_draws()&lt;/code&gt; function comes from tidybayes. The first argument took our brms model fit, &lt;code&gt;fit2&lt;/code&gt;. With the &lt;code&gt;n&lt;/code&gt; argument, we indicated we just wanted &lt;code&gt;20&lt;/code&gt; draws. With &lt;code&gt;dpar&lt;/code&gt;, we requested distributional regression parameters in the output. In our case, those were the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; values for each level of &lt;code&gt;condition&lt;/code&gt;. Here’s what that looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
## # Groups:   condition, .row [1]
##   condition  .row .chain .iteration .draw .value     mu sigma
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 a             1     NA         NA    57 0.562  0.562  0.645
## 2 a             1     NA         NA    97 0.163  0.163  0.593
## 3 a             1     NA         NA   277 0.0490 0.0490 0.682
## 4 a             1     NA         NA   305 0.386  0.386  0.607
## 5 a             1     NA         NA   360 0.162  0.162  0.563
## 6 a             1     NA         NA   496 0.0224 0.0224 0.650&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we established the lower- and upper-bounds bounds for the density lines, which were 95% intervals in this example. Within the second &lt;code&gt;mutate()&lt;/code&gt; function, we used the &lt;a href=&#34;https://purrr.tidyverse.org/reference/map2.html&#34;&gt;&lt;code&gt;purrr::map2()&lt;/code&gt;&lt;/a&gt; function to feed those two values into the first two arguments of the &lt;code&gt;seq()&lt;/code&gt; function. Those arguments, recall, are &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt;. We then hard coded &lt;code&gt;200&lt;/code&gt; into the &lt;code&gt;length.out&lt;/code&gt; argument. As a result, we turned our regular old tibble into a &lt;a href=&#34;https://tidyr.tidyverse.org/reference/nest.html&#34;&gt;nested tibble&lt;/a&gt;. In each row of our new &lt;code&gt;response&lt;/code&gt; column, we now have a &lt;span class=&#34;math inline&#34;&gt;\(200 \times 1\)&lt;/span&gt; data frame containing the &lt;code&gt;seq()&lt;/code&gt; output. If you’re new to nested data structures, I recommend checking out Hadley Wickham’s &lt;a href=&#34;https://www.youtube.com/watch?v=rz3_FDVt9eg&#34;&gt;&lt;em&gt;Managing many models with R&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 11
## # Groups:   condition, .row [1]
##   condition  .row .chain .iteration .draw .value    mu sigma  lower upper response   
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;     
## 1 a             1     NA         NA   130  0.105 0.105 0.553 -0.979  1.19 &amp;lt;dbl [200]&amp;gt;
## 2 a             1     NA         NA   647  0.121 0.121 0.602 -1.06   1.30 &amp;lt;dbl [200]&amp;gt;
## 3 a             1     NA         NA  1087  0.260 0.260 0.727 -1.17   1.68 &amp;lt;dbl [200]&amp;gt;
## 4 a             1     NA         NA  1343  0.396 0.396 0.587 -0.754  1.55 &amp;lt;dbl [200]&amp;gt;
## 5 a             1     NA         NA  1618  0.342 0.342 0.500 -0.638  1.32 &amp;lt;dbl [200]&amp;gt;
## 6 a             1     NA         NA  1701  0.238 0.238 0.569 -0.878  1.35 &amp;lt;dbl [200]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Much as the &lt;code&gt;purrr::map2()&lt;/code&gt; function allowed us to iterate over two arguments, the &lt;code&gt;purrr::pmap()&lt;/code&gt; function will allow us to iterate over an arbitrary number of arguments. In the case of our third &lt;code&gt;mutate()&lt;/code&gt; function, we’ll iterate over the first three arguments of the &lt;code&gt;dnorm()&lt;/code&gt; function. In case you forgot, those arguments are &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;mean&lt;/code&gt;, and &lt;code&gt;sd&lt;/code&gt;, respectively. Within our &lt;code&gt;list()&lt;/code&gt;, we indicated we wanted to insert into them the &lt;code&gt;response&lt;/code&gt;, &lt;code&gt;mu&lt;/code&gt;, and &lt;code&gt;sigma&lt;/code&gt; values. This returns the desired &lt;code&gt;density&lt;/code&gt; values. Since our &lt;code&gt;map2()&lt;/code&gt; and &lt;code&gt;pmap()&lt;/code&gt; operations returned a nested tibble, we then followed them up with the &lt;code&gt;unnest()&lt;/code&gt; function to make it easier to access the results.&lt;/p&gt;
&lt;p&gt;Before &lt;code&gt;unnest&lt;/code&gt;ing, our nested tibble had 100 observations. After &lt;code&gt;unnest()&lt;/code&gt;, we converted it to the long format, resulting in &lt;span class=&#34;math inline&#34;&gt;\(100 \times 200 = 20,000\)&lt;/span&gt; observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  mutate(density  = pmap(list(response, mu, sigma), dnorm)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required when using unnest().
## Please use `cols = c(response, density)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 12
## Groups: condition, .row [5]
## $ condition  &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;,…
## $ .row       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ .chain     &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .iteration &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .draw      &amp;lt;int&amp;gt; 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,…
## $ .value     &amp;lt;dbl&amp;gt; 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.…
## $ mu         &amp;lt;dbl&amp;gt; 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.1866095, 0.…
## $ sigma      &amp;lt;dbl&amp;gt; 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.5573212, 0.…
## $ lower      &amp;lt;dbl&amp;gt; -0.9057199, -0.9057199, -0.9057199, -0.9057199, -0.9057199, -0.9057199, -0.9057…
## $ upper      &amp;lt;dbl&amp;gt; 1.278939, 1.278939, 1.278939, 1.278939, 1.278939, 1.278939, 1.278939, 1.278939,…
## $ response   &amp;lt;dbl&amp;gt; -0.9057199, -0.8947417, -0.8837635, -0.8727853, -0.8618071, -0.8508289, -0.8398…
## $ density    &amp;lt;dbl&amp;gt; 0.1048678, 0.1089746, 0.1131982, 0.1175399, 0.1220008, 0.1265818, 0.1312839, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hopefully, our last two lines look familiar. We &lt;code&gt;group_by(.draw)&lt;/code&gt; just like in previous examples. However, our final &lt;code&gt;mutate()&lt;/code&gt; line is a little simpler than in previous versions. Before we had to make that intermediary variable, &lt;code&gt;g&lt;/code&gt;. Because we intend to plot these data with help from ggridges, we no longer have need for &lt;code&gt;g&lt;/code&gt;. You’ll see. But the upshot is the only reason we’re adding this last &lt;code&gt;mutate()&lt;/code&gt; line is to scale all the Gaussians to have the same maximum height the way Kruschke did.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afd &amp;lt;-
  abc %&amp;gt;%
  distinct(condition) %&amp;gt;%
  add_fitted_draws(fit2, n = 20, dpar = c(&amp;quot;mu&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;% 
  mutate(lower    = qnorm(.025, mean = mu, sd = sigma),
         upper    = qnorm(.975, mean = mu, sd = sigma)) %&amp;gt;% 
  mutate(response = map2(lower, upper, seq, length.out = 200)) %&amp;gt;% 
  mutate(density  = pmap(list(response, mu, sigma), dnorm)) %&amp;gt;% 
  unnest() %&amp;gt;% 
  group_by(.draw) %&amp;gt;% 
  mutate(density  = density * .75 / max(density))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `cols` is now required when using unnest().
## Please use `cols = c(response, density)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(afd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,000
## Columns: 12
## Groups: .draw [20]
## $ condition  &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;,…
## $ .row       &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ .chain     &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .iteration &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
## $ .draw      &amp;lt;int&amp;gt; 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,…
## $ .value     &amp;lt;dbl&amp;gt; 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.…
## $ mu         &amp;lt;dbl&amp;gt; 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.2600993, 0.…
## $ sigma      &amp;lt;dbl&amp;gt; 0.570355, 0.570355, 0.570355, 0.570355, 0.570355, 0.570355, 0.570355, 0.570355,…
## $ lower      &amp;lt;dbl&amp;gt; -0.857776, -0.857776, -0.857776, -0.857776, -0.857776, -0.857776, -0.857776, -0…
## $ upper      &amp;lt;dbl&amp;gt; 1.377974, 1.377974, 1.377974, 1.377974, 1.377974, 1.377974, 1.377974, 1.377974,…
## $ response   &amp;lt;dbl&amp;gt; -0.8577760, -0.8465410, -0.8353061, -0.8240712, -0.8128362, -0.8016013, -0.7903…
## $ density    &amp;lt;dbl&amp;gt; 0.1098804, 0.1141834, 0.1186089, 0.1231581, 0.1278322, 0.1326322, 0.1375591, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s open &lt;a href=&#34;https://github.com/clauswilke/ggridges&#34;&gt;ggridges&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggridges)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how contrary to before, we set the global y axis to our &lt;code&gt;condition&lt;/code&gt; grouping variable. It’s within the &lt;code&gt;geom_ridgeline()&lt;/code&gt; function that we now specify &lt;code&gt;height = density&lt;/code&gt;. Other than that, the main thing to point out is you might want to adjust the &lt;code&gt;ylim&lt;/code&gt; parameters. Otherwise the margins aren’t the best.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afd %&amp;gt;%
  
  ggplot(aes(x = response, y = condition)) +
  geom_ridgeline(aes(height = density, group = interaction(condition, .draw)),
                 fill = NA, size = 1/3, color = adjustcolor(&amp;quot;steelblue&amp;quot;, alpha.f = 1/2)) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  coord_cartesian(ylim = c(1.25, 5.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;“But I wanted my Gaussians tipped to the left!”, you say. Yep, we can do that, too. Three things: First, we’ll want to adjust the &lt;code&gt;height&lt;/code&gt; parameter to &lt;code&gt;-density&lt;/code&gt;. We want our Gaussians to extend under their baselines. Along with that, we need to include &lt;code&gt;min_height = NA&lt;/code&gt;. Finally, we’ll switch out &lt;code&gt;coord_cartesian()&lt;/code&gt; for good old &lt;code&gt;coord_flip()&lt;/code&gt;. And you can adjust your &lt;code&gt;ylim&lt;/code&gt; parameters as desired.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afd %&amp;gt;%
  
  ggplot(aes(x = response, y = condition)) +
  geom_ridgeline(aes(height = -density, group = interaction(condition, .draw)),
                 fill = NA, size = 1/3, color = adjustcolor(&amp;quot;steelblue&amp;quot;, alpha.f = 1/2),
                 min_height = NA) +
  geom_jitter(data = abc,
              height = .05, shape = 1, alpha = 2/3) +
  coord_flip(ylim = c(0.5, 4.75))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-20-make-rotated-gaussians-kruschke-style/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think it’s important to note that I’ve never met any of the people who helped me with this project. Academic twitter, man–it’s a good place to be.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggridges_0.5.2  tidybayes_2.3.1 brms_2.15.0     Rcpp_1.0.6      forcats_0.5.1   stringr_1.4.0  
##  [7] dplyr_1.0.5     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3     tibble_3.1.0    ggplot2_3.3.3  
## [13] tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] svUnit_1.0.3         splines_4.0.4        crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] viridisLite_0.3.0    xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7
##  [49] DT_0.16              htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3       
##  [53] arrayhelpers_1.1-0   ellipsis_0.3.1       pkgconfig_2.0.3      loo_2.4.1           
##  [57] farver_2.0.3         dbplyr_2.0.0         utf8_1.1.4           tidyselect_1.1.0    
##  [61] labeling_0.4.2       rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1       
##  [65] munsell_0.5.0        cellranger_1.1.0     tools_4.0.4          cli_2.3.1           
##  [69] generics_0.1.0       broom_0.7.5          evaluate_0.14        fastmap_1.0.1       
##  [73] yaml_2.2.1           processx_3.4.5       knitr_1.31           fs_1.5.0            
##  [77] nlme_3.1-152         mime_0.10            projpred_2.0.2       xml2_1.3.2          
##  [81] compiler_4.0.4       bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13     
##  [85] gamm4_0.2-6          curl_4.3             reprex_0.3.0         statmod_1.4.35      
##  [89] stringi_1.5.3        highr_0.8            ps_1.6.0             blogdown_1.3        
##  [93] Brobdingnag_1.2-6    lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2      
##  [97] markdown_1.1         shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1        
## [101] lifecycle_1.0.0      bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4        
## [105] R6_2.5.0             bookdown_0.21        promises_1.1.1       gridExtra_2.3       
## [109] codetools_0.2-18     boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53         
## [113] gtools_3.8.2         assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0     
## [117] multcomp_1.4-16      mgcv_1.8-33          parallel_4.0.4       hms_0.5.3           
## [121] grid_4.0.4           coda_0.19-4          minqa_1.2.4          rmarkdown_2.7       
## [125] shiny_1.5.0          lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I’ve made a lot of progress working through Kruschke’s material since the initial release of this blog post. You can find the results in an ebook, &lt;a href=&#34;https://bookdown.org/content/3686/&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

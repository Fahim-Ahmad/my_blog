---
title: Analysis of the Relationship Between Two Quantitative Variables | Correlation
author: Fahim Ahmad
date: '2020-06-26'
slug: analysis-of-the-relationship-between-two-quantitative-variables-in-r-correlation
categories:
  - R
  - Statistics
tags:
  - R
  - Statistics

output:
  html_document:
    keep_md: false
    toc: true
    self_contained: false
    theme: null
    template: "../src/blog-template.html"

katex: false
---
# Introduction

In recent posts, we discussed [measures of central tendency](../2020-04-18-what-is-central-tendency/) and [measures of dispersion](../2020-05-21-measures-of-dispersion-using-r/). But hey, that is the start of a long journey. Finding relationships between variables is more interesting than describing the distribution of the data.

While studying, for example, the relationship between GDP and life expectancy, you might be interested to know whether there exists any relationship between the two indicators? is it a positive relationship or a negative relationship? and how strong the association is?

The above questions can be answered by computing the correlation coefficient between the two indicators. Depending on the type of data, different methods of correlation exist. In this post, you will learn the Pearson correlation coefficient and the Spearman correlation coefficient.

# Pearson Correlation

Pearson correlation is a parametric correlation test that measures the strength and direction of a **linear relationship** between two variables. This method is by far the most used and best method for calculating the association between variables that follow a **normal distribution**.

If the two variables are denoted by `x` and `y`, the Pearson correlation coefficient is defined as the ratio of covariance between `x` and `y` to the product of standard deviation of `x` and standard deviation of `y`.

$$r =  (Covariance (x,y)) \space/\space (S.D.(x)×S.D.(y))$$
Where,

$${Covariance(x,y) = \frac{∑^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})}{\ {n-1}}}$$


$${S.D.(x) = \sqrt\frac{∑^n_{i=1}(x_i-\bar{x})^2}{\ {n-1}}}$$

$${S.D.(y) = \sqrt\frac{∑^n_{i=1}(y_i-\bar{y})^2}{\ {n-1}}}$$

# Spearman Rank Correlation
While the Pearson correlation is used to compute the degree of relationship between linearly related variables, the Spearman correlation indicates the monotonic relationship. The Pearson correlation is more appropriate for continuous data such as income, age, and height, while Spearman is more appropriate for ordinal data such as level of satisfaction, or level of happiness measured on a scale of 1 to 4.

In other words, the Pearson correlation is used to compute the extent of association when there is an equal interval between the adjacent units of the input variables. For instance, an increase in income from 100 to 101 is equal to the increase from 500 to 501 or an increase in age from 18 to 19 is same as an increase in 38 to 39, while the Spearman correlation is widely used with the ordinal data such that one level can be considered higher or lower than another level, but the magnitude of the difference is not necessarily known or the same. For instance, "Strong satisfaction" is higher than the "Somewhat satisfaction" and "Somewhat dissatisfaction" is higher than "Strong dissatisfaction". However, it can not be quantified how much higher/lower is one level compared to the other level.

Moreover, since the Spearman correlation is based on the ranked values rather than the raw data and does not carry any assumption about the distribution of the data points, it can be used to measure the association between two continuous variables as well when the data are not distributed normally.

The Spearman ranked correlation can be expressed as follow:

$$rho =  (Covariance (x',y')) \space/\space (S.D.(x')×S.D.(y'))$$

Where $x'$ is rank(x) and $y'$ is rank(y).

```{r eval=FALSE, include=FALSE}
$${r_s = 1-\frac{6∑d_i^2}{\ n(n^2-1)}}$$ Where $d_i$ is the difference between the rank of the variables and $n$ is number of observation.
```


# Interpreting the Correlation Coefficient

The correlation coefficient always lies between -1 and +1 and can't go outside of this range. Where the sign of the correlation coefficient denotes the direction of the relationship between two variables and the absolute value of the correlation coefficient indicates the strength of association between two variables. A `+` sign indicates the positive relationship and a `-` sign indicates the negative relationship. Furthermore, the closer the correlation coefficient to -1 or +1, the stronger the relationship, and as it goes toward 0 the relationship will be weaker.

How close is close enough to -1 or +1? I get too excited to see correlations beyond ±0.7.
You can refer to below table while interpreting the correlation coefficient:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(gt) # NOTE: I had to remove and re-install the 'rlang' package so the 'gt' worked well
clr <- "maroon"

data.table::data.table(
`Correlation coefficient` = c(-1, -0.7, -0.5, -0.3, 0, 0.3, 0.5, 0.7, 1),
Meaning = c("Perfect negative relationship", "Strong negative relationship", "Moderate negative relationship", "Weak negative relationship", "No relationship", "Weak positive relationship", "Moderate positive relationship", "Strong positive relationship", "Perfect positive relationship")
) %>% 
  gt() %>% 
  tab_style(
    # list(
      style = cell_fill(color = clr, alpha = 1),
      locations = cells_body(rows = (`Correlation coefficient` %in% c(-1, 1)))
      # )
    ) %>% 
  tab_style(
    style = cell_fill(color = clr, alpha = 0.7),
    locations = cells_body(rows = (`Correlation coefficient` %in% c(-0.7, 0.7)))
    ) %>% 
  tab_style(
    style = cell_fill(color = clr, alpha = 0.5),
    locations = cells_body(rows = (`Correlation coefficient` %in% c(-0.5, 0.5)))
    ) %>% 
  tab_style(
    style = cell_fill(color = clr, alpha = 0.3),
    locations = cells_body(rows = (`Correlation coefficient` %in% c(-0.3, 0.3)))
    ) %>% 
  tab_style(
    style = cell_fill(color = clr, alpha = 0.1),
    locations = cells_body(rows = (`Correlation coefficient` %in% c(0)))
    ) %>% 
  tab_style(list(cell_text(align = "left", color = "black"),
                 cell_borders(sides = "all")
                 ),
            locations = cells_body()
            ) %>% 
  tab_style(
    list(
      cell_fill(color = "gray", alpha = 1),
      cell_text(align = "center", color = "white"),
      cell_borders(sides = "all")
    ),
    locations = cells_column_labels(everything())
    ) %>% 
  tab_header(title = "Interpreting the Correlation Coefficient") 

```

# Computing Correlation Coefficient in R

In the example below, we have a table of time series data of global GDP per capita and life expectancy from 1960 to 2018. Let's find if if a higher GDP per capita can predict a higher life expectancy, and to what extent both indicators are correlated to each other.


```{r echo=F}
# life expectancy data
life_exp <- read.csv("data/life_exp.csv", header = F, as.is = T)
colnames(life_exp) <- life_exp[3,]
life_exp <- life_exp[-c(1:3),]
# gdp data
gdp_pc <- read.csv("data/gdp_pc_2020_07_01.csv", header = F, as.is = T)
colnames(gdp_pc) <- gdp_pc[3,]
gdp_pc <- gdp_pc[-c(1:3),]


df <- full_join(
life_exp %>%
  filter(`Country Name` %in% "World") %>%
  dplyr::select(-c(`Country Code`, `Indicator Name`, `Indicator Code`)) %>%
  tidyr::pivot_longer(names_to = "year", values_to = "life_exp", -c(`Country Name`)) %>%
  filter(!is.na(life_exp))
,
gdp_pc %>%
  filter(`Country Name` %in% "World") %>%
  dplyr::select(-c(`Country Code`, `Indicator Name`, `Indicator Code`)) %>%
  tidyr::pivot_longer(names_to = "year", values_to = "gdp_pc", -c(`Country Name`)) %>%
  filter(!is.na(gdp_pc))
,
by = c("Country Name", "year")
)

knitr::kable(dplyr::select(df, year, life_exp, gdp_pc))
```

First thing first, let's plot the data points to explore if there exists any relationship between the two indicators or not, and if the data points vary together. This is done through a scatter-plot.

```{r message=FALSE}
library(ggplot2)
df %>% 
  ggplot(aes(x = gdp_pc, y = life_exp)) +
  geom_point() +
  theme_bw() +
  labs(title = "GDP per capita and life expectancy, 1960-2018")
```

Since the points are moving from the lower-left corner to the upper-right corner. Thus, a positive relationship exists^[If there exists a positive correlation, the points will be moving from the lower-left to the upper-right corner. Similarly, if there is a negative correlation, the points will be moving down from the upper-left corner to the lower right corner. In case the points are scattered and do not form any pattern, it depicts that no relationship exists between the variables.]. However, it does not indicate the extent of the relationship between the variables.

Next, the normality test should be performed to check whether the data follow a normal distribution or not. This can be tested visually through **qq-plot** or **histogram**:

```{r fig.height=10, fig.width=10, message=FALSE}
qq1 <- df %>% ggplot(aes(sample = life_exp)) +
  geom_qq() + labs(y = "life expectancy")

qq2 <- df %>% ggplot(aes(sample = gdp_pc)) +
  geom_qq() + labs(y = "GDP per capita")

hist1 <- df %>% ggplot(aes(x = life_exp)) +
  geom_histogram(fill = "orange", alpha = 0.5) + labs(x = "life expectancy")

hist2 <- df %>% ggplot(aes(x = gdp_pc)) +
  geom_histogram(fill = "orange", alpha = 0.5) + labs(x = "GDP per capita")

library(patchwork)
(qq1 | qq2) / (hist1 / hist2) &
  theme_bw()
```


From the above plots it can easily be drawn that the data points do not follow a normal distribution, but the more precise way to check the normality of distribution is to use a statistical test. The `shapiro.test()` function performs the Shapiro-Wilk test of normality.

```{r comment=NA}
shapiro.test(df$gdp_pc)
shapiro.test(df$life_exp)
```

The null hypothesis in the Shapiro-Wilk test is that the data are normally distributed. Therefore, a p-value of less than the significance level, or less than 0.05 indicates the data points are not normally distributed.

After testing the assumptions, you can decide to choose either Pearson or Spearman correlation. The `cor.test()` function can be used for evaluating the association between variables. 

```{r message=FALSE, warning=FALSE, comment=NA}
cor.test(df$gdp_pc, df$life_exp, method = "pearson") 
cor.test(df$gdp_pc, df$life_exp, method = "spearman")
```


As shown above, both Pearson and Spearman's correlation coefficient is above 0.9 which indicates a strong correlation between the two indicators. In other words, as GDP per capita increases life expectancy increases as well.

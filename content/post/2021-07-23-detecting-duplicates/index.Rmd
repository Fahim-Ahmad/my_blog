---
title: Detecting Duplicates (base R vs. dplyr)
author: Fahim Ahmad
date: '2021-07-23'
slug: detecting-duplicates
categories:
  - R
tags:
  - R
output:
  blogdown::html_page:
    toc: yes

image:
  caption: ''
  focal_point: ''
  preview_only: no

summary: In this post, I provide an overview of duplicated( ) function from base R and the distinct( ) function from dplyr package to detect and remove duplicates.
---

Sometimes you may encounter duplicated values in the data which might cause problems depending on how you plan to use the data. In this post, I provide an overview of `duplicated()` function from base R and the `distinct()` function from **dplyr** package to detect and remove duplicates.

I will be using the following data frame as an example in this post.

```{r}
set.seed(1000)
df <- data.frame(
  ID = sample(paste0("ID-00", 1:7), 10, replace = T),
  value_1 = sample(1:7, 10, replace = T),
  value_2 = sample(2:5, 10, replace = T)
)

df
```

# Removing duplicates based on a single variable

The `duplicated()` function returns a logical vector where `TRUE` specifies which rows of the data frame are duplicates.

For instance, `duplicated(df["ID"])` returns the following vector.

```{r echo=FALSE}
(duplicate_rows <- duplicated(df["ID"], fromLast = F))
```

> **Note**: the `duplicated()` function preserves the first occurrence in the process of identifying the duplicate values, if you want to consider the duplication from the reverse side, then set the `fromLast` argument to `TRUE`.

Let's use the above vector to exclude the duplicated values.

```{r}
df[!duplicate_rows, ]
```

An alternative way to select unique values is `dplyr::distinct()` function that yields a similar result as above.

```{r}
dplyr::distinct(df, ID, .keep_all = TRUE)
```

> **Note**: The .`keep_all` argument is used to retain all other columns in the output data frame.

# Removing duplicates based on the combination of multiple variables

The above chunks of codes remove the duplicated rows based on a single column. What if we want to remove duplicates based on more than a single column?

One way is to concatenate the columns in which you want to check the presence of duplicates. For example, let's remove the rows where **value_1** and **value_2** are duplicated.


```{r}
df <- dplyr::mutate(df, value_1_2 = paste(value_1, value_2))
df[!duplicated(df[c("value_1_2")]), ]

```

Below is an efficient way of detecting duplicates based on the combination of multiple columns without concatenating the values of the columns in which we want to identify the duplicated values:

```{r eval=FALSE}
# using duplicated() function
df[!duplicated(df[c("value_1", "value_2")]), ]

# using distinct() function
dplyr::distinct(df, value_1, value_2, .keep_all = TRUE)
```

```{r echo=FALSE}
df[!duplicated(df[c("value_1", "value_2")]), ]
```


# Conclusion

It seems that both approaches work very well; however, the advantage of using `duplicated()` function from base R is it returns a logical vector identifying the duplicated rows that can be used to either drop the duplicated rows or keep only these rows for further investigation while the `distinct()` function directly removes the duplicated rows without specifying which row has duplicate values.

For instance, letâ€™s keep the duplicated ID numbers only.

```{r}
df[duplicated(df["ID"], fromLast = F), ]
```

As mentioned above, the `duplicated()` function does not assign the first occurrence in the process of identifying the duplicated values as duplicates. Thus, we need to count backward as well to consider the duplication from the reverse side. To do so, we need to set the `fromLast` argument to `TRUE`.


```{r}
df[duplicated(df["ID"], fromLast = F) | duplicated(df["ID"], fromLast = T), ]
```

